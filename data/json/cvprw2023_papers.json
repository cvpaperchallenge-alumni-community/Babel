[
    {
        "title": "Synthetic Sample Selection for Generalized Zero-Shot Learning",
        "author": "Shreyank N. Gowda",
        "abstract": "Generalized Zero-Shot Learning (GZSL) has emerged as a pivotal research domain in computer vision, owing to its capability to recognize objects that have not been seen during training. Despite the significant progress achieved by generative techniques in converting traditional GZSL to fully supervised learning, they tend to generate a large number of synthetic features that are often redundant, thereby increasing training time and decreasing accuracy. To address this issue, this paper proposes a novel approach for synthetic feature selection using reinforcement learning. In particular, we propose a transformer-based selector that is trained through proximal policy optimization (PPO) to select synthetic features based on the validation classification accuracy of the seen classes, which serves as a reward. The proposed method is model-agnostic and data-agnostic, making it applicable to both images and videos and versatile for diverse applications. Our experimental results demonstrate the superiority of our approach over existing feature-generating methods, yielding improved overall performance on multiple benchmarks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Gowda_Synthetic_Sample_Selection_for_Generalized_Zero-Shot_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/papers/Gowda_Synthetic_Sample_Selection_for_Generalized_Zero-Shot_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robustness Against Gradient Based Attacks Through Cost Effective Network Fine-Tuning",
        "author": "Akshay Agarwal, Nalini Ratha, Richa Singh, Mayank Vatsa",
        "abstract": "Adversarial perturbations aim to modify the image pixels in an imperceptible manner such that the CNN classifier misclassifies an image, whereas humans can predict the original class. Several defense algorithms against adversarial attacks are proposed in the literature, such as binary classification which aims to detect adversarial examples, and network retraining using perturbed images. The challenge with the adversarial detection approach is that once the perturbed samples are detected, they are discarded, and the system requires fresh input. On the other hand, adversarial training requires the generation of adversarial images for data augmentation and hence is computationally demanding. It is well known that training a deep CNN architecture is resource-intensive, and therefore retraining again from scratch is not feasible in resource-constrained scenarios. We propose computationally efficient fine-tuning of pre-trained networks to increase their robustness against the prevalent gradient-based attacks. The proposed fine-tuning is performed in a complete black-box fashion, where we do not know the training setting such as optimizer, batch size, and learning rate used in the training of the network. Extensive experiments using multiple CNN architectures such as VGG and ResNet show that the proposed fine-tuning provides significant robustness against various widespread gradient attacks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Agarwal_Robustness_Against_Gradient_Based_Attacks_Through_Cost_Effective_Network_Fine-Tuning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/papers/Agarwal_Robustness_Against_Gradient_Based_Attacks_Through_Cost_Effective_Network_Fine-Tuning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Learning Unbiased Classifiers From Biased Data With Meta-Learning",
        "author": "Ruggero Ragonesi, Pietro Morerio, Vittorio Murino",
        "abstract": "It is well known that large deep architectures are powerful models when adequately trained, but may exhibit undesirable behavior leading to confident incorrect predictions, even when evaluated on slightly different test examples. Test data characterized by distribution shifts (from training data distribution), outliers, and adversarial samples are among the types of data affected by this problem. This situation worsens whenever data are biased, meaning that predictions are mostly based on spurious correlations present in the data. Unfortunately, since such correlations occur in the most of data, a model is prevented from correctly generalizing the considered classes. In this work, we tackle this problem from a meta-learning perspective. Considering the dataset as composed of unknown biased and unbiased samples, we first identify these two subsets by a pseudo-labeling algorithm, even if coarsely. Subsequently, we apply a bi-level optimization algorithm in which, in the inner loop, we look for the best parameters guiding the training of the two subsets, while in the outer loop, we train the final model taking benefit from augmented data generated using Mixup. Properly tuning the contributions of biased and unbiased data, together with the regularization introduced by the mixed data has proved to be an effective training strategy to learn unbiased models, showing superior generalization capabilities. Experimental results on synthetically and realistically biased datasets surpass state-of-the-art performance, as compared to existing methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Ragonesi_Learning_Unbiased_Classifiers_From_Biased_Data_With_Meta-Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/papers/Ragonesi_Learning_Unbiased_Classifiers_From_Biased_Data_With_Meta-Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy",
        "author": "Yuhao Chen, Hayden Gunraj, E. Zhixuan Zeng, Robbie Meyer, Maximilian Gilles, Alexander Wong",
        "abstract": "Recently, there has been tremendous interest in industry 4.0 infrastructure to address labor shortages in global supply chains. Deploying artificial intelligence-enabled robotic bin picking systems in real world has become particularly important for reducing stress and physical demands of workers while increasing speed and efficiency of warehouses. To this end, artificial intelligence-enabled robotic bin picking systems may be used to automate order picking, but with the risk of causing expensive damage during an abnormal event such as sensor failure. As such, reliability becomes a critical factor for translating artificial intelligence research to real world applications and products. In this paper, we propose a reliable object detection and segmentation system with MultiModal Redundancy (MMRNet) for tackling object detection and segmentation for robotic bin picking using data from different modalities. This is the first system that introduces the concept of multimodal redundancy to address sensor failure issues during deployment. In particular, we realize the multimodal redundancy framework with a gate fusion module and dynamic ensemble learning. Finally, we present a new label-free multi-modal consistency (MC) score that utilizes the output from all modalities to measure the overall system output reliability and uncertainty. Through experiments, we demonstrate that in an event of missing modality, our system provides a much more reliable performance compared to baseline models. We also demonstrate that our MC score is a more reliability indicator for outputs during inference time compared to the model generated confidence scores that are often over-confident.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Chen_MMRNet_Improving_Reliability_for_Multimodal_Object_Detection_and_Segmentation_for_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/papers/Chen_MMRNet_Improving_Reliability_for_Multimodal_Object_Detection_and_Segmentation_for_CVPRW_2023_paper.pdf"
    },
    {
        "title": "The Casual Conversations v2 Dataset",
        "author": "Bilal Porgali, V\u00edtor Albiero, Jordan Ryda, Cristian Canton Ferrer, Caner Hazirbas",
        "abstract": "This paper introduces a new large consent-driven dataset aimed at assisting in the evaluation of algorithmic bias and robustness of computer vision and audio speech models in regards to 11 attributes that are self-provided or labeled by trained annotators. The dataset includes 26,467 videos of 5,567 unique paid participants, with an average of almost 5 videos per person, recorded in Brazil, India, Indonesia, Mexico, Vietnam, Philippines, and the USA, representing diverse demographic characteristics. The participants agreed for their data to be used in assessing fairness of AI models and provided self-reported age, gender, language/dialect, disability status, physical adornments, physical attributes and geo-location information, while trained annotators labeled apparent skin tone using the Fitzpatrick Skin Type and Monk Skin Tone scales, and voice timbre. Annotators also labeled for different recording setups and per-second activity annotations.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Porgali_The_Casual_Conversations_v2_Dataset_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/papers/Porgali_The_Casual_Conversations_v2_Dataset_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Gradient Attention Balance Network: Mitigating Face Recognition Racial Bias via Gradient Attention",
        "author": "Linzhi Huang, Mei Wang, Jiahao Liang, Weihong Deng, Hongzhi Shi, Dongchao Wen, Yingjie Zhang, Jian Zhao",
        "abstract": "Although face recognition has made impressive progress in recent years, we ignore the racial bias of the recognition system when we pursue a high level of accuracy. Previous work found that for different races, face recognition networks focus on different facial regions, and the sensitive regions of darker-skinned people are much smaller. Based on this discovery, we propose a new de-bias method based on gradient attention, called Gradient Attention Balance Network (GABN). Specifically, we use the gradient attention map (GAM) of the face recognition network to track the sensitive facial regions and make the GAMs of different races tend to be consistent through adversarial learning. This method mitigates the bias by making the network focus on similar facial regions. In addition, we also use masks to erase the Top-N sensitive facial regions, forcing the network to allocate its attention to a larger facial region. This method expands the sensitive region of darker-skinned people and further reduces the gap between GAM of darker-skinned people and GAM of Caucasians. Extensive experiments show that GABN successfully mitigates racial bias in face recognition and learns more balanced performance for people of different races.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Huang_Gradient_Attention_Balance_Network_Mitigating_Face_Recognition_Racial_Bias_via_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/papers/Huang_Gradient_Attention_Balance_Network_Mitigating_Face_Recognition_Racial_Bias_via_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Schrodinger's Camera: First Steps Towards a Quantum-Based Privacy Preserving Camera",
        "author": "Hannah Kirkland, Sanjeev J. Koppal",
        "abstract": "Privacy-preserving vision must overcome the dual challenge of utility and privacy. Too much anonymity renders the images useless, but too little privacy does not protect sensitive data. We propose a novel design for privacy preservation, where the imagery is stored in quantum states. In the future, this will be enabled by quantum imaging cameras, and, currently, storing very low resolution imagery in quantum states is possible. Quantum state imagery has the advantage of being both private and non-private till the point of measurement. This occurs even when images are manipulated, since every quantum action is fully reversible. We propose a control algorithm, based on double deep Q-learning, to learn how to anonymize the image before measurement. After learning, the RL weights are fixed, and new attack neural networks are trained from scratch to break the system's privacy. Although all our results are in simulation, we demonstrate, with these first steps, that it is possible to control both privacy and utility in a quantum-based manner.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Kirkland_Schrodingers_Camera_First_Steps_Towards_a_Quantum-Based_Privacy_Preserving_Camera_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/papers/Kirkland_Schrodingers_Camera_First_Steps_Towards_a_Quantum-Based_Privacy_Preserving_Camera_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Estimating and Maximizing Mutual Information for Knowledge Distillation",
        "author": "Aman Shrivastava, Yanjun Qi, Vicente Ordonez",
        "abstract": "In this work, we propose Mutual Information Maximization Knowledge Distillation (MIMKD). Our method uses a contrastive objective to simultaneously estimate and maximize a lower bound on the mutual information of local and global feature representations between a teacher and a student network. We demonstrate through extensive experiments that this can be used to improve the performance of low capacity models by transferring knowledge from more performant but computationally expensive models. This can be used to produce better models that can be run on devices with low computational resources. Our method is flexible, we can distill knowledge from teachers with arbitrary network architectures to arbitrary student networks. Our empirical results show that MIMKD outperforms competing approaches across a wide range of student-teacher pairs with different capacities, with different architectures, and when student networks are with extremely low capacity. We are able to obtain 74.55% accuracy on CIFAR100 with a ShufflenetV2 from a baseline accuracy of 69.8% by distilling knowledge from ResNet-50. On Imagenet we improve a ResNet-18 network from 68.88% to 70.32% accuracy (1.44%+) using a ResNet-34 teacher network.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Shrivastava_Estimating_and_Maximizing_Mutual_Information_for_Knowledge_Distillation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TCV/papers/Shrivastava_Estimating_and_Maximizing_Mutual_Information_for_Knowledge_Distillation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Training Strategies for Vision Transformers for Object Detection",
        "author": "Apoorv Singh",
        "abstract": "Vision-based Transformer have shown huge application in the perception module of autonomous driving in terms of predicting accurate 3D bounding boxes, owing to their strong capability in modeling long-range dependencies between the visual features. However Transformers, initially designed for language models, have mostly focused on the performance accuracy, and not so much on the inference-time budget. For a safety critical system like autonomous driving, real-time inference at the on-board compute is an absolute necessity. This keeps our object detection algorithm under a very tight run-time budget. In this paper, we evaluated a variety of strategies to optimize on the inference-time of vision transformers based object detection methods keeping a close-watch on any performance variations. Our chosen metric for these strategies is accuracy-runtime joint optimization. Moreover, for actual inference-time analysis we profile our strategies with float32 and float16 precision with TensorRT module. This is the most common format used by the industry for deployment of their Machine Learning networks on the edge devices. We showed that our strategies are able to improve inference-time by 63% at the cost of performance drop of mere 3% for our problem-statement. We recommend practitioners use these techniques to deploy Transformers based hefty multi-view networks on a budge-constrained robotic platform.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Singh_Training_Strategies_for_Vision_Transformers_for_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Singh_Training_Strategies_for_Vision_Transformers_for_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MobileDeRainGAN: An Efficient Semi-Supervised Approach to Single Image Rain Removal for Task-Driven Applications",
        "author": "Ruphan Swaminathan, Pradyot Korupolu",
        "abstract": "Rain removal is an essential task in computer vision, particularly for applications such as autonomous navigation to function seamlessly during rain. However, most existing single-image deraining algorithms are limited by their inability to generalize on diverse real-world rainy images, the need for real-time processing, and the lack of task-driven metric enhancement. This paper proposes MobileDeRainGAN, an efficient semi-supervised algorithm that addresses these challenges. The proposed approach includes a novel latent bridge network and multi-scale discriminator that effectively removes rain-related artifacts at different scales. Our cross-domain experiments on Rain1400 and RainCityscapes datasets demonstrate substantial improvements over state-of-the-art methods in terms of generalization and object detection scores in a semi-supervised setting. Furthermore, our approach is significantly faster and can run in real-time even on edge devices. Overall, our proposed MobileDeRainGAN algorithm offers a significant improvement in rain removal performance on real-world images while being efficient, scalable, and suitable for real-world applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Swaminathan_MobileDeRainGAN_An_Efficient_Semi-Supervised_Approach_to_Single_Image_Rain_Removal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Swaminathan_MobileDeRainGAN_An_Efficient_Semi-Supervised_Approach_to_Single_Image_Rain_Removal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Ultra-Sonic Sensor Based Object Detection for Autonomous Vehicles",
        "author": "Tommaso Nesti, Santhosh Boddana, Burhaneddin Yaman",
        "abstract": "Perception systems in autonomous vehicles (AV) have made significant advancements in recent years. Such systems leverage different sensing modalities such as cameras, LiDARs and Radars, and are powered by state-of-the-art deep learning algorithms. Ultrasonic sensors (USS) are a low-cost, durable and robust sensing technology that is particularly suitable for near-range detection in harsh weather conditions, but have received very limited attention in the perception literature. In this work, we present a novel USS-based object detection system that can enable accurate detection of objects in low-speed scenarios. The proposed pipeline involves four steps. First, the input USS data is transformed into a novel voxelized 3D point cloud leveraging the physics of USS. Next, multi-channels Bird Eye's View (BEV) images are generated via projection operators. Later, the resolution of BEV images is enhanced by means of a rolling-window, vehicle movement-aware temporal aggregation process. Finally, the image-like data representation is used to train a deep neural network to detect and localize objects in the 2D plane. We present extensive experiments showing that the proposed framework achieves satisfactory performance across both classic and custom object detection metrics, thus bridging the usecase and literature visibility gap between USS and more established sensors.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Nesti_Ultra-Sonic_Sensor_Based_Object_Detection_for_Autonomous_Vehicles_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Nesti_Ultra-Sonic_Sensor_Based_Object_Detection_for_Autonomous_Vehicles_CVPRW_2023_paper.pdf"
    },
    {
        "title": "HazardNet: Road Debris Detection by Augmentation of Synthetic Models",
        "author": "Tae Eun Choe, Jane Wu, Xiaolin Lin, Karen Kwon, Minwoo Park",
        "abstract": "We present an algorithm to detect unseen road debris using a small set of synthetic models. Early detection of road debris is critical for safe autonomous or assisted driving, yet the development of a robust road debris detection model has not been widely discussed. There are two main challenges to building a road debris detector: first, data collection of road debris is challenging since hazardous objects on the road are rare to encounter in real driving scenarios; second, the variability of road debris is broad, ranging from a very small brick to a large fallen tree. To overcome these challenges, we propose a novel approach to few-shot learning of road debris that uses semantic augmentation and domain randomization to augment real road images with synthetic models. We constrain the problem domain to uncommon objects on the road and allow the deep neural network, HazardNet, to learn the semantic meaning of road debris to eventually detect unseen road debris. Our results demonstrate that HazardNet is able to accurately detect real road debris when only trained on synthetic objects in augmented images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Choe_HazardNet_Road_Debris_Detection_by_Augmentation_of_Synthetic_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Choe_HazardNet_Road_Debris_Detection_by_Augmentation_of_Synthetic_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TorchSparse++: Efficient Point Cloud Engine",
        "author": "Haotian Tang, Shang Yang, Zhijian Liu, Ke Hong, Zhongming Yu, Xiuyu Li, Guohao Dai, Yu Wang, Song Han",
        "abstract": "Point cloud computation has become an increasingly more important workload thanks to its applications in autonomous driving. Unlike dense 2D computation, point cloud convolution has sparse and irregular computation patterns and thus requires dedicated inference system support with specialized high-performance kernels. While existing point cloud deep learning libraries have developed different dataflows for convolution on point clouds, they assume a single dataflow throughout the execution of the entire model. In this work, we systematically analyze and improve existing dataflows. Our resulting system, TorchSparse++, achieves 2.9x, 3.3x, 2.2x and 1.8x measured end-to-end speedup on an NVIDIA A100 GPU over the state-of-the-art MinkowskiEngine, SpConv 1.2, TorchSparse and SpConv v2 in inference respectively. Furthermore, TorchSparse++ is the only system to date that supports all necessary primitives for 3D segmentation, detection, and reconstruction workloads in autonomous driving. Code is publicly released at https://github.com/mit-han-lab/torchsparse.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Tang_TorchSparse_Efficient_Point_Cloud_Engine_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Tang_TorchSparse_Efficient_Point_Cloud_Engine_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improvements to Image Reconstruction-Based Performance Prediction for Semantic Segmentation in Highly Automated Driving",
        "author": "Andreas B\u00e4r, Daniel Kusuma, Tim Fingscheidt",
        "abstract": "The performance of deep neural networks is typically measured with ground truth data which is expensive and not available during operation. At the same time, safety-critical applications, such as highly automated driving, require an awareness of the current performance, especially during operation with distorted inputs. Recently, performance prediction for semantic segmentation by an image reconstruction decoder was proposed. In this work, we investigate three approaches to improve its predictive power: Parameter initialization, parameter sharing, and inter-decoder lateral connections. Our best setup establishes a new state of the art in performance prediction with image-only inputs on Cityscapes and KITTI and even excels a method exploiting both point cloud and image inputs on Cityscapes. Further, our investigations reveal that the best Pearson correlation between the segmentation quality and the reconstruction quality does not always lead to the best predictive power. Code is available at https://github.com/ifnspaml/PerfPredRecV2.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Bar_Improvements_to_Image_Reconstruction-Based_Performance_Prediction_for_Semantic_Segmentation_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Bar_Improvements_to_Image_Reconstruction-Based_Performance_Prediction_for_Semantic_Segmentation_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "RadarGNN: Transformation Invariant Graph Neural Network for Radar-Based Perception",
        "author": "Felix Fent, Philipp Bauerschmidt, Markus Lienkamp",
        "abstract": "A reliable perception has to be robust against challenging environmental conditions. Therefore, recent efforts focused on the use of radar sensors in addition to camera and lidar sensors for perception applications. However, the sparsity of radar point clouds and the poor data availability remain challenging for current perception methods. To address these challenges, a novel graph neural network is proposed that does not just use the information of the points themselves but also the relationships between the points. The model is designed to consider both point features and point-pair features, embedded in the edges of the graph. Furthermore, a general approach for achieving transformation invariance is proposed which is robust against unseen scenarios and also counteracts the limited data availability. The transformation invariance is achieved by an invariant data representation rather than an invariant model architecture, making it applicable to other methods. The proposed RadarGNN model outperforms all previous methods on the RadarScenes dataset. In addition, the effects of different invariances on the object detection and semantic segmentation quality are investigated. The code is made available as open-source software under https://github.com/TUMFTM/RadarGNN.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Fent_RadarGNN_Transformation_Invariant_Graph_Neural_Network_for_Radar-Based_Perception_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Fent_RadarGNN_Transformation_Invariant_Graph_Neural_Network_for_Radar-Based_Perception_CVPRW_2023_paper.pdf"
    },
    {
        "title": "EGA-Depth: Efficient Guided Attention for Self-Supervised Multi-Camera Depth Estimation",
        "author": "Yunxiao Shi, Hong Cai, Amin Ansari, Fatih Porikli",
        "abstract": "The ubiquitous multi-camera setup on modern autonomous vehicles provides an opportunity to construct surround-view depth. Existing methods, however, either perform independent monocular depth estimations on each camera or rely on computationally heavy self attention mechanisms. In this paper, we propose a novel guided attention architecture, EGA-Depth, which can improve both the efficiency and accuracy of self-supervised multi-camera depth estimation. More specifically, for each camera, we use its perspective view as the query to cross-reference its neighboring views to derive informative features for this camera view. This allows the model to perform attention only across views with considerable overlaps and avoid the costly computations of standard self-attention. Given its efficiency, EGA-Depth enables us to exploit higher-resolution visual features, leading to improved accuracy. Furthermore, EGA-Depth can incorporate more frames from previous time steps as it scales linearly w.r.t. the number of views and frames. Extensive experiments on two challenging autonomous driving benchmarks nuScenes and DDAD demonstrate the efficacy of our proposed EGA-Depth and show that it achieves the new state-of-the-art in self-supervised multi-camera depth estimation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Shi_EGA-Depth_Efficient_Guided_Attention_for_Self-Supervised_Multi-Camera_Depth_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Shi_EGA-Depth_Efficient_Guided_Attention_for_Self-Supervised_Multi-Camera_Depth_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FUTR3D: A Unified Sensor Fusion Framework for 3D Detection",
        "author": "Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, Hang Zhao",
        "abstract": "Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Existing multi-modal 3D detection models usually involve customized designs depending on the sensor combinations or setups. In this work, we propose the first unified end-to-end sensor fusion framework for 3D detection, named FUTR3D, which can be used in (almost) any sensor configuration. FUTR3D employs a query-based Modality-Agnostic Feature Sampler (MAFS), together with a transformer decoder with a set-to-set loss for 3D detection, thus avoiding using late fusion heuristics and post-processing tricks. We validate the effectiveness of our framework on various combinations of cameras, low-resolution LiDARs, high-resolution LiDARs, and radars. On NuScenes dataset, FUTR3D achieves better performance over specifically designed methods across different sensor combinations. Moreover, FUTR3D achieves great flexibility with different sensor configurations and enables low-cost autonomous driving. For example, only using a 4-beam LiDAR with cameras, FUTR3D (58.0 mAP) surpasses state-of-the-art 3D detection model centerpoint (56.6 mAP) using a 32-beam LiDAR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Chen_FUTR3D_A_Unified_Sensor_Fusion_Framework_for_3D_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Chen_FUTR3D_A_Unified_Sensor_Fusion_Framework_for_3D_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Joint Camera and LiDAR Risk Analysis",
        "author": "Oliver Zendel, Johannes Huemer, Markus Murschitz, Gustavo Fernandez Dominguez, Amadeus Lobe",
        "abstract": "Sensor fusion of camera-based sensors and LiDAR is widely used in autonomous systems with the premise of increased robustness. Contrarily, due to their overlapping functional principles, they also share many risk factors that may result in degraded operation. This work applies the risk analysis method Hazard and Operability study (HAZOP) to LiDAR sensors and connects it with an existing camera-based HAZOP. This systematic approach leads to a structured listing of potential sources of data quality reduction in LiDAR data. Many risk factors identified for camera-based systems (e.g. transparency or reflections) can be correlated to degradation in the corresponding LiDAR data. To validate our findings, the public dataset A2D2 is analyzed for such co-occurring camera-LiDAR risk factors. Additionally, experiments under controlled laboratory conditions are performed to quantify the impact of various identified risks. Our HAZOP results are released publicly and are intended to improve the design and usage of sensor systems as well as training and test datasets for safer autonomous systems.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Zendel_Joint_Camera_and_LiDAR_Risk_Analysis_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Zendel_Joint_Camera_and_LiDAR_Risk_Analysis_CVPRW_2023_paper.pdf"
    },
    {
        "title": "LiDAR-Based Localization on Highways Using Raw Data and Pole-Like Object Features",
        "author": "Sheng-Cheng Lee, Victor Lu, Chieh-Chih Wang, Wen-Chieh Lin",
        "abstract": "Poles on highways provide important cues for how a scan should be localized onto a map. However existing point cloud scan matching algorithms do not fully leverage such cues, leading to suboptimal matching accuracy in highway environments. To improve the ability to match in such scenarios, we include pole-like objects for lateral information and add this information to the current matching algorithm. First, we classify the points from the LiDAR sensor using the Random Forests classifier to find the points that represent poles. Each detected pole point will then generate a residual by the distance to the nearest pole in map. The pole residuals are later optimized along with the point-to-distribution residuals proposed in the normal distributions transform (NDT) using a nonlinear least squares optimization to get the localization result. Compared to the baseline (NDT), our proposed method obtains a 34% improvement in accuracy on highway scenes in the localization problem. In addition, our experiment shows that the convergence area is significantly enlarged, increasing the usability of the self-driving car localization algorithm on highway scenarios.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Lee_LiDAR-Based_Localization_on_Highways_Using_Raw_Data_and_Pole-Like_Object_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Lee_LiDAR-Based_Localization_on_Highways_Using_Raw_Data_and_Pole-Like_Object_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MotionTrack: End-to-End Transformer-Based Multi-Object Tracking With LiDAR-Camera Fusion",
        "author": "Ce Zhang, Chengjie Zhang, Yiluan Guo, Lingji Chen, Michael Happold",
        "abstract": "Multiple Object Tracking (MOT) is crucial to autonomous vehicle perception. End-to-end transformer-based algorithms, which detect and track objects simultaneously, show great potential for the MOT task. However, most existing methods focus on image-based tracking with a single object category. In this paper, we propose an end-to-end transformer-based MOT algorithm (MotionTrack) with multi-modality sensor inputs to track objects with multiple classes. Our objective is to establish a transformer baseline for the MOT in an autonomous driving environment. The proposed algorithm consists of a transformer-based data association (DA) module and a transformer-based query enhancement module to achieve MOT and Multiple Object Detection (MOD) simultaneously. The MotionTrack and its variations achieve better results (AMOTA score at 0.55) on the nuScenes dataset compared with other classical baseline models, such as the AB3DMOT, the CenterTrack, and the probabilistic 3D Kalman filter. In addition, we prove that a modified attention mechanism can be utilized for DA to accomplish the MOT, and aggregate history features to enhance the MOD performance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Zhang_MotionTrack_End-to-End_Transformer-Based_Multi-Object_Tracking_With_LiDAR-Camera_Fusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Zhang_MotionTrack_End-to-End_Transformer-Based_Multi-Object_Tracking_With_LiDAR-Camera_Fusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploiting the Complementarity of 2D and 3D Networks To Address Domain-Shift in 3D Semantic Segmentation",
        "author": "Adriano Cardace, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano",
        "abstract": "3D semantic segmentation is a critical task in many real-world applications, such as autonomous driving, robotics, and mixed reality. However, the task is extremely challenging due to ambiguities coming from the unstructured, sparse, and uncolored nature of the 3D point clouds. A possible solution is to combine the 3D information with others coming from sensors featuring a different modality, such as RGB cameras. Recent multi-modal 3D semantic segmentation networks exploit these modalities relying on two branches that process the 2D and 3D information independently, striving to maintain the strength of each modality. In this work, we first explain why this design choice is effective and then show how it can be improved to make the multi-modal semantic segmentation more robust to domain shift. Our surprisingly simple contribution achieves state-of-the-art performances on four popular multi-modal unsupervised domain adaptation benchmarks, as well as better results in a domain generalization scenario.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Cardace_Exploiting_the_Complementarity_of_2D_and_3D_Networks_To_Address_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Cardace_Exploiting_the_Complementarity_of_2D_and_3D_Networks_To_Address_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DPPD: Deformable Polar Polygon Object Detection",
        "author": "Yang Zheng, Oles Andrienko, Yonglei Zhao, Minwoo Park, Trung Pham",
        "abstract": "Regular object detection methods output rectangle bounding boxes, which are unable to accurately describe the actual object shapes. Instance segmentation methods output pixel-level labels, which are computationally expensive for real-time applications. Therefore, a polygon representation is needed to achieve precise shape alignment, while retaining low computation cost. We develop a novel Deformable Polar Polygon Object Detection method (DPPD) to detect objects in polygon shapes. In particular, our network predicts, for each object, a sparse set of flexible vertices to construct the polygon, where each vertex is represented by a pair of angle and distance in the Polar coordinate system. To enable training, both ground truth and predicted polygons are densely resampled to have the same number of vertices with equal-spaced raypoints. The resampling operation is fully differentable, allowing gradient back-propagation. Sparse polygon predicton ensures high-speed runtime inference while dense resampling allows the network to learn object shapes with high precision. The polygon detection head is established on top of an anchor-free and NMS-free network architecture. DPPD has been demonstrated successfully in various object detection tasks for autonomous driving such as traffic-sign, crosswalk, vehicle and pedestrian objects.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Zheng_DPPD_Deformable_Polar_Polygon_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Zheng_DPPD_Deformable_Polar_Polygon_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Rare Classes on nuScenes LiDAR Segmentation Through Targeted Domain Adaptation",
        "author": "Vickram Rajendran, Chuck Tang, Frits van Paasschen",
        "abstract": "We generate synthetic data in order to target improvement on specific rare classes in LiDAR segmentation without regressing performance on the existing, plentiful classes. While using auxiliary data to improve performance on a domain is not new with respect to classification, there is limited research on targeting specific classes with this technique. It is currently unclear how to extend those methods to work well on more complicated but realistic use cases for autonomous driving such as LiDAR segmentation. By upsampling specific classes in the auxiliary domain, mixing data between domains, and splitting representation building and fine-tuning, we are able to see impressive improvements on a targeted rare class without losing performance on the other classes. On the popular autonomous driving benchmark nuScenes, we use this procedure to improve performance on the rare class of cyclists by 18%, resulting in the best Cylinder3D model on the LiDAR segmentation benchmark. We also show that these techniques extend to other classes (debris) and other tasks (LiDAR object detection), giving strong evidence that this methodology generalizes well to other autonomous perception tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Rajendran_Improving_Rare_Classes_on_nuScenes_LiDAR_Segmentation_Through_Targeted_Domain_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Rajendran_Improving_Rare_Classes_on_nuScenes_LiDAR_Segmentation_Through_Targeted_Domain_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Does Image Anonymization Impact Computer Vision Training?",
        "author": "H\u00e5kon Hukkel\u00e5s, Frank Lindseth",
        "abstract": "Image anonymization is widely adapted in practice to comply with privacy regulations in many regions. However, anonymization often degrades the quality of the data, reducing its utility for computer vision development. In this paper, we investigate the impact of image anonymization for training computer vision models on key computer vision tasks (detection, instance segmentation, and pose estimation). Specifically, we benchmark the recognition drop on common detection datasets, where we evaluate both traditional and realistic anonymization for faces and full bodies. Our comprehensive experiments reflect that traditional image anonymization substantially impacts final model performance, particularly when anonymizing the full body. Furthermore, we find that realistic anonymization can mitigate this decrease in performance, where our experiments reflect a minimal performance drop for face anonymization. Our study demonstrates that realistic anonymization can enable privacy-preserving computer vision development with minimal performance degradation across a range of important computer vision benchmarks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Hukkelas_Does_Image_Anonymization_Impact_Computer_Vision_Training_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Hukkelas_Does_Image_Anonymization_Impact_Computer_Vision_Training_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Difficulty Estimation With Action Scores for Computer Vision Tasks",
        "author": "Octavio Arriaga, Sebastian Palacio, Matias Valdenegro-Toro",
        "abstract": "As more machine learning models are now being applied in real world scenarios it has become crucial to evaluate their difficulties and biases. In this paper we present an unsupervised method for calculating a difficulty score based on the accumulated loss per epoch. Our proposed method does not require any modification to the model, neither any external supervision, and it can be easily applied to a wide range of machine learning tasks. We provide results for the tasks of image classification, image segmentation, and object detection. We compare our score against similar metrics and provide theoretical and empirical evidence of their difference. Furthermore, we show applications of our proposed score for detecting incorrect labels, and test for possible biases.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Arriaga_Difficulty_Estimation_With_Action_Scores_for_Computer_Vision_Tasks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Arriaga_Difficulty_Estimation_With_Action_Scores_for_Computer_Vision_Tasks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Towards Characterizing the Semantic Robustness of Face Recognition",
        "author": "Juan C. P\u00e9rez, Motasem Alfarra, Ali Thabet, Pablo Arbel\u00e1ez, Bernard Ghanem",
        "abstract": "Deep Neural Networks (DNNs) are brittle against imperceptible perturbations to their input. Face Recognition Models (FRMs) based on DNNs inherit this vulnerability. We propose a methodology for assessing and characterizing the robustness of FRMs against semantic perturbations to their input. Our methodology causes FRMs to malfunction by designing adversarial attacks that search for identity-preserving modifications to faces. In particular, given a face, our attacks find identity-preserving variants of the face such that an FRM fails to recognize the images belonging to the same identity. We model these identity-preserving semantic modifications via direction- and magnitude-constrained perturbations in the latent space of StyleGAN. We further propose to characterize the semantic robustness of an FRM by statistically describing the perturbations that induce the FRM to malfunction. Finally, we combine our methodology with a certification technique and provide (i) theoretical guarantees on an FRM's performance, and (ii) a formal description of how an FRM may model the notion of face identity.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Perez_Towards_Characterizing_the_Semantic_Robustness_of_Face_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Perez_Towards_Characterizing_the_Semantic_Robustness_of_Face_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Deep Prototypical-Parts Ease Morphological Kidney Stone Identification and Are Competitively Robust to Photometric Perturbations",
        "author": "Daniel Flores-Araiza, Francisco Lopez-Tiro, Jonathan El-Beze, Jacques Hubert, Miguel Gonzalez-Mendoza, Gilberto Ochoa-Ruiz, Christian Daul",
        "abstract": "Identifying the type of kidney stones can allow urologists to determine their cause of formation, improving the prescription of appropriate treatments to diminish future relapses. Currently, the associated ex-vivo diagnosis (known as Morpho-constitutional Analysis, MCA) is time-consuming, expensive and requires a great deal of experience, as it requires a visual analysis component that is highly operator dependant. Recently, machine learning methods have been developed for in-vivo endoscopic stone recognition. Deep Learning (DL) based methods outperform non-DL methods in terms of accuracy but lack explainability. Despite this trade-off, when it comes to making high-stakes decisions, it's important to prioritize understandable Computer-Aided Diagnosis (CADx) that suggests a course of action based on reasonable evidence, rather than a model prescribing a course of action. In this proposal, we learn Prototypical Parts (PPs) per kidney stone subtype, which are used by the DL model to generate an output classification. Using PPs in the classification task enables case-based reasoning explanations for such output, thus making the model interpretable. In addition, we modify global visual characteristics to describe their relevance to the PPs and the sensitivity of our model's performance. With this, we provide explanations with additional information at the sample, class and model levels in contrast to previous works. Although our implementation's average accuracy is lower than state-of-the-art (SOTA) noninterpretable DL models by 1.5%, our models perform 2.8% better on perturbed images with a lower standard deviation, without adversarial training. Thus, Learning PPs has the potential to create more robust DL models. Code at: https://github.com/DanielF29/Prototipical_Parts",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Flores-Araiza_Deep_Prototypical-Parts_Ease_Morphological_Kidney_Stone_Identification_and_Are_Competitively_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Flores-Araiza_Deep_Prototypical-Parts_Ease_Morphological_Kidney_Stone_Identification_and_Are_Competitively_CVPRW_2023_paper.pdf"
    },
    {
        "title": "LD-GAN: Low-Dimensional Generative Adversarial Network for Spectral Image Generation With Variance Regularization",
        "author": "Emmanuel Martinez, Roman Jacome, Alejandra Hernandez-Rojas, Henry Arguello",
        "abstract": "Deep learning methods are state-of-the-art for spectral image (SI) computational tasks. However, these methods are constrained in their performance since available datasets are limited due to the highly expensive and long acquisition time. Usually, data augmentation techniques are employed to mitigate the lack of data. Surpassing classical augmentation methods, such as geometric transformations, GANs enable diverse augmentation by learning and sampling from the data distribution. Nevertheless, GAN-based SI generation is challenging since the high-dimensionality nature of this kind of data hinders the convergence of the GAN training yielding to suboptimal generation. To surmount this limitation, we propose low-dimensional GAN (LD-GAN), where we train the GAN employing a low-dimensional representation of the dataset with the latent space of a pretrained autoencoder network. Thus, we generate new low-dimensional samples which are then mapped to the SI dimension with the pretrained decoder network. Besides, we propose a statistical regularization to control the low-dimensional representation variance for the autoencoder training and to achieve high diversity of samples generated with the GAN. We validate our method LD-GAN as data augmentation strategy for compressive spectral imaging, SI super-resolution, and RBG to spectral tasks with improvements varying from 0.5 to 1 [dB] in each task respectively. We perform comparisons against the non-data augmentation training, traditional DA, and with the same GAN adjusted and trained to generate the full-sized SIs. The code of this paper can be found in https://github.com/romanjacome99/LD_GAN.git",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Martinez_LD-GAN_Low-Dimensional_Generative_Adversarial_Network_for_Spectral_Image_Generation_With_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Martinez_LD-GAN_Low-Dimensional_Generative_Adversarial_Network_for_Spectral_Image_Generation_With_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Wildlife Image Generation From Scene Graphs",
        "author": "Yoshio Rubio, Marco A. Contreras-Cruz",
        "abstract": "Image generation from natural language descriptions is an exciting and challenging task in computer vision and natural language processing. In this work, we propose a novel method to generate synthetic images from scene graphs in the context of wildlife scenarios. Given a scene graph, our method uses a graph convolutional network to predict semantic layouts, and a semi-parametric approach based on a cascade refinement network to synthesize the final image. We test our approach on a subset of COCO dataset, which we call COCO-Wildlife. Our results outperform the baselines, both quantitatively and qualitatively, and the visual results show the ability of our approach to generate stunning images with natural interaction between the different objects. Our findings show the potential to expand the use case of the proposed method to other contexts where scale and realism is fundamental.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Rubio_Wildlife_Image_Generation_From_Scene_Graphs_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Rubio_Wildlife_Image_Generation_From_Scene_Graphs_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SUPRA: Superpixel Guided Loss for Improved Multi-Modal Segmentation in Endoscopy",
        "author": "Rafael Mart\u00ednez-Garc\u00eda-Pe\u00f1a, Mansoor Ali Teevno, Gilberto Ochoa-Ruiz, Sharib Ali",
        "abstract": "Domain shift is a well-known problem in the medical imaging community. In particular, for endoscopic image analysis data can have different modalities that cause the performance of deep learning (DL) methods to become adversely affected. Methods developed on one modality cannot be used for a different modality without retraining. However, in real clinical settings, endoscopists switch between modalities depending on the specifics of the condition being explored. In this paper, we explore domain generalisation to enable DL methods to be used in such scenarios. To this extent, we propose to use superpixels generated with Simple Linear Iterative Clustering (SLIC), which we refer to as \"SUPRA\" for SUPeRpixel Augmented method. SUPRA first generates a preliminary segmentation mask making use of our new loss \"SLICLoss\" that encourages both an accurate and superpixel-consistent segmentation. We demonstrate that SLICLoss when combined with Binary Cross Entropy loss (BCE) can improve the model's generalisability with data that presents significant domain shift due to a change in lighting modalities. We validate this novel compound loss on a vanilla UNet using the EndoUDA dataset, which contains images for Barret's Esophagus from two modalities. We show that our method yields a relative improvement of more than 20% IoU in the target domain set compared to the baseline.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Martinez-Garcia-Pena_SUPRA_Superpixel_Guided_Loss_for_Improved_Multi-Modal_Segmentation_in_Endoscopy_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Martinez-Garcia-Pena_SUPRA_Superpixel_Guided_Loss_for_Improved_Multi-Modal_Segmentation_in_Endoscopy_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Detail-Preserving Self-Supervised Monocular Depth With Self-Supervised Structural Sharpening",
        "author": "Juan Luis Gonzalez, Jaeho Moon, Munchurl Kim",
        "abstract": "We propose to close the gap between self-supervised and fully-supervised methods for the single view depth estimation (SVDE) task in terms of the levels of detail and sharpness in the estimated depth maps. Detailed SVDE is challenging as even fully-supervised methods struggle to obtain detail-preserving depth estimates. Recent works have proposed multi-scale boosting techniques and exploiting semantic masks to improve the structural information in the estimated depth maps. In contrast, our proposed method in this paper yields detail-preserving depth estimates from a single forward pass without increasing the computational cost or requiring additional data. We achieve this by exploiting a missing component in SVDE, Self-Supervised Structural Sharpening, referred to as S4. S4 is a mechanism that encourages a similar level of detail between the RGB input and the depth/disparity output. To this extent, we propose a novel DispNet-S4 network for detail-preserving SVDE. Our network exploits un-blurring and un-noising tasks of clean input images for learning S4 without the need for either additional data (e.g., segmentation masks, matting maps, etc.) or advanced network blocks (attention, transformers, etc.). The recovered structural details in the un-blurring and un-noising operations are transferred to the estimated depth maps via adaptive convolutions to yield structurally sharpened depths that are selectively used for self-supervision. We provide extensive experimental results and ablation studies that show our proposed DispNet-S4 network can yield fine details in the depth maps while achieving state-of-the-art quantitative metrics for the challenging KITTI dataset.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Gonzalez_Detail-Preserving_Self-Supervised_Monocular_Depth_With_Self-Supervised_Structural_Sharpening_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Gonzalez_Detail-Preserving_Self-Supervised_Monocular_Depth_With_Self-Supervised_Structural_Sharpening_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Isolated Sign Language Recognition Based on Tree Structure Skeleton Images",
        "author": "David Laines, Miguel Gonzalez-Mendoza, Gilberto Ochoa-Ruiz, Gissella Bejarano",
        "abstract": "Sign Language Recognition (SLR) systems aim to be embedded in video stream platforms to recognize the sign performed in front of a camera. SLR research has taken advantage of recent advances in pose estimation models to use skeleton sequences estimated from videos instead of RGB information to predict signs. This approach can make HAR-related tasks less complex and more robust to diverse backgrounds, lightning conditions, and physical appearances. In this work, we explore the use of a spatio-temporal skeleton representation such as Tree Structure Skeleton Image (TSSI) as an alternative input to improve the accuracy of skeleton-based models for SLR. TSSI converts a skeleton sequence into an RGB image where the columns represent the joints of the skeleton in a depth-first tree traversal order, the rows represent the temporal evolution of the joints, and the three channels represent the (x, y, z) coordinates of the joints. We trained a DenseNet-121 using this type of input and compared it with other skeleton-based deep learning methods using a large-scale American Sign Language (ASL) dataset, WLASL. Our model (SL-TSSI-DenseNet) overcomes the state-of-the-art of other skeleton-based models. Moreover, when including data augmentation our proposal achieves better results than both skeleton-based and RGB-based models. We evaluated the effectiveness of our model on the Ankara University Turkish Sign Language (TSL) dataset, AUTSL, and a Mexican Sign Language (LSM) dataset. On the AUTSL dataset, the model achieves similar results to the state-of-the-art of other skeleton-based models. On the LSM dataset, the model achieves higher results than the baseline. As far as we know, our work is the first to try TSSI for sign language recognition and our results suggest it presents a real alternative for isolated sign language representation. Code has been made available at: https://github.com/davidlainesv/SL-TSSI-DenseNet.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Laines_Isolated_Sign_Language_Recognition_Based_on_Tree_Structure_Skeleton_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Laines_Isolated_Sign_Language_Recognition_Based_on_Tree_Structure_Skeleton_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "High-Level Context Representation for Emotion Recognition in Images",
        "author": "Willams de Lima Costa, Estefania Talavera, Lucas Silva Figueiredo, Veronica Teichrieb",
        "abstract": "Emotion recognition is the task of classifying perceived emotions in people. Previous works have utilized various nonverbal cues to extract features from images and correlate them to emotions. Of these cues, situational context is particularly crucial in emotion perception since it can directly influence the emotion of a person. In this paper, we propose an approach for high-level context representation extraction from images. The model relies on a single cue and a single encoding stream to correlate this representation with emotions. Our model competes with the state-of-the-art, achieving an mAP of 0.3002 on the EMOTIC dataset while also being capable of execution on consumer-grade hardware at 90 frames per second. Overall, our approach is more efficient than previous models and can be easily deployed to address real-world problems related to emotion recognition.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/de_Lima_Costa_High-Level_Context_Representation_for_Emotion_Recognition_in_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/de_Lima_Costa_High-Level_Context_Representation_for_Emotion_Recognition_in_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Zero-Shot Classification at Different Levels of Granularity",
        "author": "Mat\u00edas Molina",
        "abstract": "Zero-shot classification (ZSC) is the task of learning predictors for classes not seen during training. The different methods proposed in literature are evaluated over specific datasets with their specific class partitions, but little attention has been paid to the impact of the dataset granularity when ZSC is performed. The novelty of this work is to generate synthetic datasets by controlling their granularity level to analyze the ZSC performance afterwards. Moreover, it presents an approach that allows us to preserve the visual and semantic structures. The experiments show that ZSC performance exhibits strong differences depending on the data granularity and it reveals the relevance of both visual and semantic spaces when performing ZSC.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Molina_Zero-Shot_Classification_at_Different_Levels_of_Granularity_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LatinX/papers/Molina_Zero-Shot_Classification_at_Different_Levels_of_Granularity_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Photometric Correction for Infrared Sensors",
        "author": "Jincheng Zhang, Andrew R. Willis, Kevin Brink",
        "abstract": "Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Zhang_Photometric_Correction_for_Infrared_Sensors_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Zhang_Photometric_Correction_for_Infrared_Sensors_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Spectral Transfer Guided Active Domain Adaptation for Thermal Imagery",
        "author": "Berkcan Ustun, Ahmet Kagan Kaya, Ezgi Cakir Ayerden, Fazil Altinel",
        "abstract": "The exploitation of visible spectrum datasets has led deep networks to show remarkable success. However, real-world tasks include low-lighting conditions which arise performance bottlenecks for models trained on large-scale RGB image datasets. Thermal IR cameras are more robust against such conditions. Therefore, the usage of thermal imagery in real-world applications can be useful. Unsupervised domain adaptation (UDA) allows transferring information from a source domain to a fully unlabeled target domain. Despite substantial improvements in UDA, the performance gap between UDA and its supervised learning counterpart remains significant. By picking a small number of target samples to annotate and using them in training, active domain adaptation tries to mitigate this gap with minimum annotation expense. We propose an active domain adaptation method in order to examine the efficiency of combining the visible spectrum and thermal imagery modalities. When the domain gap is considerably large as in the visible-to-thermal task, we may conclude that the methods without explicit domain alignment cannot achieve their full potential. To this end, we propose a spectral transfer guided active domain adaptation method to select the most informative unlabeled target samples while aligning source and target domains. We used the large-scale visible spectrum dataset MS-COCO as the source domain and the thermal dataset FLIR ADAS as the target domain to present the results of our method. Extensive experimental evaluation demonstrates that our proposed method outperforms the state-of-the-art active domain adaptation methods. The code and models are publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Ustun_Spectral_Transfer_Guided_Active_Domain_Adaptation_for_Thermal_Imagery_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Ustun_Spectral_Transfer_Guided_Active_Domain_Adaptation_for_Thermal_Imagery_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Thermal Image Super-Resolution Challenge Results - PBVS 2023",
        "author": "Rafael E. Rivadeneira, Angel D. Sappa, Boris X. Vintimilla, Dai Bin, Li Ruodi, Li Shengye, Zhiwei Zhong, Xianming Liu, Junjun Jiang, Chenyang Wang",
        "abstract": "This paper presents the results of two tracks from the fourth Thermal Image Super-Resolution (TISR) challenge, held at the Perception Beyond the Visible Spectrum (PBVS) 2023 workshop. Track-1 uses the same thermal image dataset as previous challenges, with 951 training images and 50 validation images at each resolution. In this track, two evaluations were conducted: the first consists of generating a SR image from a HR thermal noisy image downsampled by four, and the second consists of generating a SR image from a mid-resolution image and compare it with its semi-registered HR image (acquired with another camera). The results of Track-1 outperformed those from last year's challenge. On the other hand, Track-2 uses a new acquired dataset consisting of 160 registered visible and thermal images of the same scenario for training and 30 validation images. This year, more than 150 teams participated in the challenge tracks, demonstrating the community's ongoing interest in this topic.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Rivadeneira_Thermal_Image_Super-Resolution_Challenge_Results_-_PBVS_2023_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Rivadeneira_Thermal_Image_Super-Resolution_Challenge_Results_-_PBVS_2023_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Mitigating Catastrophic Interference Using Unsupervised Multi-Part Attention for RGB-IR Face Recognition",
        "author": "Kshitij Nikhal, Nkiruka Uzuegbunam, Bridget Kennedy, Benjamin S. Riggan",
        "abstract": "Modern algorithms for RGB-IR facial recognition--a challenging problem where infrared probe images are matched with visible gallery images--leverage precise and accurate guidance from curated (i.e., labeled) data to bridge large spectral differences. However, supervised cross-spectral face recognition methods are often extremely sensitive due to over-fitting to labels, performing well in some settings but not in others. Moreover, when fine-tuning on data from additional settings, supervised cross-spectral face recognition are prone to catastrophic forgetting. Therefore, we propose a novel unsupervised framework for RGB-IR face recognition to minimize the cost and time inefficiencies pertaining to labeling large-scale, multi-spectral data required to train supervised cross-spectral recognition methods and to alleviate the effect of forgetting by removing over dependence on hard labels to bridge such large spectral differences. The proposed framework integrates an efficient backbone network architecture with part-based attention models, which collectively enhances common information between visible and infrared faces. Then, the framework is optimized using pseudo-labels and a new cross-spectral memory bank loss. This framework is evaluated on the ARL-VTF and TUFTS datasets, achieving 98.55% and 43.28% true accept rate, respectively. Additionally, we analyze effects of forgetting and show that our framework is less prone to these effects.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Nikhal_Mitigating_Catastrophic_Interference_Using_Unsupervised_Multi-Part_Attention_for_RGB-IR_Face_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Nikhal_Mitigating_Catastrophic_Interference_Using_Unsupervised_Multi-Part_Attention_for_RGB-IR_Face_CVPRW_2023_paper.pdf"
    },
    {
        "title": "C-PLES: Contextual Progressive Layer Expansion With Self-Attention for Multi-Class Landslide Segmentation on Mars Using Multimodal Satellite Imagery",
        "author": "Abel A. Reyes, Sidike Paheding, A. Rajaneesh, K.S. Sajinkumar, Thomas Oommen",
        "abstract": "Landslide segmentation on Earth has been a challenging computer vision task, in which the lack of annotated data or limitation on computational resources has been a major obstacle in the development of accurate and scalable artificial intelligence-based models. However, the accelerated progress in deep learning techniques and the availability of data-sharing initiatives have enabled significant achievements in landslide segmentation on Earth. With the current capabilities in technology and data availability, replicating a similar task on other planets, such as Mars, does not seem an impossible task anymore. In this research, we present C-PLES (Contextual Progressive Layer Expansion with Self-attention), a deep learning architecture for multi-class landslide segmentation in the Valles Marineris (VM) on Mars. Even though the challenges could be different from on-Earth landslide segmentation, due to the nature of the environment and data characteristics, the outcomes of this research lead to a better understanding of the geology and terrain of the planet, in addition, to providing valuable insights regarding the importance of image modality for this task. The proposed architecture combines the merits of the progressive neuron expansion with attention mechanisms in an encoder-decoder-based framework, delivering competitive performance in comparison with state-of-the-art deep learning architectures for landslide segmentation. In addition to the new multi-class segmentation architecture, we introduce a new multi-modal multi-class Martian landslide segmentation dataset for the first time.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Reyes_C-PLES_Contextual_Progressive_Layer_Expansion_With_Self-Attention_for_Multi-Class_Landslide_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Reyes_C-PLES_Contextual_Progressive_Layer_Expansion_With_Self-Attention_for_Multi-Class_Landslide_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Three-Stage Framework With Reliable Sample Pool for Long-Tailed Classification",
        "author": "Feng Cai, Keyu Wu, Haipeng Wang, Feng Wang",
        "abstract": "Synthetic Aperture Radar (SAR) imagery presents a promising solution for acquiring Earth surface information regardless of weather and daylight. However, the SAR dataset is commonly characterized by a long-tailed distribution due to the scarcity of samples from infrequent categories. In this work, we extend the problem to aerial view object classification in the SAR dataset with long-tailed distribution and a plethora of negative samples. Specifically, we propose a three-stage approach that employs a ResNet101 backbone for feature extraction, Class-balanced Focal Loss for class-level re-weighting, and reliable pseudo-labels generated through semi-supervised learning to improve model performance. Moreover, we introduce a Reliable Sample Pool (RSP) to enhance the model's confidence in predicting in-distribution data and mitigate the domain gap between the labeled and unlabeled sets. The proposed framework achieved a Top-1 Accuracy of 63.20% and an AUROC of 0.71 on the final dataset, winning the first place in track 1 of the PBVS 2023 Multi-modal Aerial View Object Classification Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Cai_A_Three-Stage_Framework_With_Reliable_Sample_Pool_for_Long-Tailed_Classification_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Cai_A_Three-Stage_Framework_With_Reliable_Sample_Pool_for_Long-Tailed_Classification_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeepMAO: Deep Multi-Scale Aware Overcomplete Network for Building Segmentation in Satellite Imagery",
        "author": "Aniruddh Sikdar, Sumanth Udupa, Prajwal Gurunath, Suresh Sundaram",
        "abstract": "Building segmentation in large-scale aerial images is challenging, especially for small buildings in dense and cluttered urban environments. Complex building structures with highly varied geometric footprints pose an additional challenge for the building segmentation task in satellite imagery. In this work, we propose to tackle the issue of detecting and segmenting small and complex-shaped buildings in Electro-Optical (EO) and SAR satellite imagery. A novel architecture Deep Multi-scale Aware Overcomplete Network (DeepMAO), is proposed that comprises an overcomplete branch that focuses on fine structural features and an undercomplete (U-Net) branch tasked to focus on coarse, semantic-rich features. Additionally, a novel self-regulating augmentation strategy, \"Loss-Mix,\" is proposed to increase pixel representation of misclassified pixels. DeepMAO is simple and efficient in accurately identifying small and geometrically complex buildings. Experimental results on SpaceNet 6 dataset, on both EO and SAR modalities, and the INRIA dataset show that DeepMAO achieves state-of-the-art building segmentation performance, including small and complex-shaped buildings with a negligible increase in the parameter count. In addition, the presence of the over-complete branch in DeepMAO helps in handling the speckle noise present in the SAR image modality.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Sikdar_DeepMAO_Deep_Multi-Scale_Aware_Overcomplete_Network_for_Building_Segmentation_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Sikdar_DeepMAO_Deep_Multi-Scale_Aware_Overcomplete_Network_for_Building_Segmentation_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Appearance Label Balanced Triplet Loss for Multi-Modal Aerial View Object Classification",
        "author": "Raghunath Sai Puttagunta, Zhu Li, Shuvra Bhattacharyya, George York",
        "abstract": "Automatic target recognition (ATR) using image data is an important computer vision task with widespread applications in remote sensing for surveillance, object tracking, urban planning, agriculture, and more. Although there have been continuous advancements in this task, there is still significant room for further advancements, particularly with aerial images. This work extracts rich information from multimodal synthetic aperture radar (SAR) and electro-optical (EO) aerial images to perform object classification. Compared to EO images, the advantages of SAR images are that they can be captured at night and in any weather condition. Compared to EO images, the disadvantage of SAR images is that they are noisy. Overcoming the noise inherent to SAR images is a challenging, but worthwhile, task because of the additional information SAR images provide the model. This work proposes a training strategy that involves the creation of appearance labels to generate triplet pairs for training the network with both triplet loss and cross-entropy loss. During the development phase of the 2023 Perception Beyond Visual Spectrum (PBVS) Multi-modal Aerial Image Object Classification (MAVOC) challenge, our ResNet-34 model achieved a top-1 accuracy of 64.29% for Track 1 and our ensemble learning model achieved a top-1 accuracy 75.84% for Track 2. These values are 542% and 247% higher than the baseline values. Overall, this work ranked 3rd in both Track 1 and Track 2.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Puttagunta_Appearance_Label_Balanced_Triplet_Loss_for_Multi-Modal_Aerial_View_Object_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Puttagunta_Appearance_Label_Balanced_Triplet_Loss_for_Multi-Modal_Aerial_View_Object_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Detecting Underwater Discrete Scatterers in Echograms With Deep Learning-Based Semantic Segmentation",
        "author": "Rhythm Vohra, Femina Senjaliya, Melissa Cote, Amanda Dash, Alexandra Branzan Albu, Julek Chawarski, Steve Pearce, Kaan Ersahin",
        "abstract": "This paper reports on an exploratory study of the automatic detection of discrete scatterers in the water column from underwater acoustic data with deep learning (DL) networks. Underwater acoustic surveys using moored singlebeam multi-frequency echosounders make environmental monitoring tasks possible in a non-invasive manner. Discrete scatterers, i.e., individual marine organisms, are particularly challenging to detect automatically due to their small size, sometimes overlapping tracks, and similarity with various types of noise. As our interest lies in identifying the presence and general location of discrete scatterers, we propose the use of a semantic segmentation paradigm over object detection or instance segmentation, and compare several state-of-the-art DL networks. We also study the effects of early and late fusion strategies to aggregate information contained in the multi-frequency data. Experiments on the Okisollo Channel Underwater Discrete Scatterers dataset, which also include schools of herring and juvenile salmon, air bubbles from wave and fish school activity, and significant noise bands, show that late fusion yields higher metrics, with DeepLabV3+ outperforming other networks in terms of precision and intersection over union (IoU) and Attention U-Net offering higher recall. The detection of discrete scatterers is a good example of a problem for which exact annotations cannot be reached due to various reasons; in several cases, network outputs seem visually more adequate than the annotations (which contain inherent noise). This opens up the way for utilizing actual detection results to improve the annotations iteratively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Vohra_Detecting_Underwater_Discrete_Scatterers_in_Echograms_With_Deep_Learning-Based_Semantic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Vohra_Detecting_Underwater_Discrete_Scatterers_in_Echograms_With_Deep_Learning-Based_Semantic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Thermal Infrared Single Image Dehazing and Blind Image Quality Assessment",
        "author": "Fabian Erlenbusch, Constanze Merkt, Bernardo de Oliveira, Alexander Gatter, Friedhelm Schwenker, Ulrich Klauck, Michael Teutsch",
        "abstract": "Image dehazing is a method to reduce the effects of haze, dust, or fog in images in order to provide a clear view of the observed scene. A large variety of traditional and machine learning-based approaches exists in the literature. However, these approaches mostly consider color images in the visual-optical spectrum. Apparently, the thermal infrared spectrum is much less affected by haze due to its longer wavelength. But atmospheric perturbations during long-range observation can cause image quality degradation in the thermal infrared (TIR) spectrum as well. In this paper, we propose a method to generate synthetic haze for TIR images. Then, we analyze the already existing blind image quality assessment measure Fog Aware Density Evaluator (FADE) for its applicability to the TIR spectrum. We further provide a comprehensive overview of the current state-of-the-art in image dehazing and empirically show that many approaches originally designed for visual-optical images perform surprisingly well when applied to the TIR spectrum. This is shown in experiments conducted on the recently published M3FD dataset.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Erlenbusch_Thermal_Infrared_Single_Image_Dehazing_and_Blind_Image_Quality_Assessment_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Erlenbusch_Thermal_Infrared_Single_Image_Dehazing_and_Blind_Image_Quality_Assessment_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multimodal Object Detection by Channel Switching and Spatial Attention",
        "author": "Yue Cao, Junchi Bin, Jozsef Hamari, Erik Blasch, Zheng Liu",
        "abstract": "Multimodal object detection has attracted great attention in recent years since the information specific to different modalities can complement each other and effectively improve the accuracy and stability of the detection model. However, compared to processing the inputs from a single modality, fusing information from multiple modalities can significantly increase the computational complexity of the model, thus impairing its efficiency. Therefore the multimodal fusion module needs to be carefully designed to enhance the performance of the detection model while keeping the computational consumption low. In this paper, we propose a novel lightweight fusion module that can efficiently fuse the inputs from different modalities using channel switching and spatial attention (CSSA). The effectiveness and generalizability of the module are tested using two public multimodal datasets LLVIP and FLIR, both of which comprise paired infrared (IR) and visible (RGB) images. The experiments demonstrate that the proposed CSSA module can substantially improve the accuracy of multimodal object detection without consuming excessive computing resources.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Cao_Multimodal_Object_Detection_by_Channel_Switching_and_Spatial_Attention_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Cao_Multimodal_Object_Detection_by_Channel_Switching_and_Spatial_Attention_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Sensor Ensemble-Guided Attention Network for Aerial Vehicle Perception Beyond Visible Spectrum",
        "author": "Alicja Kwasniewska, Anastacia MacAllister, Rey Nicolas, Javier Garza",
        "abstract": "Researchers from different market domains have made significant developments in Artificial Intelligence (AI) enabling more advanced automated sensing systems and, thus, eliminating the need for the time-consuming manual analysis of data, which is prone to human errors. However, successful deployment of such systems in real world applications requires careful design and analysis of the proposed models. This work focuses on perception done on Unmanned Aerial Vehicles (UAV) using multi-task learning. There are multiple challenges when considering such platforms. First of all, they often operate in difficult and dynamic conditions affected by various factors, such as background noises, ego-noise of the motors and occluded views. At the same time, they require high performance local compute, co-designed with optimized software solutions that meet small size, weight, and power (SWaP) requirements. Therefore, the AI models designed for such systems should not introduce computational and memory overheads to allow for real time processing at the embedded edge. Taking this into account, this work proposes a novel neural network-based system that utilizes ensemble-guided modulations of audio path fused with the infrared (IR) visual embedding using the attention mechanism. The ensemble mechanism doesn't require spawning new ensemble members, but instead operates on FiLM (Feature-wise Linear Modulation) activation, making it suitable for resource-constraints embedded edge platforms. The performed experiments show that the proposed network outperforms a single FiLM network by 15% and is more robust to noise.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Kwasniewska_Multi-Sensor_Ensemble-Guided_Attention_Network_for_Aerial_Vehicle_Perception_Beyond_Visible_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Kwasniewska_Multi-Sensor_Ensemble-Guided_Attention_Network_for_Aerial_Vehicle_Perception_Beyond_Visible_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Enhanced Thermal-RGB Fusion for Robust Object Detection",
        "author": "Wassim El Ahmar, Yahya Massoud, Dhanvin Kolhatkar, Hamzah AlGhamdi, Mohammad Alja\u2019afreh, Riad Hammoud, Robert Laganiere",
        "abstract": "Thermal imaging has seen rapid development in the last few years due to its robustness in different weather and lighting conditions and its reduced production cost. In this paper, we study the performance of different RGB-Thermal fusion methods in the task of object detection, and introduce a new RGB-Thermal fusion approach that enhances the performance by up to 9% using a sigmoid-activated gating mechanism for early fusion. We conduct our experiments on an enhanced version of the City Scene RGB-Thermal MOT Dataset where we register the RGB and corresponding thermal images in order to conduct fusion experiments. Finally, we benchmark the speed of our proposed fusion method and show that it adds negligible overhead to the model processing time. Our work would be useful for autonomous systems and any multi-model machine vision system. The improved version of the dataset, our trained models, and source code are available at https://github.com/wassimea/rgb-thermal-fusion",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Ahmar_Enhanced_Thermal-RGB_Fusion_for_Robust_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Ahmar_Enhanced_Thermal-RGB_Fusion_for_Robust_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MoundCount: A Detection-Based Approach for Automatic Counting of Planting Microsites on UAV Images",
        "author": "Ahmed Zgaren, Wassim Bouachir, Nizar Bouguila, Riad I. Hammoud",
        "abstract": "Planting by mounding is a commonly used forestry technique that improves soil quality and ensures optimal tree growth conditions. During planting operations, one of the main planning steps is to estimate the number of mechanically created mounds in each planting block. Traditional counting methods involve manual field surveys or human photo-interpretation of UAV images, which are generally subject to errors and time-consuming. In this work, we propose a new approach to count mounds on UAV orthomosaics. Our framework is designed to estimate the required number of seedlings for a given planting block, based on a visual detection approach and a global estimation module. Firstly, a deep local detection model is applied on local patches to recognize and count visible mounds. Then, an estimation model, based on global features is used to predict the final number of plant seedling required for a given plantation block. To evaluate the proposed framework in real-world conditions, we constructed a large UAV dataset, including 18 UAV orthomosaics, comprising 111,000 mounds. We have conducted extensive experiments in our dataset, including a comparison with the state-of-the-art counting methods, as well as an analysis of Human-Level Performance (HLP) in identifying and annotating mounds. The experimental results show that our model reaches the best performance in terms of MAE and MSE, by comparison to state-of-the-art automatic counting mehtods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Zgaren_MoundCount_A_Detection-Based_Approach_for_Automatic_Counting_of_Planting_Microsites_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Zgaren_MoundCount_A_Detection-Based_Approach_for_Automatic_Counting_of_Planting_Microsites_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Modal Aerial View Image Challenge: Translation From Synthetic Aperture Radar to Electro-Optical Domain Results - PBVS 2023",
        "author": "Spencer Low, Oliver Nina, Angel D. Sappa, Erik Blasch, Nathan Inkawhich",
        "abstract": "This paper unveils the discoveries and outcomes of the inaugural iteration of the Multi-modal Aerial View Image Challenge (MAVIC) aimed at image translation. The primary objective of this competition is to stimulate research efforts towards the development of models capable of translating co-aligned images between multiple modalities. To accomplish the task of image translation, the competition utilizes images obtained from both synthetic aperture radar (SAR) and electro-optical (EO) sources. Specifically, the challenge centers on the translation from the SAR modality to the EO modality, an area of research that has garnered attention. The inaugural challenge demonstrates the feasibility of the task. The dataset utilized in this challenge is derived from the UNIfied COincident Optical and Radar for recognitioN (UNICORN) dataset. We introduce an new version of the UNICORN dataset that is focused on enabling the sensor translation task. Performance evaluation is conducted using a combination of measures to ensure high fidelity and high accuracy translations.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Low_Multi-Modal_Aerial_View_Image_Challenge_Translation_From_Synthetic_Aperture_Radar_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Low_Multi-Modal_Aerial_View_Image_Challenge_Translation_From_Synthetic_Aperture_Radar_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Seeing Through the Data: A Statistical Evaluation of Prohibited Item Detection Benchmark Datasets for X-Ray Security Screening",
        "author": "Brian K. S. Isaac-Medina, Seyma Yucer, Neelanjan Bhowmik, Toby P. Breckon",
        "abstract": "The rapid progress in automatic prohibited object detection within the context of X-ray security screening, driven forward by advances in deep learning, has resulted in the first internationally-recognized, application-focused object detection performance standard (ECAC Common Testing Methodology for Automated Prohibited Item Detection Systems). However, the ever-increasing volume of detection work in this application domain is highly reliant on a limited set of large-scale benchmark detection datasets that are specific to this domain. This study provides a comprehensive quantitative analysis of the underlying distribution of the prohibited item instances in three of the most prevalent X-ray security imagery benchmark and how these correlate against the detection performance of six state-of-the-art object detectors spanning multiple contemporary object detection paradigms. We focus on object size, location and aspect ratio within the image in addition to looking at global properties such as image colour distribution. Our results show a clear correlation between false negative (missed) detections and object size with the distribution of undetected items being statistically smaller in size than those typically found in the corresponding dataset as a whole. For false positive detections, the size distribution of such false alarm instances is shown to differ from the corresponding dataset test distribution in all cases. Furthermore, we observe that one-stage, anchor-free object detectors may be more vulnerable to the detection of heavily occluded or cluttered objects than other approaches whilst the detection of smaller prohibited item instances such as bullets remains more challenging than other object types.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Isaac-Medina_Seeing_Through_the_Data_A_Statistical_Evaluation_of_Prohibited_Item_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Isaac-Medina_Seeing_Through_the_Data_A_Statistical_Evaluation_of_Prohibited_Item_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution",
        "author": "Aditya Kasliwal, Pratinav Seth, Sriya Rallabandi, Sanchit Singhal",
        "abstract": "Thermal imaging has numerous advantages over regular visible-range imaging since it performs well in low-light circumstances. Super-Resolution approaches can broaden their usefulness by replicating accurate high-resolution thermal pictures using measurements from low-cost, low-resolution thermal sensors. Because of the spectral range mismatch between the images, Guided Super-Resolution of thermal images utilizing visible range images is difficult. However, In case of failure to capture Visible Range Images can prevent the operations of applications in critical areas. We present a novel data fusion framework and regularization technique for Guided Super Resolution of Thermal images. The proposed architecture is computationally in-expensive and lightweight with the ability to maintain performance despite missing one of the modalities, i.e., high-resolution RGB image or the lower-resolution thermal image, and is designed to be robust in the presence of missing data. The proposed method presents a promising solution to the frequently occurring problem of missing modalities in a real-world scenario.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Kasliwal_CoReFusion_Contrastive_Regularized_Fusion_for_Guided_Thermal_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Kasliwal_CoReFusion_Contrastive_Regularized_Fusion_for_Guided_Thermal_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multispectral Contrastive Learning With Viewmaker Networks",
        "author": "Jasmine Bayrooti, Noah Goodman, Alex Tamkin",
        "abstract": "Contrastive learning methods have been applied to a range of domains and modalities by training models to identify similar \"views\" of data points. However, specialized scientific modalities pose a challenge for this paradigm, as identifying good views for each scientific instrument is complex and time-intensive. In this paper, we focus on applying contrastive learning approaches to a variety of remote sensing datasets. We show that Viewmaker networks, a recently proposed method for generating views without extensive domain knowledge, can produce useful views in this setting. We also present a Viewmaker variant called Divmaker, which achieves similar performance and does not require adversarial optimization. Applying both methods to four multispectral imaging problems, each with a different format, we find that Viewmaker and Divmaker can outperform cropping- and reflection-based methods for contrastive learning in every case when evaluated on downstream classification tasks. This provides additional evidence that domain-agnostic methods can empower contrastive learning to scale to real-world scientific domains. Open source code can be found at https://github.com/anonymous629/divmaker.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Bayrooti_Multispectral_Contrastive_Learning_With_Viewmaker_Networks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Bayrooti_Multispectral_Contrastive_Learning_With_Viewmaker_Networks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "IR Reasoner: Real-Time Infrared Object Detection by Visual Reasoning",
        "author": "Meryem Mine G\u00fcndo\u011fan, Tolga Aksoy, Alptekin Temizel, Ugur Halici",
        "abstract": "Thermal Infrared (IR) imagery is utilized in several applications due to their unique properties. However, there are a number of challenges, such as small target objects, image noise, lack of textural information, and background clutter, negatively affecting detection of objects in IR images. Current real-time object detection methods treat each image region separately and, in face of these challenges, this sole dependency on feature maps extracted by convolutional layers is not ideal. In this paper, we introduce a new architecture for real-time object detection in IR images by reasoning the relations between image regions by using self-attention. The proposed method, IR Reasoner, takes the spatial and semantic coherency between image regions into account to enhance the feature maps. We integrated this approach into the current state-of-the-art one-stage object detectors YOLOv4, YOLOR, and YOLOv7, and trained them from scratch on the FLIR ADAS dataset. Experimental evaluations show that the Reasoner variants perform better than the baseline models while still running in real-time. Our best performing Reasoner model YOLOv7-W6-Reasoner achieves 40.5% AP at 32.7 FPS. The code is publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Gundogan_IR_Reasoner_Real-Time_Infrared_Object_Detection_by_Visual_Reasoning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Gundogan_IR_Reasoner_Real-Time_Infrared_Object_Detection_by_Visual_Reasoning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "VisiTherS: Visible-Thermal Infrared Stereo Disparity Estimation of Human Silhouette",
        "author": "Noreen Anwar, Philippe Duplessis-Guindon, Guillaume-Alexandre Bilodeau, Wassim Bouachir",
        "abstract": "This paper presents a novel approach for visible-thermal infrared stereoscopy, focusing on the estimation of disparities of human silhouettes. Visible-thermal infrared stereo poses several challenges, including occlusions and differently textured matching regions in both spectra. Finding matches between two spectra with varying colors, textures, and shapes adds further complexity to the task. To address the aforementioned challenges, this paper proposes a novel approach where a high-resolution convolutional neural network is used to better capture relationships between the two spectra. To do so, a modified HRNet backbone is used for feature extraction. This HRNet backbone is capable of capturing fine details and textures as it extracts features at multiple scales, thereby enabling the utilization of both local and global information. For matching visible and thermal infrared regions, our method extracts features on each patch using two modified HRNet streams. Features from the two streams are then combined for predicting the disparities by concatenation and correlation. Results on public datasets demonstrate the effectiveness of the proposed approach by improving the results by approximately 18 percentage points on the <= 1 pixel error, highlighting its potential for improving accuracy in this task. The code of VisiTherS is available on GitHub at the following link: (will be added upon acceptance).",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Anwar_VisiTherS_Visible-Thermal_Infrared_Stereo_Disparity_Estimation_of_Human_Silhouette_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Anwar_VisiTherS_Visible-Thermal_Infrared_Stereo_Disparity_Estimation_of_Human_Silhouette_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Modal Aerial View Object Classification Challenge Results - PBVS 2023",
        "author": "Spencer Low, Oliver Nina, Angel D. Sappa, Erik Blasch, Nathan Inkawhich",
        "abstract": "This paper presents the findings and results of the third iteration of the Multi-modal Aerial View Object Classification (MAVOC) challenge in a detailed and comprehensive manner. The primary aim of both challenges is to encourage research into building recognition models that utilize both synthetic aperture radar (SAR) and electro-optical (EO) imagery. Participating teams are encouraged to develop multi-modal approaches that incorporate complementary information from both domains. While the 2021 challenge demonstrated the feasibility of combining both modalities, the 2022 challenge expanded on the capability of multi-modal models. The 2023 challenge introduces a refined version of the UNICORN dataset and demonstrates significant improvements made. The 2023 challenge adopts an updated UNIfied COincident Optical and Radar for recognitioN (UNICORN) dataset and competition format. Two tasks are featured: SAR classification and SAR + EO classification. In addition to measuring accuracy of models, we also introduce out-of-distribution measures to encourage model robustness. The majority of this paper is dedicated to discussing the top performing methods and evaluating their performance on our blind test set. It is worth noting that all of the top ten teams outperformed the Resnet-50 baseline. The top team for SAR classification achieved a 173% improvement over the baseline, while the top team for SAR + EO classification achieved a 175% improvement.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Low_Multi-Modal_Aerial_View_Object_Classification_Challenge_Results_-_PBVS_2023_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Low_Multi-Modal_Aerial_View_Object_Classification_Challenge_Results_-_PBVS_2023_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Meta-Learning Approach for Domain Generalisation Across Visual Modalities in Vehicle Re-Identification",
        "author": "Eleni Kamenou, Jes\u00fas Mart\u00ednez del Rinc\u00f3n, Paul Miller, Patricia Devlin-Hill",
        "abstract": "Recent advances in imaging technologies have enabled the usage of infrared spectrum data for computer vision tasks previously working with traditional RGB data, such as re-identification. Infrared spectrum data can provide complementary and consistent visual information in situations of low visibility such as night-time, or adverse environments. However, the main issue that prevents the training of multi-modal systems is the lack of available infrared spectrum data. To this end, it is important to create systems that can easily adapt to data of multiple modalities, at inference time. In this paper, we propose a domain generalisation approach for multi-modal vehicle re-identification based on the recent success of meta-learning training approaches, and evaluate the ability of the model to perform to unseen modality data at testing time. In our experiments we use RGB, near-infrared and thermal-infrared modalities using the RGBNT100 dataset and prove that our meta-learning training configuration can improve the generalisation ability of the trained model compared to traditional training settings.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Kamenou_A_Meta-Learning_Approach_for_Domain_Generalisation_Across_Visual_Modalities_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Kamenou_A_Meta-Learning_Approach_for_Domain_Generalisation_Across_Visual_Modalities_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Shape and Intensity Analysis of Glioblastoma Multiforme Tumors",
        "author": "Yi Tang Chen, Sebastian Kurtek",
        "abstract": "We use a geometric approach to characterize tumor shape and intensity along the tumor contour in the context of Glioblastoma Multiforme. Properties of the proposed shape+intensity representation include invariance to translation, scale, rotation and reparameterization, which allow for objective comparison of tumor features. Controlling for the weight of intensity information in the shape+intensity representation results in improved comparisons between tumor features of different patients who have been diagnosed with Glioblastoma Multiforme; further, it allows for identification of different partitions of the data associated with different median survival among such patients. Our findings suggest that integrating and appropriately balancing information regarding GBM tumor shape and intensity can be beneficial for disease prognosis. We evaluate the proposed statistical framework using simulated examples as well as a real dataset of Glioblastoma Multiforme tumors.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Chen_Shape_and_Intensity_Analysis_of_Glioblastoma_Multiforme_Tumors_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Chen_Shape_and_Intensity_Analysis_of_Glioblastoma_Multiforme_Tumors_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Euler Characteristic Transform Based Topological Loss for Reconstructing 3D Images From Single 2D Slices",
        "author": "Kalyan Varma Nadimpalli, Amit Chattopadhyay, Bastian Rieck",
        "abstract": "The computer vision task of reconstructing 3D images, i.e., shapes, from their single 2D image slices is extremely challenging, more so in the regime of limited data. Deep learning models typically optimize geometric loss functions, which may lead to poor reconstructions as they ignore the structural properties of the shape. To tackle this, we propose a novel topological loss function based on the Euler Characteristic Transform. This loss can be used as an inductive bias to aid the optimization of any neural network toward better reconstructions in the regime of limited data. We show the effectiveness of the proposed loss function by incorporating it into SHAPR, a state-of-the-art shape reconstruction model, and test it on two benchmark datasets, viz., Red Blood Cells and Nuclei datasets. We also show a favourable property, namely injectivity and discuss the stability of the topological loss function based on the Euler Characteristic Transform.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Nadimpalli_Euler_Characteristic_Transform_Based_Topological_Loss_for_Reconstructing_3D_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Nadimpalli_Euler_Characteristic_Transform_Based_Topological_Loss_for_Reconstructing_3D_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Topology-Aware Focal Loss for 3D Image Segmentation",
        "author": "Andac Demir, Elie Massaad, Bulent Kiziltan",
        "abstract": "The efficacy of segmentation algorithms is frequently compromised by topological errors like overlapping regions, disrupted connections, and voids. To tackle this problem, we introduce a novel loss function, namely Topology-Aware Focal Loss (TAFL), that incorporates the conventional Focal Loss with a topological constraint term based on the Wasserstein distance between the ground truth and predicted segmentation masks' persistence diagrams. By enforcing identical topology as the ground truth, the topological constraint can effectively resolve topological errors, while Focal Loss tackles class imbalance. We begin by constructing persistence diagrams from filtered cubical complexes of the ground truth and predicted segmentation masks. We subsequently utilize the Sinkhorn-Knopp algorithm to determine the optimal transport plan between the two persistence diagrams. The resultant transport plan minimizes the cost of transporting mass from one distribution to the other and provides a mapping between the points in the two persistence diagrams. We then compute the Wasserstein distance based on this travel plan to measure the topological dissimilarity between the ground truth and predicted masks. We evaluate our approach by training a 3D U-Net with the MICCAI Brain Tumor Segmentation (BraTS) challenge validation dataset, which requires accurate segmentation of 3D MRI scans that integrate various modalities for the precise identification and tracking of malignant brain tumors. Then, we demonstrate that the quality of segmentation performance is enhanced by regularizing the focal loss through the addition of a topological constraint as a penalty term.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Demir_Topology-Aware_Focal_Loss_for_3D_Image_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Demir_Topology-Aware_Focal_Loss_for_3D_Image_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Hamming Similarity and Graph Laplacians for Class Partitioning and Adversarial Image Detection",
        "author": "Huma Jamil, Yajing Liu, Turgay Caglar, Christina Cole, Nathaniel Blanchard, Christopher Peterson, Michael Kirby",
        "abstract": "Researchers typically investigate neural network representations by examining activation outputs for one or more layers of a network. Here, we investigate the potential for ReLU activation patterns (encoded as bit vectors) to aid in understanding and interpreting the behavior of neural networks. We utilize Representational Dissimilarity Matrices (RDMs) to investigate the coherence of data within the embedding spaces of a deep neural network. From each layer of a network, we extract and utilize bit vectors to construct similarity scores between images. From these similarity scores, we build a similarity matrix for a collection of images drawn from 2 classes. We then apply Fiedler partitioning to the associated Laplacian matrix to separate the classes. Our results indicate, through bit vector representations, that the network continues to refine class detectability with the last ReLU layer achieving better than 95% separation accuracy. Additionally, we demonstrate that bit vectors aid in adversarial image detection, again achieving over 95% accuracy in separating adversarial and non-adversarial images using a simple classifier.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Liu_Hamming_Similarity_and_Graph_Laplacians_for_Class_Partitioning_and_Adversarial_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Liu_Hamming_Similarity_and_Graph_Laplacians_for_Class_Partitioning_and_Adversarial_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TopFusion: Using Topological Feature Space for Fusion and Imputation in Multi-Modal Data",
        "author": "Audun Myers, Henry Kvinge, Tegan Emerson",
        "abstract": "We present a novel multi-modal data fusion technique using topological features. The method, TopFusion, leverages the flexibility of topological data analysis tools (namely persistent homology and persistence images) to map multi-modal datasets into a common feature space by forming a new multi-channel persistence image. Each channel in the image is representative of a view of the data from a modality-dependent filtration. We demonstrate that the topological perspective we take allows for more effective data reconstruction, i.e. imputation. In particular, by performing imputation in topological feature space we are able to outperform the same imputation techniques applied to raw data or alternatively derived features. We show that TopFusion representations can be used as input to downstream deep learning-based computer vision models and doing so achieves comparable performance to other fusion methods for classification on two multi-modal datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Myers_TopFusion_Using_Topological_Feature_Space_for_Fusion_and_Imputation_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Myers_TopFusion_Using_Topological_Feature_Space_for_Fusion_and_Imputation_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robust Hierarchical Symbolic Explanations in Hyperbolic Space for Image Classification",
        "author": "Ainkaran Santhirasekaram, Avinash Kori, Mathias Winkler, Andrea Rockall, Francesca Toni, Ben Glocker",
        "abstract": "Explanations for black-box models help us to understand model decisions as well as provide information on model biases and inconsistencies. Most of the current post-hoc explainability techniques provide a single level of explanation, often in terms of feature importance scores or feature attention maps in the input space. The explanations provided by these methods are also sensitive to perturbations in the input space. Our focus is on explaining deep discriminative models for images at multiple levels of abstraction, from fine-grained to fully abstract explanations. We use the natural properties of hyperbolic geometry to more efficiently model a hierarchical relationship of symbolic features with decreased distortion to generate robust hierarchical explanations. Specifically, we distill the underpinning knowledge in an image classifier by quantising the continuous latent space to form hyperbolic symbols and learn the relations between these symbols in a hierarchical manner to induce a knowledge tree. We traverse the tree to extract hierarchical explanations in terms of chains of symbols and their corresponding visual semantics.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Santhirasekaram_Robust_Hierarchical_Symbolic_Explanations_in_Hyperbolic_Space_for_Image_Classification_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Santhirasekaram_Robust_Hierarchical_Symbolic_Explanations_in_Hyperbolic_Space_for_Image_Classification_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Topology Preserving Compositionality for Robust Medical Image Segmentation",
        "author": "Ainkaran Santhirasekaram, Mathias Winkler, Andrea Rockall, Ben Glocker",
        "abstract": "Deep Learning based segmentation models for medical imaging often fail under subtle distribution shifts calling into question the robustness of these models. Medical images however have the unique feature that there is limited structural variability between patients. We propose to exploit this notion and improve the robustness of deep learning based segmentation models by constraining the latent space to a learnt dictionary of base components. We incorporate a topological prior using persistent homology in the sampling of our dictionary to ensure topological accuracy after composition of the components. We further improve robustness by deep topological supervision applied in an hierarchical manner. We demonstrate the effectiveness of our method under various perturbations and in two single domain generalisation tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Santhirasekaram_Topology_Preserving_Compositionality_for_Robust_Medical_Image_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Santhirasekaram_Topology_Preserving_Compositionality_for_Robust_Medical_Image_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Making Corgis Important for Honeycomb Classification: Adversarial Attacks on Concept-Based Explainability Tools",
        "author": "Davis Brown, Henry Kvinge",
        "abstract": "Methods for model explainability have become increasingly critical for testing the fairness and soundness of deep learning. Concept-based interpretability techniques, which use a small set of human-interpretable concept exemplars in order to measure the influence of a concept on a model's internal representation of input, are an important thread in this line of research. In this work we show that these explainability methods can suffer the same vulnerability to adversarial attacks as the models they are meant to analyze. We demonstrate this phenomenon on two well-known concept-based interpretability methods: TCAV and faceted feature visualization. We show that by carefully perturbing the examples of the concept that is being investigated, we can radically change the output of the interpretability method. The attacks that we propose can either induce positive interpretations (polka dots are an important concept for a model when classifying zebras) or negative interpretations (stripes are not an important factor in identifying images of a zebra). Our work highlights the fact that in safety-critical applications, there is need for security around not only the machine learning pipeline but also the model interpretation process.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Brown_Making_Corgis_Important_for_Honeycomb_Classification_Adversarial_Attacks_on_Concept-Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Brown_Making_Corgis_Important_for_Honeycomb_Classification_Adversarial_Attacks_on_Concept-Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Quantifying Extrinsic Curvature in Neural Manifolds",
        "author": "Francisco Acosta, Sophia Sanborn, Khanh Dao Duc, Manu Madhav, Nina Miolane",
        "abstract": "The neural manifold hypothesis postulates that the activity of a neural population forms a low-dimensional manifold whose structure reflects that of the encoded task variables. In this work, we combine topological deep generative models and extrinsic Riemannian geometry to introduce a novel approach for studying the structure of neural manifolds. This approach (i) computes an explicit parameterization of the manifolds and (ii) estimates their local extrinsic curvature--hence quantifying their shape within the neural state space. Importantly, we prove that our methodology is invariant with respect to transformations that do not bear meaningful neuroscience information, such as permutation of the order in which neurons are recorded. We show empirically that we correctly estimate the geometry of synthetic manifolds generated from smooth deformations of circles, spheres, and tori, using realistic noise levels. We additionally validate our methodology on simulated and real neural data, and show that we recover geometric structure known to exist in hippocampal place cells. We expect this approach to open new avenues of inquiry into geometric neural correlates of perception and behavior, while providing a new means to compare representations in biological and artificial neural systems.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Acosta_Quantifying_Extrinsic_Curvature_in_Neural_Manifolds_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/papers/Acosta_Quantifying_Extrinsic_Curvature_in_Neural_Manifolds_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Benchmarking Robustness to Text-Guided Corruptions",
        "author": "Mohammadreza Mofayezi, Yasamin Medghalchi",
        "abstract": "This study investigates the robustness of image classifiers to text-guided corruptions. We utilize diffusion models to edit images to different domains. Unlike other works that use synthetic or hand-picked data for benchmarking, we use diffusion models as they are generative models capable of learning to edit images while preserving their semantic content. Thus, the corruptions will be more realistic and the comparison will be more informative. Also, there is no need for manual labeling and we can create large-scale benchmarks with less effort. We define a prompt hierarchy based on the original ImageNet hierarchy to apply edits in different domains. As well as introducing a new benchmark we try to investigate the robustness of different vision models. The results of this study demonstrate that the performance of image classifiers decreases significantly in different language-based corruptions and edit domains. We also observe that convolutional models are more robust than transformer architectures. Additionally, we see that common data augmentation techniques can improve the performance on both the original data and the edited images. The findings of this research can help improve the design of image classifiers and contribute to the development of more robust machine learning systems. The code for generating the benchmark will be made available online upon publication.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Mofayezi_Benchmarking_Robustness_to_Text-Guided_Corruptions_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Mofayezi_Benchmarking_Robustness_to_Text-Guided_Corruptions_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Discovering Class-Specific GAN Controls for Semantic Image Synthesis",
        "author": "Edgar Sch\u00f6nfeld, Julio Borges, Vadim Sushko, Bernt Schiele, Anna Khoreva",
        "abstract": "Prior work has extensively studied the latent space structure of GANs for unconditional image synthesis, enabling global editing of generated images by the unsupervised discovery of interpretable latent directions. However, the discovery of latent directions for conditional GANs for semantic image synthesis (SIS) has remained unexplored. In this work, we specifically focus on addressing this gap. We propose a novel optimization method for finding spatially disentangled class-specific directions in the latent space of pretrained SIS models. We show that the latent directions found by our method can effectively control the local appearance of semantic classes, e.g., changing their internal structure, texture or color independently from each other. Visual inspection and quantitative evaluation of the discovered GAN controls on various datasets demonstrate that our method discovers a diverse set of unique and semantically meaningful latent directions for class-specific edits.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Schonfeld_Discovering_Class-Specific_GAN_Controls_for_Semantic_Image_Synthesis_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Schonfeld_Discovering_Class-Specific_GAN_Controls_for_Semantic_Image_Synthesis_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Look ATME: The Discriminator Mean Entropy Needs Attention",
        "author": "Edgardo Solano-Carrillo, Angel Bueno Rodriguez, Borja Carrillo-Perez, Yannik Steiniger, Jannis Stoppe",
        "abstract": "Generative adversarial networks (GANs) are successfully used for image synthesis but are known to face instability during training. In contrast, probabilistic diffusion models (DMs) are stable and generate high-quality images, at the cost of an expensive sampling procedure. In this paper, we introduce a simple method to allow GANs to stably converge to their theoretical optimum, while bringing in the denoising machinery from DMs. These models are combined into a simpler model (ATME) that only requires a forward pass during inference, making predictions cheaper and more accurate than DMs and popular GANs. ATME breaks an information asymmetry existing in most GAN models in which the discriminator has spatial knowledge of where the generator is failing. To restore the information symmetry, the generator is endowed with knowledge of the entropic state of the discriminator, which is leveraged to allow the adversarial game to converge towards equilibrium. We demonstrate the power of our method in several image-to-image translation tasks, showing superior performance than state-of-the-art methods at a lesser cost. Code is available at https://github.com/DLR-MI/atme.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Solano-Carrillo_Look_ATME_The_Discriminator_Mean_Entropy_Needs_Attention_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Solano-Carrillo_Look_ATME_The_Discriminator_Mean_Entropy_Needs_Attention_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeSRF: Deformable Stylized Radiance Field",
        "author": "Shiyao Xu, Lingzhi Li, Li Shen, Zhouhui Lian",
        "abstract": "When stylizing 3D scenes, current methods need to render the full-resolution images from different views and use the style loss, which is proposed for 2D style transfer and needs to be calculated on the whole image, to optimize the stylized radiance fields. It is quite inefficient when we need to stylize a large-scale scene. This paper proposes a more efficient method, DeSRF, to stylize the radiance fields, which also transfers style information to the geometry according to the input style. To achieve this goal, on the one hand, we first introduce a deformable module, which can learn the geometric style contained in the input style image and transfer it to radiance fields. On the other hand, although the style loss needs to be calculated for the entire image, actually we do not need to process all the rays when updating the stylized radiance fields. Motivated by this observation, we propose a new training strategy called Dilated Sampling (DS) for efficient stylization propagation. Experimental results show that our method works more efficiently and produces more visually-reasonable stylized 3D scenes with geometry style information compared to other existing approaches.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Xu_DeSRF_Deformable_Stylized_Radiance_Field_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Xu_DeSRF_Deformable_Stylized_Radiance_Field_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Internal Diverse Image Completion",
        "author": "Noa Alkobi, Tamar Rott Shaham, Tomer Michaeli",
        "abstract": "Image completion is widely used in photo restoration and editing applications, e.g. for object removal. Recently, there has been a surge of research on generating diverse completions for missing regions. However, existing methods require large training sets from a specific domain of interest, and often fail on general-content images. In this paper, we propose a diverse completion method that does not require a training set and can thus treat arbitrary images from any domain. Our internal diverse completion (IDC) approach draws inspiration from recent single-image generative models that are trained on multiple scales of a single image, adapting them to the extreme setting in which only a small portion of the image is available for training. We illustrate the strength of IDC on several datasets, using both user studies and quantitative comparisons.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Alkobi_Internal_Diverse_Image_Completion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Alkobi_Internal_Diverse_Image_Completion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Face Animation With an Attribute-Guided Diffusion Model",
        "author": "Bohan Zeng, Xuhui Liu, Sicheng Gao, Boyu Liu, Hong Li, Jianzhuang Liu, Baochang Zhang",
        "abstract": "Face animation has achieved much progress in computer vision. However, prevailing GAN-based methods suffer from unnatural distortions and artifacts due to sophisticated motion deformation. In this paper, we propose a Face Animation framework with an attribute-guided Diffusion Model (FADM), which is the first work to exploit the superior modeling capacity of diffusion models for photo-realistic talking-head generation. To mitigate the uncontrollable synthesis effect of the diffusion model, we design an Attribute-Guided Conditioning Network (AGCN) to adaptively combine the coarse animation features and 3D face reconstruction results, which can incorporate appearance and motion conditions into the diffusion process. These specific designs help FADM rectify unnatural artifacts and distortions, and also enrich high-fidelity facial details through iterative diffusion refinements with accurate animation attributes. FADM can flexibly and effectively improve existing animation videos. Extensive experiments on widely used talking-head benchmarks validate the effectiveness of FADM over prior arts.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Zeng_Face_Animation_With_an_Attribute-Guided_Diffusion_Model_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Zeng_Face_Animation_With_an_Attribute-Guided_Diffusion_Model_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring Compositional Visual Generation With Latent Classifier Guidance",
        "author": "Changhao Shi, Haomiao Ni, Kai Li, Shaobo Han, Mingfu Liang, Martin Renqiang Min",
        "abstract": "Diffusion probabilistic models have achieved enormous success in the field of image generation and manipulation. In this paper, we explore a novel paradigm of using the diffusion model and classifier guidance in the latent semantic space for compositional visual tasks. Specifically, we train latent diffusion models and auxiliary latent classifiers to facilitate non-linear navigation of latent representation generation for any pre-trained generative model with a semantic latent space. We demonstrate that such conditional generation achieved by latent classifier guidance provably maximizes a lower bound of the conditional log probability during training. To maintain the original semantics during manipulation, we introduce a new guidance term, which we show is crucial for achieving compositionality. With additional assumptions, we show that the non-linear manipulation reduces to a simple latent arithmetic approach. We show that this paradigm based on latent classifier guidance is agnostic to pre-trained generative models, and present competitive results for both image generation and sequential manipulation of real and synthetic images. Our findings suggest that latent classifier guidance is a promising approach that merits further exploration, even in the presence of other strong competing methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Shi_Exploring_Compositional_Visual_Generation_With_Latent_Classifier_Guidance_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Shi_Exploring_Compositional_Visual_Generation_With_Latent_Classifier_Guidance_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Controllable GAN Synthesis Using Non-Rigid Structure-From-Motion",
        "author": "Ren\u00e9 Haas, Stella Gra\u00dfhof, Sami S. Brandt",
        "abstract": "In this paper, we present an approach for combining non-rigid structure-from-motion (NRSfM) with deep generative models, and propose an efficient framework for discovering trajectories in the latent space of 2D GANs corresponding to changes in 3D geometry. Our approach uses recent advances in NRSfM and enables editing of the camera and non-rigid shape information associated with the latent codes without needing to retrain the generator. This formulation provides an implicit dense 3D reconstruction as it enables the image synthesis of novel shapes from arbitrary view angles and non-rigid structure. The method is built upon a sparse backbone, where a neural regressor is first trained to regress parameters describing the cameras and sparse non-rigid structure directly from the latent codes. The latent trajectories associated with changes in the camera and structure parameters are then identified by estimating the local inverse of the regressor in the neighborhood of a given latent code. The experiments show that our approach provides a versatile, systematic way to model, analyze, and edit the geometry and non-rigid structures of faces.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Haas_Controllable_GAN_Synthesis_Using_Non-Rigid_Structure-From-Motion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Haas_Controllable_GAN_Synthesis_Using_Non-Rigid_Structure-From-Motion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "One-Shot Unsupervised Domain Adaptation With Personalized Diffusion Models",
        "author": "Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, St\u00e9phane Lathuili\u00e8re",
        "abstract": "Adapting a segmentation model from a labeled source domain to a target domain, where a single unlabeled datum is available, is one of the most challenging problems in domain adaptation and is otherwise known as one-shot unsupervised domain adaptation (OSUDA). Most of the prior works have addressed the problem by relying on style transfer techniques, where the source images are stylized to have the appearance of the target domain. Departing from the common notion of transferring only the target \"texture\" information, we leverage text-to-image diffusion models (e.g.,Stable Diffusion) to generate a synthetic target dataset with photo-realistic images that not only faithfully depict the style of the target domain, but are also characterized by novel scenes in diverse contexts. The text interface in our method Data AugmenTation with diffUsion Models (DATUM) endows us with the possibility of guiding the generation of images towards desired semantic concepts while respecting the original spatial context of a single training image, which is not possible in existing OSUDA methods. Extensive experiments on standard benchmarks show that our DATUM surpasses the state-of-the-art OSUDA methods by up to +7.1%. The implementation is available at : https://github.com/yasserben/DATUM",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Benigmim_One-Shot_Unsupervised_Domain_Adaptation_With_Personalized_Diffusion_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Benigmim_One-Shot_Unsupervised_Domain_Adaptation_With_Personalized_Diffusion_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Vision + Language Applications: A Survey",
        "author": "Yutong Zhou, Nobutaka Shimada",
        "abstract": "Text-to-image generation has attracted significant interest from researchers and practitioners in recent years due to its widespread and diverse applications across various industries. Despite the progress made in the domain of vision and language research, the existing literature remains relatively limited, particularly with regard to advancements and applications in this field. This paper explores a relevant research track within multimodal applications, including text, vision, audio, and others. In addition to the studies discussed in this paper, we are also committed to continually updating the latest relevant papers, datasets, application projects and corresponding information at https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Zhou_Vision__Language_Applications_A_Survey_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Zhou_Vision__Language_Applications_A_Survey_CVPRW_2023_paper.pdf"
    },
    {
        "title": "GAN-Based Vision Transformer for High-Quality Thermal Image Enhancement",
        "author": "Mohamed Amine Marnissi, Abir Fathallah",
        "abstract": "Generative Adversarial Networks (GANs) have shown an outstanding ability to generate high-quality images with visual realism and similarity to real images. This paper presents a new architecture for thermal image enhancement. Precisely, the strengths of architecture-based vision transformers and generative adversarial networks are exploited. The thermal loss feature introduced in our approach is specifically used to produce high-quality images. Thermal image enhancement also relies on fine-tuning based on visible images, resulting in an overall improvement in image quality. A visual quality metric was used to evaluate the performance of the proposed architecture. Significant improvements were found over the original thermal images and other enhancement methods established on a subset of the KAIST dataset. The performance of the proposed enhancement architecture is also verified on the detection results by obtaining better performance with a considerable margin regarding different versions of the YOLO detector.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Marnissi_GAN-Based_Vision_Transformer_for_High-Quality_Thermal_Image_Enhancement_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Marnissi_GAN-Based_Vision_Transformer_for_High-Quality_Thermal_Image_Enhancement_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Diversity Is Definitely Needed: Improving Model-Agnostic Zero-Shot Classification via Stable Diffusion",
        "author": "Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, Clinton Fookes",
        "abstract": "In this work, we investigate the problem of Model-Agnostic Zero-Shot Classification (MA-ZSC), which refers to training non-specific classification architectures (downstream models) to classify real images without using any real images during training. Recent research has demonstrated that generating synthetic training images using diffusion models provides a potential solution to address MA-ZSC. However, the performance of this approach currently falls short of that achieved by large-scale vision-language models. One possible explanation is a potential significant domain gap between synthetic and real images. Our work offers a fresh perspective on the problem by providing initial insights that MA-ZSC performance can be improved by improving the diversity of images in the generated dataset. We propose a set of modifications to the text-to-image generation process using a pre-trained diffusion model to enhance diversity, which we refer to as our bag of tricks. Our approach shows notable improvements in various classification architectures, with results comparable to state-of-the-art models such as CLIP. To validate our approach, we conduct experiments on CIFAR10, CIFAR100, and EuroSAT, which is particularly difficult for zero-shot classification due to its satellite image domain. We evaluate our approach with five classification architectures, including ResNet and ViT. Our findings provide initial insights into the problem of MA-ZSC using diffusion models. All code is available at https://github.com/Jordan-HS/Diversity_is_Definitely_Needed",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Shipard_Diversity_Is_Definitely_Needed_Improving_Model-Agnostic_Zero-Shot_Classification_via_Stable_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Shipard_Diversity_Is_Definitely_Needed_Improving_Model-Agnostic_Zero-Shot_Classification_via_Stable_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Leveraging GANs for Data Scarcity of COVID-19: Beyond the Hype",
        "author": "Hazrat Ali, Christer Gr\u00f6nlund, Zubair Shah",
        "abstract": "Artificial Intelligence (AI)-based models can help in diagnosing COVID-19 from lung CT scans and X-ray images; however, these models require large amounts of data for training and validation. Many researchers studied Generative Adversarial Networks (GANs) for producing synthetic lung CT scans and X-Ray images to improve the performance of AI-based models. It is not well explored how good GAN-based methods performed to generate reliable synthetic data. This work analyzes 43 published studies that reported GANs for synthetic data generation. Many of these studies suffered data bias, lack of reproducibility, and lack of feedback from the radiologists or other domain experts. A common issue in these studies is the unavailability of the source code, hindering reproducibility. The included studies reported rescaling of the input images to train the existing GANs architecture without providing clinical insights on how the rescaling was motivated. Finally, even though GAN-based methods have the potential for data augmentation and improving the training of AI-based models, these methods fall short in terms of their use in clinical practice. This paper highlights research hotspots in countering the data scarcity problem, identifies various issues as well as potentials, and provides recommendations to guide future research. These recommendations might be useful to improve acceptability for the GAN-based approaches for data augmentation as GANs for data augmentation are increasingly becoming popular in the AI and medical imaging research community.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Ali_Leveraging_GANs_for_Data_Scarcity_of_COVID-19_Beyond_the_Hype_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Ali_Leveraging_GANs_for_Data_Scarcity_of_COVID-19_Beyond_the_Hype_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Explore the Power of Synthetic Data on Few-Shot Object Detection",
        "author": "Shaobo Lin, Kun Wang, Xingyu Zeng, Rui Zhao",
        "abstract": "Few-shot object detection (FSOD) aims to expand an object detector for novel categories given only a few instances for training. The few training samples restrict the performance of FSOD model. Recent text-to-image generation models have shown promising results in generating high-quality images. How applicable these synthetic images are for FSOD tasks remains under-explored. This work extensively studies how synthetic images generated from state-of-the-art text-to-image generators benefit FSOD tasks. We focus on two perspectives: (1) How to use synthetic data for FSOD? (2) How to find representative samples from the large-scale synthetic dataset? We design a copy-paste-based pipeline for using synthetic data. Specifically, saliency object detection is applied to the original generated image, and the minimum enclosing box is used for cropping the main object based on the saliency map. After that, the cropped object is randomly pasted on the image, which comes from the base dataset. We also study the influence of the input text of text-to-image generator and the number of synthetic images used. To construct a representative synthetic training dataset, we maximize the diversity of the selected images via a sample-based and cluster-based method. However, the severe problem of high false positives (FP) ratio of novel categories in FSOD can not be solved by using synthetic data. We propose integrating CLIP, a zero-shot recognition model, into the FSOD pipeline, which can filter 90% of FP by defining a threshold for the similarity score between the detected object and the text of the predicted category. Extensive experiments on PASCAL VOC and MS COCO validate the effectiveness of our method, in which performance gain is up to 21.9% compared to the few-shot baseline.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Lin_Explore_the_Power_of_Synthetic_Data_on_Few-Shot_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Lin_Explore_the_Power_of_Synthetic_Data_on_Few-Shot_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Diffusion-Enhanced PatchMatch: A Framework for Arbitrary Style Transfer With Diffusion Models",
        "author": "Mark Hamazaspyan, Shant Navasardyan",
        "abstract": "Diffusion models have gained immense popularity in recent years due to their impressive ability to generate high-quality images. The opportunities that diffusion models provide are numerous, from text-to-image synthesis to image restoration and enhancement, as well as image compression and inpainting. However, expressing image style in words can be a challenging task, making it difficult for diffusion models to generate images with specific style without additional optimization techniques. In this paper, we present a novel method, Diffusion-Enhanced PatchMatch (DEPM), that leverages Stable Diffusion for style transfer without any finetuning or pretraining. DEPM captures high-level style features while preserving the fine-grained texture details of the original image. By enabling the transfer of arbitrary styles during inference, our approach makes the process more flexible and efficient. Moreover, its optimization-free nature makes it accessible to a wide range of users.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Hamazaspyan_Diffusion-Enhanced_PatchMatch_A_Framework_for_Arbitrary_Style_Transfer_With_Diffusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Hamazaspyan_Diffusion-Enhanced_PatchMatch_A_Framework_for_Arbitrary_Style_Transfer_With_Diffusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Unsupervised Bidirectional Style Transfer Network Using Local Feature Transform Module",
        "author": "Kangmin Bae, Hyung-Il Kim, Yongjin Kwon, Jinyoung Moon",
        "abstract": "In this paper, we propose a bidirectional style transfer method by exchanging the style of inputs while preserving the structural information. The proposed bidirectional style transfer network consists of three modules: 1) content and style extraction module that extracts the structure and style-related features, 2) local feature transform module that aligns locally extracted feature to its original coordinate, and 3) reconstruction module that generates a newly stylized image. Given two input images, we extract content and style information from both images in a global and local manner, respectively. Note that the content extraction module removes style-related information by compressing the dimension of the feature tensor to a single channel. The style extraction module removes content information by gradually reducing the spatial size of a feature tensor. The local feature transform module exchanges the style information and spatially transforms the local features to its original location. By substituting the style information with one another in both ways (i.e., global and local) bidirectionally, the reconstruction module generates a newly stylized image without diminishing the core structure. Furthermore, we enable the proposed network to control the degree of style to be applied when exchanging the style of inputs bidirectionally. Through the experiments, we compare the bidirectionally style transferred results with existing methods quantitatively and qualitatively. We show generation results by controlling the degree of applied style and adopting various textures to an identical structure.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Bae_Unsupervised_Bidirectional_Style_Transfer_Network_Using_Local_Feature_Transform_Module_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Bae_Unsupervised_Bidirectional_Style_Transfer_Network_Using_Local_Feature_Transform_Module_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Generating Adversarial Attacks in the Latent Space",
        "author": "Nitish Shukla, Sudipta Banerjee",
        "abstract": "Adversarial attacks in the input (pixel) space typically incorporate noise margins such as L_1 or L_-norm to produce imperceptibly perturbed data that can confound deep learning networks. Such noise margins confine the magnitude of permissible noise. In this work, we propose injecting adversarial perturbations in the latent (feature) space using a generative adversarial network, removing the need for margin-based priors. Experiments on MNIST, CIFAR10, Fashion-MNIST, CIFAR100 and Stanford Dogs datasets support the effectiveness of the proposed method in generating adversarial attacks in the latent space while ensuring a high degree of visual realism with respect to pixel-based adversarial attack methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Shukla_Generating_Adversarial_Attacks_in_the_Latent_Space_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Shukla_Generating_Adversarial_Attacks_in_the_Latent_Space_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Identity-Driven Three-Player Generative Adversarial Network for Synthetic-Based Face Recognition",
        "author": "Jan Niklas Kolf, Tim Rieber, Jurek Elliesen, Fadi Boutros, Arjan Kuijper, Naser Damer",
        "abstract": "Many of the commonly used datasets for face recognition development are collected from the internet without proper user consent. Due to the increasing focus on privacy in the social and legal frameworks, the use and distribution of these datasets are being restricted and strongly questioned. These databases, which have a realistically high variability of data per identity, have enabled the success of face recognition models. To build on this success and to align with privacy concerns, synthetic databases, consisting purely of synthetic persons, are increasingly being created and used in the development of face recognition solutions. In this work, we present a three-player generative adversarial network (GAN) framework, namely IDnet, that enables the integration of identity information into the generation process. The third player in our IDnet aims at forcing the generator to learn to generate identity-separable face images. We empirically proved that our IDnet synthetic images are of higher identity discrimination in comparison to the conventional two-player GAN, while maintaining a realistic intra-identity variation. We further studied the identity link between the authentic identities used to train the generator and the generated synthetic identities, showing very low similarities between these identities. We demonstrated the applicability of our IDnet data in training face recognition models by evaluating these models on a wide set of face recognition benchmarks. In comparison to the state-of-the-art works in synthetic-based face recognition, our solution achieved comparable results to a recent rendering-based approach and outperformed all existing GAN-based approaches. The training code and the synthetic face image dataset are publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Kolf_Identity-Driven_Three-Player_Generative_Adversarial_Network_for_Synthetic-Based_Face_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Kolf_Identity-Driven_Three-Player_Generative_Adversarial_Network_for_Synthetic-Based_Face_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Normalizing Flows With the Approximate Mass for Out-of-Distribution Detection",
        "author": "Samy Chali, Inna Kucher, Marc Duranton, Jacques-Olivier Klein",
        "abstract": "Normalizing flows are generative models that show poor performance on out-of-distribution (OOD) detection tasks with a likelihood-based test. In this study we focus on the \"approximate mass\" metric. We show that while it improves OOD detection performance, it has limitations under a maximum likelihood training. To solve this limitation we modify the training objective by incorporating the approximate mass. It smooths the learnt distribution in the vicinity of training in-distribution data. We measure an average of 97.6% AUROC in our experiments on different benchmarks, showing an improvement of 16% with respect to the best baseline we tested against.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Chali_Improving_Normalizing_Flows_With_the_Approximate_Mass_for_Out-of-Distribution_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Chali_Improving_Normalizing_Flows_With_the_Approximate_Mass_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Scene Graph Driven Text-Prompt Generation for Image Inpainting",
        "author": "Tripti Shukla, Paridhi Maheshwari, Rajhans Singh, Ankita Shukla, Kuldeep Kulkarni, Pavan Turaga",
        "abstract": "Scene editing methods are undergoing a revolution, driven by text-to-image synthesis methods. Applications in media content generation have benefited from a careful set of engineered text prompts, that have been arrived at by the artists by trial and error. There is a growing need to better model prompt generation, for it to be useful for a broad range of consumer-grade applications. We propose a novel method for text prompt generation for the explicit purpose of consumer-grade image inpainting, i.e. insertion of new objects into missing regions in an image. Our approach leverages existing inter-object relationships to generate plausible textual descriptions for the missing object, that can then be used with any text-to-image generator. Given an image and a location where a new object is to be inserted, our approach first converts the given image to an intermediate scene graph. Then, we use graph convolutional networks to 'expand' the scene graph by predicting the identity and relationships of the new object to be inserted, with respect to the existing objects in the scene. The output of the expanded scene graph is cast into a textual description, which is then processed by a text-to-image generator, conditioned on the given image, to produce the final inpainted image. We conduct extensive experiments on the Visual Genome dataset, and show through qualitative and quantitative metrics that our method is superior to other methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Shukla_Scene_Graph_Driven_Text-Prompt_Generation_for_Image_Inpainting_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Shukla_Scene_Graph_Driven_Text-Prompt_Generation_for_Image_Inpainting_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Universal Guidance for Diffusion Models",
        "author": "Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein",
        "abstract": "Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, and classifier signals.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Bansal_Universal_Guidance_for_Diffusion_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Bansal_Universal_Guidance_for_Diffusion_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Face Transformer: Towards High Fidelity and Accurate Face Swapping",
        "author": "Kaiwen Cui, Rongliang Wu, Fangneng Zhan, Shijian Lu",
        "abstract": "Face swapping aims to generate swapped images that fuse the identity of source faces and the attributes of target faces. Most existing works address this challenging task through 3D modelling or generation using generative adversarial networks (GANs), but 3D modelling suffers from limited reconstruction accuracy and GANs often struggle in preserving subtle yet important identity details of source faces (e.g., skin colors, face features) and structural attributes of target faces (e.g., face shapes, facial expressions). This paper presents Face Transformer, a novel face swapping network that can accurately preserve source identities and target attributes simultaneously in the swapped face images. We introduce a transformer network for the face swapping task, which learns high-quality semantic-aware correspondence between source and target faces and maps identity features of source faces to the corresponding region in target faces. The high-quality semantic-aware correspondence enables smooth and accurate transfer of source identity information with minimal modification of target shapes and expressions. In addition, our Face Transformer incorporates a multi-scale transformation mechanism for preserving the rich fine facial details. Extensive experiments show that our Face Transformer achieves superior face swapping performance qualitatively and quantitatively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Cui_Face_Transformer_Towards_High_Fidelity_and_Accurate_Face_Swapping_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Cui_Face_Transformer_Towards_High_Fidelity_and_Accurate_Face_Swapping_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Unsupervised Style-Based Explicit 3D Face Reconstruction From Single Image",
        "author": "Heng Yu, Zolt\u00e1n \u00c1. Milacski, L\u00e1szl\u00f3 A. Jeni",
        "abstract": "Inferring 3D object structures from a single image is an ill-posed task due to depth ambiguity and occlusion. Typical resolutions in the literature include leveraging 2D or 3D ground truth for supervised learning, as well as imposing hand-crafted symmetry priors or using an implicit representation to hallucinate novel viewpoints for unsupervised methods. In this work, we propose a general adversarial learning framework for solving Unsupervised 2D to Explicit 3D Style Transfer (UE3DST). Specifically, we merge two architectures: the unsupervised explicit 3D reconstruction network of Wu et al. and the Generative Adversarial Network (GAN) named StarGAN-v2. We experiment across three facial datasets (Basel Face Model, 3DFAW, and CelebA-HQ) and show that our solution is able to outperform well-established solutions such as DepthNet in 3D reconstruction and Pix2NeRF in conditional style transfer, while we also justify the individual contributions of our model components via ablation. In contrast to the aforementioned baselines, our scheme produces features for explicit 3D rendering, which can be manipulated and utilized in downstream tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Yu_Unsupervised_Style-Based_Explicit_3D_Face_Reconstruction_From_Single_Image_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Yu_Unsupervised_Style-Based_Explicit_3D_Face_Reconstruction_From_Single_Image_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Geometric and Photometric Exploration of GAN and Diffusion Synthesized Faces",
        "author": "Maty\u00e1\u0161 Boh\u00e1\u010dek, Hany Farid",
        "abstract": "Classic computer-generated imagery is produced by modeling 3D scene geometry, the surrounding illumination, and a virtual camera. As a result, rendered images accurately capture the geometry and physics of natural scenes. In contrast, AI-generated imagery is produced by learning the statistical distribution of natural scenes from a large set of real images. Without an explicit 3D model of the world, we wondered how accurately synthesized content captures the 3D geometric and photometric properties of natural scenes. From a diverse set of real, GAN- and diffusion-synthesized faces, we estimate a 3D geometric model of the face, from which we estimate the surrounding 3D photometric environment. We also analyze 2D facial features -- eyes and mouth -- that have been traditionally difficult to accurately render. Using these models, we provide a quantitative analysis of the 3D and 2D realism of synthesized faces.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Bohacek_A_Geometric_and_Photometric_Exploration_of_GAN_and_Diffusion_Synthesized_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Bohacek_A_Geometric_and_Photometric_Exploration_of_GAN_and_Diffusion_Synthesized_CVPRW_2023_paper.pdf"
    },
    {
        "title": "RoSteALS: Robust Steganography Using Autoencoder Latent Space",
        "author": "Tu Bui, Shruti Agarwal, Ning Yu, John Collomosse",
        "abstract": "Data hiding such as steganography and invisible watermarking has important applications in copyright protection, privacy-preserved communication and content provenance. Existing works often fall short in either preserving image quality, or robustness against perturbations or are too complex to train. We propose RoSteALS, a practical steganography technique leveraging frozen pretrained autoencoders to free the payload embedding from learning the distribution of cover images. RoSteALS has a light-weight secret encoder of just 300k parameters, is easy to train, has perfect secret recovery performance and comparable image quality on three benchmarks. Additionally, RoSteALS can be adapted for novel cover-less steganography applications in which the cover image can be sampled from noise or conditioned on text prompts via a denoising diffusion process. Our model and code are available at https://github.com/TuBui/RoSteALS.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Bui_RoSteALS_Robust_Steganography_Using_Autoencoder_Latent_Space_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Bui_RoSteALS_Robust_Steganography_Using_Autoencoder_Latent_Space_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Harnessing the Power of Text-Image Contrastive Models for Automatic Detection of Online Misinformation",
        "author": "Hao Chen, Peng Zheng, Xin Wang, Shu Hu, Bin Zhu, Jinrong Hu, Xi Wu, Siwei Lyu",
        "abstract": "As growing usage of social media websites in the recent decades, the amount of news articles spreading online rapidly, resulting in an unprecedented scale of potentially fraudulent information. Although a plenty of studies have applied the supervised machine learning approaches to detect such content, the lack of gold standard training data has hindered the development. Analysing the single data format, either fake text description or fake image, is the mainstream direction for the current research. However, the misinformation in real-world scenario is commonly formed as a text-image pair where the news article/news title is described as text content, and usually followed by the related image. Given the strong ability of learning features without labelled data, contrastive learning, as a self-learning approach, has emerged and achieved success on the computer vision. In this paper, our goal is to explore the constrastive learning in the domain of misinformation identification. We developed a self-learning model and carried out the comprehensive experiments on a public data set named COSMOS. Comparing to the baseline classifier, our model shows the superior performance of non-matched image-text pair detection (approximately 10%) when the training data is insufficient. In addition, we observed the stability for contrsative learning and suggested the use of it offers large reductions in the number of training data, whilst maintaining comparable classification results.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Chen_Harnessing_the_Power_of_Text-Image_Contrastive_Models_for_Automatic_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Chen_Harnessing_the_Power_of_Text-Image_Contrastive_Models_for_Automatic_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Intriguing Properties of Synthetic Images: From Generative Adversarial Networks to Diffusion Models",
        "author": "Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, Luisa Verdoliva",
        "abstract": "Detecting fake images is becoming a major goal of computer vision. This need is becoming more and more pressing with the continuous improvement of synthesis methods based on Generative Adversarial Networks (GAN), and even more with the appearance of powerful methods based on Diffusion Models (DM). Towards this end, it is important to gain insight into which image features better discriminate fake images from real ones. In this paper we report on our systematic study of a large number of image generators of different families, aimed at discovering the most forensically relevant characteristics of real and generated images. Our experiments provide a number of interesting observations and shed light on some intriguing properties of synthetic images: (1) not only the GAN models but also the DM and VQ-GAN (Vector Quantized Generative Adversarial Networks) models give rise to visible artifacts in the Fourier domain and exhibit anomalous regular patterns in the autocorrelation; (2) when the dataset used to train the model lacks sufficient variety, its biases can be transferred to the generated images; (3) synthetic and real images exhibit significant differences in the mid-high frequency signal content, observable in their radial and angular spectral power distributions.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Corvi_Intriguing_Properties_of_Synthetic_Images_From_Generative_Adversarial_Networks_to_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Corvi_Intriguing_Properties_of_Synthetic_Images_From_Generative_Adversarial_Networks_to_CVPRW_2023_paper.pdf"
    },
    {
        "title": "EKILA: Synthetic Media Provenance and Attribution for Generative Art",
        "author": "Kar Balan, Shruti Agarwal, Simon Jenni, Andy Parsons, Andrew Gilbert, John Collomosse",
        "abstract": "We present EKILA; a decentralized framework that enables creatives to receive recognition and reward for their contributions to generative AI (GenAI). EKILA proposes a robust visual attribution technique and combines this with an emerging content provenance standard (C2PA) to address the problem of synthetic image provenance -- determining the generative model and training data responsible for an AI-generated image. Furthermore, EKILA extends the non-fungible token (NFT) ecosystem to introduce a tokenized representation for rights, enabling a triangular relationship between the asset's Ownership, Rights, and Attribution (ORA). Leveraging the ORA relationship enables creators to express agency over training consent and, through our attribution model, to receive apportioned credit, including royalty payments for the use of their assets in GenAI.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Balan_EKILA_Synthetic_Media_Provenance_and_Attribution_for_Generative_Art_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Balan_EKILA_Synthetic_Media_Provenance_and_Attribution_for_Generative_Art_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Open Set Classification of GAN-Based Image Manipulations via a ViT-Based Hybrid Architecture",
        "author": "Jun Wang, Omran Alamayreh, Benedetta Tondi, Mauro Barni",
        "abstract": "Classification of AI-manipulated content is receiving great attention, for distinguishing different types of manipulations. Most of the methods developed so far fail in the open-set scenario, that is when the algorithm used for the manipulation is not represented by the training set. In this paper, we focus on the classification of synthetic face generation and manipulation in open-set scenarios, and propose a method for classification with a rejection option. The proposed method combines the use of Vision Transformers (ViT) with a hybrid approach for simultaneous classification and localization. Feature map correlation is exploited by the ViT module, while a localization branch is employed as an attention mechanism to force the model to learn per-class discriminative features associated with the forgery when the manipulation is performed locally in the image. Rejection is performed by considering several strategies and analyzing the model output layers. The effectiveness of the proposed method is assessed for the task of classification of facial attribute editing and GAN attribution.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Wang_Open_Set_Classification_of_GAN-Based_Image_Manipulations_via_a_ViT-Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Wang_Open_Set_Classification_of_GAN-Based_Image_Manipulations_via_a_ViT-Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MTN: Forensic Analysis of MP4 Video Files Using Graph Neural Networks",
        "author": "Ziyue Xiang, Amit Kumar Singh Yadav, Paolo Bestagini, Stefano Tubaro, Edward J. Delp",
        "abstract": "MP4 video files are stored using a tree data structure. These trees contain rich information that can be used for forensic analysis. In this paper, we propose MP4 Tree Network (MTN), an approach based on an end-to-end Graph Neural Networks (GNNs) that is used for forensic analysis of MP4 trees. MTN does not use any video pixel data. MTN is trained using Self-Supervised Learning (SSL), which generates semantic-preserving node embeddings for the nodes in an MP4 tree. We also propose a data augmentation technique for MP4 trees, which helps train MTN in data-scarce scenarios. MTN achieves good performance across 3 video forensics tasks on the EVA-7K dataset. We show that MTN can gain more comprehensive understanding about the MP4 trees and is more robust to potential attacks compared to existing methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Xiang_MTN_Forensic_Analysis_of_MP4_Video_Files_Using_Graph_Neural_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Xiang_MTN_Forensic_Analysis_of_MP4_Video_Files_Using_Graph_Neural_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exposing GAN-Generated Profile Photos From Compact Embeddings",
        "author": "Shivansh Mundra, Gonzalo J. Aniano Porcile, Smit Marvaniya, James R. Verbus, Hany Farid",
        "abstract": "Generative adversarial networks (GANs) have been used to create remarkably realistic images of people. More recently, diffusion-based techniques have taken image synthesis to the next level. From only a text prompt, these techniques can synthesize any image seemingly limited only by our imagination. Along with the many clever and creative use cases, synthetically-generated faces are being used to create more convincing fake social-media profiles. We describe two related techniques that learn low-dimensional (128-D) embeddings of GAN-generated faces. We show that these embeddings capture common facial structures found in these synthetically-generated faces that are uncommon in real profile photos. These low-dimensional models, trained on a relatively small data set, achieve higher classification performance than larger and more complex state-of-the-art classifiers.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Mundra_Exposing_GAN-Generated_Profile_Photos_From_Compact_Embeddings_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Mundra_Exposing_GAN-Generated_Profile_Photos_From_Compact_Embeddings_CVPRW_2023_paper.pdf"
    },
    {
        "title": "AutoSplice: A Text-Prompt Manipulated Image Dataset for Media Forensics",
        "author": "Shan Jia, Mingzhen Huang, Zhou Zhou, Yan Ju, Jialing Cai, Siwei Lyu",
        "abstract": "Recent advancements in language-image models have led to the development of highly realistic images that can be generated from textual descriptions. However, the increased visual quality of these generated images poses a potential threat to the field of media forensics. This paper aims to investigate the level of challenge that language-image generation models pose to media forensics. To achieve this, we propose a new approach that leverages the DALL-E2 language-image model to automatically generate and splice masked regions guided by a text prompt. To ensure the creation of realistic manipulations, we have designed an annotation platform with human checking to verify reasonable text prompts. This approach has resulted in the creation of a new image dataset called AutoSplice, containing 5,894 manipulated and authentic images. Specifically, we have generated a total of 3,621 images by locally or globally manipulating real-world image-caption pairs, which we believe will provide a valuable resource for developing generalized detection methods in this area. The dataset is evaluated under two media forensic tasks: forgery detection and localization. Our extensive experiments show that most media forensic models struggle to detect the AutoSplice dataset as an unseen manipulation. However, when fine-tuned models are used, they exhibit improved performance in both tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Jia_AutoSplice_A_Text-Prompt_Manipulated_Image_Dataset_for_Media_Forensics_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Jia_AutoSplice_A_Text-Prompt_Manipulated_Image_Dataset_for_Media_Forensics_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Audio-Visual Person-of-Interest DeepFake Detection",
        "author": "Davide Cozzolino, Alessandro Pianese, Matthias Nie\u00dfner, Luisa Verdoliva",
        "abstract": "Face manipulation technology is advancing very rapidly, and new methods are being proposed day by day. The aim of this work is to propose a deepfake detector that can cope with the wide variety of manipulation methods and scenarios encountered in the real world. Our key insight is that each person has specific characteristics that a synthetic generator likely cannot reproduce. Accordingly, we extract audio-visual features which characterize the identity of a person, and use them to create a person-of-interest (POI) deepfake detector. We leverage a contrastive learning paradigm to learn the moving-face and audio segment embeddings that are most discriminative for each identity. As a result, when the video and/or audio of a person is manipulated, its representation in the embedding space becomes inconsistent with the real identity, allowing reliable detection. Training is carried out exclusively on real talking-face video; thus, the detector does not depend on any specific manipulation method and yields the highest generalization ability. In addition, our method can detect both single-modality (audio-only, video-only) and multi-modality (audio-video) attacks, and is robust to low-quality or corrupted videos. Experiments on a wide variety of datasets confirm that our method ensures a SOTA performance, especially on low quality videos. Code is publicly available on-line at https://github.com/grip-unina/poi-forensics.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Cozzolino_Audio-Visual_Person-of-Interest_DeepFake_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Cozzolino_Audio-Visual_Person-of-Interest_DeepFake_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "AI-Synthesized Voice Detection Using Neural Vocoder Artifacts",
        "author": "Chengzhe Sun, Shan Jia, Shuwei Hou, Siwei Lyu",
        "abstract": "Advancements in AI-synthesized human voices have created a growing threat of impersonation and disinformation, making it crucial to develop methods to detect synthetic human voices. This study proposes a new approach to identifying synthetic human voices by detecting artifacts of vocoders in audio signals. Most DeepFake audio synthesis models use a neural vocoder, a neural network that generates waveforms from temporal-frequency representations like mel-spectrograms. By identifying neural vocoder processing in audio, we can determine if a sample is synthesized. To detect synthetic human voices, we introduce a multi-task learning framework for a binary-class RawNet2 model that shares the feature extractor with a vocoder identification module. By treating vocoder identification as a pretext task, we constrain the feature extractor to focus on vocoder artifacts and provide discriminative features for the final binary classifier. Our experiments show that the improved RawNet2 model based on vocoder identification achieves high classification performance on the binary task overall.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Sun_AI-Synthesized_Voice_Detection_Using_Neural_Vocoder_Artifacts_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Sun_AI-Synthesized_Voice_Detection_Using_Neural_Vocoder_Artifacts_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multimodaltrace: Deepfake Detection Using Audiovisual Representation Learning",
        "author": "Muhammad Anas Raza, Khalid Mahmood Malik",
        "abstract": "By employing generative deep learning techniques, Deepfakes are created with the intent to create mistrust in society, manipulate public opinion and political decisions, and for other malicious purposes such as blackmail, scamming, and even cyberstalking. As realistic deepfake may involve manipulation of either audio or video or both, thus it is important to explore the possibility of detecting deepfakes through the inadequacy of generative algorithms to synchronize audio and visual modalities. Prevailing performant methods, either detect audio or video cues for deepfakes detection while few ensemble the results after predictions on both modalities without inspecting relationship between audio and video cues. Deepfake detection using joint audiovisual representation learning is not explored much. Therefore, this paper proposes a unified multimodal framework, Multimodaltrace, which extracts learned channels from audio and visual modalities, mixes them independently in IntrAmodality Mixer Layer (IAML), processes them jointly in IntErModality Mixer Layers (IEML) from where it is fed to multilabel classification head. Empirical results show the effectiveness of the proposed framework giving state-of-the-art accuracy of 92.9% on the FakeAVCeleb dataset. The cross-dataset evaluation of the proposed framework on World Leaders and Presidential Deepfake Detection Datasets gives an accuracy of 83.61% and 70% respectively. The study also provides insights into how the model focuses on different parts of audio and visual features through integrated gradient analysis.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Raza_Multimodaltrace_Deepfake_Detection_Using_Audiovisual_Representation_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Raza_Multimodaltrace_Deepfake_Detection_Using_Audiovisual_Representation_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Defending Low-Bandwidth Talking Head Videoconferencing Systems From Real-Time Puppeteering Attacks",
        "author": "Danial Samadi Vahdati, Tai Duc Nguyen, Matthew C. Stamm",
        "abstract": "Talking head videos have gained significant attention in recent years due to advances in AI that allow for the synthesis of realistic videos from only a single image of the speaker. Recently, researchers have proposed low bandwidth talking head video systems for use in applications such as videoconferencing and video calls. However, these systems are vulnerable to puppeteering attacks, where an attacker can control a synthetic version of a different target speaker in real-time. This can be potentially used spread misinformation or committing fraud. Because the receiver always creates a synthetic video of the speaker, deepfake detectors cannot protect against these attacks. As a result, there are currently no defenses against puppeteering in these systems. In this paper, we propose a new defense against puppeteering attacks in low-bandwidth talking head video systems by utilizing the biometric information inherent in the facial expression and pose data transmitted to the receiver. Our proposed system requires no modifications to the video transmission system and operates with low computational cost. We present experimental evidence to demonstrate the effectiveness of our proposed defense and provide a new dataset for benchmarking defenses against puppeteering attacks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Vahdati_Defending_Low-Bandwidth_Talking_Head_Videoconferencing_Systems_From_Real-Time_Puppeteering_Attacks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Vahdati_Defending_Low-Bandwidth_Talking_Head_Videoconferencing_Systems_From_Real-Time_Puppeteering_Attacks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PIC-Score: Probabilistic Interpretable Comparison Score for Optimal Matching Confidence in Single- and Multi-Biometric Face Recognition",
        "author": "Pedro C. Neto, Ana F. Sequeira, Jaime S. Cardoso, Philipp Terh\u00f6rst",
        "abstract": "In the context of biometrics, matching confidence refers to the confidence that a given matching decision is correct. Since many biometric systems operate in critical decisionmaking processes, such as in forensics investigations, accurately and reliably stating the matching confidence becomes of high importance. Previous works on biometric confidence estimation can well differentiate between high and low confidence, but lack interpretability. Therefore, they do not provide accurate probabilistic estimates of the correctness of a decision. In this work, we propose a probabilistic interpretable comparison (PIC) score that accurately reflects the probability that the score originates from samples of the same identity. We prove that the proposed approach provides optimal matching confidence. Contrary to other approaches, it can also optimally combine multiple samples in a joint PIC score which further increases the recognition and confidence estimation performance. In the experiments, the proposed PIC approach is compared against all biometric confidence estimation methods available on four publicly available databases and five state-of-the-art face recognition systems. The results demonstrate that PIC has a significantly more accurate probabilistic interpretation than similar approaches and is highly effective for multibiometric recognition. The code is publicly-available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/Neto_PIC-Score_Probabilistic_Interpretable_Comparison_Score_for_Optimal_Matching_Confidence_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/Neto_PIC-Score_Probabilistic_Interpretable_Comparison_Score_for_Optimal_Matching_Confidence_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robust Partial Fingerprint Recognition",
        "author": "Yufei Zhang, Rui Zhao, Ziyi Zhao, Naveen Ramakrishnan, Manoj Aggarwal, Gerard Medioni, Qiang Ji",
        "abstract": "Low quality capture and obstruction on fingers often result in partially visible fingerprint images, which imposes challenge for fingerprint recognition. In this work, motivated from the practical use cases, we first systematically studied different types of partial occlusion. Specifically, two major types of partial occlusion, including six granular types, and the corresponding methods to simulate each type for model evaluation and improvement were introduced. Second, we proposed a novel Robust Partial Fingerprint (RPF) recognition framework to mitigate the performance degradation due to occlusion. RPF effectively encodes the knowledge about partial fingerprints through occlusion-enhanced data augmentation, and explicitly captures the missing regions for robust feature extraction through occlusion-aware modeling. Finally, we demonstrated the effectiveness of RPF through extensive experiments. Particularly, baseline fingerprint recognition models can degrade the recognition accuracy measured in FRR @ FAR=0.1% from 14.67% to 17.57% at 10% occlusion ratio on the challenging NIST dataset, while RPF instead improves the recognition performance to 9.99% under the same occlusion ratio. Meanwhile, we presented a set of empirical analysis through visual explanation, matching score analysis, and uncertainty modeling, providing insights into the recognition model's behavior and potential directions of enhancement.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/Zhang_Robust_Partial_Fingerprint_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/Zhang_Robust_Partial_Fingerprint_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SynthASpoof: Developing Face Presentation Attack Detection Based on Privacy-Friendly Synthetic Data",
        "author": "Meiling Fang, Marco Huber, Naser Damer",
        "abstract": "Recently, significant progress has been made in face presentation attack detection (PAD), which aims to secure face recognition systems against presentation attacks, owing to the availability of several face PAD datasets. However, all available datasets are based on privacy and legally-sensitive authentic biometric data with a limited number of subjects. To target these legal and technical challenges, this work presents the first synthetic-based face PAD dataset, named SynthASpoof, as a large-scale PAD development dataset. The bona fide samples in SynthASpoof are synthetically generated and the attack samples are collected by presenting such synthetic data to capture systems in a real attack scenario. The experimental results demonstrate the feasibility of using SynthASpoof for the development of face PAD. Moreover, we boost the performance of such a solution by incorporating the domain generalization tool MixStyle into the PAD solutions. Additionally, we showed the viability of using synthetic data as a supplement to enrich the diversity of limited authentic training data and consistently enhance PAD performances. The SynthASpoof dataset, containing 25,000 bona fide and 78,800 attack samples, the implementation, and the pre-trained weights are made publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/Fang_SynthASpoof_Developing_Face_Presentation_Attack_Detection_Based_on_Privacy-Friendly_Synthetic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/Fang_SynthASpoof_Developing_Face_Presentation_Attack_Detection_Based_on_Privacy-Friendly_Synthetic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "The Universal Face Encoder: Learning Disentangled Representations Across Different Attributes",
        "author": "Sandipan Banerjee, Ajjen Joshi, Jay Turcot",
        "abstract": "Models that can learn orthogonal representations for different facial attributes (eg. pose, lighting, identity, expressions) have proven to be beneficial for both discriminative and generative tasks. In this work, we propose the universal facial encoder (UFE) that can simultaneously encode different facial attributes as disentangled features from a single face image. We propose a variety of qualitative and quantitative metrics to evaluate feature orthogonality of the UFE and demonstrate superior disentanglement compared to traditional single-attribute encoding. We also show that these features can then be used to train lightweight prediction heads for multiple downstream classification tasks. Moreover, coupling the UFE with a style-based decoder enables hallucination of new face images composed of attributes taken from different samples. As experimentally demonstrated, the UFE allows us to pick and choose these attributes from label-disjoint datasets. A catalog of such synthetic composites can be used as supplemental training data or simply as stock photos.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/Banerjee_The_Universal_Face_Encoder_Learning_Disentangled_Representations_Across_Different_Attributes_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/Banerjee_The_Universal_Face_Encoder_Learning_Disentangled_Representations_Across_Different_Attributes_CVPRW_2023_paper.pdf"
    },
    {
        "title": "BeCAPTCHA-Type: Biometric Keystroke Data Generation for Improved Bot Detection",
        "author": "Daniel DeAlcala, Aythami Morales, Ruben Tolosana, Alejandro Aci\u00e9n, Julian Fierrez, Santiago Hern\u00e1ndez, Miguel A. Ferrer, Moises Diaz",
        "abstract": "This work proposes a data driven learning model for the synthesis of keystroke biometric data. The proposed method is compared with two statistical approaches based on Universal and User-dependent models. These approaches are validated on the bot detection task, using the keystroke synthetic data to improve the training process of keystroke-based bot detection systems. Our experimental framework considers a dataset with 136 million keystroke events from 168 thousand subjects. We have analyzed the performance of the three synthesis approaches through qualitative and quantitative experiments. Different bot detectors are considered based on several supervised classifiers (Support Vector Machine, Random Forest, Gaussian Naive Bayes and a Long Short-Term Memory network) and a learning framework including human and synthetic samples. The experiments demonstrate the realism of the synthetic samples. The classification results suggest that in scenarios with large labeled data, these synthetic samples can be detected with high accuracy. However, in few-shot learning scenarios it represents an important challenge. Furthermore, these results show the great potential of the presented models.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/DeAlcala_BeCAPTCHA-Type_Biometric_Keystroke_Data_Generation_for_Improved_Bot_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/DeAlcala_BeCAPTCHA-Type_Biometric_Keystroke_Data_Generation_for_Improved_Bot_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exposing Fine-Grained Adversarial Vulnerability of Face Anti-Spoofing Models",
        "author": "Songlin Yang, Wei Wang, Chenye Xu, Ziwen He, Bo Peng, Jing Dong",
        "abstract": "Face anti-spoofing aims to discriminate the spoofing face images (e.g., printed photos and replayed videos) from live ones. However, adversarial examples greatly challenge its credibility, where adding some perturbation noise can easily change the output of the target model. Previous works conducted adversarial attack methods to evaluate the face anti-spoofing performance without any fine-grained analysis that which model architecture or auxiliary feature is vulnerable. To handle this problem, we propose a novel framework to expose the fine-grained adversarial vulnerability of the face anti-spoofing models, which consists of a multitask module and a semantic feature augmentation (SFA) module. The multitask module can obtain different semantic features for further fine-grained evaluation, but only attacking these semantic features fails to reflect the vulnerability which is related to the discrimination between spoofing and live images. We then design the SFA module to introduce the data distribution prior for more discrimination-related gradient directions for generating adversarial examples. And the discrimination-related improvement is quantitatively reflected by the increase of attack success rate, where comprehensive experiments show that SFA module increases the attack success rate by nearly 40% on average. We conduct fine-grained adversarial analysis on different annotations, geometric maps, and backbone networks (e.g., Resnet network). These fine-grained adversarial examples can be used for selecting robust backbone networks and auxiliary features. They also can be used for adversarial training, which makes it practical to further improve the accuracy and robustness of the face anti-spoofing models.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/Yang_Exposing_Fine-Grained_Adversarial_Vulnerability_of_Face_Anti-Spoofing_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/Yang_Exposing_Fine-Grained_Adversarial_Vulnerability_of_Face_Anti-Spoofing_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Closer Look at Geometric Temporal Dynamics for Face Anti-Spoofing",
        "author": "Chih-Jung Chang, Yaw-Chern Lee, Shih-Hsuan Yao, Min-Hung Chen, Chien-Yi Wang, Shang-Hong Lai, Trista Pei-Chun Chen",
        "abstract": "Face anti-spoofing (FAS) is indispensable for a face recognition system. Many texture-driven countermeasures were developed against presentation attacks (PAs), but the performance against unseen domains or unseen spoofing types is still unsatisfactory. Instead of exhaustively collecting all the spoofing variations and making binary decisions of live/spoof, we offer a new perspective on the FAS task to distinguish between normal and abnormal movements of live and spoof presentations. We propose Geometry-Aware Interaction Network (GAIN), which exploits dense facial landmarks with spatio-temporal graph convolutional network (ST-GCN) to establish a more interpretable and modularized FAS model. Additionally, with our cross-attention feature interaction mechanism, GAIN can be easily integrated with other existing methods to significantly boost performance. Our approach achieves state-of-the-art performance in the standard intra- and cross-dataset evaluations. Moreover, our model outperforms state-of-the-art methods by a large margin in the cross-dataset cross-type protocol on CASIA-SURF 3DMask (+10.26% higher AUC score), exhibiting strong robustness against domain shifts and unseen spoofing types.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/Chang_A_Closer_Look_at_Geometric_Temporal_Dynamics_for_Face_Anti-Spoofing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/Chang_A_Closer_Look_at_Geometric_Temporal_Dynamics_for_Face_Anti-Spoofing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Face Recognition Accuracy Across Demographics: Shining a Light Into the Problem",
        "author": "Haiyu Wu, V\u00edtor Albiero, K. S. Krishnapriya, Michael C. King, Kevin W. Bowyer",
        "abstract": "We explore varying face recognition accuracy across demographic groups as a phenomenon partly caused by differences in face illumination. We observe that for a common operational scenario with controlled image acquisition, there is a large difference in face region brightness between African-American and Caucasian, and also a smaller difference between male and female. We show that impostor image pairs with both faces under-exposed, or both over-exposed, have an increased false match rate (FMR). Conversely, image pairs with strongly different face brightness have a decreased similarity measure. We propose a brightness information metric to measure variation in brightness in the face and show that face brightness that is too low or too high has reduced information in the face region, providing a cause for the lower accuracy. Based on this, for operational scenarios with controlled image acquisition, illumination should be adjusted for each individual to obtain appropriate face image brightness. This is the first work that we are aware of to explore how the level of brightness of the skin region in a pair of face images (rather than a single image) impacts face recognition accuracy, and to evaluate this as a systematic factor causing unequal accuracy across demographics.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/Wu_Face_Recognition_Accuracy_Across_Demographics_Shining_a_Light_Into_the_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/Wu_Face_Recognition_Accuracy_Across_Demographics_Shining_a_Light_Into_the_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Gait Recognition From Fisheye Images",
        "author": "Chi Xu, Yasushi Makihara, Xiang Li, Yasushi Yagi",
        "abstract": "Gait recognition has been a hot topic of extensive research in video-based surveillance and forensics. Compared with traditional rectilinear cameras mainly used in existing studies, fisheye cameras have a wider field of view, and hence are more suitable for gait recognition applications in navigation robots, which enables more flexible and free surveillance scenarios. In this paper, to the best of our knowledge, we propose the first framework for gait recognition from images captured by fisheye cameras. To deal with severe image distortion and partial body occlusions induced by fisheye cameras set at lower heights, we combine a set of preprocessing procedures with a state-of-the-art model-based gait recognition method. Specifically, an input fisheye image is first expanded into a panoramic view before pedestrian detection. A person-dependent gnomonic projection is then applied to the detected human region for distortion correction. Next, background regions are attenuated to improve human model fitting accuracy in complex outdoor scenes. The resulting rectified image sequence is finally fed into the gait recognition network for human model estimation and gait feature extraction. To validate the performance, we collect a real fisheye image dataset with various views and capture scenarios, including simplified indoor and challenging outdoor scenes. Various experiments on the collected dataset demonstrate the effectiveness of the proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/html/Xu_Gait_Recognition_From_Fisheye_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Biometrics/papers/Xu_Gait_Recognition_From_Fisheye_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Adaptive Human-Centric Video Compression for Humans and Machines",
        "author": "Wei Jiang, Hyomin Choi, Fabien Racap\u00e9",
        "abstract": "We propose a novel framework to compress human-centric videos for both human viewing and machine analytics. Our system uses three coding branches to combine the power of generic face-prior learning with data-dependent detail recovery. The generic branch embeds faces into a discrete code space described by a learned high-quality (HQ) codebook, to reconstruct an HQ baseline face. The domain-adaptive branch adjusts reconstruction to fit the current data domain by adding domain-specific information through a supplementary codebook. The task-adaptive branch derives assistive details from a low-quality (LQ) input to help machine analytics on the restored face. Adaptive weights are introduced to balance the use of domain-adaptive and task-adaptive features in reconstruction, driving trade-offs among criteria including perceptual quality, fidelity, bitrate, and task accuracy. Moreover, the proposed online learning mechanism automatically adjusts the adaptive weights according to the actual compression needs. By sharing the main generic branch, our framework can extend to multiple data domains and multiple tasks more flexibly compared to conventional coding schemes. Our experiments demonstrate that at very low bitrates we can restore faces with high perceptual quality for human viewing while maintaining high recognition accuracy for machine use.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Jiang_Adaptive_Human-Centric_Video_Compression_for_Humans_and_Machines_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Jiang_Adaptive_Human-Centric_Video_Compression_for_Humans_and_Machines_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SC-NAFSSR: Perceptual-Oriented Stereo Image Super-Resolution Using Stereo Consistency Guided NAFSSR",
        "author": "Zidian Qiu, Zongyao He, Zhihao Zhan, Zilin Pan, Xingyuan Xian, Zhi Jin",
        "abstract": "Stereo image Super-Resolution (SR) has made significant progress since binocular systems are widely accepted in recent years. Most stereo SR methods focus on improving the PSNR performance, while their visual quality is over-smoothing and lack of detail. Perceptual-oriented SR methods are mainly designed for single-view images, thereby their performance decreases on stereo SR due to stereo inconsistency. We propose a perceptual-oriented stereo SR framework that considers both single-view and cross-view information, noted as SC-NAFSSR. With NAFSSR as our backbone, we combine LPIPS-based perceptual loss and VGG-based perceptual loss for perceptual training. To improve stereo consistency, we perform supervision on each Stereo Cross-Attention Module (SCAM) with stereo consistency loss, which calculates photometric loss, smoothness loss, and cycle loss using the cycle-attention maps and valid masks of SCAM. Furthermore, we propose training strategies to fully exploit the performance on perceptual-oriented stereo SR. Both extensive experiments and ablation studies demonstrate the effectiveness of our proposed method. In particular, SC-NAFSSR outperforms the SOTA methods on Flickr1024 dataset. In the NTIRE 2023 Stereo Image Super-Resolution Challenge Track 2 Perceptual & Bicubic, SC-NAFSSR ranked 2nd place on the leaderboard. Our source code is available at https://github.com/FVL2020/SC-NAFSSR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Qiu_SC-NAFSSR_Perceptual-Oriented_Stereo_Image_Super-Resolution_Using_Stereo_Consistency_Guided_NAFSSR_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Qiu_SC-NAFSSR_Perceptual-Oriented_Stereo_Image_Super-Resolution_Using_Stereo_Consistency_Guided_NAFSSR_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TransER: Hybrid Model and Ensemble-Based Sequential Learning for Non-Homogenous Dehazing",
        "author": "Trung Hoang, Haichuan Zhang, Amirsaeed Yazdani, Vishal Monga",
        "abstract": "Image dehazing is one of the most challenging imaging inverse problems that estimates the haze-free images from hazy ones. While recent transformer/convolutional neural network-based methods have shown excellent performance in handling both homogeneous and non-homogeneous dehazing problems, these networks are often trained end-to-end to estimate the haze-free image directly and require a large number of parameters. In this work, we propose a novel, lightweight two-stage deep network for non-homogeneous dehazing. In particular, our proposed method, denoted as TransER, consists of two separate deep neural networks which are TransConv Fusion Dehaze (TFD) model in Stage I and Lightweight Ensemble Reconstruction (LER) network in Stage II. The first model (TFD) using transformer-based encoder and decoders generates two estimates of the haze-free image: a parameter-based dehazed output based on the physical modeling of the problem and a pseudo haze-free output generated directly by the model in an end-to-end fashion. LER in stage II reconstructs the final dehazed output fusing the two estimates from stage I. We incorporate knowledge distillation to develop a teacher network with the same architecture as LER, allowing it to supervise the intermediate features. Extensive experiments performed on challenging real and synthetic scene image datasets (NTIRE 2019-2023, and RESIDE-indoor) demonstrate that TransER can outperform many state-of-the-art competing methods while using a significantly lower number of parameters. The source code is available at https://github.com/trungpsu1210/TransER.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Hoang_TransER_Hybrid_Model_and_Ensemble-Based_Sequential_Learning_for_Non-Homogenous_Dehazing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Hoang_TransER_Hybrid_Model_and_Ensemble-Based_Sequential_Learning_for_Non-Homogenous_Dehazing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Bicubic++: Slim, Slimmer, Slimmest - Designing an Industry-Grade Super-Resolution Network",
        "author": "Bahri Batuhan Bilecen, Mustafa Ayazoglu",
        "abstract": "We propose a real-time and lightweight single-image super-resolution (SR) network named Bicubic++. Despite using spatial dimensions of the input image across the whole network, Bicubic++ first learns quick reversible downgraded and lower resolution features of the image in order to decrease the number of computations. We also construct a training pipeline, where we apply an end-to-end global structured pruning of convolutional layers without using metrics like magnitude and gradient norms, and focus on optimizing the pruned network's PSNR on the validation set. Furthermore, we have experimentally shown that the bias terms take considerable amount of the runtime while increasing PSNR marginally, hence we have also applied bias removal to the convolutional layers. Our method adds  1dB on Bicubic upscaling PSNR for all tested SR datasets and runs with  1.17ms on RTX3090 and  2.9ms on RTX3070, for 720p inputs and 4K outputs, both in FP16 precision. Bicubic++ won NTIRE 2023 RTSR Track 2 x3 SR competition and is the fastest among all competitive methods. Being almost as fast as the standard Bicubic upsampling method, we believe that Bicubic++ can set a new industry standard.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Bilecen_Bicubic_Slim_Slimmer_Slimmest_-_Designing_an_Industry-Grade_Super-Resolution_Network_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Bilecen_Bicubic_Slim_Slimmer_Slimmest_-_Designing_an_Industry-Grade_Super-Resolution_Network_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Blind Image Inpainting via Omni-Dimensional Gated Attention and Wavelet Queries",
        "author": "Shruti S. Phutke, Ashutosh Kulkarni, Santosh Kumar Vipparthi, Subrahmanyam Murala",
        "abstract": "Blind image inpainting is a crucial restoration task that does not demand additional mask information to restore the corrupted regions. Yet, it is a very less explored research area due to the difficulty in discriminating between corrupted and valid regions. There exist very few approaches for blind image inpainting which sometimes fail at producing plausible inpainted images. Since they follow a common practice of predicting the corrupted regions and then inpaint them. To skip the corrupted region prediction step and obtain better results, in this work, we propose a novel end-to-end architecture for blind image inpainting consisting of wavelet query multi-head attention transformer block and the omni-dimensional gated attention. The proposed wavelet query multi-head attention in the transformer block provides encoder features via processed wavelet coefficients as query to the multi-head attention. Further, the proposed omni-dimensional gated attention effectively provides all dimensional attentive features from the encoder to the respective decoder. Our proposed approach is compared numerically and visually with existing state-of-the-art methods for blind image inpainting on different standard datasets. The comparative and ablation studies prove the effectiveness of the proposed approach for blind image inpainting. The testing code is available at: https://github.com/shrutiphutke/Blind_ Omni_Wav_Net",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Phutke_Blind_Image_Inpainting_via_Omni-Dimensional_Gated_Attention_and_Wavelet_Queries_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Phutke_Blind_Image_Inpainting_via_Omni-Dimensional_Gated_Attention_and_Wavelet_Queries_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Hybrid Transformer and CNN Attention Network for Stereo Image Super-Resolution",
        "author": "Ming Cheng, Haoyu Ma, Qiufang Ma, Xiaopeng Sun, Weiqi Li, Zhenyu Zhang, Xuhan Sheng, Shijie Zhao, Junlin Li, Li Zhang",
        "abstract": "Multi-stage strategies are frequently employed in image restoration tasks. While transformer-based methods have exhibited high efficiency in single-image super-resolution tasks, they have not yet shown significant advantages over CNN-based methods in stereo super-resolution tasks. This can be attributed to two key factors: first, current single-image super-resolution transformers are unable to leverage the complementary stereo information during the process; second, the performance of transformers is typically reliant on sufficient data, which is absent in common stereo-image super-resolution algorithms. To address these issues, we propose a Hybrid Transformer and CNN Attention Network (HTCAN), which utilizes a transformer-based network for single-image enhancement and a CNN-based network for stereo information fusion. Furthermore, we employ a multi-patch training strategy and larger window sizes to activate more input pixels for super-resolution. We also revisit other advanced techniques, such as data augmentation, data ensemble, and model ensemble to reduce overfitting and data bias. Finally, our approach achieved a score of 23.90dB and emerged as the winner in Track 1 of the NTIRE 2023 Stereo Image Super-Resolution Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cheng_Hybrid_Transformer_and_CNN_Attention_Network_for_Stereo_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Cheng_Hybrid_Transformer_and_CNN_Attention_Network_for_Stereo_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 HR NonHomogeneous Dehazing Challenge Report",
        "author": "Codruta O. Ancuti, Cosmin Ancuti, Florin-Alexandru Vasluianu, Radu Timofte, Han Zhou, Wei Dong, Yangyi Liu, Jun Chen, Huan Liu, Liangyan Li, Zijun Wu, Yubo Dong, Yuyan Li, Tian Qiu, Yu He, Yonghong Lu, Yinwei Wu, Zhenxiang Jiang, Songhua Liu, Xingyi Yang, Yongcheng Jing, Bilel Benjdira, Anas M. Ali, Anis Koubaa, Hao-Hsiang Yang, I-Hsiang Chen, Wei-Ting Chen, Zhi-Kai Huang, Yi-Chung Chen, Chia-Hsuan Hsieh, Hua-En Chang, Yuan-Chun Chiang, Sy-Yen Kuo, Yu Guo, Yuan Gao, Ryan Wen Liu, Yuxu Lu, Jingxiang Qu, Shengfeng He, Wenqi Ren, Trung Hoang, Haichuan Zhang, Amirsaeed Yazdani, Vishal Monga, Lehan Yang, Alex Jiahao Wu, Tiancheng Mai, Xiaofeng Cong, Xuemeng Yin, Xuefei Yin, Hazim Emad, Ahmed Abdallah, Yahya Yasser, Dalia Elshahat, Esraa Elbaz, Zhan Li, Wenqing Kuang, Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, Thomas B. Sch\u00f6n, Zhao Zhang, Yanyan Wei, Junhu Wang, Suiyi Zhao, Huan Zheng, Jin Guo, Yangfan Sun, Tianli Liu, Dejun Hao, Kui Jiang, Anjali Sarvaiya, Kalpesh Prajapati, Ratnadeep Patra, Pragnesh Barik, Chaitanya Rathod, Kishor Upla, Kiran Raja, Raghavendra Ramachandra, Christoph Busch",
        "abstract": "This study assesses the outcomes of the NTIRE 2023 Challenge on Non-Homogeneous Dehazing, wherein novel techniques were proposed and evaluated on new image dataset called HD-NH-HAZE. The HD-NH-HAZE dataset contains 50 high resolution pairs of real-life outdoor images featuring nonhomogeneous hazy images and corresponding haze-free images of the same scene. The nonhomogeneous haze was simulated using a professional setup that replicated real-world conditions of hazy scenarios. The competition had 246 participants and 17 teams that competed in the final testing phase, and the proposed solutions demonstrated the cutting-edge in image dehazing technology.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Ancuti_NTIRE_2023_HR_NonHomogeneous_Dehazing_Challenge_Report_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Ancuti_NTIRE_2023_HR_NonHomogeneous_Dehazing_Challenge_Report_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Reparameterized Residual Feature Network for Lightweight Image Super-Resolution",
        "author": "Weijian Deng, Hongjie Yuan, Lunhui Deng, Zengtong Lu",
        "abstract": "In order to solve the problem of deploying super-resolution technology on resource-limited devices, this paper explores the differences in performance and efficiency between information distillation mechanism and residual learning mechanism used in lightweight super-resolution, and proposes a lightweight super-resolution network structure based on reparameterization, named RepRFN, which can effectively reduce GPU memory consumption and improve inference speed. A multi-scale feature fusion structure is designed so that the network can learn and integrate features of various scales and high-frequency edges. We rethought the redundancy existing in the overall network framework, and removed some redundant modules without affecting the overall performance as much as possible to further reduce the complexity of the model. In addition, we introduced a loss function based on Fourier transform to transform the spatial domain of the image into the frequency domain, so that the network can supervise and learn the frequency part of the image. The experimental results show that the RepRFN designed in this paper achieves relatively low complexity while ensuring certain performance, which is conducive to the deployment of Edge devices. Code is available at https://github.com/laonafahaodange/RepRFN.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Deng_Reparameterized_Residual_Feature_Network_for_Lightweight_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Deng_Reparameterized_Residual_Feature_Network_for_Lightweight_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Back to the Future: A Night Photography Rendering ISP Without Deep Learning",
        "author": "Simone Zini, Claudio Rota, Marco Buzzelli, Simone Bianco, Raimondo Schettini",
        "abstract": "Rendering night photography pictures is a challenging task that requires advanced processing techniques. Although deep learning-based Image Signal Processing (ISP) pipelines have shown promising results, current limitations are set by the lack of proper nighttime image datasets, their high computational requirements, and low explainability. In this paper, we propose a traditional ISP pipeline for rendering visually pleasing photographs of night scenes. Our pipeline is comprised of various algorithms addressing the different challenges presented by night images, and it is characterized by a shallow structure, explainable steps, and a low parameter count, resulting in computationally efficient processing. Moreover, it does not require training data. Experiments show that our pipeline can produce more pleasing results compared to other deep learning-based ISP pipelines, as it won first place in people's choice track and third place in photographer's choice track in the NTIRE 2023 Night Photography Rendering Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zini_Back_to_the_Future_A_Night_Photography_Rendering_ISP_Without_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zini_Back_to_the_Future_A_Night_Photography_Rendering_ISP_Without_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Stereo Cross Global Learnable Attention Module for Stereo Image Super-Resolution",
        "author": "Yuanbo Zhou, Yuyang Xue, Wei Deng, Ruofeng Nie, Jiajun Zhang, Jiaqi Pu, Qinquan Gao, Junlin Lan, Tong Tong",
        "abstract": "Stereo super-resolution is a technique that utilizes corresponding information from multiple viewpoints to enhance the texture of low-resolution images. In recent years, numerous impressive works have advocated attention mechanisms based on epipolar constraints to boost the performance of stereo super-resolution. However, techniques that exclusively depend on epipolar constraint attention are insufficient to recover realistic and natural textures for heavily corrupted low resolution images. We noticed that global self-similarity features within the image and across the views can proficiently fix the texture details of low-resolution images that are severely damaged. Therefore, in the current paper, we propose a stereo cross global learnable attention module (SCGLAM), aiming to improve the performance of stereo super-resolution. The experimental outcomes show that our approach outperforms others when dealing with heavily damaged low-resolution images. The relevant code is made available on this link as open source.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zhou_Stereo_Cross_Global_Learnable_Attention_Module_for_Stereo_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhou_Stereo_Cross_Global_Learnable_Attention_Module_for_Stereo_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "BeautyREC: Robust, Efficient, and Component-Specific Makeup Transfer",
        "author": "Qixin Yan, Chunle Guo, Jixin Zhao, Yuekun Dai, Chen Change Loy, Chongyi Li",
        "abstract": "In this work, we propose a Robust, Efficient, and Component-specific makeup transfer method (abbreviated as BeautyREC). A unique departure from prior methods that leverage global attention, simply concatenate features, or implicitly manipulate features in latent space, we propose a component-specific correspondence to directly transfer the makeup style of a reference image to the corresponding components (e.g., skin, lips, eyes) of a source image, making elaborate and accurate local makeup transfer. As an auxiliary, the long-range visual dependencies of Transformer are introduced for effective global makeup transfer. Instead of the commonly used cycle structure that is complex and unstable, we employ a content consistency loss coupled with a content encoder to implement efficient single-path makeup transfer. The key insights of this study are modeling component-specific correspondence for local makeup transfer, capturing long-range dependencies for global makeup transfer, and enabling efficient makeup transfer via a single-path structure. We also contribute BeautyFace, a makeup transfer dataset to supplement existing datasets. This dataset contains 3,000 faces, covering more diverse makeup styles, face poses, and races. Each face has annotated parsing map. Extensive experiments demonstrate the effectiveness of our method against state-of-the-art methods. Besides, our method is appealing as it is with only 1M parameters, outperforming the state-of-the-art methods (BeautyGAN: 8.43M, PSGAN: 12.62M, SCGAN: 15.30M, CPM: 9.24M, SSAT: 10.48M).",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Yan_BeautyREC_Robust_Efficient_and_Component-Specific_Makeup_Transfer_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Yan_BeautyREC_Robust_Efficient_and_Component-Specific_Makeup_Transfer_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Mixer-Based Local Residual Network for Lightweight Image Super-Resolution",
        "author": "Garas Gendy, Nabil Sabor, Jingchao Hou, Guanghui He",
        "abstract": "Recently, the single image super-resolution (SISR) based on deep learning algorithm has taken more attention from the research community. There are many methods that are developed to solve this task using CNNs methods. However, most of these methods need large computational resources and consume more runtime. Due to the fact that the runtime is essential for some applications, we propose a mixer-based local residual network (MLRN) for lightweight image super-resolution (SR). The idea of the MLRN model is based on mixing channel and spatial features and mixing low and high-frequency information. This is done by designing a mixer local residual block (MLRB) to be the backbone of our model. Moreover, the bilinear up-sampling is utilized to transfer and mix low-frequency information with extracted high-frequency information. Finally, the GELU activation is used in the main model, proving its efficiency for the SR task. The experimental results show the effectiveness of the model against other state-of-the-art lightweight models. Finally, we took part in the Efficient Super-Resolution 2023 Challenge and achieved good results.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Gendy_Mixer-Based_Local_Residual_Network_for_Lightweight_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Gendy_Mixer-Based_Local_Residual_Network_for_Lightweight_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Quantum Annealing for Single Image Super-Resolution",
        "author": "Han Yao Choong, Suryansh Kumar, Luc Van Gool",
        "abstract": "This paper proposes a quantum computing-based algorithm to solve the single image super-resolution (SISR) problem. One of the well-known classical approaches for SISR relies on the well-established patch-wise sparse modeling of the problem. Yet, this field's current state of affairs is that deep neural networks (DNNs) have demonstrated far superior results than traditional approaches. Nevertheless, quantum computing is expected to become increasingly prominent for machine learning problems soon. As a result, in this work, we take the privilege to perform an early exploration of applying a quantum computing algorithm to this important image enhancement problem, i.e., SISR. Among the two paradigms of quantum computing, namely universal gate quantum computing and adiabatic quantum computing (AQC), the latter has been successfully applied to practical computer vision problems, in which quantum parallelism has been exploited to solve combinatorial optimization efficiently. This work demonstrates formulating quantum SISR as a sparse coding optimization problem, which is solved using quantum annealers accessed via the D-Wave Leap platform. The proposed AQC-based algorithm is demonstrated to achieve improved speed-up over a classical analog while maintaining comparable SISR accuracy",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Choong_Quantum_Annealing_for_Single_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Choong_Quantum_Annealing_for_Single_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Lens-to-Lens Bokeh Effect Transformation. NTIRE 2023 Challenge Report",
        "author": "Marcos V. Conde, Manuel Kolmet, Tim Seizinger, Tom E. Bishop, Radu Timofte, Xiangyu Kong, Dafeng Zhang, Jinlong Wu, Fan Wang, Juewen Peng, Zhiyu Pan, Chengxin Liu, Xianrui Luo, Huiqiang Sun, Liao Shen, Zhiguo Cao, Ke Xian, Chaowei Liu, Zigeng Chen, Xingyi Yang, Songhua Liu, Yongcheng Jing, Michael Bi Mi, Xinchao Wang, Zhihao Yang, Wenyi Lian, Siyuan Lai, Haichuan Zhang, Trung Hoang, Amirsaeed Yazdani, Vishal Monga, Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, Thomas B. Sch\u00f6n, Yuxuan Zhao, Baoliang Chen, Yiqing Xu, JiXiang Niu",
        "abstract": "We present the new Bokeh Effect Transformation Dataset (BETD), and review the proposed solutions for this novel task at the NTIRE 2023 Bokeh Effect Transformation Challenge. Recent advancements of mobile photography aim to reach the visual quality of full-frame cameras. Now, a goal in computational photography is to optimize the Bokeh effect itself, which is the aesthetic quality of the blur in out-of-focus areas of an image. Photographers create this aesthetic effect by benefiting from the lens optical properties. The aim of this work is to design a neural network capable of converting the the Bokeh effect of one lens to the effect of another lens without harming the sharp foreground regions in the image. For a given input image, knowing the target lens type, we render or transform the Bokeh effect accordingly to the lens properties. We build the BETD using two full-frame Sony cameras, and diverse lens setups. To the best of our knowledge, we are the first attempt to solve this novel task, and we provide the first BETD dataset and benchmark for it. The challenge had 99 registered participants. The submitted methods gauge the state-of-the-art in Bokeh effect rendering and transformation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Conde_Lens-to-Lens_Bokeh_Effect_Transformation._NTIRE_2023_Challenge_Report_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Conde_Lens-to-Lens_Bokeh_Effect_Transformation._NTIRE_2023_Challenge_Report_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method Based on Fast Fourier Convolution and ConvNeXt",
        "author": "Han Zhou, Wei Dong, Yangyi Liu, Jun Chen",
        "abstract": "Haze usually leads to deteriorated images with low contrast, color shift and structural distortion. We observe that many deep learning based models exhibit exceptional performance on removing homogeneous haze, but they usually fail to address the challenge of non-homogeneous dehazing. Two main factors account for this situation. Firstly, due to the intricate and non uniform distribution of dense haze, the recovery of structural and chromatic features with high fidelity is challenging, particularly in regions with heavy haze. Secondly, the existing small scale datasets for non-homogeneous dehazing are inadequate to support reliable learning of feature mappings between hazy images and their corresponding haze-free counterparts by convolutional neural network (CNN)-based models. To tackle these two challenges, we propose a novel two branch network that leverages 2D discrete wavelete transform (DWT), fast Fourier convolution (FFC) residual block and a pretrained ConvNeXt model. Specifically, in the DWT-FFC frequency branch, our model exploits DWT to capture more high-frequency features. Moreover, by taking advantage of the large receptive field provided by FFC residual blocks, our model is able to effectively explore global contextual information and produce images with better perceptual quality. In the prior knowledge branch, an ImageNet pretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model to learn more supplementary information and acquire a stronger generalization ability. The feasibility and effectiveness of the proposed method is demonstrated via extensive experiments and ablation studies. The code is available at https://github.com/zhouh115/DWT-FFC.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zhou_Breaking_Through_the_Haze_An_Advanced_Non-Homogeneous_Dehazing_Method_Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhou_Breaking_Through_the_Haze_An_Advanced_Non-Homogeneous_Dehazing_Method_Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution",
        "author": "Lei Yu, Xinpeng Li, Youwei Li, Ting Jiang, Qi Wu, Haoqiang Fan, Shuaicheng Liu",
        "abstract": "Efficient deep learning-based approaches have achieved remarkable performance in single image super-resolution. However, recent studies on efficient super-resolution have mainly focused on reducing the number of parameters and floating-point operations through various network designs. Although these methods can decrease the number of parameters and floating-point operations, they may not necessarily reduce actual running time. To address this issue, we propose a novel multi-stage lightweight network boosting method, which can enable lightweight networks to achieve outstanding performance. Specifically, we leverage enhanced high-resolution output as additional supervision to improve the learning ability of lightweight student networks. Upon convergence of the student network, we further simplify our network structure to a more lightweight level using reparameterization techniques and iterative network pruning. Meanwhile, we adopt an effective lightweight network training strategy that combines multi-anchor distillation and progressive learning, enabling the lightweight network to achieve outstanding performance. Ultimately, our proposed method achieves the fastest inference time among all participants in the NTIRE 2023 efficient super-resolution challenge while maintaining competitive super-resolution performance. Additionally, extensive experiments are conducted to demonstrate the effectiveness of the proposed components. The results show that our approach achieves comparable performance in representative dataset DIV2K, both qualitatively and quantitatively, with faster inference and fewer number of network parameters.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Yu_DIPNet_Efficiency_Distillation_and_Iterative_Pruning_for_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Yu_DIPNet_Efficiency_Distillation_and_Iterative_Pruning_for_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results",
        "author": "Andrei Dumitriu, Florin Tatui, Florin Miron, Radu Tudor Ionescu, Radu Timofte",
        "abstract": "Rip currents are the leading cause of fatal accidents and injuries on many beaches worldwide, emphasizing the importance of automatically detecting these hazardous surface water currents. In this paper, we address a novel task: rip current instance segmentation. We introduce a comprehensive dataset containing 2,466 images with newly created polygonal annotations for instance segmentation, used for training and validation. Additionally, we present a novel dataset comprising 17 drone videos (comprising about 24K frames) captured at 30 FPS, annotated with both polygons for instance segmentation and bounding boxes for object detection, employed for testing purposes. We train various versions of YOLOv8 for instance segmentation on static images and assess their performance on the test dataset (videos). The best results were achieved by the YOLOv8-nano model (runnable on a portable device), with an mAP50 of 88.94% on the validation dataset and 81.21% macro average on the test dataset. The results provide a baseline for future research in rip current segmentation. Our work contributes to the existing literature by introducing a detailed, annotated dataset, and training a deep learning model for instance segmentation of rip currents. The code, training details and the annotated dataset are made publicly available at https://github.com/Irikos/rip_currents.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Dumitriu_Rip_Current_Segmentation_A_Novel_Benchmark_and_YOLOv8_Baseline_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Dumitriu_Rip_Current_Segmentation_A_Novel_Benchmark_and_YOLOv8_Baseline_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "OPDN: Omnidirectional Position-Aware Deformable Network for Omnidirectional Image Super-Resolution",
        "author": "Xiaopeng Sun, Weiqi Li, Zhenyu Zhang, Qiufang Ma, Xuhan Sheng, Ming Cheng, Haoyu Ma, Shijie Zhao, Jian Zhang, Junlin Li, Li Zhang",
        "abstract": "360deg omnidirectional images have gained research attention due to their immersive and interactive experience, particularly in AR/VR applications. However, they suffer from lower angular resolution due to being captured by fisheye lenses with the same sensor size for capturing planar images. To solve the above issues, we propose a two-stage framework for 360deg omnidirectional image super-resolution. The first stage employs two branches: model A, which incorporates omnidirectional position-aware deformable blocks (OPDB) and Fourier upsampling, and model B, which adds a spatial frequency fusion module (SFF) to model A. Model A aims to enhance the feature extraction ability of 360deg image location characteristics, while Model B further focuses on the high-frequency information of 360deg images. The second stage performs same-resolution enhancement based on the structure of model A with a pixel unshuffle operation. In addition, we collected data from YouTube to improve the fitting ability of the transformer, and created pseudo low-resolution images using a degradation network. Our proposed method achieves superior performance and wins the NTIRE 2023 challenge of 360deg omnidirectional image super-resolution.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Sun_OPDN_Omnidirectional_Position-Aware_Deformable_Network_for_Omnidirectional_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Sun_OPDN_Omnidirectional_Position-Aware_Deformable_Network_for_Omnidirectional_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results",
        "author": "Mingdeng Cao, Chong Mou, Fanghua Yu, Xintao Wang, Yinqiang Zheng, Jian Zhang, Chao Dong, Gen Li, Ying Shan, Radu Timofte, Xiaopeng Sun, Weiqi Li, Zhenyu Zhang, Xuhan Sheng, Bin Chen, Haoyu Ma, Ming Cheng, Shijie Zhao, Wanwan Cui, Tianyu Xu, Chunyang Li, Long Bao, Heng Sun, Huaibo Huang, Xiaoqiang Zhou, Yuang Ai, Ran He, Renlong Wu, Yi Yang, Zhilu Zhang, Shuohao Zhang, Junyi Li, Yunjin Chen, Dongwei Ren, Wangmeng Zuo, Qian Wang, Hao-Hsiang Yang, Yi-Chung Chen, Zhi-Kai Huang, Wei-Ting Chen, Yuan-Chun Chiang, Hua-En Chang, I-Hsiang Chen, Chia-Hsuan Hsieh, Sy-Yen Kuo, Zebin Zhang, Jiaqi Zhang, Yuhui Wang, Shuhao Cui, Junshi Huang, Li Zhu, Shuman Tian, Wei Yu, Bingchun Luo",
        "abstract": "This report introduces two high-quality datasets Flickr360 and ODV360 for omnidirectional image and video super-resolution, respectively, and reports the NTIRE 2023 challenge on 360deg omnidirectional image and video super-resolution. Unlike ordinary 2D images/videos with a narrow field of view, omnidirectional images/videos can represent the whole scene from all directions in one shot. There exists a large gap between omnidirectional image/video and ordinary 2D image/video in both the degradation and restoration processes. The challenge is held to facilitate the development of omnidirectional image/video super-resolution by considering their special characteristics. In this challenge, two tracks are provided: one is the omnidirectional image super-resolution and the other is the omnidirectional video super-resolution. The task of the challenge is to super-resolve an input omnidirectional image/video with a magnification factor of x4. Realistic omnidirectional downsampling is applied to construct the datasets. Some general degradation(e.g., video compression) is also considered for the video track. The challenge has 100 and 56 registered participants for those two tracks. In the final testing stage, 7 and 3 participating teams submitted their results, source codes, and fact sheets. Almost all teams achieved better performance than baseline models by integrating omnidirectional characteristics, reaching compelling performance on our newly collected Flickr360 and ODV360 datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cao_NTIRE_2023_Challenge_on_360deg_Omnidirectional_Image_and_Video_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Cao_NTIRE_2023_Challenge_on_360deg_Omnidirectional_Image_and_Video_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Deep Dehazing Powered by Image Processing Network",
        "author": "Guisik Kim, Jinhee Park, Junseok Kwon",
        "abstract": "Image processing is a very fundamental technique in the field of low-level vision. However, with the development of deep learning over the past five years, most low-level vision methods tend to ignore this technique. Recent dehazing methods also refrain from using conventional image processing techniques, whereas only focusing on the development of new deep neural network (DNN) architectures. Unlike this recent trend, we show that image processing techniques are still competitive, if they are incorporated into DNNs. In this paper, we utilize conventional image processing techniques (i.e. curve adjustment, retinex decomposition, and multiple image fusion) for accurate dehazing. Moreover, we employ direct learning for stable dehazing performance. The proposed method can perform with low computational cost and easy to learn. The experimental results demonstrate that the proposed method produces accurate dehazing results compared to recent algorithms.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Kim_Deep_Dehazing_Powered_by_Image_Processing_Network_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Kim_Deep_Dehazing_Powered_by_Image_Processing_Network_CVPRW_2023_paper.pdf"
    },
    {
        "title": "High-Resolution Synthetic RGB-D Datasets for Monocular Depth Estimation",
        "author": "Aakash Rajpal, Noshaba Cheema, Klaus Illgner-Fehns, Philipp Slusallek, Sunil Jaiswal",
        "abstract": "Accurate depth maps are essential in various applications, such as autonomous driving, scene reconstruction, point-cloud creation, etc. However, monocular-depth estimation (MDE) algorithms often fail to provide enough texture & sharpness, and also are inconsistent for homogeneous scenes. These algorithms mostly use CNN or vision transformer-based architectures requiring large datasets for supervised training. But, MDE algorithms trained on available depth datasets do not generalize well and hence fail to perform accurately in diverse real-world scenes. Moreover, the ground-truth depth maps are either lower resolution or sparse leading to relatively inconsistent depth maps. In general, acquiring a high-resolution ground truth dataset with pixel-level precision for accurate depth prediction is a formidable, expensive, and time-consuming challenge. In this paper, we generate a high-resolution synthetic depth dataset (HRSD) of dimension 1920 x 1080 from Grand Theft Auto (GTA-V), which contains 100,000 color images and corresponding dense ground truth depth maps. The generated datasets are diverse and have scenes from indoors to outdoors, from homogeneous surfaces to textures. For experiments and analysis, we train the DPT algorithm, a state-of-the-art transformer-based MDE algorithm on the proposed synthetic dataset, which significantly increases the accuracy of depth maps on different scenes by 9%. Since the synthetic datasets are of higher resolution, we propose adding a feature extraction module in the transformer's encoder and incorporating an attention-based loss, further improving the accuracy by 15 %.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Rajpal_High-Resolution_Synthetic_RGB-D_Datasets_for_Monocular_Depth_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Rajpal_High-Resolution_Synthetic_RGB-D_Datasets_for_Monocular_Depth_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration",
        "author": "Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, Luc Van Gool",
        "abstract": "Plug-and-play Image Restoration (IR) has been widely recognized as a flexible and interpretable method for solving various inverse problems by utilizing any off-the-shelf denoiser as the implicit image prior. However, most existing methods focus on discriminative Gaussian denoisers. Although diffusion models have shown impressive performance for high-quality image synthesis, their potential to serve as a generative denoiser prior to the plug-and-play IR methods remains to be further explored. While several other attempts have been made to adopt diffusion models for image restoration, they either fail to achieve satisfactory results or typically require an unacceptable number of Neural Function Evaluations (NFEs) during inference. This paper proposes DiffPIR, which integrates the traditional plug-and-play method into the diffusion sampling framework. Compared to plug-and-play IR methods that rely on discriminative Gaussian denoisers, DiffPIR is expected to inherit the generative ability of diffusion models. Experimental results on three representative IR tasks, including super-resolution, image deblurring, and inpainting, demonstrate that DiffPIR achieves state-of-the-art performance on both the FFHQ and ImageNet datasets in terms of reconstruction faithfulness and perceptual quality with no more than 100 NFEs. The source code is available at https://github.com/yuanzhi-zhu/DiffPIR",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zhu_Denoising_Diffusion_Models_for_Plug-and-Play_Image_Restoration_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhu_Denoising_Diffusion_Models_for_Plug-and-Play_Image_Restoration_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SCANet: Self-Paced Semi-Curricular Attention Network for Non-Homogeneous Image Dehazing",
        "author": "Yu Guo, Yuan Gao, Wen Liu, Yuxu Lu, Jingxiang Qu, Shengfeng He, Wenqi Ren",
        "abstract": "The presence of non-homogeneous haze can cause scene blurring, color distortion, low contrast, and other degradations that obscure texture details. Existing homogeneous dehazing methods struggle to handle the non-uniform distribution of haze in a robust manner. The crucial challenge of non-homogeneous dehazing is to effectively extract the non-uniform distribution features and reconstruct the details of hazy areas with high quality. In this paper, we propose a novel self-paced semi-curricular attention network, called SCANet, for non-homogeneous image dehazing that focuses on enhancing haze-occluded regions. Our approach consists of an attention generator network and a scene reconstruction network. We use the luminance differences of images to restrict the attention map and introduce a self-paced semi-curricular learning strategy to reduce learning ambiguity in the early stages of training. Extensive quantitative and qualitative experiments demonstrate that our SCANet outperforms many state-of-the-art methods. The code is publicly available at https://github.com/gy65896/SCANet.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Guo_SCANet_Self-Paced_Semi-Curricular_Attention_Network_for_Non-Homogeneous_Image_Dehazing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Guo_SCANet_Self-Paced_Semi-Curricular_Attention_Network_for_Non-Homogeneous_Image_Dehazing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "LSDIR: A Large Scale Dataset for Image Restoration",
        "author": "Yawei Li, Kai Zhang, Jingyun Liang, Jiezhang Cao, Ce Liu, Rui Gong, Yulun Zhang, Hao Tang, Yun Liu, Denis Demandolx, Rakesh Ranjan, Radu Timofte, Luc Van Gool",
        "abstract": "The aim of this paper is to propose a large scale dataset for image restoration (LSDIR). Recent work in image restoration has focused on the design of deep neural networks. The datasets used to train these networks only contain some thousands of images, which is still incomparable with the large scale datasets for other vision tasks such as visual recognition and object detection. The small training set limits the performance of image restoration networks. To solve that problem, we collect high-resolution (HR) images from Flickr for image restoration. To ensure the pixel-level quality of the collected dataset, annotators were invited to manually inspect each of the collected image and remove the low-quality ones. The final dataset contains 84,991 high-quality training images, 1,000 validation images, and 1,000 test images. In addition, we showed that the model capacity of large networks could be fully exploited by training on the large scale dataset with significantly increased patch size and prolonged training iterations. The experimental results on image super-resolution (SR), denoising, JPEG deblocking, deblurring, and demosaicking, and real-world SR show that image restoration networks benefit a lot from the large scale dataset.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_LSDIR_A_Large_Scale_Dataset_for_Image_Restoration_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_LSDIR_A_Large_Scale_Dataset_for_Image_Restoration_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Learning Epipolar-Spatial Relationship for Light Field Image Super-Resolution",
        "author": "Ahmed Salem, Hatem Ibrahem, Hyun-Soo Kang",
        "abstract": "Light field (LF) imaging has become increasingly popular in recent years for capturing and processing visual information. A significant challenge in LF processing is super-resolution (SR), which aims to enhance the resolution of low-resolution LF images. This article proposes a new LF image super-resolution (LFSR) approach that leverages the epipolar-spatial relationship within the LF. To train a deep neural network for LFSR, the proposed method involves extracting three types of information from the LF: spatial, horizontal epipolar, and vertical epipolar. Experimental results demonstrate the effectiveness of the proposed approach compared with state-of-the-art (SOTA) performance, as evidenced by quantitative metrics and visual quality. In addition, we conducted ablation studies to assess the effectiveness of each type of information and gain insights into the underlying mechanisms of the proposed method. Our approach achieved competitive results on the NTIRE 2023 Light Field Image Super-Resolution Challenge: our proposed model was ranked 10th on the test set and 6th on the validation set among 148 participants. Paper's code is available at: https://github.com/ahmeddiefy/EpiS_LFSR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Salem_Learning_Epipolar-Spatial_Relationship_for_Light_Field_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Salem_Learning_Epipolar-Spatial_Relationship_for_Light_Field_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Quality Assessment of Enhanced Videos Guided by Aesthetics and Technical Quality Attributes",
        "author": "Mirko Agarla, Luigi Celona, Claudio Rota, Raimondo Schettini",
        "abstract": "In this work we propose a novel method to evaluate the quality of enhanced videos. Perceived quality of a video depends on both technical aspects, such as the presence of distortions like noise and blur, and non-technical factors, such as content preference and recommendation. Our approach involves the use of three deep learning based models that encode video sequences in terms of their overall technical quality, quality-related attributes, and aesthetic quality. The resulting feature vectors are adaptively combined and used as input to a Support Vector Regressor to estimate the video quality score. Quantitative results on the recently released VQA Dataset for Perceptual Video Enhancement (VDPVE) introduced for the NTIRE 2023 Quality Assessment of Video Enhancement Challenge demonstrates the effectiveness of the proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Agarla_Quality_Assessment_of_Enhanced_Videos_Guided_by_Aesthetics_and_Technical_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Agarla_Quality_Assessment_of_Enhanced_Videos_Guided_by_Aesthetics_and_Technical_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FlexiCurve: Flexible Piecewise Curves Estimation for Photo Retouching",
        "author": "Chongyi Li, Chunle Guo, Shangchen Zhou, Qiming Ai, Ruicheng Feng, Chen Change Loy",
        "abstract": "This paper presents a new method, called FlexiCurve, for photo retouching. Unlike most existing methods that perform image-to-image mapping, which requires expensive pixel-wise reconstruction, FlexiCurve takes an input image and estimates global curves to adjust the image. The adjustment curves are specially designed for performing piecewise mapping, taking nonlinear adjustment and differentiability into account. To cope with challenging and diverse properties in real-world photos, FlexiCurve is formulated to produce diverse estimations. The spatial dependencies among these estimations are implicitly modeled by a Transformer structure to improve local retouching of different regions. Thanks to the image-to-curve formulation, FlexiCurve only needs a lightweight network. Our method improves efficiency without compromising the retouching quality and losing details in the original image. The method is also appealing as it is not limited to paired training data, thus it can flexibly learn rich retouching styles from unpaired data. Extensive experiments demonstrate the efficiency, retouching performance, and flexibility of our method quantitatively and qualitatively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_FlexiCurve_Flexible_Piecewise_Curves_Estimation_for_Photo_Retouching_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_FlexiCurve_Flexible_Piecewise_Curves_Estimation_for_Photo_Retouching_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Efficient Multi-Lens Bokeh Effect Rendering and Transformation",
        "author": "Tim Seizinger, Marcos V. Conde, Manuel Kolmet, Tom E. Bishop, Radu Timofte",
        "abstract": "Many advancements of mobile cameras aim to reach the visual quality of professional DSLR cameras. Great progress was shown over the last years in optimizing the sharp regions of an image and in creating virtual portrait effects with artificially blurred backgrounds. Bokeh is the aesthetic quality of the blur in out-of-focus areas of an image. This is a popular technique among professional photographers, and for this reason, a new goal in computational photography is to optimize the Bokeh effect itself. This paper introduces EBokehNet, a efficient state-of-the-art solution for Bokeh effect transformation and rendering. Our method can render Bokeh from an all-in-focus image, or transform the Bokeh of one lens to the effect of another lens without harming the sharp foreground regions in the image. Moreover we can control the shape and strength of the effect by feeding the lens properties i.e. type (Sony or Canon) and aperture, into the neural network as an additional input. Our method is a winning solution at the NTIRE 2023 Lens-to-Lens Bokeh Effect Transformation Challenge, and state-of-the-art at the EBB benchmark.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Seizinger_Efficient_Multi-Lens_Bokeh_Effect_Rendering_and_Transformation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Seizinger_Efficient_Multi-Lens_Bokeh_Effect_Rendering_and_Transformation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SS-TTA: Test-Time Adaption for Self-Supervised Denoising Methods",
        "author": "Masud An-Nur Islam Fahim, Jani Boutellier",
        "abstract": "Even though image denoising has already been studied for decades, recent progress in deep learning has provided novel and considerably better results for this classical signal reconstruction problem. One of the most signifcant advances in recent years has been relaxing the requirement of having noise-free (clean) images in the training dataset. By leveraging self-supervised learning, recent methods already reach the reconstruction quality of classical and some supervised schemes. In this paper, we propose SS-TTA, a generic test-time adaptation policy that can be applied on top of various self-supervised denoising methods. Taking a pre-trained self-supervised denoising model and a test image as input, our SS-TTA algorithm improves the denoising performance through a proposed 'inference-guided regularization' process. Based on experiments with three synthetic and three real noise datasets, SS-TTA improves the denoising results of several state-of-the-art self-supervised methods, outperforms recent test-time adaptation approaches, and shows promising performance with supervised models. Finally, SS-TTA also generalizes to cases where the testtime noise distribution differs from the noise distribution of training images",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Fahim_SS-TTA_Test-Time_Adaption_for_Self-Supervised_Denoising_Methods_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Fahim_SS-TTA_Test-Time_Adaption_for_Self-Supervised_Denoising_Methods_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FRR-Net: A Real-Time Blind Face Restoration and Relighting Network",
        "author": "Samira Pouyanfar, Sunando Sengupta, Mahmoud Mohammadi, Ebey Abraham, Brett Bloomquist, Lukas Dauterman, Anjali Parikh, Steve Lim, Eric Sommerlade",
        "abstract": "Face restoration models that mitigate low light, mixed lighting, poor camera quality conditions can benefit various applications, including video conferencing, image capture apps, among other uses. Many different models exist to address this problem. Although recent models generate impressive and high-fidelity faces, several important challenges remain, such as model efficiency, realistic texture and facial components, low-light environments, and screen illumination on the face. To tackle these challenges, we propose a simple, yet effective model called Face Restoration and Relighting Network (FRR-Net). The FRR-Net architecture includes an encoder-decoder model with a parallel distortion classifier which predicts the distortion types during training. This model is systematically scaled to balance network depth and width for better performance and efficiency trade-off. In addition, to generate the enhanced facial region, FRR-Net also utilizes a facial segmentation mask during the training, which not only helps the model performance but can also be used for further post-production uses. Furthermore, this work integrates a wide range of data degradation techniques to generate data for training to tackle both face enhancement and relighting. We demonstrate the effectiveness of our method by comparing it with several recent face restoration models. FRR-Net is computationally efficient and can perform inference at 13ms per frame on a low-powered Neural Processing Unit making it suitable for real-time face restoration applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Pouyanfar_FRR-Net_A_Real-Time_Blind_Face_Restoration_and_Relighting_Network_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Pouyanfar_FRR-Net_A_Real-Time_Blind_Face_Restoration_and_Relighting_Network_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Pyramid Ensemble Structure for High Resolution Image Shadow Removal",
        "author": "Shuhao Cui, Junshi Huang, Shuman Tian, Mingyuan Fan, Jiaqi Zhang, Li Zhu, Xiaoming Wei, Xiaolin Wei",
        "abstract": "Existing methods for shadow removal in high-resolution images may not be effective due to challenges such as the time-consuming nature of training and the loss of visual data during image cropping or resizing, highlighting the necessity for the development of more efficient methods. In this paper, we propose a novel Pyramid Ensemble Structure (PES) for High Resolution Image Shadow Removal. Our approach takes advantage of multiple scales by constructing pyramid inputs that allow for the capturing of a wide range of shadow sizes and shapes. We then train the network in pyramid stages to enhance global information processing. Furthermore, an ensemble of different shadow removal models is employed, and the maximum value is chosen to indicate the least amount of remaining shadow in the output. Experiments on both validation and testing data sets confirm the effectiveness of our method. In the Image Shadow Removal Challenge competition, our method obtained 22.36 PSNR score (1st place) and 0.70 SSIM score (2nd place) on the test sets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cui_Pyramid_Ensemble_Structure_for_High_Resolution_Image_Shadow_Removal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Cui_Pyramid_Ensemble_Structure_for_High_Resolution_Image_Shadow_Removal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Expanding Synthetic Real-World Degradations for Blind Video Super Resolution",
        "author": "Mehran Jeelani, Sadbhawna, Noshaba Cheema, Klaus Illgner-Fehns, Philipp Slusallek, Sunil Jaiswal",
        "abstract": "Video super-resolution (VSR) techniques, especially deep-learning-based algorithms, have drastically improved over the last few years and shown impressive performance on synthetic data. However, their performance on real-world video data suffers because of the complexity of real-world degradations and misaligned video frames. Since obtaining a synthetic dataset consisting of low-resolution (LR) and high-resolution (HR) frames are easier than obtaining real-world LR and HR images, in this paper, we propose synthesizing real-world degradations on synthetic training datasets. The proposed synthetic real-world degradations (SRWD) include a combination of blur, noise, downsampling, pixel binning, and image and video compression artifacts. We then propose using a random shuffling-based strategy to simulate these degradations on the training datasets and train a single end-to-end deep neural network (DNN) on the proposed larger variation of realistic synthesized training data. Our quantitative and qualitative comparative analysis shows that the proposed training strategy using diverse realistic degradations improves the performance by 7.1 % in terms of NRQM compared to RealBasicVSR and by 3.34 % compared to BSRGAN on the VideoLQ dataset. We also introduce a new dataset that contains high-resolution real-world videos that can serve as a common ground for bench-marking.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Jeelani_Expanding_Synthetic_Real-World_Degradations_for_Blind_Video_Super_Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Jeelani_Expanding_Synthetic_Real-World_Degradations_for_Blind_Video_Super_Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Quality Assessment of Video Enhancement Challenge",
        "author": "Xiaohong Liu, Xiongkuo Min, Wei Sun, Yulun Zhang , Kai Zhang, Radu Timofte, Guangtao Zhai, Yixuan Gao, Yuqin Cao, Tengchuan Kou, Yunlong Dong, Ziheng Jia , Yilin Li, Kai Zhao, Heng Cong, Hang Shi, Zhiliang Ma, Mirko Agarla, Zhiwei Huang, Hongye Liu, Ironhead Chuang, Haotian Fan, Shiqi Zhou, Yu Lai, Wenqi Wang, Haoning Wu, Chunzheng Zhu, Shiling Zhao, Hanene Brachemi Meftah, Tengfei Shi, Azadeh Mansouri",
        "abstract": "This paper reports on the NTIRE 2023 Quality Assessment of Video Enhancement Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2023. This challenge is to address a major challenge in the field of video processing, namely, video quality assessment (VQA) for enhanced videos. The challenge uses the VQA Dataset for Perceptual Video Enhancement (VDPVE), which has a total of 1211 enhanced videos, including 600 videos with color, brightness, and contrast enhancements, 310 videos with deblurring, and 301 deshaked videos. The challenge has a total of 167 registered participants. 61 participating teams submitted their prediction results during the development phase, with a total of 3168 submissions. A total of 176 submissions were submitted by 37 participating teams during the final testing phase. Finally, 19 participating teams submitted their models and fact sheets, and detailed the methods they used. Some methods have achieved better results than baseline methods, and the winning methods have demonstrated superior prediction performance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Liu_NTIRE_2023_Quality_Assessment_of_Video_Enhancement_Challenge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Liu_NTIRE_2023_Quality_Assessment_of_Video_Enhancement_Challenge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Video Quality Assessment Based on Swin Transformer With Spatio-Temporal Feature Fusion and Data Augmentation",
        "author": "Wei Wu, Shuming Hu, Pengxiang Xiao, Sibin Deng, Yilin Li, Ying Chen, Kai Li",
        "abstract": "While video enhancement has drawn significant interest and has been extensively studied by academia and industry, the corresponding research on video quality assessment (VQA) for enhanced video has not been widely addressed. Video enhancement methods normally change the relevant metrics like brightness, contrast, color, etc., leading to the fluctuation of perceptual quality and challenging the related VQA task. In this paper, we propose a novel approach for VQA task based on Swin Transformer with improved spatio-temporal feature fusion, which precisely mines the stage-wise feature concatenation and provides competitive assessment performance. In addition, we propose an efficient data augmentation strategy to improve data diversity and further enhance assessment accuracy. Experimental results demonstrate that the proposed approach achieves state-of-the-art performance on two benchmark VQA datasets, and ranks first in CVPR NTIRE 2023 Quality Assessment for Video Enhancement Challenge, which proves that the proposed approach is not only promising in VQA for enhanced video but also ubiquitous in general VQA tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Chen_Video_Quality_Assessment_Based_on_Swin_Transformer_With_Spatio-Temporal_Feature_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Chen_Video_Quality_Assessment_Based_on_Swin_Transformer_With_Spatio-Temporal_Feature_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Saliency-Aware Stereoscopic Video Retargeting",
        "author": "Hassan Imani, Md Baharul Islam, Lai-Kuan Wong",
        "abstract": "Stereo video retargeting aims to resize an image to a desired aspect ratio. The quality of retargeted videos can be significantly impacted by the stereo video's spatial, temporal, and disparity coherence, all of which can be impacted by the retargeting process. Due to the lack of a publicly accessible annotated dataset, there is little research on deep learning-based methods for stereo video retargeting. This paper proposes an unsupervised deep learning-based stereo video retargeting network. Our model first detects the salient objects and shifts and warps all objects such that it minimizes the distortion of the salient parts of the stereo frames. We use 1D convolution for shifting the salient objects and design a stereo video Transformer to assist the retargeting process. To train the network, we use the parallax attention mechanism to fuse the left and right views and feed the retargeted frames to a reconstruction module that reverses the retargeted frames to the input frames. Therefore, the network is trained in an unsupervised manner. Extensive qualitative and quantitative experiments and ablation studies on KITTI stereo 2012 and 2015 datasets demonstrate the efficiency of the proposed method over the existing state-of-the-art methods. The code is available at https://github.com/z65451/SVR/.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Imani_Saliency-Aware_Stereoscopic_Video_Retargeting_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Imani_Saliency-Aware_Stereoscopic_Video_Retargeting_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Selective Bokeh Effect Transformation",
        "author": "Juewen Peng, Zhiyu Pan, Chengxin Liu, Xianrui Luo, Huiqiang Sun, Liao Shen, Ke Xian, Zhiguo Cao",
        "abstract": "Bokeh effect transformation is a novel task in computer vision and computational photography. It aims to convert bokeh effects from one camera lens to another. To this end, we introduce a new concept of blur ratio, which represents the ratio of the blur amount of a target image to that of a source image, and propose a novel framework SBTNet based on this concept. For cat-eye simulation and lens type transformation, a two-channel coordinate map and a two-channel one-hot map are added as extra inputs. The core of the framework is a sequence of parallel FeaNets, along with a feature selection and integration strategy, which aims to transform the blur amount with arbitrary blur ratio. The effectiveness of the proposed framework is demonstrated through extensive experiments, and our solution has achieved the top LPIPS metric in NTIRE 2023 Bokeh Effect Transformation Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Peng_Selective_Bokeh_Effect_Transformation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Peng_Selective_Bokeh_Effect_Transformation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Video Colorization Challenge",
        "author": "Xiaoyang Kang, Xianhui Lin, Kai Zhang, Zheng Hui, Wangmeng Xiang, Jun-Yan He, Xiaoming Li, Peiran Ren, Xuansong Xie, Radu Timofte, Yixin Yang, Jinshan Pan, Zhongzheng Peng, Qiyan Zhang, Jiangxin Dong, Jinhui Tang, Jinjing Li, Chichen Lin, Qipei Li, Qirong Liang, Ruipeng Gang, Xiaofeng Liu, Shuang Feng, Shuai Liu, Hao Wang, Chaoyu Feng, Furui Bai, Yuqian Zhang, Guangqi Shao, Xiaotao Wang, Lei Lei, Siqi Chen, Yu Zhang, Hanning Xu, Zheyuan Liu, Zhao Zhang, Yan Luo, Zhichao Zuo",
        "abstract": "This paper reviews the video colorization challenge on the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2023. The target of this challenge is converting grayscale videos into color videos with better colorization performance and temporal consistency. The challenge consists of two tracks. For Track 1, the goal is achieving the best FID (Frechet Inception Distance) while being constrained to maintain or improve over the baseline method in terms of the temporal-consistency metric. The Color Distribution Consistency (CDC) index is used as the temporal consistency evaluation metric in this challenge. For Track 2, the target is to obtain a solution with the best CDC result while being constrained to maintain or improve over the baseline method in terms of FID. We use DeOldify-video as the baseline method for two tracks. For the final testing phase of both tracks, six teams submitted fact sheets and executable code of their solutions. This report brings together descriptions and discussions of all these solutions.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Kang_NTIRE_2023_Video_Colorization_Challenge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Kang_NTIRE_2023_Video_Colorization_Challenge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Cross-View Hierarchy Network for Stereo Image Super-Resolution",
        "author": "Wenbin Zou, Hongxia Gao, Liang Chen, Yunchen Zhang, Mingchao Jiang, Zhongxin Yu, Ming Tan",
        "abstract": "Stereo image super-resolution aims to improve the quality of high-resolution stereo image pairs by exploiting complementary information across views. To attain superior performance, many methods have prioritized designing complex modules to fuse similar information across views, yet overlooking the importance of intra-view information for high-resolution reconstruction. It also leads to problems of wrong texture in recovered images. To address this issue, we explore the interdependencies between various hierarchies from intra-view and propose a novel method, named Cross-View-Hierarchy Network for Stereo Image Super-Resolution (CVHSSR). Specifically, we design a cross-hierarchy information mining block (CHIMB) that leverages channel attention and large kernel convolution attention to extract both global and local features from the intra-view, enabling the efficient restoration of accurate texture details. Additionally, a cross-view interaction module (CVIM) is proposed to fuse similar features from different views by utilizing cross-view attention mechanisms, effectively adapting to the binocular scene. Extensive experiments demonstrate the effectiveness of our method. CVHSSR achieves the best stereo image super-resolution performance than other state-of-the-art methods while using fewer parameters. The source code and pre-trained models are available at https://github.com/AlexZou14/CVHSSR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zou_Cross-View_Hierarchy_Network_for_Stereo_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zou_Cross-View_Hierarchy_Network_for_Stereo_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Semantic Guidance Learning for High-Resolution Non-Homogeneous Dehazing",
        "author": "Hao-Hsiang Yang, I-Hsiang Chen, Chia-Hsuan Hsieh, Hua-En Chang, Yuan-Chun Chiang, Yi-Chung Chen, Zhi-Kai Huang, Wei-Ting Chen, Sy-Yen Kuo",
        "abstract": "High-resolution non-homogeneous dehazing aims to generate a clear image from a 4000 x 6000 image with non-homogeneous haze. To the best of our knowledge, this task is a new challenge that was not addressed in the previous literature. To address this issue, we propose semantic-guided loss functions for high-resolution non-homogeneous dehazing. We find semantic information contains strong texture and color prior. Thus, we proposed to adopt the pre-trained model to generate the semantic mask to guide the neural network during the training phase. On the other hand, to handle the non-homogeneous dehazing process in the high-resolution scenario, we adjust the kernel size of the model to increase the receptive field. Furthermore, to deal with the different image sizes during the training and the testing phase, several post-processing methods are applied to improve the high-resolution non-homogeneous dehazing. Several experiments performed on challenging benchmark show that the proposed model achieves competitive performance in the NTIRE 2023 HR NonHomogeneous Dehazing Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Yang_Semantic_Guidance_Learning_for_High-Resolution_Non-Homogeneous_Dehazing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Yang_Semantic_Guidance_Learning_for_High-Resolution_Non-Homogeneous_Dehazing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Lightweight Real-Time Image Super-Resolution Network for 4K Images",
        "author": "Ganzorig Gankhuyag, Kihwan Yoon, Jinman Park, Haeng Seon Son, Kyoungwon Min",
        "abstract": "Single-image super-resolution technology has become a topic of extensive research in various applications, aiming to enhance the quality and resolution of degraded images obtained from low-resolution sensors. However, most existing studies on single-image super-resolution have primarily focused on developing deep learning networks operating on high-performance graphics processing units. Therefore, this study proposes a lightweight real-time image super-resolution network for 4K images. Furthermore, we applied a reparameterization method to improve the network performance without incurring additional computational costs. The experimental results demonstrate that the proposed network achieves a PSNR of 30.15 dB and an inference time of 4.75 ms on an RTX 3090Ti device, as evaluated on the NTIRE 2023 Real-Time Super-Resolution validation scale X3 dataset. The code is available at https://github.com/Ganzooo/LRSRN.git.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Gankhuyag_Lightweight_Real-Time_Image_Super-Resolution_Network_for_4K_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Gankhuyag_Lightweight_Real-Time_Image_Super-Resolution_Network_for_4K_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Challenge on Light Field Image Super-Resolution: Dataset, Methods and Results",
        "author": "Yingqian Wang, Longguang Wang, Zhengyu Liang, Jungang Yang, Radu Timofte, Yulan Guo, Kai Jin, Zeqiang Wei, Angulia Yang, Sha Guo, Mingzhi Gao, Xiuzhuang Zhou, Vinh Van Duong, Thuc Nguyen Huu, Jonghoon Yim, Byeungwoo Jeon, Yutong Liu, Zhen Cheng, Zeyu Xiao, Ruikang Xu, Zhiwei Xiong, Gaosheng Liu, Manchang Jin, Huanjing Yue, Jingyu Yang, Chen Gao, Shuo Zhang, Song Chang, Youfang Lin, Wentao Chao, Xuechun Wang, Guanghui Wang, Fuqing Duan, Wang Xia, Yan Wang, Peiqi Xia, Shunzhou Wang, Yao Lu, Ruixuan Cong, Hao Sheng, Da Yang, Rongshan Chen, Sizhe Wang, Zhenglong Cui, Yilei Chen, Yongjie Lu, Dongjun Cai, Ping An, Ahmed Salem, Hatem Ibrahem, Bilel Yagoub, Hyun-Soo Kang, Zekai Zeng, Heng Wu",
        "abstract": "In this report, we summarize the first NTIRE challenge on light field (LF) image super-resolution (SR), which aims at super-resolving LF images under the standard bicubic degradation with a magnification factor of 4. This challenge develops a new LF dataset called NTIRE-2023 for validation and test, and provides a toolbox called BasicLFSR to facilitate model development. Compared with single image SR, the major challenge of LF image SR lies in how to exploit complementary angular information from plenty of views with varying disparities. In total, 148 participants have registered the challenge, and 11 teams have successfully submitted results with PSNR scores higher than the baseline method LF-InterNet. These newly developed methods have set new state-of-the-art in LF image SR, e.g., the winning method achieves around 1 dB PSNR improvement over the existing state-of-the-art method DistgSSR. We report the solutions proposed by the participants, and summarize their common trends and useful tricks. We hope this challenge can stimulate future research and inspire new ideas in LF image SR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Wang_NTIRE_2023_Challenge_on_Light_Field_Image_Super-Resolution_Dataset_Methods_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Wang_NTIRE_2023_Challenge_on_Light_Field_Image_Super-Resolution_Dataset_Methods_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NAFBET: Bokeh Effect Transformation With Parameter Analysis Block Based on NAFNet",
        "author": "Xiangyu Kong, Fan Wang, Dafeng Zhang, Jinlong Wu, Zikun Liu",
        "abstract": "Bokeh effect transformation(BET) aims to transform the bokeh effect of one lens to another lens without harming the sharp foreground regions in the image. Recent studies have shown remarkable success in bokeh effect rendering. However, unlike the traditional bokeh effect rendering task, the BET task needs to transform the image into the bokeh effect of the specified lens. The existing bokeh rendering method is invalid or inefficient for BET, because each pair of lens needs to independently build different model. To address this limitation, we propose NAFBET, a scalable approach than can perform bokeh rendering for multiple lens using only a single model. NAFBET is based on the structure of the image restoration model NAFNet and expands it by adding the source and target parameter analysis block(PAB) to adapt to the BET task. This block can be very convenient to apply in UNet-based model, which can greatly improve BET performance. We did a lot of experiments to prove the effectiveness of our method. In particular, NAFBET won the 1st place in the NTIRE 2023 Bokeh effect transformation Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Kong_NAFBET_Bokeh_Effect_Transformation_With_Parameter_Analysis_Block_Based_on_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Kong_NAFBET_Bokeh_Effect_Transformation_With_Parameter_Analysis_Block_Based_on_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Single Residual Network With ESA Modules and Distillation",
        "author": "Yucong Wang, Minjie Cai",
        "abstract": "Although there are many methods based on deep learning that have superior performance on single image super-resolution (SISR), it is difficult to run in real time on devices with limited computing power. Some recent studies have found that simply relying on reducing parameters or reducing the theoretical FLOPs of the model does not speed up the inference time of the network in a practical sense. Actual speed on the device is probably a better measure than FLOPs. In this work, we propose a new single residual network (SRN). On the one hand, we try to introduce and optimize an attention mechanism module to improve the performance of the network with a relatively small speed loss. On the other hand, we find that residuals in residual blocks do not have a positive impact on networks with adjusted ESA. Therefore, the residual of the network residual block is removed, which not only improves the speed of the network, but also improves the performance of the network. Finally, we reduced the number of channels and the number of residual blocks of the classic model EDSR, and removed the last convolution before the long residual. We set this tuned EDSR as the teacher model and our newly proposed SRN as the student model. Under the joint effect of the original loss and the distillation loss, the performance of the network can be improved without losing the inference time. Combining the above strategies, our proposed model runs much faster than similarly performing models. As an example, we built a Fast and Efficient Network (SRN) and its small version SRN-S, which run 30%-37% faster than the state-of-the-art EISR model: a paper champion RLFN. Furthermore, the shallow version of SRN-S achieves the second-shortest inference time as well as the second-smallest number of activations in the NTIRE2023 challenge. Code will be available at https://github.com/wnxbwyc/SRN.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Wang_A_Single_Residual_Network_With_ESA_Modules_and_Distillation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Wang_A_Single_Residual_Network_With_ESA_Modules_and_Distillation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Challenge on Image Denoising: Methods and Results",
        "author": "Yawei Li, Yulun Zhang, Radu Timofte, Luc Van Gool, Zhijun Tu, Kunpeng Du, Hailing Wang, Hanting Chen, Wei Li, Xiaofei Wang, Jie Hu, Yunhe Wang, Xiangyu Kong, Jinlong Wu, Dafeng Zhang, Jianxing Zhang, Shuai Liu, Furui Bai, Chaoyu Feng, Hao Wang, Yuqian Zhang, Guangqi Shao, Xiaotao Wang, Lei Lei, Rongjian Xu, Zhilu Zhang, Yunjin Chen, Dongwei Ren, Wangmeng Zuo, Qi Wu, Mingyan Han, Shen Cheng, Haipeng Li, Ting Jiang, Chengzhi Jiang, Xinpeng Li, Jinting Luo, Wenjie Lin, Lei Yu, Haoqiang Fan, Shuaicheng Liu, Aditya Arora, Syed Waqas Zamir, Javier Vazquez-Corral, Konstantinos G. Derpanis, Michael S. Brown, Hao Li, Zhihao Zhao, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Bo Yang, Jingxiang Chen, Chenghua Li, Xi Zhang, Zhao Zhang, Jiahuan Ren, Zhicheng Ji, Kang Miao, Suiyi Zhao, Huan Zheng, YanYan Wei, Kangliang Liu, Xiangcheng Du, Sijie Liu, Yingbin Zheng, Xingjiao Wu, Cheng Jin, Rajeev Irny, Sriharsha Koundinya, Vighnesh Kamath, Gaurav Khandelwal, Sunder Ali Khowaja, Jiseok Yoon, Ik Hyun Lee, Shijie Chen, Chengqiang Zhao, Huabin Yang, Zhongjian Zhang, Junjia Huang, Yanru Zhang",
        "abstract": "This paper reviews the NTIRE 2023 challenge on image denoising (sigma = 50) with a focus on the proposed solutions and results. The aim is to obtain a network design capable to produce high-quality results with the best performance measured by PSNR for image denoising. Independent additive white Gaussian noise (AWGN) is assumed and the noise level is 50. The challenge had 225 registered participants, and 16 teams made valid submissions. They gauge the state-of-the-art for image denoising.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Image_Denoising_Methods_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_NTIRE_2023_Challenge_on_Image_Denoising_Methods_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Challenge on Stereo Image Super-Resolution: Methods and Results",
        "author": "Longguang Wang, Yulan Guo, Yingqian Wang, Juncheng Li, Shuhang Gu, Radu Timofte, Ming Cheng, Haoyu Ma, Qiufang Ma, Xiaopeng Sun, Shijie Zhao, Xuhan Sheng, Yukan Ding, Ming Sun, Xing Wen, Dafeng Zhang, Jia Li, Fan Wang, Zheng Xie, Zongyao He, Zidian Qiu, Zilin Pan, Zhihao Zhan, Xingyuan Xian, Zhi Jin, Yuanbo Zhou, Wei Deng, Ruofeng Nie, Jiajun Zhang, Qinquan Gao, Tong Tong, Kexin Zhang, Junpei Zhang, Rui Peng, Yanbiao Ma, Licheng Jiao, Haoran Bai, Lingshun Kong, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Pu Cao, Tianrui Huang, Lu Yang, Qing Song, Bingxin Chen, Chunhua He, Meiyun Chen, Zijie Guo, Shaojuan Luo, Chengzhi Cao, Kunyu Wang, Fanrui Zhang, Qiang Zhang, Nancy Mehta, Subrahmanyam Murala, Akshay Dudhane, Yujin Wang, Lingen Li, Garas Gendy, Nabil Sabor, Jingchao Hou, Guanghui He, Junyang Chen, Hao Li, Yukai Shi, Zhijing Yang, Wenbin Zou, Yunchen Zhang, Mingchao Jiang, Zhongxin Yu, Ming Tan, Hongxia Gao, Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, Thomas B. Sch\u00f6n, Jingxiang Chen, Bo Yang, XiSheryl Zhang, Chenghua Li, Weijun Yuan, Zhan Li, Ruting Deng, Jintao Zeng, Pulkit Mahajan, Sahaj Mistry, Shreyas Chatterjee, Vinit Jakhetiya, Badri Subudhi, Sunil Jaiswal, Zhao Zhang, Huan Zheng, Suiyi Zhao, Yangcheng Gao, Yanyan Wei, Bo Wang, Gen Li, Aijin Li, Lei Sun, Ke Chen, Congling Tang, Yunzhe Li, Jun Chen, Yuan-Chun Chiang, Yi-Chung Chen, Zhi-Kai Huang, Hao-Hsiang Yang, I-Hsiang Chen, Sy-Yen Kuo, Yiheng Wang, Gang Zhu, Xingyi Yang, Songhua Liu, Yongcheng Jing, Xingyu Hu, Jianwen Song, Changming Sun, Arcot Sowmya, Seung Ho Park, Xiaoyan Lei, Jingchao Wang, Chenbo Zhai, Yufei Zhang, Weifeng Cao, Wenlong Zhang",
        "abstract": "In this paper, we summarize the 2nd NTIRE challenge on stereo image super-resolution (SR) with a focus on new solutions and results. The task of the challenge is to super-resolve a low-resolution stereo image pair to a high-resolution one with a magnification factor of x4. Compared with single image SR, the major challenge of this challenge lies in how to exploit additional information in another viewpoint and how to maintain stereo consistency in the results. This challenge has 3 tracks, including one track on distortion (e.g., PSNR) and bicubic degradation, one track on perceptual quality (e.g., LPIPS) and bicubic degradation, as well as another track on real degradations. In total, 175, 93, and 103 participants were successfully registered for each track, respectively. In the test phase, 21, 17, and 12 teams successfully submitted results with PSNR (RGB) scores better than the baseline. This challenge establishes a new benchmark for stereo image SR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Wang_NTIRE_2023_Challenge_on_Stereo_Image_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Wang_NTIRE_2023_Challenge_on_Stereo_Image_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SCONE-GAN: Semantic Contrastive Learning-Based Generative Adversarial Network for an End-to-End Image Translation",
        "author": "Iman Abbasnejad, Fabio Zambetta, Flora Salim, Timothy Wiley, Jeffrey Chan, Russell Gallagher, Ehsan Abbasnejad",
        "abstract": "SCONE-GAN presents an end-to-end image translation, which is shown to be effective for learning to generate realistic and diverse scenery images. Most current image-to-image translation approaches are devised as two mappings: a translation from the source to target domain and another to represent its inverse. While successful in many applications, these approaches may suffer from generating trivial solutions with limited diversity. That is because these methods learn more frequent associations rather than the scene structures. To mitigate the problem, we propose SCONE-GAN that utilises graph convolutional networks to learn the objects dependencies, maintain the image structure and preserve its semantics while transferring images into the target domain. For more realistic and diverse image generation we introduce style reference image. We enforce the model to maximize the mutual information between the style image and output. The proposed method explicitly maximizes the mutual information between the related patches, thus encouraging the generator to produce more diverse images. We validate the proposed algorithm for image-to-image translation and stylizing outdoor images. Both qualitative and quantitative results demonstrate the effectiveness of our approach on four dataset.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Abbasnejad_SCONE-GAN_Semantic_Contrastive_Learning-Based_Generative_Adversarial_Network_for_an_End-to-End_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Abbasnejad_SCONE-GAN_Semantic_Contrastive_Learning-Based_Generative_Adversarial_Network_for_an_End-to-End_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Streamlined Global and Local Features Combinator (SGLC) for High Resolution Image Dehazing",
        "author": "Bilel Benjdira, Anas M. Ali, Anis Koubaa",
        "abstract": "Image Dehazing aims to remove atmospheric fog or haze from an image. Although the Dehazing models have evolved a lot in recent years, few have precisely tackled the problem of High-Resolution hazy images. For this kind of image, the model needs to work on a downscaled version of the image or on cropped patches from it. In both cases, the accuracy will drop. This is primarily due to the inherent failure to combine global and local features when the image size increases. The Dehazing model requires global features to understand the general scene peculiarities and the local features to work better with fine and pixel details. In this study, we propose the Streamlined Global and Local Features Combinator (SGLC) to solve these issues and to optimize the application of any Dehazing model to High-Resolution images. The SGLC contains two successive blocks. The first is the Global Features Generator (GFG) which generates the first version of the Dehazed image containing strong global features. The second block is the Local Features Enhancer (LFE) which improves the local feature details inside the previously generated image. When tested on the Uformer architecture for Dehazing, SGLC increased the PSNR metric by a significant margin. Any other model can be incorporated inside the SGLC process to improve its efficiency on High-Resolution input data.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Benjdira_Streamlined_Global_and_Local_Features_Combinator_SGLC_for_High_Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Benjdira_Streamlined_Global_and_Local_Features_Combinator_SGLC_for_High_Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "RB-Dust - A Reference-Based Dataset for Vision-Based Dust Removal",
        "author": "Peter Buckel, Timo Oksanen, Thomas Dietm\u00fcller",
        "abstract": "Dust in the agricultural landscape is a significant challenge and influences, for example, the environmental perception of autonomous agricultural machines. Image enhancement algorithms can be used to reduce dust. However, these require dusty and dust-free images of the same environment for validation. In fact, to date, there is no dataset that we are aware of that addresses this issue. Therefore, we present the agriscapes RB-Dust dataset, which is named after its purpose of reference-based dust removal. It is not possible to take pictures from the cabin during tillage, as this would cause shifts in the images. Because of this, we built a setup from which it is possible to take images from a stationary position close to the passing tractor. The test setup was based on a half-sided gate through which the tractor could drive. The field tests were carried out on a farm in Bavaria, Germany, during tillage. During the field tests, other parameters such as soil moisture and wind speed were controlled, as these significantly affect dust development. We validated our dataset with contrast enhancement and image dehazing algorithms and analyzed the generalizability from recordings from the moving tractor. Finally, we demonstrate the application of dust removal based on a high-level vision task, such as person classification. Our empirical study confirms the validity of RB-Dust for vision-based dust removal in agriculture.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Buckel_RB-Dust_-_A_Reference-Based_Dataset_for_Vision-Based_Dust_Removal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Buckel_RB-Dust_-_A_Reference-Based_Dataset_for_Vision-Based_Dust_Removal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "RTTLC: Video Colorization With Restored Transformer and Test-Time Local Converter",
        "author": "Jinjing Li, Qirong Liang, Qipei Li, Ruipeng Gang, Ji Fang, Chichen Lin, Shuang Feng, Xiaofeng Liu",
        "abstract": "Video colorization is a highly challenging and ill-posed problem that suffers from severe flickering artifacts and color distribution inconsistency. To resolve these issues, we propose a Restored Transformer and Test-time Local Converter network(RTTLC). Firstly, we introduce a Bi-directional Recurrent Block and a Learnable Guided Mask to our network. This leverages hidden knowledge from adjacent frames that include rich information about occlusion, resulting in significant enhancements in visual quality. Secondly, we integrate a Restored Transformer that enables the network to utilize more spatial contextual information and capture multi-scale information more accurately. Thirdly, during inference, we utilize the Test-time Local Converter(TLC) strategy to alleviate distribution shift and enhance the performance of the model. Experimental results show good performance of FID and CDC. Notably, RTTLC achieves second prize in both tracks of the NTIRE23 video colorization challenges.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_RTTLC_Video_Colorization_With_Restored_Transformer_and_Test-Time_Local_Converter_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_RTTLC_Video_Colorization_With_Restored_Transformer_and_Test-Time_Local_Converter_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Refusion: Enabling Large-Size Realistic Image Restoration With Latent-Space Diffusion Models",
        "author": "Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, Thomas B. Sch\u00f6n",
        "abstract": "This work aims to improve the applicability of diffusion models in realistic image restoration. Specifically, we enhance the diffusion model in several aspects such as network architecture, noise level, denoising steps, training image size, and optimizer/scheduler. We show that tuning these hyperparameters allows us to achieve better performance on both distortion and perceptual scores. We also propose a U-Net based latent diffusion model which performs diffusion in a low-resolution latent space while preserving high-resolution information from the original input for the decoding process. Compared to the previous latent-diffusion model which trains a VAE-GAN to compress the image, our proposed U-Net compression strategy is significantly more stable and can recover highly accurate images without relying on adversarial optimization. Importantly, these modifications allow us to apply diffusion models to various image restoration tasks, including real-world shadow removal, HR non-homogeneous dehazing, stereo super-resolution, and bokeh effect transformation. By simply replacing the datasets and slightly changing the noise network, our model, named Refusion, is able to deal with large-size images (e.g., 6000 x 4000 x 3 in HR dehazing) and produces good results on all the above restoration problems. Our Refusion achieves the best perceptual performance in the NTIRE 2023 Image Shadow Removal Challenge and wins 2nd place overall.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Luo_Refusion_Enabling_Large-Size_Realistic_Image_Restoration_With_Latent-Space_Diffusion_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Luo_Refusion_Enabling_Large-Size_Realistic_Image_Restoration_With_Latent-Space_Diffusion_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Large Kernel Distillation Network for Efficient Single Image Super-Resolution",
        "author": "Chengxing Xie, Xiaoming Zhang, Linze Li, Haiteng Meng, Tianlin Zhang, Tianrui Li, Xiaole Zhao",
        "abstract": "Efficient and lightweight single-image super-resolution (SISR) has achieved remarkable performance in recent years. One effective approach is the use of large kernel designs, which have been shown to improve the performance of SISR models while reducing their computational requirements. However, current state-of-the-art (SOTA) models still face problems such as high computational costs. To address these issues, we propose the Large Kernel Distillation Network (LKDN) in this paper. Our approach simplifies the model structure and introduces more efficient attention modules to reduce computational costs while also improving performance. Specifically, we employ the re-parameterization technique to enhance model performance without adding extra cost. We also introduce a new optimizer from other tasks to SISR, which improves training speed and performance. Our experimental results demonstrate that LKDN outperforms existing lightweight SR methods and achieves SOTA performance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Xie_Large_Kernel_Distillation_Network_for_Efficient_Single_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Xie_Large_Kernel_Distillation_Network_for_Efficient_Single_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DistgEPIT: Enhanced Disparity Learning for Light Field Image Super-Resolution",
        "author": "Kai Jin, Angulia Yang, Zeqiang Wei, Sha Guo, Mingzhi Gao, Xiuzhuang Zhou",
        "abstract": "Light Field (LF) cameras capture rich information in 4D LF images by recording both intensity and angular directions, making it crucial to learn the inherent spatial-angular correlation in low-resolution (LR) images for superior results. Despite impressive progress made by several CNN-based deep methods and pioneering Transformer-based methods for LF image super resolution (SR), most of them fail to fully leverage the LF spatial-angular correlation and tend to perform poorly in scenes with varying disparities. In this paper, we propose a hybrid method called DistgEPIT that implements an enhanced disparity learning mechanism with both convolution-based and transformer-based modules. It enables the capture of angular correlation, refinement of adjacent disparities, and extraction of essential spatial features. Additionally, we introduce a Position-Sensitive Windowing (PSW) strategy to maintain consistency of disparity between the training and inference stages, which yields an average PSNR gain of 0.2 dB by replacing the traditional padding and windowing method. Our extensive experimental comparison with necessary ablation studies demonstrates the effectiveness of our proposed method, which ranked 1st place in the NITRE2023 LF image SR challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Jin_DistgEPIT_Enhanced_Disparity_Learning_for_Light_Field_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Jin_DistgEPIT_Enhanced_Disparity_Learning_for_Light_Field_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Simple Transformer-Style Network for Lightweight Image Super-Resolution",
        "author": "Garas Gendy, Nabil Sabor, Jingchao Hou, Guanghui He",
        "abstract": "The task of single image super resolution (SISR) has taken much attention in the last few years due to the wide range of real-world applications. However, most of the recently developed methods are computationally expensive and need much more memory. To solve this issue, we propose a simple Transformer-style network (STSN) for the image super resolution (SR) task. The idea of this method is based on using convolutional modulation (Conv2Former), which is a very simple block with a linearly compared to quadratically as in Transformers. This Conv2Former is simplified the self-attention mechanism based on utilizing only convolutions and Hadamard product. Also, the original Conv2Former is further improved to be able to extract local features, which is helpful for SR task. Based on this Conv2Former and multi-layer perceptron (MLP), we propose a convolutional modulation block (Conv2FormerB) which is similar to the Transformers block. Based on this Conv2FormerB, 3 x 3 convolution and enhanced spatial attention (ESA) block, an STSN is designed for the SISR task. This STSN achieved good results in multiple SR benchmarks. Finally, our STSN model attained 5.6 x faster run time compared to LWSwinIR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Gendy_A_Simple_Transformer-Style_Network_for_Lightweight_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Gendy_A_Simple_Transformer-Style_Network_for_Lightweight_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "BokehOrNot: Transforming Bokeh Effect With Image Transformer and Lens Metadata Embedding",
        "author": "Zhihao Yang, Wenyi Lian, Siyuan Lai",
        "abstract": "Bokeh effect is an optical phenomenon that offers a pleasant visual experience, typically generated by high-end cameras with wide aperture lenses. The task of bokeh effect transformation aims to produce a desired effect in one set of lenses and apertures based on another combination. Current models are limited in their ability to render a specific set of bokeh effects, primarily transformations from sharp to blur. In this paper, we propose a novel universal method for embedding lens metadata into the model and introducing a loss calculation method using alpha masks from the newly released Bokeh Effect Transformation Dataset (BETD) [3]. Based on the above techniques, we propose the BokehOrNot model, which is capable of producing both blur-to-sharp and sharp-to-blur bokeh effect with various combinations of lenses and aperture sizes. Our proposed model outperforms current leading bokeh rendering and image restoration models and renders visually natural bokeh effects. Our code is available at: https://github. com/indicator0/bokehornot.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Yang_BokehOrNot_Transforming_Bokeh_Effect_With_Image_Transformer_and_Lens_Metadata_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Yang_BokehOrNot_Transforming_Bokeh_Effect_With_Image_Transformer_and_Lens_Metadata_CVPRW_2023_paper.pdf"
    },
    {
        "title": "AsConvSR: Fast and Lightweight Super-Resolution Network With Assembled Convolutions",
        "author": "Jiaming Guo, Xueyi Zou, Yuyi Chen, Yi Liu, Jia Hao, Jianzhuang Liu, Youliang Yan",
        "abstract": "In recent years, videos and images in 720p (HD), 1080p (FHD) and 4K (UHD) resolution have become more popular for display devices such as TVs, mobile phones and VR. However, these high resolution images cannot achieve the expected visual effect due to the limitation of the internet bandwidth, and bring a great challenge for super-resolution networks to achieve real-time performance. Following this challenge, we explore multiple efficient network designs, such as pixel-unshuffle, repeat upscaling, and local skip connection removal, and propose a fast and lightweight super-resolution network. Furthermore, by analyzing the applications of the idea of divide-and-conquer in super-resolution, we propose assembled convolutions which can adapt convolution kernels according to the input features. Experiments suggest that our method outperforms all the state-of-the-art efficient super-resolution models, and achieves optimal results in terms of runtime and quality. In addition, our method also wins the first place in NTIRE 2023 Real-Time Super-Resolution - Track 1 (x2). The code will be available at https://gitee.com/mindspore/models /tree/master/research/cv/AsConvSR",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Guo_AsConvSR_Fast_and_Lightweight_Super-Resolution_Network_With_Assembled_Convolutions_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Guo_AsConvSR_Fast_and_Lightweight_Super-Resolution_Network_With_Assembled_Convolutions_CVPRW_2023_paper.pdf"
    },
    {
        "title": "VDPVE: VQA Dataset for Perceptual Video Enhancement",
        "author": "Yixuan Gao, Yuqin Cao, Tengchuan Kou, Wei Sun, Yunlong Dong, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai",
        "abstract": "Recently, many video enhancement methods have been proposed to improve video quality from different aspects such as color, brightness, contrast, and stability. Therefore, how to evaluate the quality of the enhanced video in a way consistent with human visual perception is an important research topic. However, most video quality assessment methods mainly calculate video quality by estimating the distortion degrees of videos from an overall perspective. Few researchers have specifically proposed a video quality assessment method for video enhancement, and there is also no comprehensive video quality assessment dataset available in public. Therefore, we construct a Video quality assessment dataset for Perceptual Video Enhancement (VDPVE) in this paper. The VDPVE has 1211 videos with different enhancements, which can be divided into three sub-datasets: the first sub-dataset has 600 videos with color, brightness, and contrast enhancements; the second sub-dataset has 310 videos with deblurring; and the third sub-dataset has 301 deshaked videos. We invited 21 subjects (20 valid subjects) to rate all enhanced videos in the VDPVE. After normalizing and averaging the subjective opinion scores, the mean opinion score of each video can be obtained. Furthermore, we split the VDPVE into a training set, a validation set, and a test set, and verify the performance of several state-of-the-art video quality assessment methods on the test set of the VDPVE.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Gao_VDPVE_VQA_Dataset_for_Perceptual_Video_Enhancement_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Gao_VDPVE_VQA_Dataset_for_Perceptual_Video_Enhancement_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Challenge on Efficient Super-Resolution: Methods and Results",
        "author": "Yawei Li, Yulun Zhang, Radu Timofte, Luc Van Gool, Lei Yu, Youwei Li, Xinpeng Li, Ting Jiang, Qi Wu, Mingyan Han, Wenjie Lin, Chengzhi Jiang, Jinting Luo, Haoqiang Fan, Shuaicheng Liu, Yucong Wang, Minjie Cai, Mingxi Li, Yuhang Zhang, Xian-jun Fan, Yankai Sheng, Yanyu Mao, Nihao Zhang, Qian Wang, Mingjun Zheng, Long Sun, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Zhongbao Yang, Yan Wang, Erlin Pan, Qixuan Cai, Xinan Dai, Magauiya Zhussip, Nikolay Kalyazin, Dmitry Vyal, Xueyi Zou, Youliang Yan, Heaseo Chung, Jin Zhang, Gaocheng Yu, Feng Zhang, Hongbin Wang, Bohao Liao, Zhibo Du, Yu-liang Wu, Gege Shi, Long Peng, Yang Wang, Yang Cao, Zhengjun Zha, Zhi-Kai Huang, Yi-Chung Chen, Yuan-Chun Chiang, Hao-Hsiang Yang, Wei-Ting Chen, Hua-En Chang, I-Hsiang Chen, Chia-Hsuan Hsieh, Sy-Yen Kuo, Xin Liu, Qian Wang, Jiahao Pan, Hongyuan Yu, Weichen Yu, Lin Ge, Jiahua Dong, Yajun Zou, Zhuoyuan Wu, Binnan Han, Xiaolin Zhang, Heng Zhang, Xuanwu Yin, Kunlong Zuo, Weijian Deng, Hongjie Yuan, Zengtong Lu, Mingyu Ouyang, Wenzhuo Ma, Nian Liu, Hanyou Zheng, Yuantong Zhang, Junxi Zhang, Zhenzhong Chen, Garas Gendy, Nabil Sabor, Jingchao Hou, Guanghui He, Yurui Zhu, Xi Wang, Xueyang Fu, Zheng-Jun Zha, Daheng Yin, Mengyang Liu, Baijun Chen, Ao Li, Lei Luo, Kangjun Jin, Ce Zhu, Xiaoming Zhang, Chengxing Xie, Linze Li, Haiteng Meng, Tianlin Zhang, Tianrui Li, Xiaole Zhao, Zhao Zhang, Baiang Li, Huan Zheng, Suiyi Zhao, Yangcheng Gao, Jiahuan Ren, Kang Hu, Jingpeng Shi, Zhijian Wu, Dingjiang Huang, Jinchen Zhu, Hui Li, Qianru Xv, Tianle Liu, Shizhuang Weng, Gang Wu, Junpeng Jiang, Xianming Liu, Junjun Jiang, Mingjian Zhang, Jing Hu, Chengxu Wu, Qinrui Fan, Chengming Feng, Ziwei Luo, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang",
        "abstract": "This paper reviews the NTIRE 2023 challenge on efficient single-image super-resolution with a focus on the proposed solutions and results. The aim of this challenge is to devise a network that reduces one or several aspects such as runtime, parameters, FLOPs, activations, memory footprint, and depth of RFDN while at least maintaining the PSNR of 29.00dB on DIV2K validation datasets. The challenge had 272 registered participants, and 35 teams made valid submissions. They gauge the state-of-the-art for efficient single-image super-resolution.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_NTIRE_2023_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Attention Retractable Frequency Fusion Transformer for Image Super Resolution",
        "author": "Qiang Zhu, Pengfei Li, Qianhui Li",
        "abstract": "Transformer-based image super-resolution (SR) has offered promising performance gains over the convolutional neural network-based one due to the adoption of parameter-independent global interactions. However, the existing Transformer-based methods are limited to obtaining enough global information due to the use of self-attention within non-overlapping windows, which restricts the receptive fields. To address this issue, we construct an effective image SR model based on the attention retractable frequency Transformer with the proposed spatial-frequency fusion block. In our method, the spatial-frequency fusion block is designed to strengthen the representation ability of the Transformer and extend the receptive field to the whole image to improve the quality of SR results. Furthermore, a progressive training strategy is proposed to use image patches with different sizes to train our SR model to further improve the SR performance. The experimental results demonstrate that our proposed method outperforms the state-of-the-art methods over various benchmark datasets, both objectively and subjectively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zhu_Attention_Retractable_Frequency_Fusion_Transformer_for_Image_Super_Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhu_Attention_Retractable_Frequency_Fusion_Transformer_for_Image_Super_Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "High-Perceptual Quality JPEG Decoding via Posterior Sampling",
        "author": "Sean Man, Guy Ohayon, Theo Adrai, Michael Elad",
        "abstract": "JPEG is arguably the most popular image coding format, achieving high compression ratios via lossy quantization that may create visual artifacts degradation. Numerous attempts to remove these artifacts were conceived over the years, and common to most of these is the use of deterministic post-processing algorithms that optimize some distortion measure (e.g., PSNR, SSIM). In this paper we propose a different paradigm for JPEG artifact correction: Our method is stochastic, and the objective we target is high perceptual quality -- striving to obtain sharp, detailed and visually pleasing reconstructed images, while being consistent with the compressed input. These goals are achieved by training a stochastic conditional generator (conditioned on the compressed input), accompanied by a theoretically well-founded loss term, resulting in a sampler from the posterior distribution. Our solution offers a diverse set of plausible and fast reconstructions for a given input with perfect consistency. We demonstrate our scheme's unique properties and its superiority to a variety of alternative methods on the FFHQ and ImageNet datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Man_High-Perceptual_Quality_JPEG_Decoding_via_Posterior_Sampling_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Man_High-Perceptual_Quality_JPEG_Decoding_via_Posterior_Sampling_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer",
        "author": "Yangyi Liu, Huan Liu, Liangyan Li, Zijun Wu, Jun Chen",
        "abstract": "Recent years have witnessed an increased interest in image dehazing. Many deep learning methods have been proposed to tackle this challenge, and have made significant accomplishments dealing with homogeneous haze. However, these solutions cannot maintain comparable performance when they are applied to images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE challenges. One of the reasons for such failures is that non-homogeneous haze does not obey one of the assumptions that is required for modeling homogeneous haze. In addition, a large number of pairs of non-homogeneous hazy image and the clean counterpart is required using traditional end-to-end training approaches, while NH-HAZE23 dataset is of limited quantities. Although it is possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous dehazing datasets, we observe that it is necessary to design a proper data-preprocessing approach that reduces the distribution gaps between the target dataset and the augmented one. This finding indeed aligns with the essence of data-centric AI. With a novel network architecture and a principled data- preprocessing approach that systematically enhances data quality, we present an innovative dehazing method. Specifically, we apply RGB-channel-wise transformations on the augmented datasets, and incorporate the state-of-the-art transformers as the backbone in the two-branch framework. We conduct extensive experiments and ablation study to demonstrate the effectiveness of our proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Liu_A_Data-Centric_Solution_to_NonHomogeneous_Dehazing_via_Vision_Transformer_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Liu_A_Data-Centric_Solution_to_NonHomogeneous_Dehazing_via_Vision_Transformer_CVPRW_2023_paper.pdf"
    },
    {
        "title": "WSRD: A Novel Benchmark for High Resolution Image Shadow Removal",
        "author": "Florin-Alexandru Vasluianu, Tim Seizinger, Radu Timofte",
        "abstract": "Shadow removal is an important computer vision task, whose aim is to successfully detect the shadow affected area appearing through light occlussion, followed by a photo-realistic restoration of the image contents, textures, and details. Following decades of research, a multitude of hand-crafted restoration techniques were proposed, following different observations on shadow formation models, with scenes altered in particular conditions. However, the increased popularity of deep learning based solutions enabled a significant step forward for the shadow removal solutions, both in terms of reconstruction fidelity and perceptual properties. However, the publicly available datasets remain focused around a particularly low complexity setup, with a low variety of light occluders and affected backgrounds, and with limited representation for more complex light interactions and complex shadow patterns. Shadow removal is an important computer vision task, whose aim is to successfully detect the shadow affected area appearing through light occlussion, followed by a photo-realistic restoration of the affected image contents, textures, and details. After decades of research, a multitude of hand-crafted restoration techniques were proposed, following different observations on shadow formation models, with scenes altered in particular conditions. However, the increased popularity of deep learning based solutions enabled a significant step forward for the shadow removal solutions, both in terms of reconstruction fidelity and perceptual properties. However, the publicly available datasets remain focused around a particularly low complexity setup, with a low variety of light occluders and affected backgrounds, and with limited representation for more complex light interactions and complex shadow patterns. In this work, we propose WSRD, a novel benchmark for high resolution image shadow removal, characterized by a large variety in terms or represented objects, backgrounds and light occluders. We study more complex interactions, combining self shadows with externally casted shadows, to further extend the study of the phenomenon, its factors and effects. To prove WSRD as a relevant benchmark, we propose DNSR, a novel shadow removal method, comparing the results on WSRD with the performance level observed on other well-established benchmarks like ISTD and ISTD+. We validate our approach comparing with existing state-of-the-art (SOTA) methods, improving both in reconstruction fidelity and perceptual properties, setting a new SOTA for the field.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Vasluianu_WSRD_A_Novel_Benchmark_for_High_Resolution_Image_Shadow_Removal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Vasluianu_WSRD_A_Novel_Benchmark_for_High_Resolution_Image_Shadow_Removal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Benchmark Dataset and Effective Inter-Frame Alignment for Real-World Video Super-Resolution",
        "author": "Ruohao Wang, Xiaohui Liu, Zhilu Zhang, Xiaohe Wu, Chun-Mei Feng, Lei Zhang, Wangmeng Zuo",
        "abstract": "Video super-resolution (VSR) aiming to reconstruct a high-resolution (HR) video from its low-resolution (LR) counterpart has made tremendous progress in recent years. However, it remains challenging to deploy existing VSR methods to real-world data with complex degradations. On the one hand, there are few well-aligned real-world VSR datasets, especially with large super-resolution scale factors, which limits the development of real-world VSR tasks. On the other hand, alignment algorithms in existing VSR methods perform poorly for real-world videos, leading to unsatisfactory results. As an attempt to address the aforementioned issues, we build a real-world x4 VSR dataset, namely MVSR4x, where low- and high-resolution videos are captured with different focal length lenses of a smartphone, respectively. Moreover, we propose an effective alignment method for real-world VSR, namely EAVSR. EAVSR takes the proposed multi-layer adaptive spatial transform network (MultiAdaSTN) to refine the offsets provided by the pre-trained optical flow estimation network. Experimental results on RealVSR and MVSR4x datasets show the effectiveness and practicality of our method, and we achieve state-of-the-art performance in real-world VSR task. The dataset and code will be available at https://github.com/HITRainer/EAVSR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Wang_Benchmark_Dataset_and_Effective_Inter-Frame_Alignment_for_Real-World_Video_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Wang_Benchmark_Dataset_and_Effective_Inter-Frame_Alignment_for_Real-World_Video_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SB-VQA: A Stack-Based Video Quality Assessment Framework for Video Enhancement",
        "author": "Ding-Jiun Huang, Yu-Ting Kao, Tieh-Hung Chuang, Ya-Chun Tsai, Jing-Kai Lou, Shuen-Huei Guan",
        "abstract": "In recent years, several video quality assessment (VQA) methods have been developed, achieving high performance. However, these methods were not specifically trained for enhanced videos, which limits their ability to predict video quality accurately based on human subjective perception. To address this issue, we propose a stack-based framework for VQA that outperforms existing state-of-the-art methods on VDPVE, a dataset consisting of enhanced videos. In addition to proposing the VQA framework for enhanced videos, we also investigate its application on professionally generated content (PGC). To address copy- right issues with premium content, we create the PGCVQ dataset, which consists of videos from YouTube. We evaluate our proposed approach and state-of-the-art methods on PGCVQ, and provide new insights on the results. Our experiments demonstrate that existing VQA algorithms can be applied to PGC videos, and we find that VQA performance for PGC videos can be improved by considering the plot of a play, which highlights the importance of video semantic understanding.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Huang_SB-VQA_A_Stack-Based_Video_Quality_Assessment_Framework_for_Video_Enhancement_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Huang_SB-VQA_A_Stack-Based_Video_Quality_Assessment_Framework_for_Video_Enhancement_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Challenge on Night Photography Rendering",
        "author": "Alina Shutova, Egor Ershov, Georgy Perevozchikov, Ivan Ermakov, Nikola Bani\u0107, Radu Timofte, Richard Collins, Maria Efimova, Arseniy Terekhin, Simone Zini, Claudio Rota, Marco Buzzelli, Simone Bianco, Raimondo Schettini, Chunxia Lei, Tingniao Wang, Song Wang, Shuai Liu, Chaoyu Feng, Guangqi Shao, Hao Wang, Xiaotao Wang, Lei Lei, Lu Xu, Chao Zhang, Yasi Wang, Jin Guo, Yangfan Sun, Tianli Liu, Dejun Hao, Furkan K\u0131nl\u0131, Bar\u0131\u0219 \u00d6zcan, Furkan K\u0131ra\u00e7, Hyerin Chung, Nakyung Lee, Sung Keun Kwak, Marcos Conde, Tim Seizinger, Florin Vasluianu, Omar Elezabi, Chia-Hsuan Hsieh, Wei-Ting Chen, Hao-Hsiang Yang, Zhi-Kai Huang, Hua-En Chang, I-Hsiang Chen, Yi-Chung Chen, Yuan-Chun Chiang",
        "abstract": "This paper presents a review of the NTIRE 2023 challenge on night photography rendering. The goal of the challenge was to find solutions that process raw camera images taken in nighttime conditions conditions, and thereby produce a photo-quality output images in the standard RGB (sRGB) space. Unlike the previous year's competition, participants were not provided with a large training dataset for the target sensor. Instead, this time they were given images of a color checker illuminated by a known light source. To evaluate the results, a sufficient number of viewers were asked to assess the visual quality of the proposed solutions, considering the subjective nature of the task. The highest ranking solutions were further ranked by Richard Collins, a renowned photographer. The top ranking participants' solutions effectively represent the state-of-the-art in nighttime photography rendering. More results can be found at https://nightimaging.org/",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Shutova_NTIRE_2023_Challenge_on_Night_Photography_Rendering_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Shutova_NTIRE_2023_Challenge_on_Night_Photography_Rendering_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TSRFormer: Transformer Based Two-Stage Refinement for Single Image Shadow Removal",
        "author": "Hua-En Chang, Chia-Hsuan Hsieh, Hao-Hsiang Yang, I-Hsiang Chen, Yi-Chung Chen, Yuan-Chun Chiang, Zhi-Kai Huang, Wei-Ting Chen, Sy-Yen Kuo",
        "abstract": "Single-image shadow removal aims to remove undesired shadow information from captured images. With the development of the deep convolutional neural networks, several methods have been proposed to achieve promising performance in shadow removal. However, they still struggle with limited performance due to the non-homogeneous intensity distribution of the shadow. To address this issue, we propose a two-stage shadow removal architecture based on the transformer called TSRFormer. The proposed architecture is divided into shadow removal and content refinement networks. These two stages adopt different transformer architectures and remove the shadow based on different information to achieve effective shadow removal. Experiments performed on challenging benchmark show that the proposed model achieves the 2 nd  highest SSIM in the NTIRE 2023 Image Shadow Removal Challenge. The source code will be public after the acceptance of this paper.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Chang_TSRFormer_Transformer_Based_Two-Stage_Refinement_for_Single_Image_Shadow_Removal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Chang_TSRFormer_Transformer_Based_Two-Stage_Refinement_for_Single_Image_Shadow_Removal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Zoom-VQA: Patches, Frames and Clips Integration for Video Quality Assessment",
        "author": "Kai Zhao, Kun Yuan, Ming Sun, Xing Wen",
        "abstract": "Video quality assessment (VQA) aims to simulate the human perception of video quality, which is influenced by factors ranging from low-level color and texture details to high-level semantic content. To effectively model these complicated quality-related factors, in this paper, we decompose video into three levels (i.e., patch level, frame level, and clip level), and propose a novel Zoom-VQA architecture to perceive spatio-temporal features at different levels. It integrates three components: patch attention module, frame pyramid alignment, and clip ensemble strategy, respectively for capturing region-of-interest in the spatial dimension, multi-level information at different feature levels, and distortions distributed over the temporal dimension. Owing to the comprehensive design, Zoom-VQA obtains state-of-the-art results on four VQA benchmarks and achieves 2nd place in the NTIRE 2023 VQA challenge. Notably, Zoom-VQA has outperformed the previous best results on two subsets of LSVQ, achieving 0.8860 (+1.0%) and 0.7985 (+1.9%) of SRCC on the respective subsets. Adequate ablation studies further verify the effectiveness of each component. Codes and models are released in https://github.com/k-zha14/Zoom-VQA.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zhao_Zoom-VQA_Patches_Frames_and_Clips_Integration_for_Video_Quality_Assessment_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhao_Zoom-VQA_Patches_Frames_and_Clips_Integration_for_Video_Quality_Assessment_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Efficient Deep Models for Real-Time 4K Image Super-Resolution. NTIRE 2023 Benchmark and Report",
        "author": "Marcos V. Conde, Eduard Zamfir, Radu Timofte, Daniel Motilla, Cen Liu, Zexin Zhang, Yunbo Peng, Yue Lin, Jiaming Guo, Xueyi Zou, Yuyi Chen, Yi Liu, Jia Hao, Youliang Yan, Yuanfan Zhang, Gen Li, Lei Sun, Lingshun Kong, Haoran Bai, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Mustafa Ayazoglu, Bahri Batuhan Bilecen, Mingxi Li, Yuhang Zhang, Xianjun Fan, Yankai Sheng, Long Sun, Zibin Liu, Weiran Gou, Shaoqing Li, Ziyao Yi, Yan Xiang, Dehui Kong, Ke Xu, Ganzorig Gankhuyag, Kihwan Yoon, Jin Zhang, Gaocheng Yu, Feng Zhang, Hongbin Wang, Zhou Zhou, Jiahao Chao, Hongfan Gao, Jiali Gong, Zhengfeng Yang, Zhenbing Zeng, Chengpeng Chen, Zichao Guo, Anjin Park, Yuqing Liu, Qi Jia, Hongyuan Yu, Xuanwu Yin, Kunlong Zuo, Dongyang Zhang, Ting Fu, Zhengxue Cheng, Shiai Zhu, Dajiang Zhou, Weichen Yu, Lin Ge, Jiahua Dong, Yajun Zou, Zhuoyuan Wu, Binnan Han, Xiaolin Zhang, Heng Zhang, Ben Shao, Shaolong Zheng, Daheng Yin, Baijun Chen, Mengyang Liu, Marian-Sergiu Nistor, Yi-Chung Chen, Zhi-Kai Huang, Yuan-Chun Chiang, Wei-Ting Chen, Hao-Hsiang Yang, Hua-En Chang, I-Hsiang Chen, Chia-Hsuan Hsieh, Sy-Yen Kuo, Tu Vo, Qingsen Yan, Yun Zhu, Jinqiu Su, Yanning Zhang, Cheng Zhang, Jiaying Luo, Youngsun Cho, Nakyung Lee",
        "abstract": "This paper introduces a novel benchmark for efficient upscaling as part of the NTIRE 2023 Real-Time Image Super-Resolution (RTSR) Challenge, which aimed to upscale images from 720p and 1080p resolution to native 4K (x2 and x3 factors) in real-time on commercial GPUs. For this, we use a new test set containing diverse 4K images ranging from digital art to gaming and photography. We assessed the methods devised for 4K SR by measuring their runtime, parameters, and FLOPs, while ensuring a minimum PSNR fidelity over Bicubic interpolation. Out of the 170 participants, 25 teams contributed to this report, making it the most comprehensive benchmark to date and showcasing the latest advancements in real-time SR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Conde_Efficient_Deep_Models_for_Real-Time_4K_Image_Super-Resolution._NTIRE_2023_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Conde_Efficient_Deep_Models_for_Real-Time_4K_Image_Super-Resolution._NTIRE_2023_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Unlimited-Size Diffusion Restoration",
        "author": "Yinhuai Wang, Jiwen Yu, Runyi Yu, Jian Zhang",
        "abstract": "Recently, using diffusion models for zero-shot image restoration (IR) has become a new hot paradigm. This type of method only needs to use the pre-trained off-the-shelf diffusion models, without any finetuning, and can directly handle various IR tasks. The upper limit of the restoration performance depends on the pre-trained diffusion models, which are in rapid evolution. However, current methods only discuss how to deal with fixed-size images, but dealing with images of arbitrary sizes is very important for practical applications. This paper focuses on how to use those diffusion-based zero-shot IR methods to deal with any size while maintaining the excellent characteristics of zero-shot. A simple way to solve arbitrary size is to divide it into fixed-size patches and solve each patch independently. But this may yield significant artifacts since it neither considers the global semantics of all patches nor the local information of adjacent patches. Inspired by the Range-Null space Decomposition, we propose the Mask-Shift Restoration to address local incoherence and propose the Hierarchical Restoration to alleviate out-of-domain issues. Our simple, parameter-free approaches can be used not only for image restoration but also for image generation of unlimited sizes, with the potential to be a general tool for diffusion models.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Wang_Unlimited-Size_Diffusion_Restoration_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Wang_Unlimited-Size_Diffusion_Restoration_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Towards Real-Time 4K Image Super-Resolution",
        "author": "Eduard Zamfir, Marcos V. Conde, Radu Timofte",
        "abstract": "Over the past few years, high-definition videos and images in 720p (HD), 1080p (FHD), and 4K (UHD) resolution have become standard. While higher resolutions offer improved visual quality for users, they pose a significant challenge for super-resolution networks to achieve real-time performance on commercial GPUs. This paper presents a comprehensive analysis of super-resolution model designs and techniques aimed at efficiently upscaling images from 720p and 1080p resolutions to 4K. We begin with a simple, effective baseline architecture and gradually modify its design by focusing on extracting important high-frequency details efficiently. This allows us to subsequently downscale the resolution of deep feature maps, reducing the overall computational footprint, while maintaining high reconstruction fidelity. We enhance our method by incorporating pixel-unshuffling, a simplified and speed-up reinterpretation of the basic block proposed by NAFNet, along with structural re-parameterization. We assess the performance of the fastest version of our method in the new NTIRE 2023 Real-Time 4K Super-Resolution challenge and demonstrate its potential in comparison with state-of-the-art efficient super-resolution models when scaled up. Our method was tested successfully on high-quality content from photography, digital art, and gaming content.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zamfir_Towards_Real-Time_4K_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zamfir_Towards_Real-Time_4K_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Spatial-Angular Multi-Scale Mechanism for Light Field Spatial Super-Resolution",
        "author": "Chen Gao, Youfang Lin, Song Chang, Shuo Zhang",
        "abstract": "Light Field (LF) cameras are promising due to their ability to capture both spatial and angular information of scenes. However, the trade-off between spatial and angular resolution significantly limits the real-world applications. In this paper, we propose a spatial-angular multi-scale decoupling network to reconstruct high-resolution LF images. Considering the epipolar geometry, we propose a spatial-angular multi-scale processing approach to explore the correspondence of sub-pixel information with different disparity ranges between sub-aperture images in LFs. We extract sub-pixel information from various dimensions and fuse it to generate global high-frequency details. Finally, we combine upsampled low-frequency and high-frequency details to generate high resolution results. To further filter the correct interpolation information, we use the shear operation to change the disparity range of the LF images and fine-tune the results. Experimental results on synthetic and real-world datasets demonstrate that our method outperforms other state-of-the-art methods in visual and numerical evaluations, especially on datasets with small disparity ranges. Furthermore, our approach fully considers the epipolar geometry of the LF image, enabling us to recover information that better maintains the imaging consistency of the LF.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Gao_Spatial-Angular_Multi-Scale_Mechanism_for_Light_Field_Spatial_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Gao_Spatial-Angular_Multi-Scale_Mechanism_for_Light_Field_Spatial_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SwinFSR: Stereo Image Super-Resolution Using SwinIR and Frequency Domain Knowledge",
        "author": "Ke Chen, Liangyan Li, Huan Liu, Yunzhe Li, Congling Tang, Jun Chen",
        "abstract": "Stereo Image Super-Resolution (stereoSR) has attracted significant attention in recent years due to the extensive deployment of dual cameras in mobile phones, autonomous vehicles and robots. In this work, we propose a new StereoSR method, named SwinFSR, based on an extension of SwinIR, originally designed for single image restoration, and the frequency domain knowledge obtained by the Fast Fourier Convolution (FFC). Specifically, to effectively gather global information, we modify the Residual Swin Transformer blocks (RSTBs) in SwinIR by explicitly incorporating the frequency domain knowledge using the FFC and employing the resulting residual Swin Fourier Transformer blocks (RSFTBs) for feature extraction. Besides, for the efficient and accurate fusion of stereo views, we propose a new cross-attention module referred to as RCAM, which achieves highly competitive performance while requiring less computational cost than the state-of-the-art cross-attention modules. Extensive experimental results and ablation studies demonstrate the effectiveness and efficiency of our proposed SwinFSR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Chen_SwinFSR_Stereo_Image_Super-Resolution_Using_SwinIR_and_Frequency_Domain_Knowledge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Chen_SwinFSR_Stereo_Image_Super-Resolution_Using_SwinIR_and_Frequency_Domain_Knowledge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Image Shadow Removal Challenge Report",
        "author": "Florin-Alexandru Vasluianu, Tim Seizinger, Radu Timofte, Shuhao Cui, Junshi Huang, Shuman Tian, Mingyuan Fan, Jiaqi Zhang, Li Zhu, Xiaoming Wei, Xiaolin Wei, Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, Thomas B. Sch\u00f6n, Xiaoyi Dong, Xi Sheryl Zhang, Chenghua Li, Cong Leng, Woon-Ha Yeo, Wang-Taek Oh, Yeo-Reum Lee, Han-Cheol Ryu, Jinting Luo, Chengzhi Jiang, Mingyan Han, Qi Wu, Wenjie Lin, Lei Yu, Xinpeng Li, Ting Jiang, Haoqiang Fan, Shuaicheng Liu, Shuning Xu, Binbin Song, Xiangyu Chen, Shile Zhang, Jiantao Zhou, Zhao Zhang, Suiyi Zhao, Huan Zheng, Yangcheng Gao, Yanyan Wei, Bo Wang, Jiahuan Ren, Yan Luo, Yuki Kondo, Riku Miyata, Fuma Yasue, Taito Naruki, Norimichi Ukita, Hua-En Chang, Hao-Hsiang Yang, Yi-Chung Chen, Yuan-Chun Chiang, Zhi-Kai Huang, Wei-Ting Chen, I-Hsiang Chen, Chia-Hsuan Hsieh, Sy-Yen Kuo, Li Xianwei, Huiyuan Fu, Chunlin Liu, Huadong Ma, Binglan Fu, Huiming He, Mengjia Wang, Wenxuan She, Yu Liu, Sabari Nathan, Priya Kansal, Zhongjian Zhang, Huabin Yang, Yan Wang, Yanru Zhang, Shruti S. Phutke, Ashutosh Kulkarni, MD Raqib Khan, Subrahmanyam Murala, Santosh Kumar Vipparthi, Heng Ye, Zixi Liu, Xingyi Yang, Songhua Liu, Yinwei Wu, Yongcheng Jing, Qianhao Yu, Naishan Zheng, Jie Huang, Yuhang Long, Mingde Yao, Feng Zhao, Bowen Zhao, Nan Ye, Ning Shen, Yanpeng Cao, Tong Xiong, Weiran Xia, Dingwen Li, Shuchen Xia",
        "abstract": "This work reviews the results of the NTIRE 2023 Challenge on Image Shadow Removal. The described set of solutions were proposed for a novel dataset, which captures a wide range of object-light interactions. It consists of 1200 roughly pixel aligned pairs of real shadow free and shadow affected images, captured in a controlled environment. The data was captured in a white-box setup, using professional equipment for lights and data acquisition. The challenge had a number of 144 participants registered, out of which 19 teams were compared in the final ranking. The proposed solutions extend the work on shadow removal, improving over well-established state-of-the-art methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Vasluianu_NTIRE_2023_Image_Shadow_Removal_Challenge_Report_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Vasluianu_NTIRE_2023_Image_Shadow_Removal_Challenge_Report_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Temporal Consistent Automatic Video Colorization via Semantic Correspondence",
        "author": "Yu Zhang, Siqi Chen, Mingdao Wang, Xianlin Zhang, Chuang Zhu, Yue Zhang, Xueming Li",
        "abstract": "Video colorization task has recently attracted wide attention. Recent methods mainly work on the temporal consistency in adjacent frames or frames with small interval. However, it still faces severe challenge of the inconsistency between frames with large interval. To address this issue, we propose a novel video colorization framework, which combines semantic correspondence into automatic video colorization to keep long-range consistency. Firstly, a reference colorization network is designed to automatically colorize the first frame of each video, obtaining a reference image to supervise the following whole colorization process. Such automatically colorized reference image can not only avoid labor-intensive and time-consuming manual selection, but also enhance the similarity between reference and grayscale images. Afterwards, a semantic correspondence network and an image colorization network are introduced to colorize a series of the remaining frames with the help of the reference. Each frame is supervised by both the reference image and the immediately colorized preceding frame to improve both short-range and long-range temporal consistency. Extensive experiments demonstrate that our method outperforms other methods in maintaining temporal consistency both qualitatively and quantitatively. In the NTIRE 2023 Video Colorization Challenge, our method ranks at the 3rd place in Color Distribution Consistency (CDC) Optimization track. Code will be available online at https://github.com/bupt-ai-cz/TCVC.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zhang_Temporal_Consistent_Automatic_Video_Colorization_via_Semantic_Correspondence_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhang_Temporal_Consistent_Automatic_Video_Colorization_via_Semantic_Correspondence_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Challenge on HR Depth From Images of Specular and Transparent Surfaces",
        "author": "Pierluigi Zama Ramirez, Fabio Tosi, Luigi Di Stefano, Radu Timofte, Alex Costanzino, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Jun Shi, Dafeng Zhang, Yong A, Yixiang Jin, Dingzhe Li, Chao Li, Zhiwen Liu, Qi Zhang, Yixing Wang, Shi Yin",
        "abstract": "This paper reports about the NTIRE 2023 challenge on HR Depth From images of Specular and Transparent surfaces, held in conjunction with the New Trends in Image Restoration and Enhancement workshop (NTIRE) workshop at CVPR 2023. This challenge is held to boost the research on depth estimation, mainly to deal with two of the open issues in the field: high-resolution images and non-Lambertian surfaces characterizing specular and transparent materials. The challenge is divided into two tracks: a stereo track focusing on disparity estimation from rectified pairs and a mono track dealing with single-image depth estimation. The challenge attracted about 100 registered participants for the two tracks. In the final testing stage, 5 participating teams submitted their models and fact sheets, 2 and 3 for the Stereo and Mono tracks, respectively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Ramirez_NTIRE_2023_Challenge_on_HR_Depth_From_Images_of_Specular_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Ramirez_NTIRE_2023_Challenge_on_HR_Depth_From_Images_of_Specular_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Level Dispersion Residual Network for Efficient Image Super-Resolution",
        "author": "Yanyu Mao, Nihao Zhang, Qian Wang, Bendu Bai, Wanying Bai, Haonan Fang, Peng Liu, Mingyue Li, Shengbo Yan",
        "abstract": "Recently, single image super-resolution (SISR) has made great progress, especially through the combination of convolutional neural network (CNN) and Transformer, but the huge model complexity is not desirable for the efficient image super-resolution (EISR), nor is it affordable for edge devices. As a result, many lightweight methods have been investigated for EISR, such as distillation and pruning. However, investigating more powerful attention mechanisms is also a promising solution to improve network efficiency. In this paper, we propose a multi-level dispersion residual network (MDRN) for EISR. As the basic block of MDRN, enhanced attention distillation block (EADB) includes the proposed multi-level dispersion spatial attention (MDSA) and enhanced contrast-aware channel attention (ECCA), respectively. MDSA introduces multi-scale and variance information to obtain more accurate spatial attention distribution. ECCA effectively combines lightweight convolution layers and residual connections to improve the efficiency of channel attention. The experimental results show that the proposed methods are effective and our MDRN achieves a better balance of performance and complexity than the SOTA models. In addition, we won the first place in the model complexity track of the NTIRE 2023 Efficient SR Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Mao_Multi-Level_Dispersion_Residual_Network_for_Efficient_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Mao_Multi-Level_Dispersion_Residual_Network_for_Efficient_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ProgDTD: Progressive Learned Image Compression With Double-Tail-Drop Training",
        "author": "Ali Hojjat, Janek Haberer, Olaf Landsiedel",
        "abstract": "Progressive compression allows images to start loading as low-resolution versions, becoming clearer as more data is received. This increases user experience when, for example, network connections are slow. Today, most approaches for image compression, both classical and learned ones, are designed to be non-progressive. This paper introduces ProgDTD, a training method that transforms learned, non-progressive image compression approaches into progressive ones. The design of ProgDTD is based on the observation that the information stored within the bottleneck of a compression model commonly varies in importance. To create a progressive compression model, ProgDTD modifies the training steps to enforce the model to store the data in the bottleneck sorted by priority. We achieve progressive compression by transmitting the data in order of its sorted index. ProgDTD is designed for CNN-based learned image compression models, does not need additional parameters, and has a customizable range of progressiveness. For evaluation, we apply ProgDTDto the hyperprior model, one of the most common structures in learned image compression. Our experimental results show that ProgDTD performs comparably to its non-progressive counterparts and other state-of-the-art progressive models in terms of MS-SSIM and accuracy.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Hojjat_ProgDTD_Progressive_Learned_Image_Compression_With_Double-Tail-Drop_Training_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Hojjat_ProgDTD_Progressive_Learned_Image_Compression_With_Double-Tail-Drop_Training_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NTIRE 2023 Challenge on Image Super-Resolution (x4): Methods and Results",
        "author": "Yulun Zhang, Kai Zhang, Zheng Chen, Yawei Li, Radu Timofte, Junpei Zhang, Kexin Zhang, Rui Peng, Yanbiao Ma, Licheng Jia, Huaibo Huang, Xiaoqiang Zhou, Yuang Ai, Ran He, Yajun Qiu, Qiang Zhu, Pengfei Li, Qianhui Li, Shuyuan Zhu, Dafeng Zhang, Jia Li, Fan Wang, Chunmiao Li, TaeHyung Kim, Jungkeong Kil, Eon Kim, Yeonseung Yu, Beomyeol Lee, Subin Lee, Seokjae Lim, Somi Chae, Heungjun Choi, ZhiKai Huang, YiChung Chen, YuanChun Chiang, HaoHsiang Yang, WeiTing Chen, HuaEn Chang, I-Hsiang Chen, ChiaHsuan Hsieh, SyYen Kuo, Ui-Jin Choi, Marcos V. Conde, Sunder Ali Khowaja, Jiseok Yoon, Ik Hyun Lee, Garas Gendy, Nabil Sabor, Jingchao Hou, Guanghui He, Zhao Zhang, Baiang Li, Huan Zheng, Suiyi Zhao, Yangcheng Gao, Yanyan Wei, Jiahuan Ren, Jiayu Wei, Yanfeng Li, Jia Sun, Zhanyi Cheng, Zhiyuan Li, Xu Yao, Xinyi Wang, Danxu Li, Xuan Cui, Jun Cao, Cheng Li, Jianbin Zheng, Anjali Sarvaiya, Kalpesh Prajapati, Ratnadeep Patra, Pragnesh Barik, Chaitanya Rathod, Kishor Upla, Kiran Raja, Raghavendra Ramachandra, Christoph Busch",
        "abstract": "This paper reviews the NTIRE 2023 challenge on image super-resolution (x4), focusing on the proposed solutions and results. The task of image super-resolution (SR) is to generate a high-resolution (HR) output from a corresponding low-resolution (LR) input by leveraging prior information from paired LR-HR images. The aim of the challenge is to obtain a network design/solution capable to produce high-quality results with the best performance (e.g., PSNR). We want to explore how high performance we can achieve regardless of computational cost (e.g., model size and FLOPs) and data. The track of the challenge was to measure the restored HR images with the ground truth HR images on DIV2K testing dataset. The ranking of the teams is determined directly by the PSNR value. The challenge has attracted 192 registered participants, where 15 teams made valid submissions. They achieve state-of-the-art performance in single image super-resolution.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zhang_NTIRE_2023_Challenge_on_Image_Super-Resolution_x4_Methods_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhang_NTIRE_2023_Challenge_on_Image_Super-Resolution_x4_Methods_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Prioritised Moderation for Online Advertising",
        "author": "Phanideep Gampa, Akash Anil Valsangkar, Shailesh Choubey, Pooja A.",
        "abstract": "Online advertisement industry aims to build a preference for a product over its competitors by making consumers aware of the product at internet scale. However, the ads that violate the applicable laws and location specific regulations can have serious business impact with legal implications. At the same time, customers are at risk of getting exposed to egregious ads resulting in a bad user experience. Due to the limited and costly human bandwidth, moderating ads at the industry scale is a challenging task. Typically at Amazon Advertising, we deal with ad moderation workflows where the ad distributions are skewed by non defective ads. It is desirable to increase the review time that the human moderators spend on moderating genuine defective ads. Hence prioritisation of deemed defective ads for human moderation is crucial for the effective utilisation of human bandwidth in the ad moderation workflow. To incorporate the business knowledge and to better deal with the possible overlaps between the policies, we formulate this as a policy gradient ranking algorithm with custom scalar rewards. Our extensive experiments demonstrate that these techniques show a substantial gain in number of defective ads caught against various tabular classification algorithms, resulting in effective utilisation of human moderation bandwidth.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MMCM/html/Gampa_Prioritised_Moderation_for_Online_Advertising_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MMCM/papers/Gampa_Prioritised_Moderation_for_Online_Advertising_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CrisisHateMM: Multimodal Analysis of Directed and Undirected Hate Speech in Text-Embedded Images From Russia-Ukraine Conflict",
        "author": "Aashish Bhandari, Siddhant B. Shah, Surendrabikram Thapa, Usman Naseem, Mehwish Nasim",
        "abstract": "Text-embedded images are frequently used on social media to convey opinions and emotions, but they can also be a medium for disseminating hate speech, propaganda, and extremist ideologies. During the Russia-Ukraine war, both sides used text-embedded images extensively to spread propaganda and hate speech. To aid in moderating such content, this paper introduces CrisisHateMM, a novel multimodal dataset of over 4,700 text-embedded images from the Russia-Ukraine conflict, annotated for hate and non-hate speech. The hate speech is annotated for directed and undirected hate speech, with directed hate speech further annotated for individual, community, and organizational targets. We benchmark the dataset using unimodal and multimodal algorithms, providing insights into the effectiveness of different approaches for detecting hate speech in text-embedded images. Our results show that multimodal approaches outperform unimodal approaches in detecting hate speech, highlighting the importance of combining visual and textual features. This work provides a valuable resource for researchers and practitioners in automated content moderation and social media analysis. The CrisisHateMM dataset and codes are made publicly available at https://github.com/aabhandari/CrisisHateMM.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MMCM/html/Bhandari_CrisisHateMM_Multimodal_Analysis_of_Directed_and_Undirected_Hate_Speech_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MMCM/papers/Bhandari_CrisisHateMM_Multimodal_Analysis_of_Directed_and_Undirected_Hate_Speech_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Sparse Multimodal Vision Transformer for Weakly Supervised Semantic Segmentation",
        "author": "Jo\u00eblle Hanna, Michael Mommert, Damian Borth",
        "abstract": "Vision Transformers have proven their versatility and utility for complex computer vision tasks, such as land cover segmentation in remote sensing applications. While performing on par or even outperforming other methods like Convolutional Neural Networks (CNNs), Transformers tend to require even larger datasets with fine-grained annotations (e.g., pixel-level labels for land cover segmentation). To overcome this limitation, we propose a weakly-supervised vision Transformer that leverages image-level labels to learn a semantic segmentation task to reduce the human annotation load. We achieve this by slightly modifying the architecture of the vision Transformer through the use of gating units in each attention head to enforce sparsity during training and thereby retaining only the most meaningful heads. This allows us to directly infer pixel-level labels from image-level labels by post-processing the un-pruned attention heads of the model and refining our predictions by iteratively training a segmentation model with high fidelity. Training and evaluation on the DFC2020 dataset shows that our method not only generates high-quality segmentation masks using image-level labels, but also performs on par with fully-supervised training relying on pixel-level labels. Finally, our results show that our method is able to perform weakly-supervised semantic segmentation even on small-scale datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Hanna_Sparse_Multimodal_Vision_Transformer_for_Weakly_Supervised_Semantic_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Hanna_Sparse_Multimodal_Vision_Transformer_for_Weakly_Supervised_Semantic_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "GeoMultiTaskNet: Remote Sensing Unsupervised Domain Adaptation Using Geographical Coordinates",
        "author": "Valerio Marsocci, Nicolas Gonthier, Anatol Garioud, Simone Scardapane, Cl\u00e9ment Mallet",
        "abstract": "Land cover maps are a pivotal element in a wide range of Earth Observation (EO) applications. However, annotating large datasets to develop supervised systems for remote sensing (RS) semantic segmentation is costly and time-consuming. Unsupervised Domain Adaption (UDA) could tackle these issues by adapting a model trained on a source domain, where labels are available, to a target domain, without annotations. UDA, while gaining importance in computer vision, is still under-investigated in RS. Thus, we propose a new lightweight model, GeoMultiTaskNet, based on two contributions: a GeoMultiTask module (GeoMT), which utilizes geographical coordinates to align the source and target domains, and a Dynamic Class Sampling (DCS) strategy, to adapt the semantic segmentation loss to the frequency of classes. This approach is the first to use geographical metadata for UDA in semantic segmentation. It reaches state-of-the-art performances (47,22% mIoU), reducing at the same time the number of parameters (33M), on a subset of the FLAIR dataset, a recently proposed dataset properly shaped for RS UDA, used for the first time ever for research scopes here.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Marsocci_GeoMultiTaskNet_Remote_Sensing_Unsupervised_Domain_Adaptation_Using_Geographical_Coordinates_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Marsocci_GeoMultiTaskNet_Remote_Sensing_Unsupervised_Domain_Adaptation_Using_Geographical_Coordinates_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Masked Vision Transformers for Hyperspectral Image Classification",
        "author": "Linus Scheibenreif, Michael Mommert, Damian Borth",
        "abstract": "Transformer architectures have become state-of-the-art models in computer vision and natural language processing. To a significant degree, their success can be attributed to self-supervised pre-training on large scale unlabeled datasets. This work investigates the use of self-supervised masked image reconstruction to advance transformer models for hyperspectral remote sensing imagery. To facilitate self-supervised pre-training, we build a large dataset of unlabeled hyperspectral observations from the EnMAP satellite and systematically investigate modifications of the vision transformer architecture to optimally leverage the characteristics of hyperspectral data. We find significant improvements in accuracy on different land cover classification tasks over both standard vision and sequence transformers using (i) blockwise patch embeddings, (ii) spatial-spectral self-attention, (iii) spectral positional embeddings and (iv) masked self-supervised pre-training. The resulting model outperforms standard transformer architectures by +5% accuracy on a labeled subset of our EnMAP data and by +15% on Houston2018 hyperspectral dataset, making it competitive with a strong 3D convolutional neural network baseline. In an ablation study on label-efficiency based on the Houston2018 dataset, self-supervised pre-training significantly improves transformer accuracy when little labeled training data is available. The self-supervised model outperforms randomly initialized transformers and the 3D convolutional neural network by +7-8% when only 0.1-10% of the training labels are available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Scheibenreif_Masked_Vision_Transformers_for_Hyperspectral_Image_Classification_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Scheibenreif_Masked_Vision_Transformers_for_Hyperspectral_Image_Classification_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Handheld Burst Super-Resolution Meets Multi-Exposure Satellite Imagery",
        "author": "Jamy Lafenetre, Ngoc Long Nguyen, Gabriele Facciolo, Thomas Eboli",
        "abstract": "Image resolution is an important criterion for many applications based on satellite imagery. In this work, we adapt a state-of-the-art kernel regression technique for smartphone camera burst super-resolution to satellites. This technique leverages the local structure of the image to optimally steer the fusion kernels, limiting blur in the final high-resolution prediction, denoising the image, and recovering details up to a zoom factor of 2. We extend this approach to the multi-exposure case to predict from a sequence of multi-exposure low-resolution frames a high-resolution and noise-free one. Experiments on both single and multi-exposure scenarios show the merits of the approach. Since the fusion is learning-free, the proposed method is ensured to not hallucinate details, which is crucial for many remote sensing applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Lafenetre_Handheld_Burst_Super-Resolution_Meets_Multi-Exposure_Satellite_Imagery_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Lafenetre_Handheld_Burst_Super-Resolution_Meets_Multi-Exposure_Satellite_Imagery_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Deep Unfolding for Hypersharpening Using a High-Frequency Injection Module",
        "author": "Jamila Mifdal, Marc Tom\u00e1s-Cruz, Alessandro Sebastianelli, Bartomeu Coll, Joan Duran",
        "abstract": "The fusion of multi-source data with different spatial and spectral resolutions is a crucial task in many remote sensing and computer vision applications. Model-based fusion methods are more interpretable and flexible than pure data-driven networks, but their performance depends greatly on the established fusion model and the hand-crafted prior. In this work, we propose an end-to-end trainable model-based network for hyperspectral and panchromatic image fusion. We introduce an energy functional that takes into account classical observation models and incorporates a high-frequency injection constraint. The resulting optimization function is solved by a forward-backward splitting algorithm and unfolded into a deep-learning framework that uses two modules trained in parallel to ensure both data observation fitting and constraint compliance. Extensive experiments are conducted on the remote-sensing hyperspectral PRISMA dataset and on the CAVE dataset, proving the superiority of the proposed deep unfolding network qualitatively and quantitatively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Tomas-Cruz_Deep_Unfolding_for_Hypersharpening_Using_a_High-Frequency_Injection_Module_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Tomas-Cruz_Deep_Unfolding_for_Hypersharpening_Using_a_High-Frequency_Injection_Module_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Modal Multi-Objective Contrastive Learning for Sentinel-1/2 Imagery",
        "author": "Jonathan Prexl, Michael Schmitt",
        "abstract": "The field of spaceborne Earth observation offers, due to constant monitoring of the Earth's surface, a huge amount of unlabeled data. At the same time, for many applications, there still exists a shortage of high-quality labelled datasets. This is one of the major bottlenecks for progress in developing globally applicable deep learning models for analysing the dynamics of our planet from space. In recent years self-supervised representation learning revealed itself to state a very powerful way of incorporating unlabeled data into the typical supervised machine learning workflow. Still, many questions on how to adapt commonly used approaches to domain-specific properties of Earth observation data remain. In this work, we introduce and study approaches to incorporate multi-modal Earth observation data into a contrastive self-supervised learning framework by forcing inter- and intra-modality similarity in the loss function. Further, we introduce a batch-sampling strategy that leverages the geo-coding of the imagery in order to obtain harder negative pairs for the contrastive learning problem. We show through extensive experiments that various domain-specific downstream problems are benefitting from the above-mentioned contributions.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Prexl_Multi-Modal_Multi-Objective_Contrastive_Learning_for_Sentinel-12_Imagery_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Prexl_Multi-Modal_Multi-Objective_Contrastive_Learning_for_Sentinel-12_Imagery_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Solar Irradiance Anticipative Transformer",
        "author": "Thomas M. Mercier, Tasmiat Rahman, Amin Sabet",
        "abstract": "This paper proposes an anticipative transformer-based model for short-term solar irradiance forecasting. Given a sequence of sky images, our proposed vision transformer encodes features of consecutive images, feeding into a transformer decoder to predict irradiance values associated with future unseen sky images. We show that our model effectively learns to attend only to relevant features in images in order to forecast irradiance. Moreover, the proposed anticipative transformer captures long-range dependencies between sky images to achieve a forecasting skill of 21.45 % on a 15 minute ahead prediction for a newly introduced dataset of all-sky images when compared to a smart persistence model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Mercier_Solar_Irradiance_Anticipative_Transformer_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Mercier_Solar_Irradiance_Anticipative_Transformer_CVPRW_2023_paper.pdf"
    },
    {
        "title": "UnCRtainTS: Uncertainty Quantification for Cloud Removal in Optical Satellite Time Series",
        "author": "Patrick Ebel, Vivien Sainte Fare Garnot, Michael Schmitt, Jan Dirk Wegner, Xiao Xiang Zhu",
        "abstract": "Clouds and haze often occlude optical satellite images, hindering continuous, dense monitoring of the Earth's surface. Although modern deep learning methods can implicitly learn to ignore such occlusions, explicit cloud removal as pre-processing enables manual interpretation and allows training models when only few annotations are available. Cloud removal is challenging due to the wide range of occlusion scenarios---from scenes partially visible through haze, to completely opaque cloud coverage. Furthermore, integrating reconstructed images in downstream applications would greatly benefit from trustworthy quality assessment. In this paper, we introduce UnCRtainTS, a method for multi-temporal cloud removal combining a novel attention-based architecture, and a formulation for multivariate uncertainty prediction. These two components combined set a new state-of-the-art performance in terms of image reconstruction a public cloud removal dataset. Additionally, we show how the well-calibrated predicted uncertainties enable a precise control of the reconstruction quality.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Ebel_UnCRtainTS_Uncertainty_Quantification_for_Cloud_Removal_in_Optical_Satellite_Time_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Ebel_UnCRtainTS_Uncertainty_Quantification_for_Cloud_Removal_in_Optical_Satellite_Time_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Comprehensive Quality Assessment of Optical Satellite Imagery Using Weakly Supervised Video Learning",
        "author": "Valerie J. Pasquarella, Christopher F. Brown, Wanda Czerwinski, William J. Rucklidge",
        "abstract": "Identifying high-quality (i.e., relatively clear) measurements of surface conditions is a near-universal first step in working with optical satellite imagery. Many cloud masking algorithms have been developed to characterize the likelihood that reflectance measurements for individual pixels were influenced by clouds, cloud shadows, and other atmospheric effects. However, due to the continuous density of the atmospheric volume, we argue that quantification of occlusion and corruption effects is better treated as a regression problem rather than a discretized classification problem as done in prior work. We propose a space-time context network trained using a bootstrapping procedure that leverages millions of automatically-mined video sequences informed by a weakly supervised measure of atmospheric similarity. We find that our approach outperforms existing machine learning and physical basis cloud and cloud shadow detection algorithms, producing state-of-the-art results for Sentinel-2 imagery on two different out-of-distribution reference datasets. The resulting product offers a flexible quality assessment (QA) solution that enables both standard cloud and cloud shadow masking via thresholding and more complex image grading for compositing or downstream models. By way of generality, minimal supervision, and scale of our training data, our approach has the potential to significantly improve the utility and usability of optical remote sensing imagery.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Pasquarella_Comprehensive_Quality_Assessment_of_Optical_Satellite_Imagery_Using_Weakly_Supervised_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Pasquarella_Comprehensive_Quality_Assessment_of_Optical_Satellite_Imagery_Using_Weakly_Supervised_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Inferring the Past: A Combined CNN-LSTM Deep Learning Framework To Fuse Satellites for Historical Inundation Mapping",
        "author": "Jonathan Giezendanner, Rohit Mukherjee, Matthew Purri, Mitchell Thomas, Max Mauerman, A.K.M. Saiful Islam, Beth Tellman",
        "abstract": "Mapping floods using satellite data is crucial for managing and mitigating flood risks. Satellite imagery enables rapid and accurate analysis of large areas, providing critical information for emergency response and disaster management. Historical flood data derived from satellite imagery can inform long-term planning, risk management strategies, and insurance-related decisions. The Sentinel-1 satellite is effective for flood detection, but for longer time series, other satellites such as MODIS can be used in combination with deep learning models to accurately identify and map past flood events. We here develop a combined CNN-LSTM deep learning framework to fuse Sentinel-1 derived fractional flooded area with MODIS data in order to infer historical floods over Bangladesh. The results show how our framework outperforms a CNN-only approach and takes advantage of not only space, but also time in order to predict the fractional inundated area. The model is applied to historical MODIS data to infer the past 20 years of inundation extents over Bangladesh and compared to a thresholding algorithm and a physical model. Our fusion model outperforms both models in consistency and capacity to predict peak inundation extents.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Giezendanner_Inferring_the_Past_A_Combined_CNN-LSTM_Deep_Learning_Framework_To_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Giezendanner_Inferring_the_Past_A_Combined_CNN-LSTM_Deep_Learning_Framework_To_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeepSim-Nets: Deep Similarity Networks for Stereo Image Matching",
        "author": "Mohamed Ali Chebbi, Ewelina Rupnik, Marc Pierrot-Deseilligny, Paul Lopes",
        "abstract": "We present three multi-scale similarity learning architectures, or DeepSim networks. These models learn pixel-level matching with a contrastive loss and are agnostic to the geometry of the considered scene. We establish a middle ground between hybrid and end-to-end approaches by learning to densely allocate all corresponding pixels of an epipolar pair at once. Our features are learnt on large image tiles to be expressive and capture the scene's wider context. We also demonstrate that curated sample mining can enhance the overall robustness of the predicted similarities and improve the performance on radiometrically homogeneous areas. We run experiments on aerial and satellite datasets. Our DeepSim-Nets outperform the baseline hybrid approaches and generalize better to unseen scene geometries than end-to-end methods. Our flexible architecture can be readily adopted in standard multi-resolution image matching pipelines. The code is available at https://github.com/DaliCHEBBI/DeepSimNets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Chebbi_DeepSim-Nets_Deep_Similarity_Networks_for_Stereo_Image_Matching_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Chebbi_DeepSim-Nets_Deep_Similarity_Networks_for_Stereo_Image_Matching_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Seasonal Domain Shift in the Global South: Dataset and Deep Features Analysis",
        "author": "Georgios Voulgaris, Andy Philippides, Jonathan Dolley, Jeremy Reffin, Fiona Marshall, Novi Quadrianto",
        "abstract": "Domain shifts during seasonal variations are an important aspect affecting the robustness of aerial scene classification and so it is crucial that such variation is captured within aerial scene datasets. This is more evident in geographic locations in the global South, where aerial coverage is scarcer and the rural and semi-urban landscape varies dramatically between wet and dry seasons. As current datasets do not offer the ability to experiment with domain shifts due to seasonal variations, this work proposes a labelled dataset for classifying land use from aerial images, comprising both wet and dry season data from Ghaziabad in India. Moreover, we conduct a thorough investigation into how image features, namely colour, shape, and texture, influence the accuracy of scene classification. We demonstrate that a combination of an architecture that extracts salient features, with the implementation of a larger receptive field improves classification performance when applied to both shallow or deep architectures by extracting invariant feature representations across domains.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Voulgaris_Seasonal_Domain_Shift_in_the_Global_South_Dataset_and_Deep_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Voulgaris_Seasonal_Domain_Shift_in_the_Global_South_Dataset_and_Deep_CVPRW_2023_paper.pdf"
    },
    {
        "title": "APPLeNet: Visual Attention Parameterized Prompt Learning for Few-Shot Remote Sensing Image Generalization Using CLIP",
        "author": "Mainak Singha, Ankit Jha, Bhupendra Solanki, Shirsha Bose, Biplab Banerjee",
        "abstract": "In recent years, the success of large-scale vision-language models (VLMs) such as CLIP has led to their increased usage in various computer vision tasks. These models enable zero-shot inference through carefully crafted instructional text prompts without task-specific supervision. However, the potential of VLMs for generalization tasks in remote sensing (RS) has not been fully realized. To address this research gap, we propose a novel image-conditioned prompt learning strategy called the Visual Attention Parameterized Prompts Learning Network (APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning in RS scene classification and disentangles visual style and content primitives for domain generalization tasks. To achieve this, APPLeNet combines visual content features obtained from different layers of the vision encoder and style properties obtained from feature statistics of domain-specific batches. An attention-driven injection module is further introduced to generate visual tokens from this information. We also introduce an anti-correlation regularizer to ensure discrimination among the token embeddings, as this visual information is combined with the textual tokens. To validate APPLeNet, we curated four available RS benchmarks and introduced experimental protocols and datasets for three domain generalization tasks. Our results consistently outperform the relevant literature.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Jha_APPLeNet_Visual_Attention_Parameterized_Prompt_Learning_for_Few-Shot_Remote_Sensing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Jha_APPLeNet_Visual_Attention_Parameterized_Prompt_Learning_for_Few-Shot_Remote_Sensing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Cascaded Zoom-In Detector for High Resolution Aerial Images",
        "author": "Akhil Meethal, Eric Granger, Marco Pedersoli",
        "abstract": "Detecting objects in aerial images is challenging because they are typically composed of crowded small objects distributed non-uniformly over high-resolution (in terms of pixel size) images. Density cropping is a widely used method to improve this small object detection where the crowded small object regions are extracted and processed in high image-resolution. However, this is typically accomplished by adding other learnable components, thus complicating the training and inference over a standard detection process. In this paper, we propose an efficient Cascaded Zoom-in (CZ) detector that re-purposes the detector itself for density-guided training and inference. During training, density crops are located, labeled as a new class, and employed to augment the training dataset. During inference, the density crops are first detected along with the base class objects, and then input for a second stage of inference. This approach is easily integrated into any detector, and creates no significant change in the standard detection process, like the uniform cropping approach popular in aerial image detection. Experimental results on the aerial images of the challenging VisDrone and DOTA datasets verify the benefits of the proposed approach. The proposed CZ detector also provides state-of-the-art results over uniform cropping and other density cropping methods on the VisDrone dataset, increasing the detection mAP of small objects by more than 3 percentage points.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Meethal_Cascaded_Zoom-In_Detector_for_High_Resolution_Aerial_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Meethal_Cascaded_Zoom-In_Detector_for_High_Resolution_Aerial_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Date Earth Observation NeRF: The Detail Is in the Shadows",
        "author": "Roger Mar\u00ed, Gabriele Facciolo, Thibaud Ehret",
        "abstract": "We introduce Earth Observation NeRF (EO-NeRF), a new method for digital surface modeling and novel view synthesis from collections of multi-date remote sensing images. In contrast to previous variants of NeRF proposed in the literature for satellite images, EO-NeRF outperforms the altitude accuracy of advanced pipelines for 3D reconstruction from multiple satellite images, including classic and learned stereovision methods. This is largely due to a rendering of building shadows that is strictly consistent with the scene geometry and independent from other transient phenomena. In addition to that, a number of strategies are also proposed with the aim to exploit raw satellite images. We add model parameters to circumvent usual pre-processing steps, such as the relative radiometric normalization of the input images and the bundle adjustment for refining the camera models. We evaluate our method on different areas of interest using sets of 10-20 pre-processed and raw pansharpened WorldView-3 images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Mari_Multi-Date_Earth_Observation_NeRF_The_Detail_Is_in_the_Shadows_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Mari_Multi-Date_Earth_Observation_NeRF_The_Detail_Is_in_the_Shadows_CVPRW_2023_paper.pdf"
    },
    {
        "title": "L1BSR: Exploiting Detector Overlap for Self-Supervised Single-Image Super-Resolution of Sentinel-2 L1B Imagery",
        "author": "Ngoc Long Nguyen, J\u00e9r\u00e9my Anger, Axel Davy, Pablo Arias, Gabriele Facciolo",
        "abstract": "High-resolution satellite imagery is a key element for many Earth monitoring applications. Satellites such as Sentinel-2 feature characteristics that are favorable for super-resolution algorithms such as aliasing and band-misalignment. Unfortunately the lack of reliable high-resolution (HR) ground truth limits the application of deep learning methods to this task. In this work we propose L1BSR, a deep learning-based method for single-image super-resolution and band alignment of Sentinel-2 L1B 10m bands. The method is trained with self-supervision directly on real L1B data by leveraging overlapping areas in L1B images produced by adjacent CMOS detectors, thus not requiring HR ground truth. Our self-supervised loss is designed to enforce the super-resolved output image to have all the bands correctly aligned. This is achieved via a novel cross-spectral registration network (CSR) which computes an optical flow between images of different spectral bands. The CSR network is also trained with self-supervision using an Anchor-Consistency loss, which we also introduce in this work. We demonstrate the performance of the proposed approach on synthetic and real L1B data, where we show that it obtains comparable results to supervised methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Nguyen_L1BSR_Exploiting_Detector_Overlap_for_Self-Supervised_Single-Image_Super-Resolution_of_Sentinel-2_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Nguyen_L1BSR_Exploiting_Detector_Overlap_for_Self-Supervised_Single-Image_Super-Resolution_of_Sentinel-2_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Real-Time Segmenting Human Portrait at Anywhere",
        "author": "Ruifeng Yuan, Yuhao Cheng, Yiqiang Yan, Haiyan Liu",
        "abstract": "Real-time portrait segmentation is an important task for a wide range of human-centered applications. With the increase of mobile devices, such as mobile phones and personal computers, more and more human-centered applications are transferred to running on these devices to provide users with a better experience. So, lightweight model designing becomes indispensable for building applications on these limited-resource platforms. In this work, we propose a real-time segmentation U-shape architecture with a Re-parameter Compress Residual module (RCR module) and a bypass branch that can further improve the segmentation efficiency. In order to speed up during the inference phase, the RCR module is compressed during inference, and the bypass branch adds the missing edge information improving the learning skill of the network. Based on the experiments on the EG1800 and P3M-10K dataset compared with the state-of-the-art methods, the proposed method achieves better performance with less number of parameters. Specifically, our method reduces the number of parameters around 50%while maintaining comparable high accuracy, and the details will be described in the experiment part.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/html/Yuan_Real-Time_Segmenting_Human_Portrait_at_Anywhere_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/papers/Yuan_Real-Time_Segmenting_Human_Portrait_at_Anywhere_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications",
        "author": "Mustafa Munir, William Avery, Radu Marculescu",
        "abstract": "Traditionally, convolutional neural networks (CNN) and vision transformers (ViT) have dominated computer vision. However, recently proposed vision graph neural networks (ViG) provide a new avenue for exploration. Unfortunately, for mobile applications, ViGs are computationally expensive due to the overhead of representing images as graph structures. In this work, we propose a new graph-based sparse attention mechanism, Sparse Vision Graph Attention (SVGA), that is designed for ViGs running on mobile devices. Additionally, we propose the first hybrid CNN-GNN architecture for vision tasks on mobile devices, MobileViG, which uses SVGA. Extensive experiments show that MobileViG beats existing ViG models and existing mobile CNN and ViT architectures in terms of accuracy and/or speed on image classification, object detection, and instance segmentation tasks. Our fastest model, MobileViG-Ti, achieves 75.7% top-1 accuracy on ImageNet-1K with 0.78 ms inference latency on iPhone 13 Mini NPU (compiled with CoreML), which is faster than MobileNetV2x1.4 (1.02 ms, 74.7% top-1) and MobileNetV2x1.0 (0.81 ms, 71.8% top-1). Our largest model, MobileViG-B obtains 82.6% top-1 accuracy with only 2.30 ms latency, which is faster and more accurate than the similarly sized EfficientFormer-L3 model (2.77 ms, 82.4%). Our work proves that well designed hybrid CNN-GNN architectures can be a new avenue of exploration for designing models that are extremely fast and accurate on mobile devices. Our code is publicly available at https://github.com/SLDGroup/MobileViG.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/html/Munir_MobileViG_Graph-Based_Sparse_Attention_for_Mobile_Vision_Applications_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/papers/Munir_MobileViG_Graph-Based_Sparse_Attention_for_Mobile_Vision_Applications_CVPRW_2023_paper.pdf"
    },
    {
        "title": "VideoMatt: A Simple Baseline for Accessible Real-Time Video Matting",
        "author": "Jiachen Li, Marianna Ohanyan, Vidit Goel, Shant Navasardyan, Yunchao Wei, Humphrey Shi",
        "abstract": "Recently, real-time video matting has received growing attention from academia and industry as a new research area on the rise. However, most current state-of-the-art solutions are trained and evaluated on private or inaccessible matting datasets, which makes it hard for future researchers to conduct fair comparisons among different models. Moreover, most methods are built upon image matting models with various tricks across frames to boost matting quality. For real-time video matting models, simple and effective temporal modeling methods must be explored better. As a result, we first composite a new video matting benchmark that is purely based on publicly accessible datasets for training and testing. We further empirically investigate various temporal modeling methods and compare their performance in matting accuracy and inference speed. We name our method VideoMatt: a simple and strong real-time video matting baseline model based on a newly-composited accessible benchmark. Extensive experiments show that our VideoMatt variants reach better trade-offs between inference speed and matting quality compared with other state-of-the-art methods for real-time trimap-free video matting. We release the VideoMatt benchmark at https://drive.google.com/file/d/1QT4KHeGW3YrtBs1_7zovdCwCAofQ_GIj/view?usp=sharing.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/html/Li_VideoMatt_A_Simple_Baseline_for_Accessible_Real-Time_Video_Matting_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/papers/Li_VideoMatt_A_Simple_Baseline_for_Accessible_Real-Time_Video_Matting_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DIFT: Dynamic Iterative Field Transforms for Memory Efficient Optical Flow",
        "author": "Risheek Garrepalli, Jisoo Jeong, Rajeswaran C. Ravindran, Jamie Menjay Lin, Fatih Porikli",
        "abstract": "Recent advancements in neural network-based optical flow estimation often come with prohibitively high computational and memory requirements, presenting challenges in their model adaptation for mobile and low-power use cases. In this paper, we introduce a lightweight low-latency and memory-efficient model, Dynamic Iterative Field Transforms (DIFT), for optical flow estimation feasible for edge applications such as mobile, XR, micro UAVs, robotics & cameras. DIFT follows an iterative refinement framework leveraging variable resolution of cost volumes for correspondence estimation. We propose a memory efficient solution for cost volume processing to reduce peak memory. Also, we present a novel dynamic coarse-to-fine cost volume processing during various stages of refinement to avoid multiple levels of cost volumes. We demonstrate first realtime cost-volume based optical flow DL architecture on Snapdragon 8 Gen 1 HTP efficient mobile AI accelerator with 32 inf/sec and 5.89 EPE on KITTI with manageable accuracy-performance tradeoffs.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/html/Garrepalli_DIFT_Dynamic_Iterative_Field_Transforms_for_Memory_Efficient_Optical_Flow_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/papers/Garrepalli_DIFT_Dynamic_Iterative_Field_Transforms_for_Memory_Efficient_Optical_Flow_CVPRW_2023_paper.pdf"
    },
    {
        "title": "High-Efficiency Device-Cloud Collaborative Transformer Model",
        "author": "Penghao Jiang, Ke Xin, Chunxi Li, Yinsi Zhou",
        "abstract": "Natural Language Processing (NLP) experts have had significant success with unsupervised language pre-training techniques. However, compared to typical NLP models, modern self-attention models require far more computational and memory resources than conventional NLP models, making pre-training or even fine-tuning them quite costly. It drastically restricts their success and uses in a variety of fields. To improve the efficiency, we propose Device-Cloud Collaborative Transformer for an efficient language model, which is a framework across cloud and device, and is designed to encourage learning of representations that generalize better to many different tasks. Specifically, we design Device-Cloud Collaborative Transformer architecture of large language models that benefits both cloud modeling and device modeling. Experimental results demonstrate the effectiveness of our proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/html/Jiang_High-Efficiency_Device-Cloud_Collaborative_Transformer_Model_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/papers/Jiang_High-Efficiency_Device-Cloud_Collaborative_Transformer_Model_CVPRW_2023_paper.pdf"
    },
    {
        "title": "QuickSRNet: Plain Single-Image Super-Resolution Architecture for Faster Inference on Mobile Platforms",
        "author": "Guillaume Berger, Manik Dhingra, Antoine Mercier, Yashesh Savani, Sunny Panchal, Fatih Porikli",
        "abstract": "In this work, we present QuickSRNet, an efficient super-resolution architecture for real-time applications on mobile platforms. Super-resolution clarifies, sharpens, and upscales an image to higher resolution. Applications such as gaming and video playback along with the ever-improving display capabilities of TVs, smartphones, and VR headsets are driving the need for efficient upscaling solutions. While existing deep learning-based super-resolution approaches achieve impressive results in terms of visual quality, enabling real-time DL-based super-resolution on mobile devices with compute, thermal, and power constraints is challenging. To address these challenges, we propose QuickSRNet, a simple yet effective architecture that provides better accuracy-to-latency trade-offs than existing neural architectures for single-image super resolution. We present training tricks to speed up existing residual-based super-resolution architectures while maintaining robustness to quantization. Our proposed architecture produces 1080p outputs via 2x upscaling in 2.2 ms on a modern smartphone, making it ideal for high-fps real-time applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/html/Berger_QuickSRNet_Plain_Single-Image_Super-Resolution_Architecture_for_Faster_Inference_on_Mobile_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/papers/Berger_QuickSRNet_Plain_Single-Image_Super-Resolution_Architecture_for_Faster_Inference_on_Mobile_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Pareto-Aware Neural Architecture Generation for Diverse Computational Budgets",
        "author": "Yong Guo, Yaofo Chen, Yin Zheng, Qi Chen, Peilin Zhao, Junzhou Huang, Jian Chen, Mingkui Tan",
        "abstract": "Designing feasible and effective architectures under diverse computational budgets, incurred by different applications/devices, is essential for deploying deep models in real-world applications. To achieve this goal, existing methods often perform an independent architecture search process for each target budget, which is very inefficient yet unnecessary. More critically, these independent search processes cannot share their learned knowledge (i.e., the distribution of good architectures) with each other and thus often result in limited search results. To address these issues, we propose a Pareto-aware Neural Architecture Generator (PNAG) which only needs to be trained once and dynamically produces the Pareto optimal architecture for any given budget via inference. To train our PNAG, we learn the whole Pareto frontier by jointly finding multiple Pareto optimal architectures under diverse budgets. Such a joint search algorithm not only greatly reduces the overall search cost but also improves the search results. Extensive experiments on three hardware platforms (i.e., mobile device, CPU, and GPU) show the superiority of our method over existing methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Guo_Pareto-Aware_Neural_Architecture_Generation_for_Diverse_Computational_Budgets_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Guo_Pareto-Aware_Neural_Architecture_Generation_for_Diverse_Computational_Budgets_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A2-Aug: Adaptive Automated Data Augmentation",
        "author": "Lujun Li, Anggeng Li",
        "abstract": "Data augmentation is a promising way to enhance the generalization ability of deep learning models. Many proxy-free and proxy-based automated augmentation methods are proposed to search for the best augmentation for target datasets. However, the proxy-free methods require lots of searching overhead, while the proxy-based methods introduce optimization gaps with the actual task. In this paper, we explore a new proxy-free approach that only needs a small number of searches (  5 vs 100 of RandAugment) to alleviate these issues. Specifically, we propose Adaptive Automated Augmentation (A^2-Aug), a simple and effective proxy-free framework, which seeks to mine the adaptive ensemble knowledge of multiple augmentations to further improve the adaptability of each candidate augmentation. Firstly, A^2-Aug automatically learns the ensemble logit from multiple candidate augmentations, which is jointly optimized and adaptive to target tasks. Secondly, the adaptive ensemble logit is used to distill each logit of input augmentation via KL divergence. In this way, these a few candidate augmentations can implicitly learn strong adaptability for the target datasets, which enjoy similar effects with many searches of RandAugment. Finally, equipped with joint training via separate BatchNorm and normalized distillation, A^2-Aug obtains state-of-the-art performance with less training budget. In experiments, our A^2-Aug achieves 4% performance gain on CIFAR-100, which substantially outperforms other methods. On ImageNet, we obtain a top-1 accuracy of 79.2% for ResNet-50, a 1.6% boosting over the AutoAugment with at least 25x faster training speed.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Li_A2-Aug_Adaptive_Automated_Data_Augmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Li_A2-Aug_Adaptive_Automated_Data_Augmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Hardware-Aware NAS by Genetic Optimisation With a Design Space Exploration Simulator",
        "author": "Lotte Hendrickx, Arne Symons, Wiebe Van Ranst, Marian Verhelst, Toon Goedem\u00e9",
        "abstract": "Neural Architecture Search (NAS) has shown its potential in aiding in the development of more efficient neural networks. In regard to hardware, efficiency often equates to power usage or latency. Over the years many researchers have incorporated hardware performance into their NAS experiments. However, accurately modelling hardware performance is a challenge in itself. We look to the field of design space exploration (DSE) for more precise performance metrics on neural network accelerators and incorporate the results into our NAS search. Our experiments show that doing so achieves a significant reduction in latency and energy consumption. The approach we propose also enables detailed insight in the breakdown of the energy consumption and latency of the optimised model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Hendrickx_Hardware-Aware_NAS_by_Genetic_Optimisation_With_a_Design_Space_Exploration_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Hendrickx_Hardware-Aware_NAS_by_Genetic_Optimisation_With_a_Design_Space_Exploration_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Systematic Architectural Design of Scale Transformed Attention Condenser DNNs via Multi-Scale Class Representational Response Similarity Analysis",
        "author": "Andrew Hryniowski, Alexander Wong",
        "abstract": "Self-attention mechanisms are commonly included in a convolutional neural networks to achieve an improved efficiency performance balance. However, adding self-attention mechanisms adds additional hyperparameters to tune for the application at hand. In this work we propose a novel type of DNN analysis called Multi-Scale Class Representational Response Similarity Analysis (ClassRepSim) which can be used to identify specific design interventions that lead to more efficient self-attention convolutional neural network architectures. Using insights grained from ClassRepSim we propose the Spatial Transformed Attention Condenser (STAC) module, a novel attention-condenser based self-attention module. We show that adding STAC modules to ResNet style architectures can result in up to a 1.6% increase in top-1 accuracy compared to vanilla ResNet models and up to a 0.5% increase in top-1 accuracy compared to SENet models on the ImageNet64x64 dataset, at the cost of up to 1.7% increase in FLOPs and 2x the number of parameters. In addition, we demonstrate that results from ClassRepSim analysis can be used to select an effective parameterization of the STAC module resulting in competitive performance compared to an extensive parameter search.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Hryniowski_Systematic_Architectural_Design_of_Scale_Transformed_Attention_Condenser_DNNs_via_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Hryniowski_Systematic_Architectural_Design_of_Scale_Transformed_Attention_Condenser_DNNs_via_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring the Potential of Neural Dataset Search",
        "author": "Ryosuke Yamada, Risa Shinoda, Hirokatsu Kataoka",
        "abstract": "Although we have witnessed Neural Architecture Search (NAS), which automatically explores architecture for best performance, the discussion has not advanced considering a dataset. We discuss the potential of Neural Dataset Search (NDS), which explores the appropriate configuration in a pre-training dataset to achieve a better pre-training effect. The NDS is designed to train in order to find the optimal parameters in the pre-training dataset for a given network architecture and downstream tasks. This allows for predicting the optimal pre-training parameters for a new unseen task in one shot. Thus, the NDS has the potential to bottom up the effectiveness of the pre-training. Therefore, this paper focuses on formula-driven supervised learning, and as a first consideration, we verify the appropriate configuration in Residual Network (ResNet) and Fractal DataBase (FractalDB). From the experimental results, we confirmed that the FractalDB generation parameters that provide the best pre-training effect are different for each ResNet- 18, 50, 152 . These observations reveal that there is an adapted image representation or dataset structure (e.g., input size, parameter, category) for a particular architecture. We hope these results will encourage further research on NDS that fully exploits the pre-training of synthetic images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Yamada_Exploring_the_Potential_of_Neural_Dataset_Search_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Yamada_Exploring_the_Potential_of_Neural_Dataset_Search_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-Task Learning in Computer Vision Tasks for Robotic Grasping on the Edge",
        "author": "Alexander Wong, Yifan Wu, Saad Abbasi, Saeejith Nair, Yuhao Chen, Mohammad Javad Shafiee",
        "abstract": "Multi-task learning has shown considerable promise for improving the performance of deep learning-driven vision systems for the purpose of robotic grasping. However, high architectural and computational complexity can result in poor suitability for deployment on embedded devices that are typically leveraged in robotic arms for real-world manufacturing and warehouse environments. As such, the design of highly efficient multi-task deep neural network architectures tailored for computer vision tasks for robotic grasping on the edge is highly desired for widespread adoption in manufacturing environments. Motivated by this, we propose Fast GraspNeXt, a fast self-attention neural network architecture tailored for embedded multi-task learning in computer vision tasks for robotic grasping. To build Fast GraspNeXt, we leverage a generative network architecture search strategy with a set of architectural constraints customized to achieve a strong balance between multi-task learning performance and embedded inference efficiency. Experimental results on the MetaGraspNet benchmark dataset show that the Fast GraspNeXt network design achieves the highest performance (average precision (AP), accuracy, and mean squared error (MSE)) across multiple computer vision tasks when compared to other efficient multi-task network architecture designs, while having only 17.8M parameters (about >5X smaller), 259 GFLOPs (as much as >5X lower) and as much as >3.15X faster on a NVIDIA Jetson TX2 embedded processor.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Wong_Fast_GraspNeXt_A_Fast_Self-Attention_Neural_Network_Architecture_for_Multi-Task_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Wong_Fast_GraspNeXt_A_Fast_Self-Attention_Neural_Network_Architecture_for_Multi-Task_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PerfHD: Efficient ViT Architecture Performance Ranking Using Hyperdimensional Computing",
        "author": "Dongning Ma, Pengfei Zhao, Xun Jiao",
        "abstract": "Neural Architecture Search (NAS) aims at identifying the optimal network architecture for a specific need in an automated manner, which serves as an alternative to the manual process of model development, selection, evaluation and performance estimation. However, evaluating performance of candidate architectures in the search space during NAS, which often requires training and ranking a mass of architectures, is often prohibitively computation-demanding. To reduce this cost, recent works propose to estimate and rank the architecture performance without actual training or inference. In this paper, we present PerfHD, an efficient-while-accurate architecture performance ranking approach using hyperdimensional computing for the emerging vision transformer (ViT), which has demonstrated state-of-the-art (SOTA) performance in vision tasks. Given a set of ViT models, PerfHD can accurately and quickly rank their performance solely based on their hyper-parameters without training. We develop two encoding schemes for PerfHD, Gram-based and Record-based, to encode the features from candidate ViT architecture parameters. Using the VIMER-UFO benchmark dataset of eight tasks from a diverse range of domains, we compare PerfHD with four SOTA methods. Experimental results show that PerfHD can rank nearly 100K ViT models in about just 1 minute, which is up to 10X faster than SOTA methods, while achieving comparable or even superior ranking accuracy. We open-source PerfHD in PyTorch implementation at https://github.com/VU-DETAIL/PerfHD.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Ma_PerfHD_Efficient_ViT_Architecture_Performance_Ranking_Using_Hyperdimensional_Computing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Ma_PerfHD_Efficient_ViT_Architecture_Performance_Ranking_Using_Hyperdimensional_Computing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection",
        "author": "Wentao Zhu, Yufang Huang, Xiufeng Xie, Wenxian Liu, Jincan Deng, Debing Zhang, Zhangyang Wang, Ji Liu",
        "abstract": "The short-form videos have explosive popularity and have dominated the new social media trends. Prevailing short-video platforms, e.g., TikTok, Instagram Reels, and YouTube Shorts, have changed the way we consume and create content. For video content creation and understanding, the shot boundary detection (SBD) is one of the most essential components in various scenarios. In this work, we release a new public Short video sHot bOundary deTection dataset, named SHOT, consisting of 853 complete short videos and 11,606 shot annotations, with 2,716 high quality shot boundary annotations in 200 test videos. Leveraging this new data wealth, we propose to optimize the model design for video SBD, by conducting neural architecture search in a search space encapsulating various advanced 3D ConvNets and Transformers. Our proposed approach, named AutoShot, achieves higher F1 scores than previous state-of-the-art approaches, e.g., outperforming TransNetV2 by 4.2%, when being derived and evaluated on our newly constructed SHOT dataset. Moreover, to validate the generalizability of the AutoShot architecture, we directly evaluate it on another three public datasets: ClipShots, BBC and RAI, and the F1 scores of AutoShot outperform previous state-of-the-art approaches by 1.1%, 0.9% and 1.2%, respectively. The SHOT dataset and code will be released.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Zhu_AutoShot_A_Short_Video_Dataset_and_State-of-the-Art_Shot_Boundary_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NAS/papers/Zhu_AutoShot_A_Short_Video_Dataset_and_State-of-the-Art_Shot_Boundary_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "An Extended Study of Human-Like Behavior Under Adversarial Training",
        "author": "Paul Gavrikov, Janis Keuper, Margret Keuper",
        "abstract": "Neural networks have a number of shortcomings. Amongst the severest ones is the sensitivity to distribution shifts which allows models to be easily fooled into wrong predictions by small perturbations to inputs that are often imperceivable to humans and do not have to carry semantic meaning. Adversarial training poses a partial solution to address this issue by training models on worst-case perturbations. Yet, recent work has also pointed out that the reasoning in neural networks is different from humans. Humans identify objects by shape, while neural nets mainly employ texture cues. Exemplarily, a model trained on photographs will likely fail to generalize to datasets containing sketches. Interestingly, it was also shown that adversarial training seems to favorably increase the shift toward shape bias. In this work, we revisit this observation and provide an extensive analysis of this effect on various architectures, the common L_2- and L_-training, and Transformer-based models. Further, we provide a possible explanation for this phenomenon from a frequency perspective.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Gavrikov_An_Extended_Study_of_Human-Like_Behavior_Under_Adversarial_Training_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Gavrikov_An_Extended_Study_of_Human-Like_Behavior_Under_Adversarial_Training_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Universal Watermark Vaccine: Universal Adversarial Perturbations for Watermark Protection",
        "author": "Jianbo Chen, Xinwei Liu, Siyuan Liang, Xiaojun Jia, Yuan Xun",
        "abstract": "As computing ability continues to rapidly develop, neural networks have found widespread use in various fields. However, in the realm of visible watermarking for image copyright protection, neural networks have made image protection through watermarking less effective. Some research has even shown that watermarks can be removed without damaging to the original image, posing a significant threat to digital copyright protection. In response, the community has introduced adversarial perturbations for watermark protection, but these are sample-specific and time-consuming in real-world scenarios. To address this issue, we propose a new universal adversarial perturbation for watermark removal networks that offers two options. The first option involves adding perturbations to the entire host image, bringing the output of the watermark removal network closer to the original image and providing protection. The second option involves adding perturbations only to the watermark position, reducing the impact of the perturbation on the image and enhancing stealthiness. Our experiments demonstrate that our method effectively resists watermark removal networks and has good generalizability across different images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Chen_Universal_Watermark_Vaccine_Universal_Adversarial_Perturbations_for_Watermark_Protection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Chen_Universal_Watermark_Vaccine_Universal_Adversarial_Perturbations_for_Watermark_Protection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs",
        "author": "Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H.S. Torr, Bernard Ghanem",
        "abstract": "In this paper we investigate the frequency sensitivity of Deep Neural Networks (DNNs) when presented with clean samples versus poisoned samples. Our analysis shows significant disparities in frequency sensitivity between these two types of samples. Building on these findings, we propose FREAK, a frequency-based poisoned sample detection algorithm that is simple yet effective. Our experimental results demonstrate the efficacy of FREAK not only against frequency backdoor attacks but also against some spatial attacks. Our work is just the first step in leveraging these insights. We believe that our analysis and proposed defense mechanism will provide a foundation for future research and development of backdoor defenses.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Al_Kader_Hammoud_Dont_FREAK_Out_A_Frequency-Inspired_Approach_to_Detecting_Backdoor_Poisoned_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Al_Kader_Hammoud_Dont_FREAK_Out_A_Frequency-Inspired_Approach_to_Detecting_Backdoor_Poisoned_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Certified Adversarial Robustness Within Multiple Perturbation Bounds",
        "author": "Soumalya Nandi, Sravanti Addepalli, Harsh Rangwani, R. Venkatesh Babu",
        "abstract": "Randomized smoothing (RS) is a well known certified defense against adversarial attacks, which creates a smoothed classifier by predicting the most likely class under random noise perturbations of inputs during inference. While initial work focused on robustness to L2 norm perturbations using noise sampled from a Gaussian distribution, subsequent works have shown that different noise distributions can result in robustness to other Lp norm bounds as well. In general, a specific noise distribution is optimal for defending against a given Lp norm based attack. In this work, we aim to improve the certified adversarial robustness against multiple perturbation bounds simultaneously. Towards this, we firstly present a novel certification scheme, that effectively combines the certificates obtained using different noise distributions to obtain optimal results against multiple perturbation bounds. We further propose a novel training noise distribution along with a regularized training scheme to improve the certification within both L1 and L2 perturbation norms simultaneously. Contrary to prior works, we compare the certified robustness of different training algorithms across the same natural (clean) accuracy, rather than across fixed noise levels used for training and certification. We also empirically invalidate the argument that training and certifying the classifier with the same amount of noise gives the best results. The proposed approach achieves improvements on the ACR (Average Certified Radius) metric across both L1 and L2 perturbation bounds. Code available at https://github.com/val-iisc/NU-Certified-Robustness",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Nandi_Certified_Adversarial_Robustness_Within_Multiple_Perturbation_Bounds_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Nandi_Certified_Adversarial_Robustness_Within_Multiple_Perturbation_Bounds_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robustness With Query-Efficient Adversarial Attack Using Reinforcement Learning",
        "author": "Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Sahand Ghorbanpour, Vineet Gundecha, Antonio Guillen, Ricardo Luna, Avisek Naug",
        "abstract": "A measure of robustness against naturally occurring distortions is key to safety, success, and trustworthiness of machine learning models on deployment. We propose an adversarial black-box attack that adds minimum Gaussian noise distortions to input images to make machine learning models misclassify. We used a Reinforcement Learning (RL) agent as a smart hacker to explore the input images to add minimum distortions to the most sensitive regions to induce misclassification. The agent employs a smart policy also to remove noises introduced earlier, which has less impact on the trained model at a given state. This novel approach is equivalent to doing a deep tree search to add noises without an exhaustive search, leading to faster and optimal convergence. Also, this adversarial attack method effectively measures the robustness of image classification models with the misclassification inducing minimum L2 distortion of Gaussian noise similar to many naturally occurring distortions. Furthermore, the proposed black-box L2 adversarial attack tool beats state-of-the-art competitors in terms of the average number of queries by a significant margin with a 100% success rate while maintaining a very competitive L2 score, despite limiting distortions to Gaussian noise. For the ImageNet dataset, the average number of queries achieved by the proposed method for ResNet-50, Inception-V3, and VGG-16 models are 42%, 32%, and 31% better than the state-of-the-art \"Square-Attack\" approach while maintaining a competitive L2. Demo: https://tinyurl.com/yr8f7x9t",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Sarkar_Robustness_With_Query-Efficient_Adversarial_Attack_Using_Reinforcement_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Sarkar_Robustness_With_Query-Efficient_Adversarial_Attack_Using_Reinforcement_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Generating Adversarial Samples in Mini-Batches May Be Detrimental to Adversarial Robustness",
        "author": "Timothy Redgrave, Colton Crum",
        "abstract": "Neural networks have been proven to be both highly effective within computer vision, and highly vulnerable to adversarial attacks. Consequently, as the use of neural networks increases due to their unrivaled performance, so too does the threat posed by adversarial attacks. In this work, we build towards addressing the challenge of adversarial robustness by exploring the relationship between the mini-batch size used during adversarial sample generation and the strength of the adversarial samples produced. We demonstrate that an increase in mini-batch size results in a decrease in the efficacy of the samples produced, and we draw connections between these observations and the phenomenon of vanishing gradients. Next, we formulate loss functions such that adversarial sample strength is not degraded by mini-batch size. Our findings highlight a potential risk for underestimating the true (practical) strength of adversarial attacks, and a risk of overestimating a model's robustness. We share our codes to let others replicate our experiments and to facilitate further exploration of the connections between batch size and adversarial sample strength.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Redgrave_Generating_Adversarial_Samples_in_Mini-Batches_May_Be_Detrimental_to_Adversarial_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Redgrave_Generating_Adversarial_Samples_in_Mini-Batches_May_Be_Detrimental_to_Adversarial_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Deep Convolutional Sparse Coding Networks for Interpretable Image Fusion",
        "author": "Zixiang Zhao, Jiangshe Zhang, Haowen Bai, Yicheng Wang, Yukun Cui, Lilun Deng, Kai Sun, Chunxia Zhang, Junmin Liu, Shuang Xu",
        "abstract": "Image fusion is a significant problem in many fields including digital photography, computational imaging and remote sensing, to name but a few. Recently, deep learning has emerged as an important tool for image fusion. This paper presents CSCFuse, which contains three deep convolutional sparse coding (CSC) networks for three kinds of image fusion tasks (i.e., infrared and visible image fusion, multi-exposure image fusion, and multi-spectral image fusion). The CSC model and the iterative shrinkage and thresholding algorithm are generalized into dictionary convolution units. As a result, all hyper-parameters are learned from data. Our extensive experiments and comprehensive comparisons reveal the superiority of CSCFuse with regard to quantitative evaluation and visual inspection.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Zhao_Deep_Convolutional_Sparse_Coding_Networks_for_Interpretable_Image_Fusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Zhao_Deep_Convolutional_Sparse_Coding_Networks_for_Interpretable_Image_Fusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring Diversified Adversarial Robustness in Neural Networks via Robust Mode Connectivity",
        "author": "Ren Wang, Yuxuan Li, Sijia Liu",
        "abstract": "This paper proposes a new method called robust mode connectivity (RMC) to enhance the adversarial robustness of neural networks (NNs) by exploring a wider range of parameter space. While adversarial training methods have shown promising results in enhancing the robustness of NNs against perturbations, they are limited by considering only a single type of perturbation during training and having limited search capability. RMC aims to address this limitation by considering multiple L_p norm perturbations (p=1,2,) and building on the concept of mode connectivity to identify a path of NNs with high robustness against different types of perturbations. The proposed method employs a multi steepest descent (MSD) algorithm to explore the parameter space and achieve diversified adversarial robustness. Experimental results on various datasets and architectures demonstrate the effectiveness of RMC.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Wang_Exploring_Diversified_Adversarial_Robustness_in_Neural_Networks_via_Robust_Mode_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Wang_Exploring_Diversified_Adversarial_Robustness_in_Neural_Networks_via_Robust_Mode_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Implications of Solution Patterns on Adversarial Robustness",
        "author": "Hengyue Liang, Buyun Liang, Ju Sun, Ying Cui, Tim Mitchell",
        "abstract": "Empirical robustness evaluation (RE) of deep learning models against adversarial perturbations involves solving non-trivial constrained optimization problems. Recent works have shown that these RE problems can be reliably solved by a general-purpose constrained-optimization solver PyGRANSO with Constraint-Folding (PWCF). In this paper, we take advantage of PWCF and other existing numerical RE algorithms to explore the distinct solution patterns in solving the RE problems with various combinations of losses, perturbation models, and optimization algorithms. We then provide extensive discussions on the implications of these patterns on current robustness evaluation and adversarial training.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Liang_Implications_of_Solution_Patterns_on_Adversarial_Robustness_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Liang_Implications_of_Solution_Patterns_on_Adversarial_Robustness_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Adversarial Defense in Aerial Detection",
        "author": "Yuwei Chen, Shiyong Chu",
        "abstract": "The excellent performance of artificial intelligence algorithms in target detection greatly improves the efficiency of detection. However, this alternative to human processing of image information faces many challenges, one of which is adversarial examples (AE). For aerial detection, it is a function widely used in many fields to obtain detection pictures of optical, infrared, and synthetic aperture radars (SAR) from high altitudes to identify ground targets. But in the current research results, optical sensors, infrared sensors, and SAR will be attacked by adversarial patches and perturbation. When these attacks exist, it is risky to let intelligent algorithms perform aerial detection. This paper will focus on the characteristics of each detection mode and propose Adaptive Defense Pipeline (ADP) in addition to improving algorithm robustness through training. According to different weather conditions, the ADP sets the weight coefficients of the detection results of multiple sensors to synthesize the detection results, and on this basis, the second confirmation is added. At the same time, we compare the traditional aerial detection results of a single sensor with the weighted results using ADP and verify that the proposed method could indeed improve the efficiency of aerial detection using artificial intelligence algorithms in an adversarial environment.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Chen_Adversarial_Defense_in_Aerial_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Chen_Adversarial_Defense_in_Aerial_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "How Many Dimensions Are Required To Find an Adversarial Example?",
        "author": "Charles Godfrey, Henry Kvinge, Elise Bishoff, Myles Mckay, Davis Brown, Tim Doster, Eleanor Byler",
        "abstract": "Past work exploring adversarial vulnerability have focused on situations where an adversary can perturb all dimensions of model input. On the other hand, a range of recent works consider the case where either (i) an adversary can perturb a limited number of input parameters or (ii) a subset of modalities in a multimodal problem. In both of these cases, adversarial examples are effectively constrained to a subspaceVin the ambient input spaceX. Motivated by this, in this work we investigate how adversarial vulnerability depends on dim(V). In particular, we show that the adversarial success of standard PGD attacks with Lp norm constraints behaves like a monotonically increasing function of epsilon*(dim(V)/dimX)^q where epsilon is the perturbation budget and (1/p)+(1/q) = 1, provided p >1 (the case p= 1 presents additional subtleties which we analyze in some detail). This functional form can be easily derived from a simple toy linear model, and as such our results land further credence to arguments that adversarial examples are endemic to locally linear models on high dimensional spaces.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Godfrey_How_Many_Dimensions_Are_Required_To_Find_an_Adversarial_Example_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Godfrey_How_Many_Dimensions_Are_Required_To_Find_an_Adversarial_Example_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-Fitting Perspective",
        "author": "Zhengbao He, Tao Li, Sizhe Chen, Xiaolin Huang",
        "abstract": "Although fast adversarial training provides an efficient approach for building robust networks, it may suffer from a serious problem known as catastrophic overfitting (CO), where multi-step robust accuracy suddenly collapses to zero. In this paper, we for the first time decouple single-step adversarial examples into data-information and self-information, which reveals an interesting phenomenon called \"self-fitting\". Self-fitting, i.e., the network learns the self-information embedded in single-step perturbations, naturally leads to the occurrence of CO. When self-fitting occurs, the network experiences an obvious \"channel differentiation\" phenomenon that some convolution channels accounting for recognizing self-information become dominant, while others for data-information are suppressed. In this way, the network can only recognize images with sufficient self-information and loses generalization ability to other types of data. Based on self-fitting, we provide new insights into the existing methods to mitigate CO and extend CO to multi-step adversarial training. Our findings reveal a self-learning mechanism in adversarial training and open up new perspectives for suppressing different kinds of information to mitigate CO.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/He_Investigating_Catastrophic_Overfitting_in_Fast_Adversarial_Training_A_Self-Fitting_Perspective_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/He_Investigating_Catastrophic_Overfitting_in_Fast_Adversarial_Training_A_Self-Fitting_Perspective_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Pilot Study of Query-Free Adversarial Attack Against Stable Diffusion",
        "author": "Haomin Zhuang, Yihua Zhang, Sijia Liu",
        "abstract": "Despite the record-breaking performance in Text-to-Image (T2I) generation by Stable Diffusion, less research attention is paid to its adversarial robustness. In this work, we study the problem of adversarial attack generation for Stable Diffusion and ask if an adversarial text prompt can be obtained even in the absence of end-to-end model queries. We call the resulting problem 'query-free attack generation'. To resolve this problem, we show that the vulnerability of T2I models is rooted in the lack of robustness of text encoders, e.g., the CLIP text encoder used for attacking Stable Diffusion. Based on such insight, we propose both untargeted and targeted query-free attacks, where the former is built on the most influential dimensions in the text embedding space, which we call steerable key dimensions. By leveraging the proposed attacks, we empirically show that only a five-character perturbation to the text prompt is able to cause the significant content shift of synthesized images using Stable Diffusion. Moreover, we show that the proposed target attack can precisely steer the diffusion model to scrub the targeted image content without causing much change in untargeted image content.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Zhuang_A_Pilot_Study_of_Query-Free_Adversarial_Attack_Against_Stable_Diffusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AML/papers/Zhuang_A_Pilot_Study_of_Query-Free_Adversarial_Attack_Against_Stable_Diffusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Continual Learning for LiDAR Semantic Segmentation: Class-Incremental and Coarse-To-Fine Strategies on Sparse Data",
        "author": "Elena Camuffo, Simone Milani",
        "abstract": "During the last few years, continual learning (CL) strategies for image classification and segmentation have been widely investigated designing innovative solutions to tackle catastrophic forgetting, like knowledge distillation and self-inpainting. However, the application of continual learning paradigms to point clouds is still unexplored and investigation is required, especially using architectures that capture the sparsity and uneven distribution of LiDAR data. The current paper analyzes the problem of class incremental learning applied to point cloud semantic segmentation, comparing approaches and state-of-the-art architectures. To the best of our knowledge, this is the first example of class-incremental continual learning for LiDAR point cloud semantic segmentation. Different CL strategies were adapted to LiDAR point clouds and tested, tackling both classic fine-tuning scenarios and the Coarse-to-Fine learning paradigm. The framework has been evaluated through two different architectures on SemanticKITTI, obtaining results in line with state-of-the-art CL strategies and standard offline learning.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Camuffo_Continual_Learning_for_LiDAR_Semantic_Segmentation_Class-Incremental_and_Coarse-To-Fine_Strategies_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Camuffo_Continual_Learning_for_LiDAR_Semantic_Segmentation_Class-Incremental_and_Coarse-To-Fine_Strategies_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Are Labels Needed for Incremental Instance Learning?",
        "author": "Mert Kilickaya, Joaquin Vanschoren",
        "abstract": "In this paper, we learn to classify visual object instances, incrementally and via self-supervision (self-incremental). Our learner observes a single instance at a time, which is then discarded from the dataset. Incremental instance learning is challenging, since longer learning sessions exacerbate forgetfulness, and labeling instances is cumbersome. We overcome these challenges via three contributions: i). We propose VINIL, a self-incremental learner that can learn object instances sequentially, ii). We equip VINIL with self-supervision to by-pass the need for instance labelling, iii). We compare VINIL to label-supervised variants on two large-scale benchmarks [??], and show that VINIL significantly improves accuracy while reducing forgetfulness.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Kilickaya_Are_Labels_Needed_for_Incremental_Instance_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Kilickaya_Are_Labels_Needed_for_Incremental_Instance_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Closer Look at Rehearsal-Free Continual Learning",
        "author": "James Seale Smith, Junjiao Tian, Shaunak Halbe, Yen-Chang Hsu, Zsolt Kira",
        "abstract": "Continual learning is a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes which may disappear from the training data for extended periods of time (a phenomenon known as the catastrophic forgetting problem). Current approaches for continual learning of a single expanding task (aka class-incremental continual learning) require extensive rehearsal of previously seen data to avoid this degradation of knowledge. Unfortunately, rehearsal comes at a cost to memory, and it may also violate data-privacy. Instead, we explore combining knowledge distillation and parameter regularization in new ways to achieve strong continual learning performance without rehearsal. Specifically, we take a deep dive into common continual learning techniques: prediction distillation, feature distillation, L2 parameter regularization, and EWC parameter regularization. We first disprove the common assumption that parameter regularization techniques fail for rehearsal-free continual learning of a single, expanding task. Next, we explore how to leverage knowledge from a pre-trained model in rehearsal-free continual learning and find that vanilla L2 parameter regularization outperforms EWC parameter regularization and feature distillation. Finally, we explore the recently popular ImageNet-R benchmark, and show that L2 parameter regularization implemented in self-attention blocks of a ViT transformer outperforms recent popular prompting for continual learning methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Smith_A_Closer_Look_at_Rehearsal-Free_Continual_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Smith_A_Closer_Look_at_Rehearsal-Free_Continual_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Simulating Task-Free Continual Learning Streams From Existing Datasets",
        "author": "Aristotelis Chrysakis, Marie-Francine Moens",
        "abstract": "Task-free continual learning is the subfield of machine learning that focuses on learning online from a stream whose distribution changes continuously over time. In contrast, previous works evaluate task-free continual learning using streams with distributions that change not continuously, but only at a few distinct points in time. In order to address the discrepancy between the definition and evaluation of task-free continual learning, we propose a principled algorithm that can permute any labeled dataset into a stream that is continuously nonstationary. We empirically show that the streams generated by our algorithm are less structured than the ones conventionally used in the literature. Moreover, we use our simulated task-free streams to benchmark multiple methods applicable to the task-free setting. We hope that our work will allow other researchers to better evaluate learning performance on continuously nonstationary streams.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Chrysakis_Simulating_Task-Free_Continual_Learning_Streams_From_Existing_Datasets_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Chrysakis_Simulating_Task-Free_Continual_Learning_Streams_From_Existing_Datasets_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Lifelong Learning of Task-Parameter Relationships for Knowledge Transfer",
        "author": "Shikhar Srivastava, Mohammad Yaqub, Karthik Nandakumar",
        "abstract": "The ability to acquire new skills and knowledge continually is one of the defining qualities of the human brain, which is critically missing in most modern machine vision systems. In this work, we focus on knowledge transfer in the lifelong learning setting. We propose a lifelong learner that models the similarities between the optimal weight spaces of tasks and exploits this in order to enable knowledge transfer across tasks in a continual learning setting. To characterize the 'task-parameter relationships', we propose a metric called adaptation rate integral (ARI), which measures the expected rate of adaptation over a finite number of steps for a (task, parameter) pair. These task-parameter relationships are learned using an auxiliary network trained on guided explorations of parameter space. The learned auxiliary network is then used to heuristically select the best parameter sets on seen tasks, which are consolidated using a hypernetwork. Given a new (unseen) task, knowledge transfer occurs through the selection of the most suitable parameter set from the hypernetwork that can be rapidly finetuned. We show that the proposed approach can improve knowledge transfer between tasks across standard benchmarks without any increase in overall model capacity, while naturally mitigating catastrophic forgetting.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Srivastava_Lifelong_Learning_of_Task-Parameter_Relationships_for_Knowledge_Transfer_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Srivastava_Lifelong_Learning_of_Task-Parameter_Relationships_for_Knowledge_Transfer_CVPRW_2023_paper.pdf"
    },
    {
        "title": "D3Former: Debiased Dual Distilled Transformer for Incremental Learning",
        "author": "Abdelrahman Mohamed, Rushali Grandhe, K. J. Joseph, Salman Khan, Fahad Khan",
        "abstract": "In class incremental learning (CIL) setting, groups of classes are introduced to a model in each learning phase. The goal is to learn a unified model performant on all the classes observed so far. Given the recent popularity of Vision Transformers (ViTs) in conventional classification settings, an interesting question is to study their continual learning behaviour. In this work, we develop a Debiased Dual Distilled Transformer for CIL dubbed D3Former. The proposed model leverages a hybrid nested ViT design to ensure data efficiency and scalability to small as well as large datasets. In contrast to a recent ViT-based CIL approach, our D3Former does not dynamically expand its architecture when new tasks are learned and remains suitable for a large number of incremental tasks. The improved CIL behaviour of the D3Former owes to two fundamental changes to the ViT design. First, we treat incremental learning as a long-tail classification problem where the majority samples from new classes vastly outnumber the limited exemplars available for old classes. To avoid the bias against the minority old classes, we propose to dynamically adjust logits to emphasize on retaining the representations relevant to old tasks. Second, we propose to preserve the configuration of spatial attention maps as the learning progresses across tasks. This helps in reducing catastrophic forgetting by constraining the model to retain attention on the most discriminative regions. D3Former obtains favorable results on incremental versions of CIFAR-100, MNIST, SVHN, and ImageNet datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Mohamed_D3Former_Debiased_Dual_Distilled_Transformer_for_Incremental_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Mohamed_D3Former_Debiased_Dual_Distilled_Transformer_for_Incremental_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SCALE: Online Self-Supervised Lifelong Learning Without Prior Knowledge",
        "author": "Xiaofan Yu, Yunhui Guo, Sicun Gao, Tajana Rosing",
        "abstract": "Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries), which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. To address the challenges, we propose Self-Supervised Contrastive Lifelong Learning (SCALE) which can extract and memorize representations on the fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgetting loss, and an online memory update for uniform subset selection. All three components are designed to work collaboratively to maximize learning performance. We perform comprehensive experiments of SCALE under iid and four non-iid data streams. The results show that SCALE outperforms the state-of-the-art algorithm in all settings with improvements up to 3.83%, 2.77% and 5.86% in terms of kNN accuracy on CIFAR-10, CIFAR-100, and TinyImageNet datasets. We release the implementation at https://github.com/Orienfish/SCALE.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Yu_SCALE_Online_Self-Supervised_Lifelong_Learning_Without_Prior_Knowledge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Yu_SCALE_Online_Self-Supervised_Lifelong_Learning_Without_Prior_Knowledge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Continual Domain Adaptation Through Pruning-Aided Domain-Specific Weight Modulation",
        "author": "Prasanna B, Sunandini Sanyal, R. Venkatesh Babu",
        "abstract": "In this paper, we propose to develop a method to address unsupervised domain adaptation (UDA) in a practical setting of continual learning (CL). The goal is to update the model on continually changing domains while preserving domain-specific knowledge to prevent catastrophic forgetting of past-seen domains. To this end, we build a framework for preserving domain-specific features utilizing the inherent model capacity via pruning. We also perform effective inference using a novel batch-norm based metric to predict the final model parameters to be used accurately. Our approach achieves not only state-of-the-art performance but also prevents catastrophic forgetting of past domains significantly. Our code is made publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/B_Continual_Domain_Adaptation_Through_Pruning-Aided_Domain-Specific_Weight_Modulation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/B_Continual_Domain_Adaptation_Through_Pruning-Aided_Domain-Specific_Weight_Modulation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "How Efficient Are Today's Continual Learning Algorithms?",
        "author": "Md Yousuf Harun, Jhair Gallardo, Tyler L. Hayes, Christopher Kanan",
        "abstract": "Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forgetting.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Harun_How_Efficient_Are_Todays_Continual_Learning_Algorithms_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Harun_How_Efficient_Are_Todays_Continual_Learning_Algorithms_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Density Map Distillation for Incremental Object Counting",
        "author": "Chenshen Wu, Joost van de Weijer",
        "abstract": "In this paper, we investigate the problem of incremental learning for object counting, where a method must learn to count a variety of object classes from a sequence of datasets. A naive approach to incremental object counting would suffer from catastrophic forgetting, where it would suffer from a dramatic performance drop on previous tasks. In this paper, we propose a new exemplar-free functional regularization method, called Density Map Distillation (DMD). During training, we introduce a new counter head for each task and introduce a distillation loss to prevent forgetting of previous tasks. As an additional novelty, we introduce a cross-task adaptor that projects the features of the current backbone to the previous backbone. This projector allows for the learning of new features while the backbone retains the relevant features for previous tasks. Finally, we set up experiments of incremental learning for counting new objects. Results confirm that our method greatly reduces catastrophic forgetting and outperforms existing methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Wu_Density_Map_Distillation_for_Incremental_Object_Counting_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Wu_Density_Map_Distillation_for_Incremental_Object_Counting_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CoVIO: Online Continual Learning for Visual-Inertial Odometry",
        "author": "Niclas V\u00f6disch, Daniele Cattaneo, Wolfram Burgard, Abhinav Valada",
        "abstract": "Visual odometry is a fundamental task for many applications on mobile devices and robotic platforms. Since such applications are oftentimes not limited to predefined target domains and learning-based vision systems are known to generalize poorly to unseen environments, methods for continual adaptation during inference time are of significant interest. In this work, we introduce CoVIO for online continual learning of visual-inertial odometry. CoVIO effectively adapts to new domains while mitigating catastrophic forgetting by exploiting experience replay. In particular, we propose a novel sampling strategy to maximize image diversity in a fixed-size replay buffer that targets the limited storage capacity of embedded devices. We further provide an asynchronous version that decouples the odometry estimation from the network weight update step enabling continuous inference in real time. We extensively evaluate CoVIO on various real-world datasets demonstrating that it successfully adapts to new domains while outperforming previous methods. The code of our work is publicly available at http://continual-slam.cs.uni-freiburg.de.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Vodisch_CoVIO_Online_Continual_Learning_for_Visual-Inertial_Odometry_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Vodisch_CoVIO_Online_Continual_Learning_for_Visual-Inertial_Odometry_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Just a Glimpse: Rethinking Temporal Information for Video Continual Learning",
        "author": "Lama Alssum, Juan Le\u00f3n Alc\u00e1zar, Merey Ramazanova, Chen Zhao, Bernard Ghanem",
        "abstract": "Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the video stream and store them in the replay memory. In this paper, we propose SMILE a novel replay mechanism for effective video continual learning based on individual/single frames. Through extensive experimentation, we show that under extreme memory constraints, video diversity plays a more significant role than temporal information. Therefore, our method focuses on learning from a small number of frames that represent a large number of unique videos. On three representative video datasets, Kinetics, UCF101, and ActivityNet, the proposed method achieves state-of-the-art performance, outperforming the previous state-of-the-art by up to 21.49%.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Alssum_Just_a_Glimpse_Rethinking_Temporal_Information_for_Video_Continual_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Alssum_Just_a_Glimpse_Rethinking_Temporal_Information_for_Video_Continual_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Online Distillation With Continual Learning for Cyclic Domain Shifts",
        "author": "Joachim Houyon, Anthony Cioppa, Yasir Ghunaim, Motasem Alfarra, Ana\u00efs Halin, Maxim Henry, Bernard Ghanem, Marc Van Droogenbroeck",
        "abstract": "In recent years, online distillation has emerged as a powerful technique for adapting real-time deep neural networks on the fly using a slow, but accurate teacher model. However, a major challenge in online distillation is catastrophic forgetting when the domain shifts, which occurs when the student model is updated with data from the new domain and forgets previously learned knowledge. In this paper, we propose a solution to this issue by leveraging the power of continual learning methods to reduce the impact of domain shifts. Specifically, we integrate several state-of-theart continual learning methods in the context of online distillation and demonstrate their effectiveness in reducing catastrophic forgetting. Furthermore, we provide a detailed analysis of our proposed solution in the case of cyclic domain shifts. Our experimental results demonstrate the efficacy of our approach in improving the robustness and accuracy of online distillation, with potential applications in domains such as video surveillance or autonomous driving. Overall, our work represents an important step forward in the field of online distillation and continual learning, with the potential to significantly impact real-world applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Houyon_Online_Distillation_With_Continual_Learning_for_Cyclic_Domain_Shifts_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Houyon_Online_Distillation_With_Continual_Learning_for_Cyclic_Domain_Shifts_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CLVOS23: A Long Video Object Segmentation Dataset for Continual Learning",
        "author": "Amir Nazemi, Zeyad Moustafa, Paul Fieguth",
        "abstract": "Continual learning in real-world scenarios is a major challenge. A general continual learning model should have a constant memory size and no predefined task boundaries, as is the case in semi-supervised Video Object Segmentation (VOS), where continual learning challenges particularly present themselves in working on long video sequences. In this article, we first formulate the problem of semi-supervised VOS, specifically online VOS, as a continual learning problem, and then secondly provide a public VOS dataset, CLVOS23, focusing on continual learning. Finally, we propose and implement a regularization-based continual learning approach on LWL, an existing online VOS baseline, to demonstrate the efficacy of continual learning when applied to online VOS and to establish a CLVOS23 baseline. We apply the proposed baseline to the Long Videos dataset as well as to two short video VOS datasets, DAVIS16 and DAVIS17. To the best of our knowledge, this is the first time that VOS has been defined and addressed as a continual learning problem.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Nazemi_CLVOS23_A_Long_Video_Object_Segmentation_Dataset_for_Continual_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CLVision/papers/Nazemi_CLVOS23_A_Long_Video_Object_Segmentation_Dataset_for_Continual_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi Event Localization by Audio-Visual Fusion With Omnidirectional Camera and Microphone Array",
        "author": "Wenru Zheng, Ryota Yoshihashi, Rei Kawakami, Ikuro Sato, Asako Kanezaki",
        "abstract": "Audio-visual fusion is a promising approach for identifying multiple events occurring simultaneously at different locations in the real world. Previous studies on audio-visual event localization (AVE) have been built on datasets that only have monaural or stereo channels in the audio; thus, it was hard to distinguish the direction of audio when different sounds are heard from multiple locations. In this paper, we develop a multi-event localization method using multi-channel audio and omnidirectional images. To take full advantage of the spatial correlation between the features in the two modalities, our method employs early fusion we propose a new fusion method that can retain audio direction and background information in images. We also created a new dataset of multi-label events containing around 660 omnidirectional videos with multi-channel audio, which was used to showcase the effectiveness of the proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Zheng_Multi_Event_Localization_by_Audio-Visual_Fusion_With_Omnidirectional_Camera_and_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Zheng_Multi_Event_Localization_by_Audio-Visual_Fusion_With_Omnidirectional_Camera_and_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval",
        "author": "Jae Myung Kim, A. Sophia Koepke, Cordelia Schmid, Zeynep Akata",
        "abstract": "Cross-modal retrieval methods are the preferred tool to search databases for the text that best matches a query image and vice versa However, image-text retrieval models commonly learn to memorize spurious correlations in the training data, such as frequent object co-occurrence, instead of looking at the real underlying reasons of the prediction in the image. For image-text retrieval, this manifests in retrieved sentences that mention objects that are not present in the query image. In this work, we introduce ODmAP@k, an object decorrelation metric that measures a model's robustness to spurious correlations in the training data. We use automatic image and text manipulations to control the presence of such object correlations in designated test data. Additionally, our data synthesis technique is used to tackle model biases due to spurious correlations of semantically unrelated objects in the training data. We apply our proposed pipeline, which involves the finetuning of image-text retrieval frameworks on carefully designed synthetic data, to three state-of-the-art models for image-text retrieval. This results in significant improvements for all three models, both in terms of the standard retrieval performance and in terms of our object decorrelation metric. The code is available at https://github.com/ExplainableML/Spurious_CM_Retrieval.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Kim_Exposing_and_Mitigating_Spurious_Correlations_for_Cross-Modal_Retrieval_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Kim_Exposing_and_Mitigating_Spurious_Correlations_for_Cross-Modal_Retrieval_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Adapting Grounded Visual Question Answering Models to Low Resource Languages",
        "author": "Ying Wang, Jonas Pfeiffer, Nicolas Carion, Yann LeCun, Aishwarya Kamath",
        "abstract": "While huge progress has been made on a variety of vision and language tasks in recent years, most major advances have been restricted to the English language due to the scarcity of relevant training and evaluation datasets in other languages. A popular approach to address this gap, has been to utilize machine-translated multi-modal datasets or multi-lingual text-only datasets for pre-training. This approach not only fails to exploit existing pre-trained state-of-the-art English multi-modal models, but also is not a viable solution for low-resource languages where translation quality is not as reliable. Therefore, we propose xMDETR, a multi-lingual grounded vision-language model based on the state-of-the-art model MDETR, by adapting it to new languages without machine-translated data, while also keeping most of the pre-trained weights frozen. xMDETR leverages mono-lingual pre-trained MDETR to achieve results competitive to state of the art on xGQA, a standard multilingual VQA benchmark. It is also interpretable, providing bounding boxes for key phrases in the multi-lingual questions. Our method utilizes several architectural as well as data-driven techniques such as training a new embedding space with a Masked Language Modeling (MLM) objective, code-switching, and adapters for efficient and modular training. We also explore contrastive losses to enforce the bridging of multi-modal and multi-lingual representations on multi-lingual multi-modal data, when available. We evaluate xMDETR on xGQA in both zero-shot and few-shot settings, improving results on Portuguese, Indonesian and Bengali, while remaining competitive on other languages.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Wang_Adapting_Grounded_Visual_Question_Answering_Models_to_Low_Resource_Languages_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Wang_Adapting_Grounded_Visual_Question_Answering_Models_to_Low_Resource_Languages_CVPRW_2023_paper.pdf"
    },
    {
        "title": "The MONET Dataset: Multimodal Drone Thermal Dataset Recorded in Rural Scenarios",
        "author": "Luigi Riz, Andrea Caraffa, Matteo Bortolon, Mohamed Lamine Mekhalfi, Davide Boscaini, Andr\u00e9 Moura, Jos\u00e9 Antunes, Andr\u00e9 Dias, Hugo Silva, Andreas Leonidou, Christos Constantinides, Christos Keleshis, Dante Abate, Fabio Poiesi",
        "abstract": "We present MONET, a new multimodal dataset captured using a thermal camera mounted on a drone that flew over rural areas, and recorded human and vehicle activities. We captured MONET to study the problem of object localisation and behaviour understanding of targets undergoing large-scale variations and being recorded from different and moving viewpoints. Target activities occur in two different land sites, each with unique scene structures and cluttered backgrounds. MONET consists of approximately 53K images featuring 162K manually annotated bounding boxes. Each image is timestamp-aligned with drone metadata that includes information about attitudes, speed, altitude, and GPS coordinates. MONET is different from previous thermal drone datasets because it features multimodal data, including rural scenes captured with thermal cameras containing both person and vehicle targets, along with trajectory information and metadata. We assessed the difficulty of the dataset in terms of transfer learning between the two sites and evaluated nine object detection algorithms to identify the open challenges associated with this type of data.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Riz_The_MONET_Dataset_Multimodal_Drone_Thermal_Dataset_Recorded_in_Rural_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Riz_The_MONET_Dataset_Multimodal_Drone_Thermal_Dataset_Recorded_in_Rural_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SSGVS: Semantic Scene Graph-to-Video Synthesis",
        "author": "Yuren Cong, Jinhui Yi, Bodo Rosenhahn, Michael Ying Yang",
        "abstract": "As a natural extension of the image synthesis task, video synthesis has attracted a lot of interest recently. Many image synthesis works utilize class labels or text as guidance. However, neither labels nor text can provide explicit temporal guidance, such as when an action starts or ends. To overcome this limitation, we introduce semantic video scene graphs as input for video synthesis, as they represent the spatial and temporal relationships between objects in the scene. Since video scene graphs are usually temporally discrete annotations, we propose a video scene graph (VSG) encoder that not only encodes the existing video scene graphs but also predicts the graph representations for unlabeled frames. The VSG encoder is pre-trained with different contrastive multi-modal losses. A semantic scene graph-to-video synthesis framework (SSGVS), based on the pre-trained VSG encoder, VQ-VAE, and auto-regressive Transformer, is proposed to synthesize a video given an initial scene image and a non-fixed number of semantic scene graphs. We evaluate SSGVS and other state-of-the-art video synthesis models on the Action Genome dataset and demonstrate the positive significance of video scene graphs in video synthesis.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Cong_SSGVS_Semantic_Scene_Graph-to-Video_Synthesis_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Cong_SSGVS_Semantic_Scene_Graph-to-Video_Synthesis_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TFRGAN: Leveraging Text Information for Blind Face Restoration With Extreme Degradation",
        "author": "Chengxing Xie, Qian Ning, Weisheng Dong, Guangming Shi",
        "abstract": "Blind face restoration aims to recover high-quality face images from unknown degraded low-quality images. Previous works that are based on geometric or generative priors have achieved impressive performance, but the task remains challenging, particularly when it comes to restoring severely degraded faces. To address this issue, we propose a novel approach TFRGAN, that leverages textual information to improve the restoration of extremely degraded face images. Specifically, we propose to generate a better and more accurate latent code for StyleGAN2 prior via fusing the text and image information in the latent code space. Besides, extracted textual features are used to modulate the decoding features to obtain more realistic and natural facial images with more reasonable details. Experimental results demonstrate the superiority of the proposed method for restoring severely degraded face images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Xie_TFRGAN_Leveraging_Text_Information_for_Blind_Face_Restoration_With_Extreme_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Xie_TFRGAN_Leveraging_Text_Information_for_Blind_Face_Restoration_With_Extreme_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Dynamic Multimodal Fusion",
        "author": "Zihui Xue, Radu Marculescu",
        "abstract": "Deep multimodal learning has achieved great progress in recent years. However, current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data. In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference. To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency. Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach. For instance, DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches. We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Xue_Dynamic_Multimodal_Fusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Xue_Dynamic_Multimodal_Fusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SEM-POS: Grammatically and Semantically Correct Video Captioning",
        "author": "Asmar Nadeem, Adrian Hilton, Robert Dawes, Graham Thomas, Armin Mustafa",
        "abstract": "Generating grammatically and semantically correct captions in video captioning is a challenging task. The captions generated from the existing methods are either word-by-word that do not align with grammatical structure or miss key information from the input videos. To address these issues, we introduce a novel global-local fusion network, with a Global-Local Fusion Block (GLFB) that encodes and fuses features from different parts of speech (POS) components with visual-spatial features. We use novel combinations of different POS components - 'determinant + subject', 'auxiliary verb', 'verb', and 'determinant + object' for supervision of the POS blocks - Det + Subject, Aux Verb, Verb, and Det + Object respectively. The novel global-local fusion network together with POS blocks helps align the visual features with language description to generate grammatically and semantically correct captions. Extensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT datasets demonstrate that the proposed approach generates more grammatically and semantically correct captions compared to the existing methods, achieving the new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate the impact of the contributions on the proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Nadeem_SEM-POS_Grammatically_and_Semantically_Correct_Video_Captioning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Nadeem_SEM-POS_Grammatically_and_Semantically_Correct_Video_Captioning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robust Multiview Multimodal Driver Monitoring System Using Masked Multi-Head Self-Attention",
        "author": "Yiming Ma, Victor Sanchez, Soodeh Nikan, Devesh Upadhyay, Bhushan Atote, Tanaya Guha",
        "abstract": "Driver Monitoring Systems (DMSs) are crucial for safe hand-over actions in Level-2+ self-driving vehicles. State-of-the-art DMSs leverage multiple sensors mounted at different locations to monitor the driver and the vehicle's interior scene and employ decision-level fusion to integrate these heterogenous data. However, this fusion method may not fully utilize the complementarity of different data sources and may overlook their relative importance. To address these limitations, we propose a novel multiview multimodal driver monitoring system based on feature-level fusion through multi-head self-attention (MHSA). We demonstrate its effectiveness by comparing it against four alternative fusion strategies (Sum, Conv, SE, and AFF). We also present a novel GPU-friendly supervised contrastive learning framework SuMoCo to learn better representations. Furthermore, We fine-grained the test split of the DAD dataset to enable the multi-class recognition of drivers' activities. Experiments on this enhanced database demonstrate that 1) the proposed MHSA-based fusion method (AUC-ROC: 97.0%) outperforms all baselines and previous approaches, and 2) training MHSA with patch masking can improve its robustness against modality/view collapses. The code and annotations are publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/html/Ma_Robust_Multiview_Multimodal_Driver_Monitoring_System_Using_Masked_Multi-Head_Self-Attention_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Ma_Robust_Multiview_Multimodal_Driver_Monitoring_System_Using_Masked_Multi-Head_Self-Attention_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Causalainer: Causal Explainer for Automatic Video Summarization",
        "author": "Jia-Hong Huang, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hung Chen, Marcel Worring",
        "abstract": "The goal of video summarization is to automatically shorten videos such that it conveys the overall story without losing relevant information. In many application scenarios, improper video summarization can have a large impact. For example in forensics, the quality of the generated video summary will affect an investigator's judgment while in journalism it might yield undesired bias. Because of this, modeling explainability is a key concern. One of the best ways to address the explainability challenge is to uncover the causal relations that steer the process and lead to the result. Current machine learning-based video summarization algorithms learn optimal parameters but do not uncover causal relationships. Hence, they suffer from a relative lack of explainability. In this work, a Causal Explainer, dubbed Causalainer, is proposed to address this issue. Multiple meaningful random variables and their joint distributions are introduced to characterize the behaviors of key components in the problem of video summarization. In addition, helper distributions are introduced to enhance the effectiveness of model training. In visual-textual input scenarios, the extra input can decrease the model performance. A causal semantics extractor is designed to tackle this issue by effectively distilling the mutual information from the visual and textual inputs. Experimental results on commonly used benchmarks demonstrate that the proposed method achieves state-of-the-art performance while being more explainable.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/html/Huang_Causalainer_Causal_Explainer_for_Automatic_Video_Summarization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/papers/Huang_Causalainer_Causal_Explainer_for_Automatic_Video_Summarization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Abstract Visual Reasoning Enabled by Language",
        "author": "Giacomo Camposampiero, Lo\u00efc Houmard, Benjamin Estermann, Jo\u00ebl Mathys, Roger Wattenhofer",
        "abstract": "While artificial intelligence (AI) models have achieved human or even superhuman performance in many well-defined applications, they still struggle to show signs of broad and flexible intelligence. The Abstraction and Reasoning Corpus (ARC), a visual intelligence benchmark introduced by Francois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific program searches to brute-force solutions for the tasks present in ARC. In this work, we propose a general learning-based framework for solving ARC. It is centered on transforming tasks from the vision to the language domain. This composition of language and vision allows for pre-trained models to be leveraged at each stage, enabling a shift from handcrafted priors towards the learned priors of the models. While not yet beating state-of-the-art models on ARC, we demonstrate the potential of our approach, for instance, by solving some ARC tasks that have not been solved previously.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/html/Camposampiero_Abstract_Visual_Reasoning_Enabled_by_Language_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/papers/Camposampiero_Abstract_Visual_Reasoning_Enabled_by_Language_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Is Multimodal Vision Supervision Beneficial to Language?",
        "author": "Avinash Madasu, Vasudev Lal",
        "abstract": "Vision (image & video) - Language (VL) pre-training is the recent popular paradigm that achieved state-of-the-art results on multi-modal tasks like image-retrieval, video-retrieval, visual question answering etc. These models are trained in an unsupervised way and greatly benefit from the complementary modality supervision. In this paper, we explore if the language representations trained using vision supervision perform better than vanilla language representations on Natural Language Understanding and common-sense reasoning benchmarks. We experiment with a diverse set of image-text models such as ALBEF, BLIP, METER and video-text models like ALPRO, Frozen in Time, VIOLET. We compare the performance of language representations of stand-alone text encoders of these models to the language representations of text encoders learnt through vision supervision. Our experiments suggest that vanilla language representations show superior performance on most of the tasks. These results shed light on the current drawbacks of the vision-language models.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/html/Madasu_Is_Multimodal_Vision_Supervision_Beneficial_to_Language_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/papers/Madasu_Is_Multimodal_Vision_Supervision_Beneficial_to_Language_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Learning CLIP Guided Visual-Text Fusion Transformer for Video-Based Pedestrian Attribute Recognition",
        "author": "Jun Zhu, Jiandong Jin, Zihan Yang, Xiaohao Wu, Xiao Wang",
        "abstract": "Existing pedestrian attribute recognition (PAR) algorithms are mainly developed based on a static image. However, the performance is not reliable for images with challenging factors, such as heavy occlusion, motion blur, etc. In this work, we propose to understand human attributes using video frames that can make full use of temporal information. Specifically, we formulate the video-based PAR as a vision-language fusion problem and adopt pre-trained big models CLIP to extract the feature embeddings of given video frames. To better utilize the semantic information, we take the attribute list as another input and transform the attribute words/phase into the corresponding sentence via split, expand, and prompt. Then, the text encoder of CLIP is utilized for language embedding. The averaged visual tokens and text tokens are concatenated and fed into a fusion Transformer for multi-modal interactive learning. The enhanced tokens will be fed into a classification head for pedestrian attribute prediction. Extensive experiments on a large-scale video-based PAR dataset fully validated the effectiveness of our proposed framework. Both the source code and pre-trained models will be released.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/html/Zhu_Learning_CLIP_Guided_Visual-Text_Fusion_Transformer_for_Video-Based_Pedestrian_Attribute_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/papers/Zhu_Learning_CLIP_Guided_Visual-Text_Fusion_Transformer_for_Video-Based_Pedestrian_Attribute_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Where Are They Looking in the 3D Space?",
        "author": "Nora Horanyi, Linfang Zheng, Eunji Chong, Ale\u0161 Leonardis, Hyung Jin Chang",
        "abstract": "We propose a novel depth-aware joint attention target estimation framework that estimates the attention target in 3D space. Our goal is to mimic human's ability to understand where each person is looking in their proximity. In this work, we tackle the previously unexplored problem of utilising a depth prior along with a 3D joint FOV probability map to estimate the joint attention target of people in the scene. We leverage the insight that besides the 2D image content, strong gaze-related constraints exist in the depth order of the scene and different subject-specific attributes. Extensive experiments show that our method outperforms favourably against existing joint attention target estimation methods on the VideoCoAtt benchmark dataset. Despite the proposed framework being designed for joint attention target estimation, we show that it outperforms single attention target estimation methods on both the GazeFollow image and the VideoAttentionTarget video benchmark datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/html/Horanyi_Where_Are_They_Looking_in_the_3D_Space_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Horanyi_Where_Are_They_Looking_in_the_3D_Space_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multimodal Integration of Human-Like Attention in Visual Question Answering",
        "author": "Ekta Sood, Fabian K\u00f6gel, Philipp M\u00fcller, Dominike Thomas, Mihai B\u00e2ce, Andreas Bulling",
        "abstract": "Human-like attention as a supervisory signal to guide neural attention has shown significant promise but is currently limited to unimodal integration - even for inherently multimodal tasks such as visual question answering (VQA). We present the Multimodal Human-Like Attention Network (MULAN) - the first method for multimodal integration of human-like attention on image and text during training of VQA models. MULAN integrates attention predictions from two state-of-the-art text and image saliency models into neural self-attention layers of a recent transformer-based VQA model. Through evaluations on the challenging VQAv2 dataset, we show that MULAN is competitive to state of the art in its model class - achieving 73.98% accuracy on test-std and 73.72% on test-dev with approximately 80% fewer trainable parameters than prior work. Overall, our work underlines the potential of integrating multimodal human-like attention into neural attention mechanisms for VQA.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/html/Sood_Multimodal_Integration_of_Human-Like_Attention_in_Visual_Question_Answering_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Sood_Multimodal_Integration_of_Human-Like_Attention_in_Visual_Question_Answering_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Kappa Angle Regression With Ocular Counter-Rolling Awareness for Gaze Estimation",
        "author": "Shiwei Jin, Ji Dai, Truong Nguyen",
        "abstract": "Conventional appearance-based 3D gaze estimation methods generally use the roll of the head pose to represent the eyeball's roll status by default. To reduce degrees of freedom of head poses, a normalization step was proposed to apply global transformations to images to make heads upright and eyelids horizontal. However, due to the ocular countering-rolling (OCR) response, the eyeball will rotate in the opposite direction when the head tilts to the side. After normalization, the eyeball will have an extra roll compared to the roll status of the eyeball when the head is not tilted. This roll from the OCR response causes a changed orientation of the eyeball in normalized eye images, which represents the roll status of the anatomical structure inside the eyeball and consequently affects gaze directions. Thus in this work, we propose a pipeline to regress the person-dependent anatomical variation as a calibration process with considering the OCR response, which can work with our proposed eye-image-based person-independent gaze estimator trained with real and synthetic eye images. The proposed method firstly brings the OCR response into the gaze estimation task, achieving better performances on the two benchmark datasets with fewer parameters under the real-time scenarios. With a replacement of a deeper network, compared to state-of-the-art methods, the proposed method is more efficient, achieving a). better average estimate (3.9% and 2.5% improvement), b). much better standard deviation (lower by 59.0% and 44.2%) and c). a much lower number of parameters (reduced by 88.0%).",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/html/Jin_Kappa_Angle_Regression_With_Ocular_Counter-Rolling_Awareness_for_Gaze_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Jin_Kappa_Angle_Regression_With_Ocular_Counter-Rolling_Awareness_for_Gaze_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "GazeCaps: Gaze Estimation With Self-Attention-Routed Capsules",
        "author": "Hengfei Wang, Jun O. Oh, Hyung Jin Chang, Jin Hee Na, Minwoo Tae, Zhongqun Zhang, Sang-Il Choi",
        "abstract": "Gaze estimation is the task of estimating eye gaze from facial features. People tend to infer gaze by considering different facial properties from the whole image and their relations. However, existing methods rarely consider these various properties. In this paper, we propose a novel GazeCaps framework that represents various facial properties as different capsules. The capsules respond sensitively to transforms of facial properties by vectorial expression, which is effective for gaze estimation in which many facial components are nonlinearly transformed according to the direction of the head in addition to the perspective. Furthermore, we propose a Self-Attention Routing (SAR) module which can dynamically allocate attention to different capsules that contain important information and can be optimized as a single process without iterations. Through rigorous experiments, we confirm that the proposed method achieves state-of-the-art performance on various benchmarks. We also detail the generalization performance of the proposed model through a cross-dataset evaluation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/html/Wang_GazeCaps_Gaze_Estimation_With_Self-Attention-Routed_Capsules_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Wang_GazeCaps_Gaze_Estimation_With_Self-Attention-Routed_Capsules_CVPRW_2023_paper.pdf"
    },
    {
        "title": "EFE: End-to-End Frame-To-Gaze Estimation",
        "author": "Haldun Balim, Seonwook Park, Xi Wang, Xucong Zhang, Otmar Hilliges",
        "abstract": "Despite the recent development of learning-based gaze estimation methods, most methods require one or more eye or face region crops as inputs and produce a gaze direction vector as output. Cropping results in a higher resolution in the eye regions and having fewer confounding factors (such as clothing and hair) is believed to benefit the final model performance. However, this eye/face patch cropping process is expensive, erroneous, and implementation-specific for different methods. In this paper, we propose a frame-to-gaze network that directly predicts both 3D gaze origin and 3D gaze direction from the raw frame out of the camera without any face or eye cropping. Our method demonstrates that direct gaze regression from the raw downscaled frame, from FHD/HD to VGA/HVGA resolution, is possible despite the challenges of having very few pixels in the eye region. The proposed method achieves comparable results to state-of-the-art methods in Point-of-Gaze (PoG) estimation on three public gaze datasets: GazeCapture, MPIIFaceGaze, and EVE, and generalizes well to extreme camera view changes.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/html/Balim_EFE_End-to-End_Frame-To-Gaze_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Balim_EFE_End-to-End_Frame-To-Gaze_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "IPD-Net: SO(3) Invariant Primitive Decompositional Network for 3D Point Clouds",
        "author": "Ramesh Ashok Tabib, Nitishkumar Upasi, Tejas Anvekar, Dikshit Hegde, Uma Mudenagudi",
        "abstract": "In this paper, we propose IPD-Net: Invariant Primitive Decompositional Network, a SO(3) invariant framework for decomposition of a point cloud. The human cognitive system is able to identify and interpret familiar objects regardless of their orientation and abstraction. Recent research aims to bring this capability to machines for understanding the 3D world. In this work, we present a framework inspired by human cognition to decompose point clouds into four primitive 3D shapes (plane, cylinder, cone, and sphere) and enable machines to understand the objects in various orientations. We employ Implicit Invariant Features (IIF) to learn local geometric relations by implicitly representing the point cloud with enhanced geometric information invariant towards SO(3) rotations. We also use Spatial Rectification Unit (SRU) to extract invariant global signatures. We demonstrate the results of our proposed methodology for SO(3) invariant decomposition on TraceParts Dataset, and show the generalizability of proposed IPD-Net as plugin for downstream task on classification of point clouds. We compare the results of classification with state-of-the-art methods on benchmark dataset (ModelNet40).",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/html/Tabib_IPD-Net_SO3_Invariant_Primitive_Decompositional_Network_for_3D_Point_Clouds_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/papers/Tabib_IPD-Net_SO3_Invariant_Primitive_Decompositional_Network_for_3D_Point_Clouds_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Attention-Based Part Assembly for 3D Volumetric Shape Modeling",
        "author": "Chengzhi Wu, Junwei Zheng, Julius Pfrommer, J\u00fcrgen Beyerer",
        "abstract": "Modeling a 3D volumetric shape as an assembly of decomposed shape parts is much more challenging, but semantically more valuable than direct reconstruction from a full shape representation. The neural network needs to implicitly learn part relations coherently, which is typically performed by dedicated network layers that can generate transformation matrices for each part. In this paper, we propose a VoxAttention network architecture for attention-based part assembly. We further propose a variant of using channel-wise part attention and show the advantages of this approach. Experimental results show that our method outperforms most state-of-the-art methods for the part relation-aware 3D shape modeling task.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/html/Wu_Attention-Based_Part_Assembly_for_3D_Volumetric_Shape_Modeling_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/papers/Wu_Attention-Based_Part_Assembly_for_3D_Volumetric_Shape_Modeling_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Octree Transformer: Autoregressive 3D Shape Generation on Hierarchically Structured Sequences",
        "author": "Moritz Ibing, Gregor Kobsik, Leif Kobbelt",
        "abstract": "Autoregressive models have proven to be very powerful in NLP text generation tasks and lately have gained popularity for image generation as well. However, they have seen limited use for the synthesis of 3D shapes so far. This is mainly due to the lack of a straightforward way to linearize 3D data as well as to scaling problems with the length of the resulting sequences when describing complex shapes. In this work we address both of these problems. We use octrees as a compact hierarchical shape representation that can be sequentialized by traversal ordering. Moreover, we introduce an adaptive compression scheme, that significantly reduces sequence lengths and thus enables their effective generation with a transformer, while still allowing fully autoregressive sampling and parallel training. We demonstrate the performance of our model by comparing against the state-of-the-art in shape generation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/html/Ibing_Octree_Transformer_Autoregressive_3D_Shape_Generation_on_Hierarchically_Structured_Sequences_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/papers/Ibing_Octree_Transformer_Autoregressive_3D_Shape_Generation_on_Hierarchically_Structured_Sequences_CVPRW_2023_paper.pdf"
    },
    {
        "title": "3DSSR: 3D Subscene Retrieval",
        "author": "Reza Asad, Manolis Savva",
        "abstract": "We present the task of 3D subscene retrieval (3DSSR). In this task, a user specifies a query object and a set of context objects in a 3D scene. Then, a system retrieves and ranks subscenes from a database of 3D scenes that best correspond to the configuration defined by the query. This formulation generalizes prior work on context-based 3D object retrieval and 3D scene retrieval. To tackle this task we present PointCrop: a self-supervised point cloud encoder training scheme that enables retrieval of geometrically similar subscenes without relying on object category supervision. We evaluate PointCrop against alternative methods and baselines through a suite of evaluation metrics that measure the degree of subscene correspondence. Our experiments show that PointCrop training outperforms supervised and prior self-supervised training paradigms by 4.33% and 9.11% in mAP respectively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/html/Asad_3DSSR_3D_Subscene_Retrieval_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/papers/Asad_3DSSR_3D_Subscene_Retrieval_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SepicNet: Sharp Edges Recovery by Parametric Inference of Curves in 3D Shapes",
        "author": "Kseniya Cherenkova, Elona Dupont, Anis Kacem, Ilya Arzhannikov, Gleb Gusev, Djamila Aouada",
        "abstract": "3D scanning as a technique to digitize objects in reality and create their 3D models, is used in many fields and areas. Though the quality of 3D scans depends on the technical characteristics of the 3D scanner, the common drawback is the smoothing of fine details, or the edges of an object. We introduce SepicNet, a novel deep network for the detection and parametrization of sharp edges in 3D shapes as primitive curves. To make the network end-to-end trainable, we formulate the curve fitting in a differentiable manner. We develop an adaptive point cloud sampling technique that captures the sharp features better than uniform sampling. The experiments were conducted on a newly introduced large-scale dataset of 50k 3D scans, where the sharp edge annotations were extracted from their parametric CAD models, and demonstrate significant improvement over state-of-the-art methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/html/Cherenkova_SepicNet_Sharp_Edges_Recovery_by_Parametric_Inference_of_Curves_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/papers/Cherenkova_SepicNet_Sharp_Edges_Recovery_by_Parametric_Inference_of_Curves_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "OO-dMVMT: A Deep Multi-View Multi-Task Classification Framework for Real-Time 3D Hand Gesture Classification and Segmentation",
        "author": "Federico Cunico, Federico Girella, Andrea Avogaro, Marco Emporio, Andrea Giachetti, Marco Cristani",
        "abstract": "Continuous mid-air hand gesture recognition based on captured hand pose streams is fundamental for human-computer interaction, particularly in AR / VR. However, many of the methods proposed to recognize heterogeneous hand gestures are tested only on the classification task, and the real-time low-latency gesture segmentation in a continuous stream is not well addressed in the literature. For this task, we propose the On-Off deep Multi-View Multi-Task paradigm (OO-dMVMT). The idea is to exploit multiple time-local views related to hand pose and movement to generate rich gesture descriptions, along with using heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the classical MVMT paradigm, where all of the multiple tasks have to be active at each time, by allowing specific tasks to switch on/off depending on whether they can apply to the input. We show that OO-dMVMT defines the new SotA on continuous/online 3D skeleton-based gesture recognition in terms of gesture classification accuracy, segmentation accuracy, false positives, and decision latency while maintaining real-time operation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/html/Cunico_OO-dMVMT_A_Deep_Multi-View_Multi-Task_Classification_Framework_for_Real-Time_3D_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/papers/Cunico_OO-dMVMT_A_Deep_Multi-View_Multi-Task_Classification_Framework_for_Real-Time_3D_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Face Image Lighting Enhancement Using a 3D Model",
        "author": "Qiulin Chen, Jan P. Allebach",
        "abstract": "Image enhancement helps to generate balanced lighting distributions over faces. Our goal is to get an illuminance-balanced enhanced face image from a single view. Traditionally, image enhancement methods ignore the 3D geometry of the face or require a complicated multi-view geometry. Other methods cause color tone shifting or over saturation. Inspired by the new research achievements in face alignment and face 3D modeling, we propose an improved face image enhancement method by leveraging 3D face models. Given a face image as input, our method will first estimate its lighting distribution. Then we build an optimization process to refine the distribution. Finally, we generate an illuminance-balanced face image from a single view. Experiments on the FiveK dataset demonstrate that our method performs well and compares favorably with other methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/html/Chen_Face_Image_Lighting_Enhancement_Using_a_3D_Model_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/papers/Chen_Face_Image_Lighting_Enhancement_Using_a_3D_Model_CVPRW_2023_paper.pdf"
    },
    {
        "title": "BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects",
        "author": "Martin Sundermeyer, Tom\u00e1\u0161 Hoda\u0148, Yann Labb\u00e9, Gu Wang, Eric Brachmann, Bertram Drost, Carsten Rother, Ji\u0159\u00ed Matas",
        "abstract": "We present the evaluation methodology, datasets and results of the BOP Challenge 2022, the fourth in a series of public competitions organized with the goal to capture the status quo in the field of 6D object pose estimation from an RGB/RGB-D image. In 2022, we witnessed another significant improvement in the pose estimation accuracy -- the state of the art, which was 56.9 AR in 2019 (Vidal et al.) and 69.8 AR in 2020 (CosyPose), moved to new heights of 83.7 AR (GDRNPP). Out of 49 pose estimation methods evaluated since 2019, the top 18 are from 2022. Methods based on point pair features, which were introduced in 2010 and achieved competitive results even in 2020, are now clearly outperformed by deep learning methods. The synthetic-to-real domain gap was again significantly reduced, with 82.7 AR achieved by GDRNPP trained only on synthetic images from BlenderProc. The fastest variant of GDRNPP reached 80.5 AR with an average time per image of 0.23s. Since most of the recent methods for 6D object pose estimation begin by detecting/segmenting objects, we also started evaluating 2D object detection and segmentation performance based on the COCO metrics. Compared to the Mask R-CNN results from CosyPose in 2020, detection improved from 60.3 to 77.3 AP and segmentation from 40.5 to 58.7 AP. The online evaluation system stays open.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/html/Sundermeyer_BOP_Challenge_2022_on_Detection_Segmentation_and_Pose_Estimation_of_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/papers/Sundermeyer_BOP_Challenge_2022_on_Detection_Segmentation_and_Pose_Estimation_of_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Three Recipes for Better 3D Pseudo-GTs of 3D Human Mesh Estimation in the Wild",
        "author": "Gyeongsik Moon, Hongsuk Choi, Sanghyuk Chun, Jiyoung Lee, Sangdoo Yun",
        "abstract": "Recovering 3D human mesh in the wild is greatly challenging as in-the-wild (ITW) datasets provide only 2D pose ground truths (GTs). Recently, 3D pseudo-GTs have been widely used to train 3D human mesh estimation networks as the 3D pseudo-GTs enable 3D mesh supervision when training the networks on ITW datasets. However, despite the great potential of the 3D pseudo-GTs, there has been no extensive analysis that investigates which factors are important to make more beneficial 3D pseudo-GTs. In this paper, we provide three recipes to obtain highly beneficial 3D pseudo-GTs of ITW datasets. The main challenge is that only 2D-based weak supervision is allowed when obtaining the 3D pseudo-GTs. Each of our three recipes addresses the challenge in each aspect: depth ambiguity, sub-optimality of weak supervision, and implausible articulation. Experimental results show that simply re-training state-of-the-art networks with our new 3D pseudo-GTs elevates their performance to the next level without bells and whistles. The 3D pseudo-GT is publicly available in https://github.com/mks0601/NeuralAnnot_RELEASE.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/html/Moon_Three_Recipes_for_Better_3D_Pseudo-GTs_of_3D_Human_Mesh_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/papers/Moon_Three_Recipes_for_Better_3D_Pseudo-GTs_of_3D_Human_Mesh_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Dual Attention Poser: Dual Path Body Tracking Based on Attention",
        "author": "Xinhan Di, Xiaokun Dai, Xinkang Zhang, Xinrong Chen",
        "abstract": "Currently, mixed reality head-mounted displays tracking the full body of users is an important human-computer interaction mode through the pose of the head and the hands. Unfortunately, users' virtual representation and experience is limited due to high reconstruction error when simple transformer network architecture is applied. In this paper, we present a novel model, named Dual Attention Poser, which can learn the whole body reconstruction at a high accuracy. The proposed model consists of three key modules. Among them, dual-path attention encoder is designed to extract feature of the sparse signals. Cross attention mixer module enable the fusion of representation in the double path. Attention-gated-mlp decoder is applied to decode the hidden feature from the sparse input through attention gate. Test results on the AMASS dataset show that Dual Attention Poser can reduce the error by up to 18.2% in comparison with the state-of-the-art results.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/html/Zhang_Dual_Attention_Poser_Dual_Path_Body_Tracking_Based_on_Attention_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/papers/Zhang_Dual_Attention_Poser_Dual_Path_Body_Tracking_Based_on_Attention_CVPRW_2023_paper.pdf"
    },
    {
        "title": "3DSAINT Representation for 3D Point Clouds",
        "author": "Chandra Kambhamettu",
        "abstract": "This paper introduces a Sphere-based representation to model a 3D scene and show its performance on various tasks, including Structure from Motion (SfM) and 3D scene classification. A significant target application of this work is Mixed Reality, where 3D data can be efficiently represented, and synthetic and real data can be mixed for an immersive experience. Over the past few decades, 3D big data has garnered increased attention in computer vision. Acquiring, representing, reconstructing, querying, classifying, and visualizing 3D models for Mixed Reality has become crucial for many applications, such as medicine, architecture, entertainment, and bioinformatics. With the ever-increasing amount of data that the 3D scanners produce, storing, processing, and transmitting the data becomes challenging. Techniques that exploit the shape information need to be developed to model, classify and visualize the data. Our work offers a novel multi-scale surface representation based on spheres, with the ultimate goal of helping scientists to see and work with 3D data in Mixed Reality more effectively and efficiently. Om Sai Ram",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/html/Kambhamettu_3DSAINT_Representation_for_3D_Point_Clouds_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/papers/Kambhamettu_3DSAINT_Representation_for_3D_Point_Clouds_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MIPI 2023 Challenge on Nighttime Flare Removal: Methods and Results",
        "author": "Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Qingpeng Zhu, Qianhui Sun, Wenxiu Sun, Chen Change Loy, Jinwei Gu, Shuai Liu, Hao Wang, Chaoyu Feng, Luyang Wang, Guangqi Shao, Chenguang Zhang, Xiaotao Wang, Lei Lei, Dafeng Zhang, Xiangyu Kong, Guanqun Liu, Mengmeng Bai, Jia Ouyang, Xiaobing Wang, Jiahui Yuan, Xinpeng Li, Chengzhi Jiang, Ting Jiang, Wenjie Lin, Qi Wu, Mingyan Han, Jinting Luo, Lei Yu, Haoqiang Fan, Shuaicheng Liu, Bo Yan, Zhuang Li, Yadong Li, Hongbin Wang, Soonyong Song, Minghan Fu, Rayyan Azam Khan, Fangxiang Wu, Zhao Zhang, Suiyi Zhao, Huan Zheng, Yangcheng Gao, Yanyan Wei, Jiahuan Ren, Bo Wang, Yan Luo, Shuaibo Gao, Wenhui Wu, Sicong Kang, Nikhil Akalwadi, Ankit Raichur, Vinod Patil, Allabakash G, Swaroop A, Amogh Joshi, Chaitra Desai, Ramesh Ashok Tabib, Ujwala Patil, Uma Mudenagudi, Sicheng Li, Ruoxi Zhu, Jiazheng Lian, Shusong Xu, Zihao Liu, Sabari Nathan, Priya Kansal",
        "abstract": "Developing and integrating advanced image sensors with novel algorithms in camera systems are prevalent with the increasing demand for computational photography and imaging on mobile platforms. However, the lack of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI). With the success of the 1st MIPI Workshop@ECCV 2022, we introduce the second MIPI challenge including four tracks focusing on novel image sensors and imaging algorithms. In this paper, we summarize and review the Nighttime Flare Removal track on MIPI 2023. In total, 120 participants were successfully registered, and 11 teams submitted results in the final testing phase. The developed solutions in this challenge achieved state-of-the-art performance on Nighttime Flare Removal. A detailed description of all models developed in this challenge is provided in this paper. More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2023/.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Dai_MIPI_2023_Challenge_on_Nighttime_Flare_Removal_Methods_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Dai_MIPI_2023_Challenge_on_Nighttime_Flare_Removal_Methods_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MIPI 2023 Challenge on RGBW Remosaic: Methods and Results",
        "author": "Qianhui Sun, Qingyu Yang, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Yuekun Dai, Wenxiu Sun, Qingpeng Zhu, Chen Change Loy, Jinwei Gu, Yuqing Liu, Hongyuan Yu, Weichen Yu, Zhen Dong, Binnan Han, Qi Jia, Xuanwu Yin, Kunlong Zuo, Yaqi Wu, Zhihao Fan, Fanqing Meng, Xun Wu, Jiawei Zhang, Feng Zhang, Mingyan Han, Jinting Luo, Qi Wu, Ting Jiang, Chengzhi Jiang, Wenjie Lin, Xinpeng Li, Lei Yu, Haoqiang Fan, Shuaicheng Liu",
        "abstract": "Developing and integrating advanced image sensors with novel algorithms in camera systems are prevalent with the increasing demand for computational photography and imaging on mobile platforms. However, the lack of high-quality data for research and the rare opportunity for an in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI). With the success of the 1st MIPI Workshop@ECCV 2022, we introduce the second MIPI challenge, including four tracks focusing on novel image sensors and imaging algorithms. This paper summarizes and reviews the RGBW Joint Remosaic and Denoise track on MIPI 2023. In total, 81 participants were successfully registered, and 4 teams submitted results in the final testing phase. The final results are evaluated using objective metrics, including PSNR, SSIM, LPIPS, and KLD. A detailed description of the top three models developed in this challenge is provided in this paper. More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2023/.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Sun_MIPI_2023_Challenge_on_RGBW_Remosaic_Methods_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Sun_MIPI_2023_Challenge_on_RGBW_Remosaic_Methods_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Efficient Multi-Exposure Image Fusion via Filter-Dominated Fusion and Gradient-Driven Unsupervised Learning",
        "author": "Kaiwen Zheng, Jie Huang, Hu Yu, Feng Zhao",
        "abstract": "Multi exposure image fusion (MEF) aims to produce images with a high dynamic range of visual perception by integrating complementary information from different exposure levels, bypassing common sensors' physical limits. Despite the marvelous progress made by deep learning-based methods, few considerations have been given to the innovation of fusion paradigms, leading to insufficient model capacity utilization. This paper proposes a novel filter prediction-dominated fusion paradigm toward a simple yet effective MEF. Precisely, we predict a series of spatial-adaptive filters conditioned on the hierarchically represented features to perform an image-level dynamic fusion. The proposed paradigm has the following merits over the previous: 1) it circumvents the risk of information loss arising from the implicit encoding and decoding processes within the neural network, and 2) it better integrates local information to obtain better continuous spatial representations than the weight map-based paradigm. Furthermore, we propose a Gradient-driven Image Fidelity (GIF) loss for unsupervised MEF. Empowered by the exploitation of informative property in the gradient domain, GIF is able to implement a stable distortion-free optimization process. Experimental results demonstrate that our method achieves the best visual performance compared to the state-of-the-art while achieving an almost 30% improvement in inference time. The code is available at https://github.com/keviner1/FFMEF.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Zheng_Efficient_Multi-Exposure_Image_Fusion_via_Filter-Dominated_Fusion_and_Gradient-Driven_Unsupervised_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Zheng_Efficient_Multi-Exposure_Image_Fusion_via_Filter-Dominated_Fusion_and_Gradient-Driven_Unsupervised_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Asymmetric Color Transfer With Consistent Modality Learning",
        "author": "Kaiwen Zheng, Jie Huang, Man Zhou, Feng Zhao",
        "abstract": "The mono-color dual-lens system widely exists in the smartphone that captures asymmetric stereo image pairs, including high-resolution (HR) monochrome images and low-resolution (LR) color images. Asymmetric color transfer aims to reconstruct an HR color image by transferring the color information of the LR color image to the HR monochrome image. However, the inconsistency of spectral resolution and spatial resolution between stereo image pairs poses a challenge for establishing reliable stereo correspondence for precise color transfer. Previous works have not adequately addressed this issue. In this paper, we propose a dual-modality consistency learning framework to assist the establishment of reliable stereo correspondence. According to the complementarity of color and frequency information between stereo images, a dual-branch Stereo Information Complementary Module (SICM) is devised to perform the consistent modality learning in feature domain. Specifically, we meticulously design the stereo frequency and color modulation mechanism equipped in the SICM for capturing the information complementarity between dual-modal features. Furthermore, a parallax attention distillation is proposed to drive consistent modality learning for better stero matching. Extensive experiments demonstrate that our model outperforms the state-of-the-art methods in the Flickr1024 dataset and has superior generalization ability over the KITTI dataset and real-world scenarios. The code is available at https://github.com/keviner1/SICNet.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Zheng_Asymmetric_Color_Transfer_With_Consistent_Modality_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Zheng_Asymmetric_Color_Transfer_With_Consistent_Modality_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FF-Former: Swin Fourier Transformer for Nighttime Flare Removal",
        "author": "Dafeng Zhang, Jia Ouyang, Guanqun Liu, Xiaobing Wang, Xiangyu Kong, Zhezhu Jin",
        "abstract": "In the process of removing nighttime flare, it is crucial to have a large receptive field due to the fact that flare can occupy a substantial portion of an image, even potentially the entire image. However, the conventional window-based Transformer approaches restrict the receptive field within the window, limiting its ability to capture global features. And the flare can cause the dark regions to become brighter and result in a loss of contrast and alteration of the frequency characteristics of the image. To address these challenges, we introduce FF-Former, which is based on Fast Fourier Convolution (FFC) and is designed to extract global frequency features for enhancing nighttime flare removal. To achieve this, we incorporate a Spatial Frequency Block (SFB) after the Swin Transformer, which forms the Swin Fourier Transformer Block (SFTB). This configuration enables the establishment of long dependencies and the extraction of global features. Unlike the traditional Transformer, which relies on global self-attention, the SFB module only performs convolution computation, making it both effective and efficient. Additionally, during the training phase, we optimize the loss function to preserve the light source points after nighttime flare removal. Experimental results on both real-world and synthetic benchmarks demonstrate that the proposed FF-Former significantly improves the performance of nighttime flare removal.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Zhang_FF-Former_Swin_Fourier_Transformer_for_Nighttime_Flare_Removal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Zhang_FF-Former_Swin_Fourier_Transformer_for_Nighttime_Flare_Removal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MIPI 2023 Challenge on RGBW Fusion: Methods and Results",
        "author": "Qianhui Sun, Qingyu Yang, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Yuekun Dai, Wenxiu Sun, Qingpeng Zhu, Chen Change Loy, Jinwei Gu, Hongyuan Yu, Yuqing Liu, Weichen Yu, Lin Ge, Xiaolin Zhang, Qi Jia, Heng Zhang, Xuanwu Yin, Kunlong Zuo, Qi Wu, Wenjie Lin, Ting Jiang, Chengzhi Jiang, Mingyan Han, Xinpeng Li, Jinting Luo, Lei Yu, Haoqiang Fan, Shuaicheng Liu, Kunyu Wang, Chengzhi Cao, Yuanshen Guan, Jiyuan Xia, Ruikang Xu, Mingde Yao, Zhiwei Xiong",
        "abstract": "Developing and integrating advanced image sensors with novel algorithms in camera systems are prevalent with the increasing demand for computational photography and imaging on mobile platforms. However, the lack of high-quality data for research and the rare opportunity for an in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI). With the success of the 1st MIPI Workshop@ECCV 2022, we introduce the second MIPI challenge, including four tracks focusing on novel image sensors and imaging algorithms. This paper summarizes and reviews the RGBW Joint Fusion and Denoise track on MIPI 2023. In total, 69 participants were successfully registered, and 4 teams submitted results in the final testing phase. The final results are evaluated using objective metrics, including PSNR, SSIM, LPIPS, and KLD. A detailed description of the models developed in this challenge is provided in this paper. More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2023/.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Sun_MIPI_2023_Challenge_on_RGBW_Fusion_Methods_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Sun_MIPI_2023_Challenge_on_RGBW_Fusion_Methods_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MIPI 2023 Challenge on RGB+ToF Depth Completion: Methods and Results",
        "author": "Qingpeng Zhu, Wenxiu Sun, Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Qianhui Sun, Chen Change Loy, Jinwei Gu, Yi Yu, Yangke Huang, Kang Zhang, Meiya Chen, Yu Wang, Yongchao Li, Hao Jiang, Amrit Kumar Muduli, Vikash Kumar, Kunal Swami, Pankaj Kumar Bajpai, Yunchao Ma, Jiajun Xiao, Zhi Ling",
        "abstract": "Depth completion from RGB images and sparse Time-of-Flight (ToF) measurements is an important problem in computer vision and robotics. While traditional methods for depth completion have relied on stereo vision or structured light techniques, recent advances in deep learning have enabled more accurate and efficient completion of depth maps from RGB images and sparse ToF measurements. To evaluate the performance of different depth completion methods, we organized an RGB+sparse ToF depth completion competition. The competition aimed to encourage research in this area by providing a standardized dataset and evaluation metrics to compare the accuracy of different approaches. In this report, we present the results of the competition and analyze the strengths and weaknesses of the top-performing methods. We also discuss the implications of our findings for future research in RGB+sparse ToF depth completion. We hope that this competition and report will help to advance the state-of-the-art in this important area of research. More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2023/.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Zhu_MIPI_2023_Challenge_on_RGBToF_Depth_Completion_Methods_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Zhu_MIPI_2023_Challenge_on_RGBToF_Depth_Completion_Methods_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "OTST: A Two-Phase Framework for Joint Denoising and Remosaicing in RGBW CFA",
        "author": "Zhihao Fan, Xun Wu, Fanqing Meng, Yaqi Wu, Feng Zhang",
        "abstract": "RGBW, a newly emerged type of Color Filter Array (CFA), possesses strong low-light photography capabilities. RGBW CFA shows significant application value when low-light sensitivity is critical, such as in security cameras and smartphones. However, the majority of commercial image signal processors (ISP) are primarily designed for Bayer CFA, research pertaining to RGBW CFA is very rare. To address above limitations, in this study, we propose a two-phase framework named OTST for the RGBW Joint Denoising and Remosaicing (RGBW-JRD) task. For the denoising stage, we propose Omni-dimensional Dynamic Convolution based Half-Shuffle Transformer (ODC-HST) which can fully utilize image's long-range dependencies to dynamically remove the noise. For the remosaicing stage, we propose a Spatial Compressive Transformer (SCT) to efficiently capture both local and global dependencies across spatial and channel dimensions. Experimental results demonstrate that our two-phase RGBW-JRD framework outperforms existing RGBW denoising and remosaicing solutions across a wide range of noise levels. In addition, the proposed approach ranks the 2nd place in MIPI 2023 RGBW Joint Remosaic and Denoise competition.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Fan_OTST_A_Two-Phase_Framework_for_Joint_Denoising_and_Remosaicing_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Fan_OTST_A_Two-Phase_Framework_for_Joint_Denoising_and_Remosaicing_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Hard-Negative Sampling With Cascaded Fine-Tuning Network To Boost Flare Removal Performance in the Nighttime Images",
        "author": "Soonyong Song, Heechul Bae",
        "abstract": "When light passes through a camera lens, it creates a residue called \"flare\" due to the interaction between foreign substances on the lens surface and internal glasses. At night, images can be distorted by flare due to multiple light sources, and research has been conducted using neural networks to remove the flare and solve this problem. However, to our knowledge, research on this approach has only recently begun, and the results are still limited, with only a few models available for use. Further research is needed to determine if the existing models provide optimal results. As part of the mentioned research, we propose a cascaded neural network structure as a means of fine-tuning earlier models to improve their performance. We optimize the performance of the proposed model by constructing triplets using the outputs of two identical neural networks and applying contrastive learning. To demonstrate the superiority of the proposed method, we quantitatively evaluated it by measuring PSNR and SSIM. We also visually compared the differences in image details after removing the flare. Experimental results confirmed that the images reconstructed by the proposed model were superior in terms of PSNR and SSIM in streak regions, compared to the results generated by the reference model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Song_Hard-Negative_Sampling_With_Cascaded_Fine-Tuning_Network_To_Boost_Flare_Removal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Song_Hard-Negative_Sampling_With_Cascaded_Fine-Tuning_Network_To_Boost_Flare_Removal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Region-Based Appearance and Flow Characteristics for Anomaly Detection in Infrared Surveillance Imagery",
        "author": "Yona Falinie A. Gaus, Neelanjan Bhowmik, Brian K. S. Isaac-Medina, Hubert P. H. Shum, Amir Atapour-Abarghouei, Toby P. Breckon",
        "abstract": "Anomaly detection is a classical problem within automated visual surveillance, namely the determination of the normal from the abnormal when operational data availability is highly biased towards one class (normal) due to both insufficient sample size, and inadequate distribution coverage for the other class (abnormal). In this work, we propose the dual use of both visual appearance and localized motion characteristics, derived from optic flow, applied on a per-region basis to facilitate object-wise anomaly detection within this context. Leveraging established object localization techniques from a region proposal network, optic flow is extracted from each object region and combined with appearance in the far infrared (thermal) band to give a 3-channel spatiotemporal tensor representation for each object, i.e., one thermal channel for spatial appearance with two optic flow magnitude channels for temporal motion. This formulation is used as the basis for training contemporary semi-supervised anomaly detection approaches in a region-based manner such that anomalous objects can be detected as a combination of appearance and/or motion within the scene. Evaluation is performed using the Long-Term infrared (thermal) Imaging (LTD) benchmark dataset against which successful detection of both anomalous object appearance and motion characteristics are demonstrated using a range of semi-supervised anomaly detection approaches.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Gaus_Region-Based_Appearance_and_Flow_Characteristics_for_Anomaly_Detection_in_Infrared_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Gaus_Region-Based_Appearance_and_Flow_Characteristics_for_Anomaly_Detection_in_Infrared_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Anomaly Detection With Domain Adaptation",
        "author": "Ziyi Yang, Iman Soltani, Eric Darve",
        "abstract": "Despite great advances have been made in the field of domain adaptation (DA), the vast majority of current methods in DA solve classical ML tasks, e.g. classification. In this paper, we study a novel research direction: semi-supervised anomaly detection with domain adaptation. Given a set of normal data from a source domain and a limited number of normal examples from a target domain, the goal is to have a well-performing anomaly detector in the target domain. We then present the Invariant Representation Anomaly Detection (IRAD) to solve this problem where we first learn to extract a domain-invariant representation. The extraction is achieved by an across-domain encoder trained together with source-specific encoders and generators by adversarial learning. An anomaly detector is then trained using the learnt representations. We evaluate IRAD extensively on anomaly detection datasets, object recognition datasets and digits benchmarks. Experimental results show that IRAD outperforms baseline models by a wide margin across different datasets. We derive a theoretical lower bound for the joint error that explains the performance decay from overtraining and also an upper bound for the generalization error.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Yang_Anomaly_Detection_With_Domain_Adaptation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Yang_Anomaly_Detection_With_Domain_Adaptation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SANO: Score-Based Diffusion Model for Anomaly Localization in Dermatology",
        "author": "Alvaro Gonzalez-Jimenez, Simone Lionetti, Marc Pouly, Alexander A. Navarini",
        "abstract": "Supervised learning for dermatology requires a large volume of annotated images, but collecting clinical data is costly, and it is virtually impossible to cover all clinical cases. Unsupervised anomaly localization circumvents this problem by learning the healthy data distribution. However, algorithms which use a generative model and localize pathologic regions based on a reconstruction error are not robust to domain shift, which is a problem for dermatology due to the low level of standardization expected in many applications. Our method, SANO, uses score-based diffusion models to produce a log-likelihood gradient map highlighting areas that contain abnormalities. A segmentation mask can then be calculated based on deviations from typical values observed during training. After benchmarking SANO on an industrial dataset, we train it on a public non-clinical dataset of healthy hand images without ornaments, evaluate it on the task of detecting jewelry within images from the same dataset, and prove its robustness by using it on clinical pictures to localize hand eczema. We demonstrate that SANO outperforms competing approaches from the literature without introducing additional computational costs.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Gonzalez-Jimenez_SANO_Score-Based_Diffusion_Model_for_Anomaly_Localization_in_Dermatology_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Gonzalez-Jimenez_SANO_Score-Based_Diffusion_Model_for_Anomaly_Localization_in_Dermatology_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Denoising Diffusion Models for Out-of-Distribution Detection",
        "author": "Mark S. Graham, Walter H.L. Pinaya, Petru-Daniel Tudosiu, Parashkev Nachev, Sebastien Ourselin, Jorge Cardoso",
        "abstract": "Out-of-distribution detection is crucial to the safe deployment of machine learning systems. Currently, unsupervised out-of-distribution detection is dominated by generative-based approaches that make use of estimates of the likelihood or other measurements from a generative model. Reconstruction-based methods offer an alternative approach, in which a measure of reconstruction error is used to determine if a sample is out-of-distribution. However, reconstruction-based approaches are less favoured, as they require careful tuning of the model's information bottleneck - such as the size of the latent dimension - to produce good results. In this work, we exploit the view of denoising diffusion probabilistic models (DDPM) as denoising autoencoders where the bottleneck is controlled externally, by means of the amount of noise applied. We propose to use DDPMs to reconstruct an input that has been noised to a range of noise levels, and use the resulting multi-dimensional reconstruction error to classify out-of-distribution inputs. We validate our approach both on standard computer-vision datasets and on higher dimension medical datasets. Our approach outperforms not only reconstruction-based methods, but also state-of-the-art generative-based approaches. Code is available at https://github.com/marksgraham/ddpm-ood.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Self-Supervised Normalizing Flows for Image Anomaly Detection and Localization",
        "author": "Li-Ling Chiu, Shang-Hong Lai",
        "abstract": "Image anomaly detection aims to detect out-of-distribution instances. Most existing methods treat anomaly detection as an unsupervised task because anomalous training data and labels are usually scarce or unavailable. Recently, image synthesis has been used to generate anomalous samples which deviate from normal sample distribution for model training. By using the synthesized anomalous training samples, we present a novel self-supervised normalizing flow-based density estimation model, which is trained by maximizing the likelihood of normal images and minimizing the likelihood of synthetic anomalous images. By adding constraints to abnormal samples in our loss function, our model training is focused on normal samples rather than synthetic samples. Moreover, we improve the transformation subnet of the affine coupling layers in our flow-based model by dynamic stacking convolution and self-attention blocks. We evaluate our method on MVTec-AD, BTAD, and DAGM datasets and achieve state-of-the-art performance compared to flow-based and self-supervised methods on both anomaly detection and localization tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Chiu_Self-Supervised_Normalizing_Flows_for_Image_Anomaly_Detection_and_Localization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Chiu_Self-Supervised_Normalizing_Flows_for_Image_Anomaly_Detection_and_Localization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring the Importance of Pretrained Feature Extractors for Unsupervised Anomaly Detection and Localization",
        "author": "Lars Heckler, Rebecca K\u00f6nig, Paul Bergmann",
        "abstract": "Modeling the distribution of descriptors obtained by pretrained feature extractors is a popular approach for unsupervised visual anomaly detection. While recent work primarily focuses on the development of new methods that build on such extractors, the importance of the selected feature space itself has not been sufficiently studied. We therefore conduct a systematic analysis of current anomaly detection methods with respect to different feature extractors, their intermediate layers, and pretraining protocols. We show that the investigated methods are highly sensitive to the particular choice of feature space. We further demonstrate that using an optimal feature selection strategy can significantly improve the anomaly detection performance, up to a point where selecting a single feature layer outperforms computationally expensive ensembling approaches.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Heckler_Exploring_the_Importance_of_Pretrained_Feature_Extractors_for_Unsupervised_Anomaly_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Heckler_Exploring_the_Importance_of_Pretrained_Feature_Extractors_for_Unsupervised_Anomaly_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Memory-Efficient and GPU-Oriented Visual Anomaly Detection With Incremental Dimension Reduction",
        "author": "Teng-Yok Lee, Yusuke Nagai, Akira Minezawa",
        "abstract": "Recent studies show that the image features from pre-trained convolution neural network (CNN) can be used for anomaly detection, even without fine-tuning. A common type of methods divides the image space into patches, and estimates the distribution of CNN-based features per patch of all training data. While this types of methods can achieve high accuracies, the high dimensionality of CNN features causes overhead to both computing and storage. In this paper, we present an incremental algorithm to reduce the dimensionality of CNN features during the training. As our algorithm ultimately computes the Truncated PCA of the features, it only maintains the truncated singular values and vectors during the training. Besides, to efficiently update the truncated singular values/vectors of all patches, we further optimize the algorithm in order to fully utilize GPUs for parallel execution. We show that with our approach, we can achieve high accuracies on the texture classes of MVTec AD with small memory footprint and extreme high speed (around 200FPS) on a single GPU.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Lee_Memory-Efficient_and_GPU-Oriented_Visual_Anomaly_Detection_With_Incremental_Dimension_Reduction_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Lee_Memory-Efficient_and_GPU-Oriented_Visual_Anomaly_Detection_With_Incremental_Dimension_Reduction_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FewSOME: One-Class Few Shot Anomaly Detection With Siamese Networks",
        "author": "Niamh Belton, Misgina Tsighe Hagos, Aonghus Lawlor, Kathleen M. Curran",
        "abstract": "Recent Anomaly Detection techniques have progressed the field considerably but at the cost of increasingly complex training pipelines. Such techniques require large amounts of training data, resulting in computationally expensive algorithms that are unsuitable for settings where only a small amount of normal samples are available. We propose 'FEW Shot anOMaly dEtection' (FewSOME), a deep One-Class Anomaly Detection algorithm with the ability to accurately detect anomalies having trained on 'few' examples of the normal class and no examples of the anomalous class. We describe FewSOME to be of low complexity given its low data requirement and short training time. FewSOME is aided by pretrained weights with an architecture based on Siamese Networks. By means of an ablation study, we demonstrate how our proposed loss, 'Stop Loss', improves the robustness of FewSOME. Our experiments demonstrate that FewSOME performs at state-of-the-art level on benchmark datasets MNIST, CIFAR-10, F-MNIST and MVTec AD while training on only 30 normal samples, a minute fraction of the data that existing methods are trained on. Moreover, our extensive experiments show FewSOME to be robust to contaminated datasets. We also report F1 score and Balanced Accuracy in addition to AUC as a benchmark for future techniques to be compared against. Code available; https://github.com/niamhbelton/FewSOME.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Belton_FewSOME_One-Class_Few_Shot_Anomaly_Detection_With_Siamese_Networks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Belton_FewSOME_One-Class_Few_Shot_Anomaly_Detection_With_Siamese_Networks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Task Learning Based Video Anomaly Detection With Attention",
        "author": "Mohammad Baradaran, Robert Bergevin",
        "abstract": "Multi-task learning based video anomaly detection methods combine multiple proxy tasks in different branches to detect video anomalies in different situations. Most existing methods suffer from one of these shortcomings: I) Combination of proxy tasks in their methods is not in a complementary and explainable way. II) Class of the object is not effectively considered. III) All motion anomaly cases are not covered. IV) Context information is not engaged in anomaly detection. To address these shortcomings, we propose a novel multi-task learning based method that combines complementary proxy tasks to better consider the motion and appearance features. In one branch, motivated by the abilities of the semantic segmentation and future frame prediction tasks, we combine them into a novel task of future semantic segmentation prediction to learn normal object classes and consistent motion patterns, and to detect respective anomalies simultaneously. In the second branch, we leverage optical flow magnitude estimation for motion anomaly detection and we propose an attention mechanism to engage context information in normal motion modeling and to detect motion anomalies with attention to object parts, the direction of motion, and the distance of the objects from the camera. Our qualitative results show that the proposed method considers the object class effectively and learns motion with attention to the aforementioned determinant factors which results in precise motion modeling and better motion anomaly detection. Additionally, quantitative results show the superiority of our method compared with state-of-the-art methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Baradaran_Multi-Task_Learning_Based_Video_Anomaly_Detection_With_Attention_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Baradaran_Multi-Task_Learning_Based_Video_Anomaly_Detection_With_Attention_CVPRW_2023_paper.pdf"
    },
    {
        "title": "On Advantages of Mask-Level Recognition for Outlier-Aware Segmentation",
        "author": "Matej Grci\u0107, Josip \u0160ari\u0107, Sini\u0161a \u0160egvi\u0107",
        "abstract": "Most dense recognition approaches bring a separate decision in each particular pixel. These approaches deliver competitive performance in usual closed-set setups. However, important applications in the wild typically require strong performance in presence of outliers. We show that this demanding setup greatly benefits from mask-level predictions, even in the case of non-finetuned baseline models. Moreover, we propose an alternative formulation of dense recognition uncertainty that effectively reduces false positive responses at semantic borders. The proposed formulation produces a further improvement over a very strong baseline and sets the new state of the art in outlier-aware semantic segmentation with and without training on negative data. Our contributions also lead to performance improvement in a recent panoptic setup. In-depth experiments confirm that our approach succeeds due to implicit aggregation of pixel-level cues into mask-level predictions.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Grcic_On_Advantages_of_Mask-Level_Recognition_for_Outlier-Aware_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Grcic_On_Advantages_of_Mask-Level_Recognition_for_Outlier-Aware_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Are We Certain It's Anomalous?",
        "author": "Alessandro Flaborea, Bardh Prenkaj, Bharti Munjal, Marco Aurelio Sterpa, Dario Aragona, Luca Podo, Fabio Galasso",
        "abstract": "The progress in modelling time series and, more generally, sequences of structured data has recently revamped research in anomaly detection. The task stands for identifying abnormal behaviors in financial series, IT systems, aerospace measurements, and the medical domain, where anomaly detection may aid in isolating cases of depression and attend the elderly. Anomaly detection in time series is a complex task since anomalies are rare due to highly non-linear temporal correlations and since the definition of anomalous is sometimes subjective. Here we propose the novel use of Hyperbolic uncertainty for Anomaly Detection (HypAD). HypAD learns self-supervisedly to reconstruct the input signal. We adopt best practices from the state-of-the-art to encode the sequence by an LSTM, jointly learned with a decoder to reconstruct the signal, with the aid of GAN critics. Uncertainty is estimated end-to-end by means of a hyperbolic neural network. By using uncertainty, HypAD may assess whether it is certain about the input signal but it fails to reconstruct it because this is anomalous; or whether the reconstruction error does not necessarily imply anomaly, as the model is uncertain, e.g. a complex but regular input signal. The novel key idea is that a detectable anomaly is one where the model is certain but it predicts wrongly. HypAD outperforms the current state-of-the-art for univariate anomaly detection on established benchmarks based on data from NASA, Yahoo, Numenta, Amazon, and Twitter. It also yields state-of-the-art performance on a multivariate dataset of anomaly activities in elderly home residences, and it outperforms the baseline on SWaT. Overall, HypAD yields the lowest false alarms at the best performance rate, thanks to successfully identifying detectable anomalies.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Flaborea_Are_We_Certain_Its_Anomalous_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Flaborea_Are_We_Certain_Its_Anomalous_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Back to the Feature: Classical 3D Features Are (Almost) All You Need for 3D Anomaly Detection",
        "author": "Eliahu Horwitz, Yedid Hoshen",
        "abstract": "Despite significant advances in image anomaly detection and segmentation, few methods use 3D information. We utilize a recently introduced 3D anomaly detection dataset to evaluate whether or not using 3D information is a lost opportunity. First, we present a surprising finding: standard color-only methods outperform all current methods that are explicitly designed to exploit 3D information. This is counter-intuitive as even a simple inspection of the dataset shows that color-only methods are insufficient for images containing geometric anomalies. This motivates the question: how can anomaly detection methods effectively use 3D information? We investigate a range of shape representations including hand-crafted and deep-learning-based; we demonstrate that rotation invariance plays the leading role in the performance. We uncover a simple 3D-only method that beats all recent approaches while not using deep learning, external pre-training datasets, or color information. As the 3D-only method cannot detect color and texture anomalies, we combine it with color-based features, significantly outperforming previous state-of-the-art. Our method, dubbed BTF (Back to the Feature) achieves pixel-wise ROCAUC: 99.3% and PRO: 96.4% on MVTec 3D-AD.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Horwitz_Back_to_the_Feature_Classical_3D_Features_Are_Almost_All_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Horwitz_Back_to_the_Feature_Classical_3D_Features_Are_Almost_All_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Unified Transformer Based Tracker for Anti-UAV Tracking",
        "author": "Qianjin Yu, Yinchao Ma, Jianfeng He, Dawei Yang, Tianzhu Zhang",
        "abstract": "Recently, the need for advanced anti-UAV techniques is increasing due to the rising threat of unauthorized drone intrusion. Object tracking, specifically in thermal infrared (TIR) videos, offers a potential solution to this issue. However, the tracked target often suffers dramatic scale variation, frequent target disappearance, and camera movement which severely influence tracking performance. Therefore, we propose a Unified Transformer-based Tracker, dubbed UTTracker, which contains the following four modules. Firstly, a multi-region local tracking module is designed with temporal cues for tackling target appearance variation and multi-region search for tracking targets in multi proposals. Complementarily, a global detection module is introduced to meet the challenge of target frequent disappearance. Meanwhile, a background correction module is incorporated to align the backgrounds between adjacent frames for alleviating camera movement. Particularly, a dynamic small object detection module for tracking the small target that lacks appearance information. Thanks to the designed modules, our UTTracker can achieve robust UAV tracking in TIR scenarios. Numerous experiments on the 1st and 2nd anti-UAV benchmarks demonstrate the effectiveness of UTTracker. Notably, UTTracker is the foundation of the 2ndplace winning entry in the 3rd Anti-UAV Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/Yu_A_Unified_Transformer_Based_Tracker_for_Anti-UAV_Tracking_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/papers/Yu_A_Unified_Transformer_Based_Tracker_for_Anti-UAV_Tracking_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Global-Local Tracking Framework Driven by Both Motion and Appearance for Infrared Anti-UAV",
        "author": "Yifan Li, Dian Yuan, Meng Sun, Hongyu Wang, Xiaotao Liu, Jing Liu",
        "abstract": "Unmanned aerial vehicles (UAVs) have been widely used in various application domains, but unauthorized UAVs may pose a threat to public safety due to violation of aviation regulations. Therefore, how to design an effective UAV tracking method for anti-UAV is a crucial part of the UAV defense system. In this paper, we propose a Global-Local Tracking Framework driven by both Motion and Appearance (GLTF-MA) including four modules to deal with the practical difficulties in infrared anti-UAV. Firstly, a Periodic Global Detection (PGD) module is periodically performed to re-locate UAVs in the whole image to account for frequent appearance/disappearance and unstable flight paths of UAVs. Meanwhile, a Multi-stage Local Tracking (MLT) module containing a priori stage switching mechanism, motion-appearance matching mechanism, and a motion estimation punisher is routinely implemented to deal with the tiny size of UAVs and background interference. Next, a Target Disappearance Judgement (TDJ) module is performed to give a robust target disappearance flag, followed by a Bounding Box Refinement (BBR) module to refine the target box when the TDJ module thinks the target exists. Extensive experiments demonstrate the superiority of GLTF-MA over other competing counterparts, especially when the UAV is low resolution and moves quickly.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/Li_A_Global-Local_Tracking_Framework_Driven_by_Both_Motion_and_Appearance_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/papers/Li_A_Global-Local_Tracking_Framework_Driven_by_Both_Motion_and_Appearance_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Real-Time and Lightweight Method for Tiny Airborne Object Detection",
        "author": "Yanyi Lyu, Zhunga Liu, Huandong Li, Dongxiu Guo, Yimin Fu",
        "abstract": "With wide applications of unmanned aerial vehicles (UAVs), the detection of airborne objects has become crucial to ensure the flight safety of UAVs and prevent their illegal use. Although object detection has achieved great success in past years, it is still a challenging problem to detect tiny airborne objects. To solve this problem, we propose a simple and effective Tiny Airborne object Detection (TAD) method. It locates potential objects using inconsistent motion cues between airborne objects and backgrounds instead of the low-quality representation of tiny objects. This enables TAD to sensitively detect tiny objects with limited appearance information. Specifically, we first establish correspondences of pixels between adjacent frames based on the local similarity of spatial feature vectors to achieve motion modeling. Next, the local similarity of motion patterns is computed to explicitly describe the motion consistency of each position with its surrounding pixels. Then, a simple network is used to output the heatmap that reflects the probability of object presence. A higher probability of containing an object will be assigned to positions with a greater difference in motion from their surrounding pixels. Finally, an independent network branch is employed to regress center offsets and scale information of objects, which are used to correct the error in the estimated object position from the heatmap and obtain the final bounding box, respectively. Experiments on three challenging datasets demonstrate that the proposed method can achieve advanced performance. Notably, TAD is highly lightweight, and the detection speed is significantly better than existing methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/Lyu_A_Real-Time_and_Lightweight_Method_for_Tiny_Airborne_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/papers/Lyu_A_Real-Time_and_Lightweight_Method_for_Tiny_Airborne_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Video Tiny-Object Detection Guided by the Spatial-Temporal Motion Information",
        "author": "Xin Yang, Gang Wang, Weiming Hu, Jin Gao, Shubo Lin, Liang Li, Kai Gao, Yizheng Wang",
        "abstract": "Detecting tiny/small objects (e.g., drone targets) in videos is highly desired in many realistic scenarios. Nevertheless, current object detection algorithms can hardly recognize tiny targets against extremely complex backgrounds. To address this problem, we propose a motion-guided video tiny-object detection method (MG-VTOD), in which the spatial-temporal motion strength maps play an important role in object searching and locating. Inspired by the biological retinal structure, we compute the motion strength using a sequential frame cube that has been aligned and registered. Subsequently, the motion strength maps are employed to enhance the potential areas of the moving targets, thereby facilitating the target detection procedure. Experimental results obtained on the Anti-UAV-2021 dataset validate that the proposed MG-VTOD method significantly outperforms the competing object detection methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/Yang_Video_Tiny-Object_Detection_Guided_by_the_Spatial-Temporal_Motion_Information_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/papers/Yang_Video_Tiny-Object_Detection_Guided_by_the_Spatial-Temporal_Motion_Information_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Strong Detector With Simple Tracker",
        "author": "Zongheng Tang, Yulu Gao, Zizheng Xun, Fengguang Peng, Yifan Sun, Si Liu, Bo Li",
        "abstract": "Unmanned aerial vehicle (UAV) tracking is a research direction with practical application value and has received sufficient attention in recent years. Challenges such as complex backgrounds, small targets, and motion blur in UAV tracking make it difficult to directly apply existing tracking or detection methods. For example, some state-of-the-art (SOTA) single-object tracking methods such as Ostrack perform poorly when encountering target disappearance or camera offset. Existing detection methods are also difficult to apply directly to this task. This paper proposes a detection-based method with cascading post-processing modules to solve this task. Our entire process includes generating detection candidate boxes, adjusting candidate box scores through video classification, connecting candidate boxes between different frames through a simple tracker, and determining moving targets in the video through background modeling, followed by single-object tracking as post-processing to adjust the results. We finally achieved first place in the 3rd Anti-UAV challenge track1 and top three in track2.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/Tang_Strong_Detector_With_Simple_Tracker_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/papers/Tang_Strong_Detector_With_Simple_Tracker_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Motion Matters: Difference-Based Multi-Scale Learning for Infrared UAV Detection",
        "author": "Ruian He, Shili Zhou, Ri Cheng, Yuqi Sun, Weimin Tan, Bo Yan",
        "abstract": "Unmanned Aerial Vehicle (UAV) detection in the wild is a challenging task due to the presence of background noise and the varying size of the object. To address these obstacles, we propose a novel learning framework for robust UAV detectors, which we call Difference-based Multi-scale Learning (DML). We argue that motion information matters in UAV detection because of the low recognition in one frame. Our method utilizes the frame difference of multiple previous frames, extracting motion information and blocking background noise. We also fuse multiple spatial-temporal scales for training and inferencing, enabling fusion from different sources. In addition, to better evaluate the performance of UAV detection in different scales, we propose Multi-Scale Average Precision (MSAP) metric to aggregate the detection accuracy over multiple scales. Through extensive experiments, we demonstrate that our proposed approach improves the detection accuracy of baseline models. Notably, we achieve SOTA performance in the 3rd Anti-UAV Challenge, with 2nd place in Track 2 and 4th place in Track 1.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/He_Motion_Matters_Difference-Based_Multi-Scale_Learning_for_Infrared_UAV_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/papers/He_Motion_Matters_Difference-Based_Multi-Scale_Learning_for_Infrared_UAV_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "The Second Monocular Depth Estimation Challenge",
        "author": "Jaime Spencer, C. Stella Qian, Michaela Trescakova, Chris Russell, Simon Hadfield, Erich W. Graf, Wendy J. Adams, Andrew J. Schofield, James Elder, Richard Bowden, Ali Anwar, Hao Chen, Xiaozhi Chen, Kai Cheng, Yuchao Dai, Huynh Thai Hoa, Sadat Hossain, Jianmian Huang, Mohan Jing, Bo Li, Chao Li, Baojun Li, Zhiwen Liu, Stefano Mattoccia, Siegfried Mercelis, Myungwoo Nam, Matteo Poggi, Xiaohua Qi, Jiahui Ren, Yang Tang, Fabio Tosi, Linh Trinh, S. M. Nadim Uddin, Khan Muhammad Umair, Kaixuan Wang, Yufei Wang, Yixing Wang, Mochu Xiang, Guangkai Xu, Wei Yin, Jun Yu, Qi Zhang, Chaoqiang Zhao",
        "abstract": "This paper discusses the results for the second edition of the Monocular Depth Estimation Challenge (MDEC). This edition was open to methods using any form of supervision, including fully-supervised, self-supervised, multi-task or proxy depth. The challenge was based around the SYNS-Patches dataset, which features a wide diversity of environments with high-quality dense ground-truth. This includes complex natural environments, e.g. forests or fields, which are greatly underrepresented in current benchmarks. The challenge received eight unique submissions that outperformed the provided SotA baseline on any of the pointcloud- or image-based metrics. The top supervised submission improved relative F-Score by 27.62%, while the top self-supervised improved it by 16.61%. Supervised submissions generally leveraged large collections of datasets to improve data diversity. Self-supervised submissions instead updated the network architecture and pretrained backbones. These results represent a significant progress in the field, while highlighting avenues for future research, such as reducing interpolation artifacts at depth boundaries, improving self-supervised indoor performance and overall natural image accuracy.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/MDEC/html/Spencer_The_Second_Monocular_Depth_Estimation_Challenge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/MDEC/papers/Spencer_The_Second_Monocular_Depth_Estimation_Challenge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Self-Supervised Learning for Accurate Liver View Classification in Ultrasound Images With Minimal Labeled Data",
        "author": "Abder-Rahman Ali, Anthony E. Samir, Peng Guo",
        "abstract": "Conventional B-mode \"grey scale\" medical ultrasound and shear wave elastography (SWE) are widely used for chronic liver disease diagnosis and risk stratification. Liver disease is very common and is clinically and socially important. As a result, multiple medical device manufacturers have proposed or developed AI systems for ultrasound image analysis. However, many abdominal ultrasound images do not include views of the liver, necessitating manual data curation for model development. To optimize the efficiency of real-time processing, a pre-processing liver view detection step is necessary before feeding the image to the AI system. Deep learning techniques have shown great promise for image classification, yet labeling large datasets for training classification models is time-consuming and expensive. In this paper, we present a self-supervised learning method for image classification that utilizes a large set of unlabeled abdominal ultrasound images to learn image representations. These representations are then applied on the downstream task of liver view classification, resulting in efficient classification and alleviation of the labeling burden. In comparison to two state-of- the-art (SOTA) models, ResNet-18 and MLP-Mixer, when trained for 100 epochs the proposed SimCLR+LR approach demonstrated outstanding performance when only labeling \"one\" image per class, achieving an accuracy similar to MLP-Mixer (86%) and outperforming the performance of ResNet-18 (70.2%), when trained on 854 (with liver: 495, without liver: 359) B-mode images. When trained on the whole dataset for 1000 epochs, SimCLR+LR and ResNet- 18 achieved an accuracy of 98.7% and 79.3%, respectively. These findings highlight the potential of the SimCLR+LR approach as a superior alternative to traditional supervised learning methods for liver view classification. Our proposed method has the ability to reduce both the time and cost associated with data labeling, as it eliminates the need for human labor.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/html/Ali_Self-Supervised_Learning_for_Accurate_Liver_View_Classification_in_Ultrasound_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/papers/Ali_Self-Supervised_Learning_for_Accurate_Liver_View_Classification_in_Ultrasound_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound",
        "author": "Blake VanBerlo, Brian Li, Alexander Wong, Jesse Hoey, Robert Arntfield",
        "abstract": "Self-supervised pretraining has been observed to improve performance in supervised learning tasks in medical imaging. This study investigates the utility of self-supervised pretraining prior to conducting supervised fine-tuning for the downstream task of lung sliding classification in M-mode lung ultrasound images. We propose a novel pairwise relationship that couples M-mode images constructed from the same B-mode image and investigate the utility of data augmentation procedure specific to M-mode lung ultrasound. The results indicate that self-supervised pretraining yields better performance than full supervision, most notably for feature extractors not initialized with ImageNet-pretrained weights. Moreover, we observe that including a vast volume of unlabelled data results in improved performance on external validation datasets, underscoring the value of self-supervision for improving generalizability in automatic ultrasound interpretation. To the authors' best knowledge, this study is the first to characterize the influence of self-supervised pretraining for M-mode ultrasound.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/html/VanBerlo_Exploring_the_Utility_of_Self-Supervised_Pretraining_Strategies_for_the_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/papers/VanBerlo_Exploring_the_Utility_of_Self-Supervised_Pretraining_Strategies_for_the_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Image Inpainting With Hypergraphs for Resolution Improvement in Scanning Acoustic Microscopy",
        "author": "Ayush Somani, Pragyan Banerjee, Manu Rastogi, Anowarul Habib, Krishna Agarwal, Dilip K. Prasad",
        "abstract": "Scanning Acoustic Microscopy (SAM) uses high-frequency acoustic waves to generate non-ionizing, label-free images of the surface and internal structures of industrial objects and biological specimens. The resolution of SAM images is limited by several factors such as the frequency of excitation signals, the signal-to-noise ratio, and the pixel size. We propose to use a hypergraphs image inpainting technique for SAM that fills in missing information to improve the resolution of the SAM image. We compared the performance of our technique with four other different techniques based on generative adversarial networks (GANs), including AOTGAN, DeepFill v2, Edge-Connect and DMFN. Our results show that the hypergraphs image inpainting model provides the SOTA average SSIM of 0.82 with a PSNR of 27.96 for 4x image size enhancement over the raw SAM image. We emphasize the importance of hypergraphs' interpretability to bridge the gap between human and machine perception, particularly for robust image recovery tools for acoustic scan imaging. We show that combining SAM with hypergraphs can yield more noise-robust explanations.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/html/Somani_Image_Inpainting_With_Hypergraphs_for_Resolution_Improvement_in_Scanning_Acoustic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/papers/Somani_Image_Inpainting_With_Hypergraphs_for_Resolution_Improvement_in_Scanning_Acoustic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Deep Learning-Based Approach To Increase Efficiency in the Acquisition of Ultrasonic Non-Destructive Testing Datasets",
        "author": "Nick Luiken, Matteo Ravasi",
        "abstract": "Ultrasonic phased array systems traditionally acquire data in a sequential fashion. Although using different excitation delays for each pulsing element can be used to steer the emitted wavefield into e.g., plane waves or focused beams, the overall frame rate of the system is dominated by the choice of the firing time between two consecutive experiments. Inspired from a technology in reflection seismology, we propose the use of simultaneous shooting to increase the flexibility of acquiring ultrasonic data in non-destructive testing (NDT) applications. Simultaneous shooting is an acquisition setup whereby separate transmit sequences are performed simultaneously at a reduced time interval leading to entangled data that may yield artifacts in subsequent imaging products. The data can be untangled by a process called deblending, which is a heavily underdetermined linear inverse problem. We solve the deblending problem using the recently introduced SSDeblend algorithm. This algorithm combines the physics of simultaneous shooting with a powerful self-supervised denoiser specifically tailored to remove the so-called blending noise. We conduct an experiment on an openly available Full Matrix Capture dataset and show that one can speed up the acquisition by at least a factor of 2 with little loss of quality on the resulting final image.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/html/Luiken_A_Deep_Learning-Based_Approach_To_Increase_Efficiency_in_the_Acquisition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/papers/Luiken_A_Deep_Learning-Based_Approach_To_Increase_Efficiency_in_the_Acquisition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Deep Learning Video Classification of Lung Ultrasound Features Associated With Pneumonia",
        "author": "Daniel E. Shea, Sourabh Kulhare, Rachel Millin, Zohreh Laverriere, Courosh Mehanian, Charles B. Delahunt, Dipayan Banik, Xinliang Zheng, Meihua Zhu, Ye Ji, Travis Ostbye, Martha-Marie S. Mehanian, Atinuke Uwajeh, Adeseye M. Akinsete, Fen Wang, Matthew P. Horning",
        "abstract": "Ultrasound (US) imaging holds promise as a low-cost versatile, non-invasive point-of-care diagnostic modality in low- and middle-income countries (LMICs). Still, lung US can be challenging to interpret because air bronchograms are anechoic and the US images mostly contain artifacts rather than lung anatomy. To help overcome these barriers, advances in computer vision and machine learning (ML) provide tools to automatically recognize abnormal US lung features, offering valuable information to healthcare workers for point-of-care diagnosis. This paper describes deep learning algorithms that target three key US features associated with lung pathology: pleural effusion, lung consolidation, and B-lines. The algorithms were developed and validated using a large and varied dataset of 22,400 US lung scans (videos) from 762 patients of all ages (newborn to adult) in Nigeria and China. The architectures include effective methods for leveraging frame-level and video-level annotations, are light enough to deploy on mobile or embedded devices and have high accuracy (e.g., AUCs 0.90). Coupled with portable US devices, we demonstrate that they can provide expert-level clinical assistance for diagnosis of pneumonia, which is the leading cause of both childhood mortality and adult hospitalization in LMICs. We also discuss some of the challenges associated with determining ground truth for pneumonia, which impact the question of how to leverage ML models for lung US to support clinical diagnosis of pneumonia.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/html/Shea_Deep_Learning_Video_Classification_of_Lung_Ultrasound_Features_Associated_With_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/papers/Shea_Deep_Learning_Video_Classification_of_Lung_Ultrasound_Features_Associated_With_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DOAD: Decoupled One Stage Action Detection Network",
        "author": "Shuning Chang, Pichao Wang, Fan Wang, Jiashi Feng, Mike Zheng Shou",
        "abstract": "Localizing people and recognizing their actions from videos is a challenging task towards high-level video understanding. Existing methods are mostly two-stage based, with one stage for person bounding box generation and the other stage for action recognition. However, such two-stage methods are generally with low efficiency. We observe that directly unifying detection and action recognition normally suffers from (i) inferior learning due to different desired properties of context representation for detection and action recognition; (ii) optimization difficulty with insufficient training data. In this work, we present a decoupled one-stage network dubbed DOAD, to mitigate above issues and improve the efficiency for spatio-temporal action detection. To achieve it, we decouple detection and action recognition into two branches. Specifically, one branch focuses on detection representation for actor detection, and the other one for action recognition. For the action branch, we design a transformer-based module (TransPC) to model pairwise relationships between people and context. Different from commonly used vector-based dot product in self-attention, it is built upon a novel matrix-based key and value for Hadamard attention to model person-context information. It not only exploits relationships between person pairs but also takes into account context and relative position information. The results on AVA and UCF101-24 datasets show that our method is competitive with two-stage state-of-the-art methods with significant efficiency improvement.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/html/Chang_DOAD_Decoupled_One_Stage_Action_Detection_Network_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/papers/Chang_DOAD_Decoupled_One_Stage_Action_Detection_Network_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Global Motion Understanding in Large-Scale Video Object Segmentation",
        "author": "Volodymyr Fedynyak, Yaroslav Romanus, Oles Dobosevych, Igor Babin, Roman Riazantsev",
        "abstract": "In this paper, we show that transferring knowledge from other domains of video understanding combined with large-scale learning can improve robustness of Video Object Segmentation (VOS) under complex circumstances. Namely, we focus on integrating scene global motion knowledge to improve large-scale semi-supervised Video Object Segmentation. Prior works on VOS mostly rely on direct comparison of semantic and contextual features to perform dense matching between current and past frames, passing over actual motion structure. On the other hand, Optical Flow Estimation task aims to approximate the scene motion field, exposing global motion patterns which are typically undiscoverable during all pairs similarity search. We present WarpFormer, an architecture for semi-supervised Video Object Segmentation that exploits existing knowledge in motion understanding to conduct smoother propagation and more accurate matching. Our framework employs a generic pretrained Optical Flow Estimation network whose prediction is used to warp both past frames and instance segmentation masks to the current frame domain. Consequently, warped segmentation masks are refined and fused together aiming to inpaint occluded regions and eliminate artifacts caused by flow field imperfects. Additionally, we employ novel large-scale MOSE 2023 dataset to train model on various complex scenarios. Our method demonstrates strong performance on DAVIS 2016/2017 validation (93.0% and 85.9%), DAVIS 2017 test-dev (80.6%) and YouTube-VOS 2019 validation (83.8%) that is competitive with alternative state-of-the-art methods while using much simpler memory mechanism and instance understanding logic.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/html/Fedynyak_Global_Motion_Understanding_in_Large-Scale_Video_Object_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/papers/Fedynyak_Global_Motion_Understanding_in_Large-Scale_Video_Object_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Annotation Attention Model for Video Summarization",
        "author": "Hacene Terbouche, Maryan Morel, Mariano Rodriguez, Alice Othmani",
        "abstract": "In the last decade, the supply of online video content exploded. Automatic video summarization has become necessary to allow content consumers to briefly glance at the video's content. However, the notion of video summary is subjective and thus requires multiple annotators to define the ground truth. Existing video summarization techniques are limited in many ways. First, existing summarization techniques aggregate multiple annotations using the average operation and use these estimates to train a learning model to make predictions on unseen videos. Second, the use of RNN-based architecture to model long-range dependencies. Third, the amount of annotated data available for general video summarization is too small to train visual models from scratch. To mitigate these issues, this work proposes a new end-to-end probabilistic framework called Multi-Annotation Attention Model (MAAM) optimized using the Expectation-Maximization algorithm where the true label is treated as a latent variable. The MAAM framework has several advantages: (i) it exploits multiple annotations from different human-labelers and thus combines model training with the label aggregation, (ii) it models the temporal dynamics representations of videos through an attention mechanism, and (iii) benefits from the power of pretrained visual encoders namely the Vision Transformer (ViT). The proposed approach is evaluated on two public datasets TVSum and SumMe. Our method significantly outperforms state-of-the-art methods on both datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/html/Terbouche_Multi-Annotation_Attention_Model_for_Video_Summarization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/papers/Terbouche_Multi-Annotation_Attention_Model_for_Video_Summarization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A New Dataset and Approach for Timestamp Supervised Action Segmentation Using Human Object Interaction",
        "author": "Saif Sayed, Reza Ghoddoosian, Bhaskar Trivedi, Vassilis Athitsos",
        "abstract": "This paper focuses on leveraging Human Object Interaction (HOI) information to improve temporal action segmentation under timestamp supervision, where only one frame is annotated for each action segment. This information is obtained from an off-the-shelf pre-trained HOI detector, that requires no additional HOI-related annotations in our experimental datasets. Our approach generates pseudo labels by expanding the annotated timestamps into intervals and allows the system to exploit the spatio-temporal continuity of human interaction with an object to segment the video. We also propose the (3+1)Real-time Cooking (ReC) dataset as a realistic collection of videos from 30 participants cooking 15 breakfast items. Our dataset has three main properties: 1) to our knowledge, the first to offer synchronized third and first person videos, 2) it incorporates diverse actions and tasks, and 3) it consists of high resolution frames to detect fine-grained information. In our experiments we benchmark state-of-the-art segmentation methods under different levels of supervision on our dataset. We also quantitatively show the advantages of using HOI information, as our framework improves its baseline segmentation method on several challenging datasets with varying view-points, providing improvements of up to 10.9% and 5.3% in F1 score and frame-wise accuracy respectively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/html/Sayed_A_New_Dataset_and_Approach_for_Timestamp_Supervised_Action_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LSHVU/papers/Sayed_A_New_Dataset_and_Approach_for_Timestamp_Supervised_Action_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Lanelet2 for nuScenes: Enabling Spatial Semantic Relationships and Diverse Map-Based Anchor Paths",
        "author": "Alexander Naumann, Felix Hertlein, Daniel Grimm, Maximilian Zipfl, Steffen Thoma, Achim Rettinger, Lavdim Halilaj, Juergen Luettin, Stefan Schmid, Holger Caesar",
        "abstract": "Motion prediction and planning are key components to enable autonomous driving. Although high definition (HD) maps provide important contextual information that constrains the action space of traffic participants, most approaches are not able to fully exploit this heterogeneous information. In this work, we enrich the existing road geometry of the popular nuScenes dataset and convert it into the open-source map framework Lanelet2. This allows easy access to the road topology and thus, enables the usage of (1) spatial semantic information, such as agents driving on intersecting roads and (2) map-generated anchor paths for target vehicles that can help to improve trajectory prediction performance. Further, we present DMAP, a simple, yet effective approach for diverse map-based anchor path generation and filtering. We show that combining DMAP with ground truth velocity profile information yields high-quality motion prediction results on nuScenes (MinADE5=1.09, MissRate5,2=0.18, Offroad rate=0.00). While it is obviously unfair to compare us against the state-of-the-art, it shows that our HD map accurately depicts the road geometry and topology. Future approaches can leverage this by focusing on data-driven sampling of map-based anchor paths and estimating velocity profiles. Moreover, our HD map can be used for map construction tasks and supplement perception. Code and data are made publicly available at https://felixhertlein.github.io/lanelet4nuscenes.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Naumann_Lanelet2_for_nuScenes_Enabling_Spatial_Semantic_Relationships_and_Diverse_Map-Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Naumann_Lanelet2_for_nuScenes_Enabling_Spatial_Semantic_Relationships_and_Diverse_Map-Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "An Improved Association Pipeline for Multi-Person Tracking",
        "author": "Daniel Stadler, J\u00fcrgen Beyerer",
        "abstract": "The association task of assigning detections to tracks in multi-person tracking has recently been improved by integration of a second matching stage for low-confident detections that are usually discarded in the tracking process. Despite its success, we find that this two stage matching has some weaknesses. For example, high-confident detections are preferred over low-confident detections in any case, even if the low-confident ones are more accurate. Therefore, a Combined Matching (CM) is proposed which considers all possible assignments simultaneously in a single matching stage and thus improves the association accuracy. Moreover, shortcomings of existing motion and appearance distance combinations are identified and a novel Combined Distance (CD) for motion and appearance information is introduced that significantly outperforms previous fusion approaches. Furthermore, we propose an Occlusion Aware Initialization (OAI) which prevents the start of ghost tracks from duplicate detections under occlusion. The effectiveness of our components is shown with extensive ablative experiments and the competitiveness of our tracker is demonstrated on the MOT17 and MOT20 benchmarks, where the current state-of-the-art is notably surpassed.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Stadler_An_Improved_Association_Pipeline_for_Multi-Person_Tracking_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Stadler_An_Improved_Association_Pipeline_for_Multi-Person_Tracking_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Correlation Pyramid Network for 3D Single Object Tracking",
        "author": "Mengmeng Wang, Teli Ma, Xingxing Zuo, Jiajun Lv, Yong Liu",
        "abstract": "In recent years, 3D LiDAR-based single object tracking (SOT) has gained increasing attention as it plays a crucial role in 3D applications such as autonomous driving. The central problem is how to learn a target-aware representation from the sparse and incomplete point clouds. In this paper, we propose a novel Correlation Pyramid Network (CorpNet) with a unified encoder and a motion-factorized decoder. Specifically, the encoder introduces multi-level self attentions and cross attentions in its main branch to enrich the template and search region features and realize their fusion and interaction, respectively. Additionally, considering the sparsity characteristics of the point clouds, we design a lateral correlation pyramid structure for the encoder to keep as many points as possible by integrating hierarchical correlated features. The output features of the search region from the encoder can be directly fed into the decoder for predicting target locations without any extra matcher. Moreover, in the decoder of CorpNet, we disentangle the 3D convolution into successive 2D and 1D convolution blocks and attach a BEV prediction head with an extra z-axis prediction head to explicitly learn the movement of the up axis and the x-y plane together. Extensive experiments on two commonly-used datasets (KITTI and NuScenes) show our CorpNet achieves state-of-the-art results while running in real-time.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Wang_Correlation_Pyramid_Network_for_3D_Single_Object_Tracking_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Wang_Correlation_Pyramid_Network_for_3D_Single_Object_Tracking_CVPRW_2023_paper.pdf"
    },
    {
        "title": "LDFA: Latent Diffusion Face Anonymization for Self-Driving Applications",
        "author": "Marvin Klemp, Kevin R\u00f6sch, Royden Wagner, Jannik Quehl, Martin Lauer",
        "abstract": "In order to protect vulnerable road users (VRUs), such as pedestrians or cyclists, it is essential that intelligent transportation systems (ITS) accurately identify them. Therefore, datasets used to train perception models of ITS must contain a significant number of vulnerable road users. However, data protection regulations require that individuals are anonymized in such datasets. In this work, we introduce a novel deep learning-based pipeline for face anonymization in the context of ITS. In contrast to related methods, we do not use generative adversarial networks (GANs) but build upon recent advances in diffusion models. We propose a two-stage method, which contains a face detection model followed by a latent diffusion model to generate realistic face in-paintings. To demonstrate the versatility of anonymized images, we train segmentation methods on anonymized data and evaluate them on non-anonymized data. Our experiments reveal that our pipeline is better suited to anonymize data for segmentation than naive methods and performes comparably with recent GAN-based methods. Moreover, face detectors achieve higher mAP scores for faces anonymized by our method compared to naive or recent GAN-based methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Klemp_LDFA_Latent_Diffusion_Face_Anonymization_for_Self-Driving_Applications_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Klemp_LDFA_Latent_Diffusion_Face_Anonymization_for_Self-Driving_Applications_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Integrated Perception and Planning for Autonomous Vehicle Navigation: An Optimization-Based Approach",
        "author": "Shubham Kedia, Yu Zhou, Sambhu H. Karumanchi",
        "abstract": "We propose an optimization-based integrated perception and planning framework for autonomous vehicle navigation that achieves real-time state estimation and path planning with high accuracy and robustness. Our Simultaneous Localization And Mapping (SLAM) module is based on Error-State Extended Kalman Filter (ES-EKF) for LiDAR-Inertial sensor fusion. The SLAM system generates a cost map using Euclidean Distance Transform (EDT) that directly encodes environmental constraints as a cost map. A non-linear trajectory optimization problem is formulated with the cost function and solved in real-time using the direct collocation approach. Our results on the KITTI dataset demonstrate the effectiveness of our framework.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Kedia_Integrated_Perception_and_Planning_for_Autonomous_Vehicle_Navigation_An_Optimization-Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Kedia_Integrated_Perception_and_Planning_for_Autonomous_Vehicle_Navigation_An_Optimization-Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Object Tracking by Self-Supervised Learning Appearance Model",
        "author": "Kaer Huang, Kanokphan Lertniphonphan, Feng Chen, Jian Li, Zhepeng Wang",
        "abstract": "In recent years, dominant multi-object tracking (MOT) and segmentation (MOTS) methods mainly follow the tracking-by-detection paradigm. Transformer-based end to end (E2E) solutions bring some ideas to MOT and MOTS, but they can not achieve a new state-of-the-art (SOTA) performance in major MOT and MOTS benchmarks. Detection and association are two main modules of the tracking-by-detection paradigm. Association techniques mainly depend on the combination of motion and appearance information. As deep learning has been recently developed, the performance of the detection and appearance model is rapidly improved. These trends made us consider whether we can achieve SOTA based on only high-performance detection and appearance model. Our paper mainly focuses on exploring this direction based on CBNetV2 with Swin-B as a detection model and MoCo-v2 as a self-supervised appearance model. Motion information and IoU mapping were removed during the association. Our method achieves SOTA results on 2 mainstream MOT datasets and 1 MOTS dataset which is BDD100K MOT, WAYMO 2D Tracking, BDD100K MOTS. Our method yielded a significant improvement of +10.7% and +33.7%, respectively on BDD 100K MOT and MOTS benchmark. The proposed method won first place in BDD100K Multiple Object Tracking (MOT) challenges at CVPR 2022 Workshop on Autonomous Driving. Our method also won first place in BDD100K Multiple Object Tracking (MOT) and Multiple Object Tracking and Segmentation (MOTS) challenges at ECCV 2022 Self-supervised Learning for Next-Generation Industry-level Autonomous Driving (SSLAD) Workshop. We hope our simple and effective method can give some insights to the MOT and MOTS research community.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Huang_Multi-Object_Tracking_by_Self-Supervised_Learning_Appearance_Model_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Huang_Multi-Object_Tracking_by_Self-Supervised_Learning_Appearance_Model_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Benchmarking the Robustness of LiDAR-Camera Fusion for 3D Object Detection",
        "author": "Kaicheng Yu, Tang Tao, Hongwei Xie, Zhiwei Lin, Tingting Liang, Bing Wang, Peng Chen, Dayang Hao, Yongtao Wang, Xiaodan Liang",
        "abstract": "To achieve autonomous driving, developing 3D detection fusion methods, which aim to fuse the camera and LiDAR information, has draw great research interest in recent years. As a common practice, people rely on large-scale datasets to fairly compare the performance of different methods. While these datasets have been carefully cleaned to ideally minimize any potential noise, we observe that they cannot truly reflect the data seen on a real autonomous vehicle, whose data tends to be noisy due to various reasons. This hinders the ability to simply estimate the robust performance under realistic noisy settings. To this end, we collect a series of real-world cases with noisy data distribution, and systematically formulate a robustness benchmark toolkit. It that can simulate these cases on any clean dataset, which has the camera and LiDAR input modality. We showcase the effectiveness of our toolkit by establishing two novel robustness benchmarks on widely-adopted datasets, nuScenes and Waymo, then holistically evaluate the state-of-the-art fusion methods. We discover that: i) most fusion methods, when solely developed on these data, tend to fail inevitably when there is a disruption to the LiDAR input; ii) the improvement of the camera input is significantly inferior to the LiDAR one. We publish the robust fusion dataset, benchmark, detailed documents and instructions on https://anonymous-benchmark.github.io/robust-benchmark-website2/.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Yu_Benchmarking_the_Robustness_of_LiDAR-Camera_Fusion_for_3D_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Yu_Benchmarking_the_Robustness_of_LiDAR-Camera_Fusion_for_3D_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DynStatF: An Efficient Feature Fusion Strategy for LiDAR 3D Object Detection",
        "author": "Yao Rong, Xiangyu Wei, Tianwei Lin, Yueyu Wang, Enkelejda Kasneci",
        "abstract": "Augmenting LiDAR input with multiple previous frames provides richer semantic information and thus boosts performance in 3D object detection, However, crowded point clouds in multi-frames can hurt the precise position information due to the motion blur and inaccurate point projection. In this work, we propose a novel feature fusion strategy, DynStaF (Dynamic-Static Fusion), which enhances the rich semantic information provided by the multi-frame (dynamic branch) with the accurate location information from the current single-frame (static branch). To effectively extract and aggregate complimentary features, DynStaF contains two modules, Neighborhood Cross Attention (NCA) and Dynamic-Static Interaction (DSI), operating through a dual pathway architecture. NCA takes the features in the static branch as queries and the features in the dynamic branch as keys (values). When computing the attention, we address the sparsity of point clouds and take only neighborhood positions into consideration. NCA fuses two features at different feature map scales, followed by DSI providing the comprehensive interaction. To analyze our proposed strategy DynStaF, we conduct extensive experiments on the nuScenes dataset. On the test set, DynStaF increases the performance of PointPillars in NDS by a large margin from 57.7% to 61.6%. When combined with CenterPoint, our framework achieves 61.0% mAP and 67.7% NDS, leading to the state-of-the-art performance without bells and whistles.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Rong_DynStatF_An_Efficient_Feature_Fusion_Strategy_for_LiDAR_3D_Object_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Rong_DynStatF_An_Efficient_Feature_Fusion_Strategy_for_LiDAR_3D_Object_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Pixel-Level Contrastive Learning of Driving Videos With Optical Flow",
        "author": "Tomoya Takahashi, Shingo Yashima, Kohta Ishikawa, Ikuro Sato, Rio Yokota",
        "abstract": "Recognition of the external environment through cameras and LIDAR play a central role in the safety of autonomous driving. The accuracy of such recognition has drastically improved with the advent of deep learning, but is still insufficient for fully autonomous driving. Even though it is possible to collect large amounts of driving data [1, 11, 14, 18], the cost to annotate such data is prohibitive. Recent efforts have focused on self-supervised learning, which does not require annotated data. In this work, we improve the accuracy of self-supervised learning on driving data by combing pixel-wise contrastive learning (PixPro) with optical flow. Unlike most self-supervised methods, PixPro is trained on pixel-level pretext tasks, which yields better accuracy on downstream tasks requiring dense pixel predictions. However, PixPro does not consider the large change in scale of objects, commonly found in driving data. We show that by incorporating optical flow into the pixel-wise contrastive pre-training, we can improve the performance of downstream tasks such as semantic segmentation on CityScapes. We found that using the optical flow between temporarily distant frames can help learn the invariance between large scale changes, which allows us to exceed the performance of the original PixPro method. Our code is released upon acceptance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Takahashi_Pixel-Level_Contrastive_Learning_of_Driving_Videos_With_Optical_Flow_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Takahashi_Pixel-Level_Contrastive_Learning_of_Driving_Videos_With_Optical_Flow_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Contrastive Learning for Depth Prediction",
        "author": "Rizhao Fan, Matteo Poggi, Stefano Mattoccia",
        "abstract": "Depth prediction is at the core of several computer vision applications, such as autonomous driving and robotics. It is often formulated as a regression task in which depth values are estimated through network layers. Unfortunately, the distribution of values on depth maps is seldom explored. Therefore, this paper proposes a novel framework combining contrastive learning and depth prediction, allowing us to pay more attention to depth distribution and consequently enabling improvements to the overall estimation process. Purposely, we propose a window-based contrastive learning module, which partitions the feature maps into non-overlapping windows and constructs contrastive loss within each one. Forming and sorting positive and negative pairs, then enlarging the gap between the two in the representation space, constraints depth distribution to fit the feature of the depth map. Experiments on KITTI and NYU datasets demonstrate the effectiveness of our framework.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Fan_Contrastive_Learning_for_Depth_Prediction_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Fan_Contrastive_Learning_for_Depth_Prediction_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Consistency and Accuracy of CelebA Attribute Values",
        "author": "Haiyu Wu, Grace Bezold, Manuel G\u00fcnther, Terrance Boult, Michael C. King, Kevin W. Bowyer",
        "abstract": "We report the first systematic analysis of the experimental foundations of facial attribute classification.Two annotators independently assigning attribute values shows that only 12 of 40 common attributes are assigned values with >= 95% consistency, and three (high cheekbones, pointed nose, oval face) have essentially random consistency. Of 5,068 duplicate face appearances in CelebA, attributes have contradicting values on from 10 to 860 of the 5,068 duplicates. Manual audit of a subset of CelebA estimates error rates as high as 40% for (no beard=false), even though the labeling consistency experiment indicates that no beard could be assigned with >= 95% consistency. Selecting the mouth slightly open (MSO) for deeper analysis, we estimate the error rate for (MSO=true) at about 20% and (MSO=false) at about 2%. A corrected version of the MSO attribute values enables learning a model that achieves higher accuracy than previously reported for MSO. Corrected values for CelebA MSO are available at https:// github.com/ HaiyuWu/ CelebAMSO.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Wu_Consistency_and_Accuracy_of_CelebA_Attribute_Values_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Wu_Consistency_and_Accuracy_of_CelebA_Attribute_Values_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications",
        "author": "Weiyu Feng, Seth Z. Zhao, Chuanyu Pan, Adam Chang, Yichen Chen, Zekun Wang, Allen Y. Yang",
        "abstract": "Digital twin is a problem of augmenting real objects with their digital counterparts. It can underpin a wide range of applications in augmented reality (AR), autonomy, and UI/UX. A critical component in a good digital-twin system is real-time, accurate 3D object tracking. Most existing works solve 3D object tracking through the lens of robotic grasping, employ older generations of depth sensors, and measure performance metrics that may not apply to other digital-twin applications such as in AR. In this work, we create a novel RGB-D dataset, called Digital Twin Tracking Dataset (DTTD), to enable further research of the problem and extend potential solutions towards longer ranges and mm localization accuracy. To reduce point cloud noise from the input source, we select the latest Microsoft Azure Kinect as the state-of-the-art time-of-flight (ToF) camera. In total, 103 scenes of 10 common off-the-shelf objects with rich textures are recorded, with each frame annotated with a per-pixel semantic segmentation and ground-truth object poses provided by a commercial motion capturing system. Through extensive experiments with model-level and dataset-level analysis, we demonstrate that DTTD can help researchers develop future object tracking methods and analyze new challenges. The dataset, data generation, annotation, and model evaluation pipeline are made publicly available as open source code at: https://github.com/augcog/DTTDv1.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Feng_Digital_Twin_Tracking_Dataset_DTTD_A_New_RGBDepth_3D_Dataset_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Feng_Digital_Twin_Tracking_Dataset_DTTD_A_New_RGBDepth_3D_Dataset_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring Video Frame Redundancies for Efficient Data Sampling and Annotation in Instance Segmentation",
        "author": "Jihun Yoon, Min-Kook Choi",
        "abstract": "In recent years, deep neural network architectures and learning algorithms have greatly improved the performance of computer vision tasks. However, acquiring and annotating large-scale datasets for training such models can be expensive. In this work, we explore the potential of reducing dataset sizes by leveraging redundancies in video frames, specifically for instance segmentation. To accomplish this, we investigate two sampling strategies for extracting keyframes, uniform frame sampling with adjusted stride (UFS) and adaptive frame sampling (AFS), which employs visual (Optical flow, SSIM) or semantic (feature representations) dissimilarities measured by learning free methods. In addition, we show that a simple copy-paste augmentation can bridge the big mAP gap caused by frame reduction. We train and evaluate Mask R-CNN with the BDD100K MOTS dataset and verify the potential of reducing training data by extracting keyframes in the video. With only 20% of the data, we achieve similar performance to the full dataset mAP; with only 33% of the data, we surpass it. Lastly, based on our findings, we offer practical solutions for developing effective sampling methods and data annotation strategies for instance segmentation models. Supplementary on https://github.com/jihun-yoon/EVFR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Yoon_Exploring_Video_Frame_Redundancies_for_Efficient_Data_Sampling_and_Annotation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Yoon_Exploring_Video_Frame_Redundancies_for_Efficient_Data_Sampling_and_Annotation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Compensation Learning in Semantic Segmentation",
        "author": "Timo Kaiser, Christoph Reinders, Bodo Rosenhahn",
        "abstract": "Label noise and ambiguities between similar classes are challenging problems in developing new models and annotating new data for semantic segmentation. In this paper, we propose Compensation Learning in Semantic Segmentation, a framework to identify and compensate ambiguities as well as label noise. More specifically, we add a ground truth depending and globally learned bias to the classification logits and introduce a novel uncertainty branch for neural networks to induce the compensation bias only to relevant regions. Our method is employed into state-of-the-art segmentation frameworks and several experiments demonstrate that our proposed compensation learns inter-class relations that allow global identification of challenging ambiguities as well as the exact localization of subsequent label noise. Additionally, it enlarges robustness against label noise during training and allows target-oriented manipulation during inference. We evaluate the proposed method on Cityscapes, KITTI-STEP, ADE20k, and COCO-stuff10k.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Kaiser_Compensation_Learning_in_Semantic_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Kaiser_Compensation_Learning_in_Semantic_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Scoring Your Prediction on Unseen Data",
        "author": "Yuhao Chen, Shen Zhang, Renjie Song",
        "abstract": "The performance of deep neural networks can vary substantially when evaluated on datasets different from the training data. This presents a crucial challenge in evaluating models on unseen data without access to labels. Previous methods compute a single model-based indicator at the dataset level and use regression methods to predict performance. To evaluate the model more accurately, we propose a sample-level label-free model evaluation method for better prediction on unseen data, named Scoring Your Prediction (SYP). Specifically, SYP introduces low-level image-based features (e.g., blurriness) to model image quality that is important for classification. We complementarily combine model-based indicators and image-based indicators to enhance sample representation. Additionally, we predict the probability that each sample is correctly classified using a neural network named oracle model. Compared to other existing methods, the proposed method outperforms them on 40 unlabeled datasets transformed by CIFAR-10. Especially, SYP lowers RMSE by 1.83-3.97 for ResNet-56 evaluation and 2.32-9.74 for RepVGG-A0 evaluation compared with latest methods. Note that our scheme won the championship on the DataCV Challenge at CVPR 2023. Source code is avaliabe at https://github.com/megvii-research/SYP.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Chen_Scoring_Your_Prediction_on_Unseen_Data_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Chen_Scoring_Your_Prediction_on_Unseen_Data_CVPRW_2023_paper.pdf"
    },
    {
        "title": "K-Means Clustering Based Feature Consistency Alignment for Label-Free Model Evaluation",
        "author": "Shuyu Miao, Lin Zheng, Jingjing Liu, Hong Jin",
        "abstract": "The label-free model evaluation aims to predict the model performance on various test sets without relying on ground truths. The main challenge of this task is the absence of labels in the test data, unlike in classical supervised model evaluation. This paper presents our solutions for the 1st DataCV Challenge of the Visual Dataset Understanding workshop at CVPR 2023. Firstly, we propose a novel method called K-means Clustering Based Feature Consistency Alignment (KCFCA), which is tailored to handle the distribution shifts of various datasets. KCFCA utilizes the K-means algorithm to cluster labeled training sets and unlabeled test sets, and then aligns the cluster centers with feature consistency. Secondly, we develop a dynamic regression model to capture the relationship between the shifts in distribution and model accuracy. Thirdly, we design an algorithm to discover the outlier model factors, eliminate the outlier models, and combine the strengths of multiple autoeval models. On the DataCV Challenge leaderboard, our approach secured 2nd place with an RMSE of 6.8526. Our method significantly improved over the best baseline method by 36% (6.8526 vs. 10.7378). Furthermore, our method achieves a relatively more robust and optimal single model performance on the validation dataset.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Miao_K-Means_Clustering_Based_Feature_Consistency_Alignment_for_Label-Free_Model_Evaluation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Miao_K-Means_Clustering_Based_Feature_Consistency_Alignment_for_Label-Free_Model_Evaluation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "WEDGE: A Multi-Weather Autonomous Driving Dataset Built From Generative Vision-Language Models",
        "author": "Aboli Marathe, Deva Ramanan, Rahee Walambe, Ketan Kotecha",
        "abstract": "The open road poses many challenges to autonomous perception, including poor visibility from extreme weather conditions. Models trained on good-weather datasets frequently fail at detection in these out-of-distribution settings. To aid adversarial robustness in perception, we introduce WEDGE (WEather images by DALL-E GEneration): a synthetic dataset generated with a vision-language generative model via prompting. WEDGE consists of 3360 images in 16 extreme weather conditions manually annotated with 16513 bounding boxes, supporting research in the tasks of weather classification and 2D object detection. We have analyzed WEDGE from research standpoints, verifying its effectiveness for extreme-weather autonomous perception. We establish baseline performance for classification and detection with 53.87% test accuracy and 45.41 mAP. Most importantly, WEDGE can be used to fine-tune state-of-the-art detectors, improving SOTA performance on real-world weather benchmarks (such as DAWN) by 4.48 AP for well-generated classes like trucks. WEDGE has been collected under OpenAI's terms of use and is released for public use under the CC BY-NC-SA 4.0 license. The repository for this work and dataset is available at https://infernolia.github.io/WEDGE.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Marathe_WEDGE_A_Multi-Weather_Autonomous_Driving_Dataset_Built_From_Generative_Vision-Language_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Marathe_WEDGE_A_Multi-Weather_Autonomous_Driving_Dataset_Built_From_Generative_Vision-Language_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Privileged Knowledge Distillation for Dimensional Emotion Recognition in the Wild",
        "author": "Muhammad Haseeb Aslam, Muhammad Osama Zeeshan, Marco Pedersoli, Alessandro L. Koerich, Simon Bacon, Eric Granger",
        "abstract": "Automated emotion recognition (AER) has a growing number of applications, ranging from behavior analysis in assistive robotics and smart e-learning to depression or pain detection and e-health. Systems for multimodal AER typically outperform unimodal approaches due to the complementary and redundant semantic information across modalities like visual, audio, language, physiological, etc. However, in practice, only a subset of these modalities is available at inference time, and using multiple modalities increases systems complexity. This paper focuses on video- based AER, and aims to enhance the accuracy of unimodal systems by leveraging the Learning Under Privileged In formation (LUPI) paradigm with information from multiple modalities. Without loss of generality, the audio modality is considered as privileged information (only available during training) in this study, and a new multimodal to un modal privileged knowledge distillation (M2PKD) mechanism is introduced. In this paper, the teacher network is comprised of a multimodal model that processes audio visual information and distills the learned knowledge to a unimodal visual student network. We validate our proposed PKD approach on the challenging RECOLA and Affwild2 datasets for video-based AER, using weak and strong baseline AER architectures, as well as joint cross-attention fusion methods. The proposed multimodal PKD method increases the absolute average concordance correlation coefficient accuracy by 8% on RECOLA, and 2% increase in the arousal dimension is observed on Affwild2.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FGAHI/html/Aslam_Privileged_Knowledge_Distillation_for_Dimensional_Emotion_Recognition_in_the_Wild_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FGAHI/papers/Aslam_Privileged_Knowledge_Distillation_for_Dimensional_Emotion_Recognition_in_the_Wild_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Human Gesture and Gait Analysis for Autism Detection",
        "author": "Sania Zahan, Zulqarnain Gilani, Ghulam Mubashar Hassan, Ajmal Mian",
        "abstract": "The biggest challenge in diagnosing autism is the diversity of the condition and the difficulty of early detection. Atypical gait and gesture patterns are dominant behavioral characteristics of autism and can provide crucial insights for diagnosis. Furthermore, these data can be collected efficiently in a non-intrusive way, facilitating early intervention to optimize positive outcomes. Existing research mainly focuses on associating facial and eye-gaze features with autism. However, very few studies have investigated movement and gesture patterns which can reveal subtle variations and characteristics that are specific to autism. To address this gap, we present an analysis of gesture and gait activity in videos to identify children with autism and quantify the severity of their condition by regressing autism diagnostic observation schedule scores. Our proposed architecture addresses two key factors: (1) an effective feature representation to manifest irregular gesture patterns and (2) a two-stream co-learning framework to enable a comprehensive understanding of its relation to autism from diverse perspectives without explicitly using additional data modality. Experimental results demonstrate the efficacy of utilizing gesture and gait-activity videos for autism analysis.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FGAHI/html/Zahan_Human_Gesture_and_Gait_Analysis_for_Autism_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FGAHI/papers/Zahan_Human_Gesture_and_Gait_Analysis_for_Autism_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Online LiDAR-to-Vehicle Alignment Using Lane Markings and Traffic Signs",
        "author": "Yao Hu, Xinyu Du, Shengbing Jiang",
        "abstract": "Highly automated vehicles with multiple environmental sensors require all the sensors aligned online to the same coordinate to ensure driving performance and improve customer convenience, especially when misalignment occurs during driving due to degradation, ageing, vibration, or accidents. The alignment between the LiDAR and the ego vehicle is one of several types of alignments. In this paper, an online alignment approach using road elements, e.g., lane markings and traffic signs, in aggregated LiDAR point cloud is developed. The optimization process to minimize the variance of aggregated point cloud for each road element is employed to automatically calculate the alignment parameters. To improve the algorithm robustness and accuracy, several excitation conditions occurred in daily driving are identified by algorithm sensitivity analysis with small input perturbations. The road elements are detected using unique designed heuristic algorithms from the distorted point cloud due to the inaccurate alignment parameters during optimization. The whole solution is validated by the data collected from several test vehicles, and the validation results demonstrate the effectiveness and robustness of the proposed solution.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/html/Hu_Online_LiDAR-to-Vehicle_Alignment_Using_Lane_Markings_and_Traffic_Signs_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/papers/Hu_Online_LiDAR-to-Vehicle_Alignment_Using_Lane_Markings_and_Traffic_Signs_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Network Specialization via Feature-Level Knowledge Distillation",
        "author": "Gaowen Liu, Yuzhang Shang, Yuguang Yao, Ramana Kompella",
        "abstract": "State-of-the-art model specialization methods are mainly based on fine-tuning a pre-trained machine learning model to fit the specific needs of a particular task or application. Or by modifying the architecture of the model itself. However, these methods are not preferable in industrial applications because of the model's large size and the complexity of the training process. In this paper, the difficulty of network specialization is attributed to overfitting caused by a lack of data, and we propose a novel model specialization method by Knowledge Distillation (SKD). The proposed methods merge transfer learning and model compression into one stage. Specifically, we distill and transfer knowledge at the feature map level, circumventing logit-level inconsistency between teacher and student. We empirically investigate and prove the effects of the three parts: Models can be specialized to customer use cases by knowledge distillation. knowledge distillation can effectively regularize the knowledge transfer process to a smaller, task-specific model. Compared with classical methods such as training a model from scratch and model fine-tuning, our methods achieve comparable and much better results and have better training efficiency on the CIFAR-100 dataset for image classification tasks. This paper proves the great potential of model specialization by knowledge distillation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/html/Liu_Network_Specialization_via_Feature-Level_Knowledge_Distillation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/papers/Liu_Network_Specialization_via_Feature-Level_Knowledge_Distillation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeepSmooth: Efficient and Smooth Depth Completion",
        "author": "Sriram Krishna, Basavaraja Shanthappa Vandrotti",
        "abstract": "Accurate and consistent depth maps are essential for numerous applications across domains such as robotics, Augmented Reality and others. High-quality depth maps that are spatially and temporally consistent enable tasks such as Spatial Mapping, Video Portrait effects and more generally, 3D Scene Understanding. Depth data acquired from sensors is often incomplete and contains holes whereas depth estimated from RGB images can be inaccurate. This work focuses on Depth Completion, the task of filling holes in depth data using color images. Most work in depth completion formulates the task at the frame level, individually filling each frame's depth. This results in undesirable flickering artifacts when the RGB-D video stream is viewed as a whole and has detrimental effects on downstream tasks. We propose DeepSmooth, a model that spatio-temporally propagates information to fill in depth maps. Using an EfficientNet and pseudo 3D-Conv based architecture, and a loss function which enforces consistency across space and time, the proposed solution produces smooth depth maps.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/html/Krishna_DeepSmooth_Efficient_and_Smooth_Depth_Completion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/papers/Krishna_DeepSmooth_Efficient_and_Smooth_Depth_Completion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PanopticVis: Integrated Panoptic Segmentation for Visibility Estimation at Twilight and Night",
        "author": "Hidetomo Sakaino",
        "abstract": "Visibility affects traffic flow and control on city roads, highways, and runways. Visibility distance or level is an important measure for predicting the risk on the road. Particularly, it is known that traffic accidents can be raised at foggy twilight and night. Cameras monitor visual conditions like fog. However, only a few papers have tackled such nighttime vision with visibility estimation. This paper proposes a Panoptic Segmentation-based foggy night visibility estimation integrating multiple Deep Learning models: DeepReject/Depth/ Scene/Vis/Fog using single images. We call PanopticVis. DeepFog is trained for no-fog and heavy fog. DeepVis for medium fog is trained by annotated visibility physical scales in a regression manner. DeepDepth is improved to be robust to strong local illumination. DeepScene panoptic-segments scenes with stuff and things, booted by DeepDepth. DeepReject conducts adversarial visual conditions: strong illumination and darkness. Notably, the proposed multiple Deep Learning framework provides high efficiency in memory, cost, and easy-to-maintenance. Unlike previous synthetic test images, experimental results show the effectiveness of the proposed integrated multiple Deep Learning approaches for estimating visibility distances on real foggy night roads. The superiority of PanopticVis is demonstrated over state-of-the-art panoptic-based Deep Learning models in terms of stability, robustness, and accuracy.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/html/Sakaino_PanopticVis_Integrated_Panoptic_Segmentation_for_Visibility_Estimation_at_Twilight_and_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/papers/Sakaino_PanopticVis_Integrated_Panoptic_Segmentation_for_Visibility_Estimation_at_Twilight_and_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ST-RoomNet: Learning Room Layout Estimation From Single Image Through Unsupervised Spatial Transformations",
        "author": "Hatem Ibrahem, Ahmed Salem, Hyun-Soo Kang",
        "abstract": "Room layout estimation is an important task for the 3D reconstruction of indoor scenes and augmented reality applications. The layout of the room is usually estimated by predicting the keypoints of the room corners, room planer segmentation (floor, ceiling, right, left, and front walls), or line detection. In this paper, we propose a novel way to estimate the room layout from monocular RGB images using spatial transformation networks (STN). Since it is commonly known that the room has a cuboid layout, we train a convolutional neural network to predict unsupervised perspective transformation parameters that can transform a reference cuboid layout to the required room layout based on the deep features of the input room image. We show that the proposed method is simple and efficient in learning the room layout without the need to perform segmentation, line detection, or keypoint estimation. We test the proposed method on two challenging benchmarks; LSUN Room Layout and Hedau dataset showing that our method can achieve pixel accuracy error of 5.24% on LSUN and 7.10% on Hedau at a speed of (10 15 fps) outperforming the state-of-the-art methods in room layout estimation task.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/html/Ibrahem_ST-RoomNet_Learning_Room_Layout_Estimation_From_Single_Image_Through_Unsupervised_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VOCVALC/papers/Ibrahem_ST-RoomNet_Learning_Room_Layout_Estimation_From_Single_Image_Through_Unsupervised_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Implicit Epipolar Geometric Function Based Light Field Continuous Angular Representation",
        "author": "Lin Zhong, Bangcheng Zong, Qiming Wang, Junle Yu, Wenhui Zhou",
        "abstract": "Light field plays an important role in many different applications such as virtual reality, microscopy and computational photography. However, low angular resolution limits the further application of light field. The existing state of the art light field angular super-resolution reconstruction methods can only achieve limited fixed-scale angular super-resolution. This paper focuses on a continuous arbitrary-scale light field angular super-resolution via introducing the implicit neural representation into the light field two-plane parametrization. Specifically, we first formulate a 4D implicit epipolar geometric function for light field continuous angular representation. Considering it is difficult and inefficient to directly learn this 4D implicit function, a divide-and-conquer learning strategy and a spatial information embedded encoder are then proposed to convert the 4D implicit function learning into a joint learning of 2D local implicit functions. Furthermore, we design a special epipolar geometric convolution block (EPIBlock) to encode the light field epipolar constraint information. Experiments on both synthetic and real-world light field datasets demonstrate that our method exhibits not only significant superiority in fixed-scale angular super-resolution, but also achieves arbitrary high magnification light field super-resolution while still maintaining the clear light field epipolar geometric structure.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Zhong_Implicit_Epipolar_Geometric_Function_Based_Light_Field_Continuous_Angular_Representation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Zhong_Implicit_Epipolar_Geometric_Function_Based_Light_Field_Continuous_Angular_Representation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Data-Driven Approach Based on Dynamic Mode Decomposition for Efficient Encoding of Dynamic Light Fields",
        "author": "Joshitha Ravishankar, Sally Khaidem, Mansi Sharma",
        "abstract": "Dynamic light fields provide a richer, more realistic 3D representation of a moving scene. However, this leads to higher data rates since excess storage and transmission requirements are needed. We propose a novel approach to efficiently represent and encode dynamic light field data for display applications based on dynamic mode decomposition (DMD). Acquired images are firstly obtained through optimized coded aperture patterns for each temporal frame/camera viewpoint of a dynamic light field. The underlying spatial, angular, and temporal correlations are effectively exploited by a data-driven DMD on these acquired images arranged as time snapshots. Next, High Efficiency Video Coding (HEVC) removes redundancies in light field data, including intra-frame and inter-frame redundancies, while maintaining high reconstruction quality. The proposed scheme is the first of its kind to treat light field videos as mathematical dynamical systems, leverage on dynamic modes of acquired images, and gain flexible coding at various bitrates. Experimental results demonstrate our scheme's superior compression efficiency and bitrate savings compared to the direct encoding of acquired images using HEVC codec.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Ravishankar_A_Data-Driven_Approach_Based_on_Dynamic_Mode_Decomposition_for_Efficient_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Ravishankar_A_Data-Driven_Approach_Based_on_Dynamic_Mode_Decomposition_for_Efficient_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Light Field Synthesis From a Monocular Image Using Variable LDI",
        "author": "Junhyeong Bak, In Kyu Park",
        "abstract": "Recent advancements in learning-based novel view synthesis enable users to synthesize light field from a monocular image without special equipment. Moreover, the state-of-the-art techniques including multiplane image (MPI) show outstanding performance in synthesizing accurate light field from a monocular image. In this study, we propose a new variable layered depth image (VLDI) representation to generate precise light field synthesis results using only a few layers. Our method exploits LDI representation built on a new two-stream halfway fusion network and transformation process. This framework has an efficient structure that directly generates the region that does not require network prediction from inputs. As a result, the proposed method allows us to acquire high-quality light field easily and quickly. Experimental results show that the proposed method outperforms the previous works quantitatively and qualitatively for diverse examples.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Bak_Light_Field_Synthesis_From_a_Monocular_Image_Using_Variable_LDI_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Bak_Light_Field_Synthesis_From_a_Monocular_Image_Using_Variable_LDI_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Toward Real-World Light Field Super-Resolution",
        "author": "Zeyu Xiao, Ruisheng Gao, Yutong Liu, Yueyi Zhang, Zhiwei Xiong",
        "abstract": "Deep learning has opened up new possibilities for light field super-resolution (SR), but existing methods trained on synthetic datasets with simple degradations (e.g., bicubic downsampling) suffer from poor performance when applied to complex real-world scenarios. To address this problem, we introduce LytroZoom, the first real-world light field SR dataset capturing paired low- and high-resolution light fields of diverse indoor and outdoor scenes using a Lytro ILLUM camera. Additionally, we propose the Omni-Frequency Projection Network (OFPNet), which decomposes the omni-frequency components and iteratively enhances them through frequency projection operations to address spatially variant degradation processes present in all frequency components. Experiments demonstrate that models trained on LytroZoom outperform those trained on synthetic datasets and are generalizable to diverse content and devices. Quantitative and qualitative evaluations verify the superiority of OFPNet. We believe this work will inspire future research in real-world light field SR. Code and dataset are available at https://github.com/zeyuxiao1997/RealLFSR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Xiao_Toward_Real-World_Light_Field_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Xiao_Toward_Real-World_Light_Field_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "LFNAT 2023 Challenge on Light Field Depth Estimation: Methods and Results",
        "author": "Hao Sheng, Yebin Liu, Jingyi Yu, Gaochang Wu, Wei Xiong, Ruixuan Cong, Rongshan Chen, Longzhao Guo, Yanlin Xie, Shuo Zhang, Song Chang, Youfang Lin, Wentao Chao, Xuechun Wang, Guanghui Wang, Fuqing Duan, Tun Wang, Da Yang, Zhenglong Cui, Sizhe Wang, Mingyuan Zhao, Qiong Wang, Qianyu Chen, Zhengyu Liang, Yingqian Wang, Jungang Yang, Xueting Yang, Junli Deng",
        "abstract": "This paper reviews the 1st LFNAT challenge on light field depth estimation, which aims at predicting disparity information of central view image in a light field (i.e., pixel offset between central view image and adjacent view image). Compared to multi-view stereo matching, light field depth estimation emphasizes efficient utilization of the 2D angular information from multiple regularly varying views. This challenge specifies UrbanLF light field dataset as the sole data source. There are two phases in total: submission phase and final evaluation phase, in which 75 registered participants successfully submit their predicted results in the first phase and 7 eligible teams compete in the second phase. The performance of all submissions is carefully reviewed and shown in this paper as a new standard for the current state-of-the-art in light field depth estimation. Moreover, the implementation details of these methods are also provided to stimulate related advanced research.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Sheng_LFNAT_2023_Challenge_on_Light_Field_Depth_Estimation_Methods_and_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Sheng_LFNAT_2023_Challenge_on_Light_Field_Depth_Estimation_Methods_and_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-View Semantic Information Guidance for Light Field Image Segmentation",
        "author": "Yiming Li, Ruixuan Cong, Sizhe Wang, Mingyuan Zhao, Yang Zhang, Fangping Li, Hao Sheng",
        "abstract": "One of the great important fields of computer vision is semantic segmentation. As for single image semantic segmentation, due to limited available information, it appears poor performance when the occlusion and similar color interference occur, and has difficulty exploiting the rich scene information. In comparison, the special micro-len array structure of light field camera can record multi-view information of the scene, which provides us with a new solution to solve this issue. In this paper, we propose a multi-view semantic information guidance network (MSIGNet) for light field semantic segmentation. It can effectively utilize semantic information from multi-view images to guide pixel feature of center view image. First, we extract feature of each view image and further obtain semantic probability. Then all probabilities are aggregated through a self-adaptive multi-view probability fusion module. Last, the resulting coarse fusion representation interacts with center view feature to obtain the refined segmentation result. The proposed method shows excellent performance on both real-world and synthetic light field datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Li_Multi-View_Semantic_Information_Guidance_for_Light_Field_Image_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Li_Multi-View_Semantic_Information_Guidance_for_Light_Field_Image_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Disentangling Local and Global Information for Light Field Depth Estimation",
        "author": "Xueting Yang, Junli Deng, Rongshan Chen, Ruixuan Cong, Wei Ke, Hao Sheng",
        "abstract": "Accurate depth estimation from light field images is essential for various applications. Deep learning-based techniques have shown great potential in addressing this problem while still face challenges such as sensitivity to occlusions and difficulties in handling untextured areas. To overcome these limitations, we propose a novel approach that utilizes both local and global features in the cost volume for depth estimation. Specifically, our hybrid cost volume network consists of two complementary sub-modules: a 2D ContextNet for global context information and a matching cost volume for local feature information. We also introduce an occlusion-aware loss that accounts for occlusion areas to improve depth estimation quality. We demonstrate the effectiveness of our approach on the UrbanLF and HCInew datasets, showing significant improvements over existing methods, especially in occluded and untextured regions. Our method disentangles local feature and global semantic information explicitly, reducing the occlusion and untextured area reconstruction error and improving the accuracy of depth estimation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Yang_Disentangling_Local_and_Global_Information_for_Light_Field_Depth_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Yang_Disentangling_Local_and_Global_Information_for_Light_Field_Depth_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "EPI-Guided Cost Construction Network for Light Field Disparity Estimation",
        "author": "Tun Wang, Rongshan Chen, Ruixuan Cong, Da Yang, Zhenglong Cui, Fangping Li, Hao Sheng",
        "abstract": "Recent learning-based light field (LF) disparity estimation methods construct cost volume by sequentially shifting each sub-aperture image (SAI) with a series of predefined offsets. They only use the visual information of SAIs and lose the geometry of LF. In this paper, we design a simple network that can cleverly integrate EPI features with cost volume to estimate the disparity. Firstly, we propose an efficient EPI extraction module to use abundant line characteristics. Secondly, we offer an EPI-Cost volume construction module that can create volume guided by the EPI line and the color consistency of images. Finally, after completing it, we adopt an intervolume fusion module to considerably correlate the validity of EPI lines in both directions. Experimental results show the proposed method achieves state-of-the-art performance in the quantitative and qualitative evaluation of the UrbanLF-Syn dataset.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Wang_EPI-Guided_Cost_Construction_Network_for_Light_Field_Disparity_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Wang_EPI-Guided_Cost_Construction_Network_for_Light_Field_Disparity_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CNT-NeRF: Carbon Nanotube Forest Depth Layer Decomposition in SEM Imagery Using Generative Adversarial Networks",
        "author": "Nguyen P. Nguyen, Ramakrishna Surya, Prasad Calyam, Kannappan Palaniappan, Matthew Maschmann, Filiz Bunyak",
        "abstract": "Carbon nanotube (CNT) forests are imaged using scanning electron microscopes (SEMs) that project their multi- layered 3D structure into a single 2D image. Image analytics, particularly instance segmentation is needed to quantify structural characteristics and to predict correlations be- tween structural morphology and physical properties. The inherent complexity of individual CNT structures is further increased in CNT forests due to density of CNTs, interactions between CNTs, occlusions, and lack of 3D information to resolve correspondences when multiple CNTs from different depths appear to cross in 2D. In this paper, we pro- pose CNT-NeRF, a generative adversarial network (GAN) for simultaneous depth layer decomposition and segmentation of CNT forests in SEM images. The proposed network is trained using a multi-layer, photo-realistic synthetic dataset obtained by transferring the style of real CNT images to physics-based simulation data. Experiments show promising depth layer decomposition and accurate CNT segmentation results not only for the front layer but also for the partially occluded middle and back layers. This achievement is a significant step towards automated, image-based CNT forest structure characterization and physical property prediction.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Nguyen_CNT-NeRF_Carbon_Nanotube_Forest_Depth_Layer_Decomposition_in_SEM_Imagery_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Nguyen_CNT-NeRF_Carbon_Nanotube_Forest_Depth_Layer_Decomposition_in_SEM_Imagery_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Fashion-Specific Ambiguous Expression Interpretation With Partial Visual-Semantic Embedding",
        "author": "Ryotaro Shimizu, Takuma Nakamura, Masayuki Goto",
        "abstract": "A novel technology named fashion intelligence system has been proposed to quantify ambiguous expressions unique to fashion, such as \"casual,\" \"adult-casual,\" and \"office-casual,\" and to support users' understanding of fashion. However, the existing visual-semantic embedding (VSE) model, which is the basis of its system, does not support situations in which images are composed of multiple parts such as hair, tops, pants, skirts, and shoes. We propose partial VSE, which enables sensitive learning for each part of the fashion outfits. This enables five types of practical functionalities, particularly image-retrieval tasks in which changes are made only to the specified parts and image-reordering tasks that focus on the specified parts by the single model. Based on both the multiple unique qualitative and quantitative evaluation experiments, we show the effectiveness of the proposed model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Shimizu_Fashion-Specific_Ambiguous_Expression_Interpretation_With_Partial_Visual-Semantic_Embedding_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Shimizu_Fashion-Specific_Ambiguous_Expression_Interpretation_With_Partial_Visual-Semantic_Embedding_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Shape of You: Precise 3D Shape Estimations for Diverse Body Types",
        "author": "Rohan Sarkar, Achal Dave, Gerard Medioni, Benjamin Biggs",
        "abstract": "This paper presents Shape of You (SoY), an approach to improve the accuracy of 3D body shape estimation for vision-based clothing recommendation systems. While existing methods have successfully estimated 3D poses, there remains a lack of work in precise shape estimation, particularly for diverse human bodies. To address this gap, we propose two loss functions that can be readily integrated into parametric 3D human reconstruction pipelines. Additionally, we propose a test-time optimization routine that further improves quality. Our method improves over the recent SHAPY method by 17.7% on the challenging SSP-3D dataset. We consider our work to be a step towards a more accurate 3D shape estimation system that works reliably on diverse body types and holds promise for practical applications in the fashion industry.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Sarkar_Shape_of_You_Precise_3D_Shape_Estimations_for_Diverse_Body_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Sarkar_Shape_of_You_Precise_3D_Shape_Estimations_for_Diverse_Body_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Diffusart: Enhancing Line Art Colorization With Conditional Diffusion Models",
        "author": "Hernan Carrillo, Micha\u00ebl Cl\u00e9ment, Aur\u00e9lie Bugeau, Edgar Simo-Serra",
        "abstract": "Colorization of line art drawings is an important task in illustration and animation workflows. However, this highly laborious process is mainly done manually, limiting the creative productivity. This paper presents a novel interactive approach for line art colorization using conditional Diffusion Probabilistic Models (DPMs). In our proposed approach, the user provides initial color strokes for colorizing the line art. The strokes are then integrated into the conditional DPM-based colorization process by means of a coupled implicit and explicit conditioning strategy to generates diverse and high-quality colorized images. We evaluate our proposal and show it outperforms existing state-of-the-art approaches using the FID, LPIPS and SSIM metrics.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Carrillo_Diffusart_Enhancing_Line_Art_Colorization_With_Conditional_Diffusion_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Carrillo_Diffusart_Enhancing_Line_Art_Colorization_With_Conditional_Diffusion_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "KBody: Balanced Monocular Whole-Body Estimation",
        "author": "Nikolaos Zioulis, James F. O'Brien",
        "abstract": "KBody is a method for fitting a low-dimensional body model to an image. It follows a predict-and-optimize approach, relying on data-driven model estimates for the constraints that will be used to solve for the body's parameters. Compared to other approaches, it introduces virtual joints to identify higher quality correspondences and disentangles the optimization between the pose and shape parameters to achieve a more balanced result in terms of pose and shape capturing capacity, as well as pixel alignment.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Zioulis_KBody_Balanced_Monocular_Whole-Body_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Zioulis_KBody_Balanced_Monocular_Whole-Body_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Name Your Style: Text-Guided Artistic Style Transfer",
        "author": "Zhi-Song Liu, Li-Wen Wang, Wan-Chi Siu, Vicky Kalogeiton",
        "abstract": "Image style transfer has attracted widespread attention in the past few years. Despite its remarkable results, it requires additional style images available as references, making it less flexible and inconvenient. Using text is the most natural way to describe the style. More importantly, text can describe implicit abstract styles, like styles of specific artists or art movements. In this paper, we propose a text-driven image style transfer (TxST) that leverages advanced image-text encoders to control arbitrary style transfer. We introduce a contrastive training strategy to effectively extract style descriptions from the image-text model (i.e., CLIP), which aligns stylization with the text description. To this end, we also propose a style attention module that explores cross-attentions to fuse style and content features. Finally, we achieve an arbitrary artist-aware image style transfer to learn and transfer specific artistic characters such as Picasso, oil painting, or a rough sketch. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods on both image and textual styles. Moreover, it can mimic the styles of one or many artists to achieve attractive results, thus highlighting a promising direction in image style transfer.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Liu_Name_Your_Style_Text-Guided_Artistic_Style_Transfer_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Liu_Name_Your_Style_Text-Guided_Artistic_Style_Transfer_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Image Reference-Guided Fashion Design With Structure-Aware Transfer by Diffusion Models",
        "author": "Shidong Cao, Wenhao Chai, Shengyu Hao, Gaoang Wang",
        "abstract": "Image-based fashion design with AI techniques has attracted increasing attention in recent years. We focus on a new fashion design task, where we aim to transfer a reference appearance image onto a clothing image while preserving the structure of the clothing image. It is a challenging task since there are no reference images available for the newly designed output fashion images. Although diffusion-based image translation or neural style transfer (NST) has enabled flexible style transfer, it is often difficult to maintain the original structure of the image realistically during the reverse diffusion, especially when the referenced appearance image greatly differs from the common clothing appearance. To tackle this issue, we present a novel diffusion model-based unsupervised structure-aware transfer method to semantically generate new clothes from a given clothing image and a reference appearance image. In specific, we decouple the foreground clothing with automatically generated semantic masks by conditioned labels. And the mask is further used as guidance in the denoising process to preserve the structure information. Moreover, we use the pre-trained Vision Transformer (ViT) for both appearance and structure guidance. Our experimental results show that the proposed method outperforms state-of-the-art baseline models, generating more realistic images in the fashion design task.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Cao_Image_Reference-Guided_Fashion_Design_With_Structure-Aware_Transfer_by_Diffusion_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Cao_Image_Reference-Guided_Fashion_Design_With_Structure-Aware_Transfer_by_Diffusion_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Gatha: Relational Loss for Enhancing Text-Based Style Transfer",
        "author": "Surgan Jandial, Shripad Deshmukh, Abhinav Java, Simra Shahid, Balaji Krishnamurthy",
        "abstract": "Text-based style transfer is a promising area of research that enables the generation of stylistic images from plain text descriptions. However, the existing text-based style transfer techniques do not account for the subjective nature of prompt descriptions or the nuances of style-specific vocabulary during the optimization process. This severely limits the stylistic expression of the predominant models. In this paper, we address this gap by proposing Gatha, which incorporates subjectivity by introducing an additional loss function that enforces the relationship between stylized images and a proxy style set to be similar to the relationship between the text description and the proxy style set. We substantiate the effectiveness of Gatha through both qualitative and quantitative analysis against the existing state-of-the-art models and show that our approach allows for consistently improved stylized images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Jandial_Gatha_Relational_Loss_for_Enhancing_Text-Based_Style_Transfer_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Jandial_Gatha_Relational_Loss_for_Enhancing_Text-Based_Style_Transfer_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FashionVQA: A Domain-Specific Visual Question Answering System",
        "author": "Min Wang, Ata Mahjoubfar, Anupama Joshi",
        "abstract": "Humans apprehend the world through various sensory modalities, yet language is their predominant communication channel. Machine learning systems need to draw on the same multimodal richness to have informed discourses with humans in natural language; this is particularly true for systems specialized in visually-dense information, such as dialogue, recommendations, and search engines for clothing. To this end, we train a visual question-answering (VQA) system to answer complex natural language questions about apparel in fashion photoshoot images. The key to the successful training of our VQA model is the automatic creation of a visual question-answering dataset with 168 million samples from item attributes of 207 thousand images using diverse templates. The sample generation employs a strategy that considers the difficulty of the question-answer pairs to emphasize challenging concepts. We see that using the same transformer for encoding the question and decoding the answer, as in language models, achieves maximum accuracy, showing that visual language models (VLMs) make the optimal visual question-answering systems for our dataset. The accuracy of the best model surpasses the human expert level. Our approach for generating a large-scale multimodal domain-specific dataset provides a path for training specialized models capable of communicating in natural language. The training of such domain-expert models, e.g., our fashion VLM model, cannot rely solely on the large-scale general-purpose datasets collected from the web.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Wang_FashionVQA_A_Domain-Specific_Visual_Question_Answering_System_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Wang_FashionVQA_A_Domain-Specific_Visual_Question_Answering_System_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SHIFT15M: Fashion-Specific Dataset for Set-to-Set Matching With Several Distribution Shifts",
        "author": "Masanari Kimura, Takuma Nakamura, Yuki Saito",
        "abstract": "Set-to-set matching is the problem of matching two different sets of items based on some criteria. Especially when each item in the set is high-dimensional, such as an image, set-to-set matching is treated as one of the applied problems to be solved by utilizing neural networks. Most machine learning-based set-to-set matching generally assumes that the training and test data follow the same distribution. However, such assumptions are often violated in real-world machine learning problems. In this paper, we propose SHIFT15M, a dataset that can be used to properly evaluate set-to-set matching models in situations where the distribution of data changes between training and testing. Some benchmark experiments show that the performance of naive methods drops due to the effects of the distribution shift. In addition, we provide software to handle the SHIFT15M dataset in a very simple way. The URL for the software will appear after this manuscript is published.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Kimura_SHIFT15M_Fashion-Specific_Dataset_for_Set-to-Set_Matching_With_Several_Distribution_Shifts_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Kimura_SHIFT15M_Fashion-Specific_Dataset_for_Set-to-Set_Matching_With_Several_Distribution_Shifts_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FreqHPT: Frequency-Aware Attention and Flow Fusion for Human Pose Transfer",
        "author": "Liyuan Ma, Tingwei Gao, Haibin Shen, Kejie Huang",
        "abstract": "Human pose transfer is a challenging task that synthesizes images in various target poses while preserving the original appearance. This is typically achieved through aligning the source texture and supplementing it to the target pose. However, most of previous alignment methods only rely on either attention or flow, thereby failing to fully leverage distinctive strengths of these two methods. Moreover, the receptive field of these methods is generally limited in supplementation, resulting in the lack of global texture consistency. To address this issue, observing that attention and flow exhibit distinct characteristics in terms of their frequency distribution, Frequency-aware Human Pose Transfer (FreqHPT) is proposed in this paper. FreqHPT investigates the complementarity between attention and flow from the frequency perspective for improving texture-preserving pose transfer. To this end, FreqHPT first transforms the features from attention and flow into the wavelet domain and then fuses them over multi-frequency bands in an adaptive manner. Subsequently, FreqHPT globally refines the fused features in the Fourier space for texture supplement, enhancing the overall semantic consistency. Extensive experiments on the DeepFashion dataset demonstrate the superiority of FreqHPT in generating texture-preserving and realistic pose transfer images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Ma_FreqHPT_Frequency-Aware_Attention_and_Flow_Fusion_for_Human_Pose_Transfer_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Ma_FreqHPT_Frequency-Aware_Attention_and_Flow_Fusion_for_Human_Pose_Transfer_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DETR-Based Layered Clothing Segmentation and Fine-Grained Attribute Recognition",
        "author": "Hao Tian, Yu Cao, P. Y. Mok",
        "abstract": "Clothing segmentation and fine-grained attribute recognition are challenging tasks at the crossing of computer vision and fashion, which segment the entire ensemble clothing instances as well as recognize detailed attributes of the clothing products from any input human images. Many new models have been developed for the tasks in recent years, nevertheless the segmentation accuracy is less than satisfactory in case of layered clothing or fashion products in different scales. In this paper, a new DEtection TRansformer (DETR) based method is proposed to segment and recognize fine-grained attributes of ensemble clothing instances with high accuracy. In this model, we propose a multi-layered attention module by aggregating features of different scales, determining the various scale components of a single instance, and merging them together. We train our model on the Fashionpedia dataset and demonstrate our method surpasses SOTA models in tasks of layered clothing segmentation and fine-grained attribute recognition.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Tian_DETR-Based_Layered_Clothing_Segmentation_and_Fine-Grained_Attribute_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Tian_DETR-Based_Layered_Clothing_Segmentation_and_Fine-Grained_Attribute_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SkiLL: Skipping Color and Label Landscape: Self Supervised Design Representations for Products in E-Commerce",
        "author": "Vinay K. Verma, Dween Rabius Sanny, Shreyas Sunil Kulkarni, Prateek Sircar, Abhishek Singh, Deepak Gupta",
        "abstract": "Understanding the design of a product without human supervision is a crucial task for e-commerce services. Such a capability can help in multiple downstream e-commerce tasks like product recommendations, design trend analysis, image-based search, and visual information retrieval, etc. For this task, getting fine-grain label data is costly and not scalable for the e-commerce product. In this paper, we leverage knowledge distillation based self-supervised learning (SSL) approach to learn design representations. These representations do not require human annotation for training and focus on only design related attributes of a product and ignore attributes like color, orientation, etc. We propose a global and task specific local augmentation space which captures the desired image information and provides robust visual embedding. We evaluated our model for the three highly diverse datasets, and also propose and measure a quantitative metric to evaluate the model's color invariant feature learning ability. In all scenarios, our proposed approach outperforms the recent SSL model by upto 8.6% in terms of accuracy.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Verma_SkiLL_Skipping_Color_and_Label_Landscape_Self_Supervised_Design_Representations_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/papers/Verma_SkiLL_Skipping_Color_and_Label_Landscape_Self_Supervised_Design_Representations_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Shape-Net: Room Layout Estimation From Panoramic Images Robust to Occlusion Using Knowledge Distillation With 3D Shapes As Additional Inputs",
        "author": "Mizuki Tabata, Kana Kurata, Junichiro Tamamatsu",
        "abstract": "Estimating the layout of a room from a single-shot panoramic image is important in virtual/augmented reality and furniture layout simulation. This involves identifying three-dimensional (3D) geometry, such as the location of corners and boundaries, and performing 3D reconstruction. However, occlusion is a common issue that can negatively impact room layout estimation, and this has not been thoroughly studied to date. It is possible to obtain 3D shape information of rooms as drawings of buildings and coordinates of corners from image datasets, thus we propose providing both 2D panoramic and 3D information to a model to effectively deal with occlusion. However, simply feeding 3D information to a model is not sufficient to utilize the shape information for an occluded area. Therefore, we improve the model by introducing 3D Intersection over Union (IoU) loss to effectively use 3D information. In some cases, drawings are not available or the construction deviates from a drawing. Considering such practical cases, we propose a method for distilling knowledge from a model trained with both images and 3D information to a model that takes only images as input. The proposed model, which is called Shape-Net, achieves state-of-the-art (SOTA) performance on benchmark datasets. We also confirmed its effectiveness in dealing with occlusion through significantly improved accuracy on images with occlusion compared with existing models.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CIVILS/html/Tabata_Shape-Net_Room_Layout_Estimation_From_Panoramic_Images_Robust_to_Occlusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CIVILS/papers/Tabata_Shape-Net_Room_Layout_Estimation_From_Panoramic_Images_Robust_to_Occlusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "U2RLE: Uncertainty-Guided 2-Stage Room Layout Estimation",
        "author": "Pooya Fayyazsanavi, Zhiqiang Wan, Will Hutchcroft, Ivaylo Boyadzhiev, Yuguang Li, Jana Kosecka, Sing Bing Kang",
        "abstract": "While the existing deep learning-based room layout estimation techniques demonstrate good overall accuracy, they are less effective for distant floor-wall boundary. To tackle this problem, we propose a novel uncertainty-guided approach for layout boundary estimation introducing new two-stage CNN architecture termed U2RLE. The initial stage predicts both floor-wall boundary and its uncertainty and is followed by the refinement of boundaries with high positional uncertainty using a different, distance-aware loss. Finally, outputs from the two stages are merged to produce the room layout. Experiments using ZInD and Structure3D datasets show that U2RLE improves over current state-of-the-art, being able to handle both near and far walls better. In particular, U2RLE outperforms current state-of-the-art techniques for the most distant walls.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CIVILS/html/Fayyazsanavi_U2RLE_Uncertainty-Guided_2-Stage_Room_Layout_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CIVILS/papers/Fayyazsanavi_U2RLE_Uncertainty-Guided_2-Stage_Room_Layout_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PanopticRoad: Integrated Panoptic Road Segmentation Under Adversarial Conditions",
        "author": "Hidetomo Sakaino",
        "abstract": "Segmentation becomes one of the most important methods for scene understanding. Segmentation plays a central role in recognizing things and stuff in a scene. Among all things and stuff in a scene, the road guides vehicles in the cities and highways. Most segmentation models, i.e., semantic, instance, and panoptic segmentation, have focused on images with clear daytime weather conditions. Few papers have tackled nighttime vision under adversarial conditions, i.e., fog, rain, snow, strong illumination, and disaster events. Moreover, further segmentation of road conditions like dry, wet, and snow is still challenging under such invisible conditions. Weather impacts not only visibility but also roads and their surrounding environment, causing vital disasters with obstacles on the road, i.e., rocks and water. This paper proposes PanopticRoad with five Deep Learning-based modules for road condition segmentation under adversarial conditions: DeepReject/Scene/Snow/Depth/Road. Integration of them helps refine the failure of local road conditions where weather and physical constraints are applied. Using foggy and heavy snowfall nighttime road images and disaster images, the superiority of PanopticRoad is demonstrated over state-of-the-art panoptic-based and adaptive domain-based Deep Learning models in terms of stability, robustness, and accuracy.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PVUW/html/Sakaino_PanopticRoad_Integrated_Panoptic_Road_Segmentation_Under_Adversarial_Conditions_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PVUW/papers/Sakaino_PanopticRoad_Integrated_Panoptic_Road_Segmentation_Under_Adversarial_Conditions_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Motion-State Alignment for Video Semantic Segmentation",
        "author": "Jinming Su, Ruihong Yin, Shuaibin Zhang, Junfeng Luo",
        "abstract": "In recent years, video semantic segmentation has made great progress with advanced deep neural networks. However, there still exist two main challenges i.e., information inconsistency and computation cost. To deal with the two difficulties, we propose a novel motion-state alignment framework for video semantic segmentation to keep both motion and state consistency. In the framework, we first construct a motion alignment branch armed with an efficient decoupled transformer to capture dynamic semantics, guaranteeing region-level temporal consistency. Then, a state alignment branch composed of a stage transformer is designed to enrich feature spaces for the current frame to extract static semantics and achieve pixel-level state consistency. Next, by a semantic assignment mechanism, the region descriptor of each semantic category is gained from dynamic semantics and linked with pixel descriptors from static semantics. Benefiting from the alignment of these two kinds of effective information, the proposed method picks up dynamic and static semantics in a targeted way, so that video semantic regions are consistently segmented to obtain precise locations with low computational complexity. Extensive experiments on Cityscapes and CamVid datasets show that the proposed approach outperforms state-of-the-art methods and validates the effectiveness of the motion-state alignment framework.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PVUW/html/Su_Motion-State_Alignment_for_Video_Semantic_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PVUW/papers/Su_Motion-State_Alignment_for_Video_Semantic_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Perceive, Excavate and Purify: A Novel Object Mining Framework for Instance Segmentation",
        "author": "Jinming Su, Ruihong Yin, Xingyue Chen, Junfeng Luo",
        "abstract": "Recently, instance segmentation has made great progress with the rapid development of deep neural networks. However, there still exist two main challenges including discovering indistinguishable objects and modeling the relationship between instances. To deal with these difficulties, we propose a novel object mining framework for instance segmentation. In this framework, we first introduce the semantics perceiving subnetwork to capture pixels that may belong to an obvious instance from the bottom up. Then, we propose an object excavating mechanism to discover indistinguishable objects. In the mechanism, preliminary perceived semantics are regarded as original instances with classifications and locations, and then indistinguishable objects around these original instances are mined, which ensures that hard objects are fully excavated. Next, an instance purifying strategy is put forward to model the relationship between instances, which pulls the similar instances close and pushes away different instances to keep intra-instance similarity and inter-instance discrimination. In this manner, the same objects are combined as the one instance and different objects are distinguished as independent instances. Extensive experiments on the COCO dataset show that the proposed approach outperforms state-of-the-art methods, which validates the effectiveness of the proposed object mining framework.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PVUW/html/Su_Perceive_Excavate_and_Purify_A_Novel_Object_Mining_Framework_for_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PVUW/papers/Su_Perceive_Excavate_and_Purify_A_Novel_Object_Mining_Framework_for_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Bush Detection for Vision-Based UGV Guidance in Blueberry Orchards: Data Set and Methods",
        "author": "Vladan Filipovi\u0107, Dimitrije Stefanovi\u0107, Nina Pajevi\u0107, \u017deljana Grbovi\u0107, Nemanja Djuric, Marko Pani\u0107",
        "abstract": "Object detection has reached strong performance in the last decade, having seen its usage spreading to various application areas, such as medicine, transportation, sports, and others. However, one of the more underutilized areas where advanced detection methods have yet to fully fulfill their promise is in the area of agriculture, where a strong potential exists for applying learned models to achieve practical, real-world impact affecting a large number of people. In this work, we focus on this application area and consider the problem of orchard guidance for ground robots, focusing on obstacle and plant detection from RGB camera images. First, we present an overview of public data sets used to train models to detect relevant objects from camera images and other sensor inputs. Then we introduce a novel data set collected in blueberry orchards that contains camera images in various conditions and provides blueberry bushes as targets for detection. The introduced data set provides the research community with a novel task of blueberry bush detection, which was not commonly considered thus far due to the lack of relevant data sets. We describe a detailed analysis of the data set, and finally provide an experimental study with several state-of-the-art deep object detection models, that set a baseline for the performance on this novel data set. The data set is made available online, enriching the variability of the existing tasks in the field and supporting further development of smart agriculture applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Filipovic_Bush_Detection_for_Vision-Based_UGV_Guidance_in_Blueberry_Orchards_Data_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Filipovic_Bush_Detection_for_Vision-Based_UGV_Guidance_in_Blueberry_Orchards_Data_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DNA: Deformable Neural Articulations Network for Template-Free Dynamic 3D Human Reconstruction From Monocular RGB-D Video",
        "author": "Khoa Vo, Trong-Thang Pham, Kashu Yamazaki, Minh Tran, Ngan Le",
        "abstract": "In this paper, we present a novel Deformable Neural Articulations Network (DNA-Net), which is a template-free learning-based method for dynamic 3D human reconstruction from a single RGB-D sequence. Our proposed DNA-Net includes a Neural Articulation Prediction Network (NAP-Net), which is capable of representing non-rigid motions of a human by learning to predict a set of articulated bones to follow movements of the human in the input video. Moreover, DNA-Net also include Signed Distance Field Network (SDF-Net) and Apearance Network (Color-Net), which take advantage of the powerful neural implicit functions in modeling 3D geometries and appearance. Finally, to avoid the reliance on external optical flow estimators to obtain deformation cues like previous related works, we propose a novel training loss, namely Easy-to-Hard Geometric-based, which is a simple strategy that inherits the merits of Chamfer distance to achieve good deformation guidance while still avoiding its limitation of local mismatches sensitivity. DNA-Net is trained end-to-end in a self-supervised manner directly on the input video to obtain 3D reconstructions of the input objects. Quantitative results on videos of DeepDeform dataset show that DNA-Net outperforms related state-of-the-art methods with an adequate gaps, qualitative results additionally prove that our method can reconstruct human shapes with high fidelity and details.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Vo_DNA_Deformable_Neural_Articulations_Network_for_Template-Free_Dynamic_3D_Human_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Vo_DNA_Deformable_Neural_Articulations_Network_for_Template-Free_Dynamic_3D_Human_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Unified Model for Continuous Conditional Video Prediction",
        "author": "Xi Ye, Guillaume-Alexandre Bilodeau",
        "abstract": "Different conditional video prediction tasks, like video future frame prediction and video frame interpolation, are normally solved by task-related models even though they share many common underlying characteristics. Furthermore, almost all conditional video prediction models can only achieve discrete prediction. In this paper, we propose a unified model that addresses these two issues at the same time. We show that conditional video prediction can be formulated as a neural process, which maps input spatio-temporal coordinates to target pixel values given context spatio-temporal coordinates and context pixel values. Specifically, we feed the implicit neural representation of coordinates and context pixel features into a Transformer-based non-autoregressive conditional video prediction model. Our task-specific models outperform previous work for video future frame prediction and video interpolation on multiple datasets. Importantly, the model is able to interpolate or predict with an arbitrary high frame rate, i.e., continuous prediction. Our source code is available at https://npvp.github.io.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Ye_A_Unified_Model_for_Continuous_Conditional_Video_Prediction_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Ye_A_Unified_Model_for_Continuous_Conditional_Video_Prediction_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DPOSE: Online Keypoint-CAM Guided Inference for Driver Pose Estimation With GMM-Based Balanced Sampling",
        "author": "Yuyu Guo, Yancheng Bai, Daiqi Shi, Yang Cai, Wei Bian",
        "abstract": "Human pose estimation (HPE) is an essential component of Driving Monitoring Systems (DMS) for real-time recognition of driving behavior. To achieve this, HPE is typically integrated with other tasks such as detection and head pose regression, into a single lightweight model that can be easily deployed on edge-side devices. However, oversimplified designs of lightweight HPE models may cause overfitting on generalized samples, rendering them unable to handle rare samples, particularly in the case of the dataset with the imbalanced distribution. In this paper, we propose an optimization scheme for a proprietary HPE task in DMS scenarios. Our method involves a pose-wise hard mining strategy to balance the pose distribution. Additionally, we introduce an online keypoint independent grad-cam loss, which constrains the gradient-based activation feature map of each keypoint prediction to its corresponding semantic region. We evaluate our approach using a benchmark dataset for DMS tasks and achieve outstanding results. Our code will be publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Guo_DPOSE_Online_Keypoint-CAM_Guided_Inference_for_Driver_Pose_Estimation_With_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Guo_DPOSE_Online_Keypoint-CAM_Guided_Inference_for_Driver_Pose_Estimation_With_CVPRW_2023_paper.pdf"
    },
    {
        "title": "StillFast: An End-to-End Approach for Short-Term Object Interaction Anticipation",
        "author": "Francesco Ragusa, Giovanni Maria Farinella, Antonino Furnari",
        "abstract": "Anticipation problem has been studied considering different aspects such as predicting humans' locations, predicting hands and objects trajectories, and forecasting actions and human-object interactions. In this paper, we studied the short-term object interaction anticipation problem from the egocentric point of view, proposing a new end-to-end architecture named StillFast. Our approach simultaneously processes a still image and a video detecting and localizing next-active objects, predicting the verb which describes the future interaction and determining when the interaction will start. Experiments on the large-scale egocentric dataset EGO4D show that our method outperformed state-of-the-art approaches on the considered task. Our method is ranked first in the public leaderboard of the EGO4D short term object interaction anticipation challenge 2022 and it is the official baseline for the 2023 one. Please see the project web page for code and additional details: https://iplab.dmi.unict.it/stillfast/.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Ragusa_StillFast_An_End-to-End_Approach_for_Short-Term_Object_Interaction_Anticipation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Ragusa_StillFast_An_End-to-End_Approach_for_Short-Term_Object_Interaction_Anticipation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "3D-IntPhys: Towards More Generalized 3D-Grounded Visual Intuitive Physics Under Challenging Scenes",
        "author": "Haotian Xue, Antonio Torralba, Joshua Tenenbaum, Daniel Yamins, Yunzhu Li, Hsiao-Yu Tung",
        "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models from videos of complex scenes with fluids. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, using which we can impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks acquired using color prior. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We generate datasets including three challenging scenarios involving fluid, granular materials, and rigid objects in the simulation. The datasets do not include any dense particle information so most previous 3D-based intuitive physics pipelines can barely deal with that. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that once trained, our model can achieve strong generalization in complex scenarios under extrapolate settings.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Xue_3D-IntPhys_Towards_More_Generalized_3D-Grounded_Visual_Intuitive_Physics_Under_Challenging_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Xue_3D-IntPhys_Towards_More_Generalized_3D-Grounded_Visual_Intuitive_Physics_Under_Challenging_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CIPF: Crossing Intention Prediction Network Based on Feature Fusion Modules for Improving Pedestrian Safety",
        "author": "Je-Seok Ham, Dae Hoe Kim, NamKyo Jung, Jinyoung Moon",
        "abstract": "As the development of autonomous driving technology continues, pedestrian safety is becoming an increasingly important issue. The ability of an autonomous car to accurately predict whether a pedestrian will cross the road is essential for ensuring their safety, as the vehicle can slow down in time or stop to avoid any potential accidents. However, predicting pedestrian behavior is a complex task influenced by various environmental and contextual factors. To deal with this issue, we propose a novel method, Crossing Intention Prediction based on feature Fusion modules (CIPF) that combines eight different input features extracted from both pedestrians and vehicles through three fusion modules using RNN layers and attention mechanisms. We demonstrated state-of-the-art performance of prediction accuracy in the PIE dataset, which is the most widely used for pedestrian crossing intention prediction. We also demonstrated the superiority of the performance of our CIPF network through qualitative and quantitative analysis. In particular, we also performed ablation studies on the verification of the effectiveness of the eight input features, the validity of VGG encoders, and performance comparison of our CIPF over time by adjusting the prediction time.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Ham_CIPF_Crossing_Intention_Prediction_Network_Based_on_Feature_Fusion_Modules_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Ham_CIPF_Crossing_Intention_Prediction_Network_Based_on_Feature_Fusion_Modules_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Best Practices for 2-Body Pose Forecasting",
        "author": "Muhammad Rameez Ur Rahman, Luca Scofano, Edoardo De Matteis, Alessandro Flaborea, Alessio Sampieri, Fabio Galasso",
        "abstract": "The task of collaborative human pose forecasting stands for predicting the future poses of multiple interacting people, given those in previous frames. Predicting two people in interaction, instead of each separately, promises better performance, due to their body-body motion correlations. But the task has remained so far mostly unexplored. In this paper, we review the progress in human pose forecasting and provide an in-depth assessment of the single-person practices that perform best for 2-body collaborative motion forecasting. Our study confirms the positive impact of frequency input representations, space-time separable and fully-learnable interaction adjacencies for the encoding GCN, and FC decoding. Other single-person practices do not transfer to 2-body, so the proposed best ones do not include hierarchical body modeling nor attention-based interaction encoding. We further contribute a novel initialization procedure for the 2-body spatial interaction parameters of the encoder, which benefits performance and stability. Altogether, our proposed 2-body pose forecasting best practices yield a performance improvement of 21.9% over the state-of-the-art on the most recent ExPI dataset, whereby the novel initialization accounts for 3.5%. See our project page at https://www.pinlab.org/bestpractices2body.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Rahman_Best_Practices_for_2-Body_Pose_Forecasting_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Rahman_Best_Practices_for_2-Body_Pose_Forecasting_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ZEBRA: Explaining Rare Cases Through Outlying Interpretable Concepts",
        "author": "Pedro Madeira, Andr\u00e9 Carreiro, Alex Gaudio, Lu\u00eds Rosado, Filipe Soares, Asim Smailagic",
        "abstract": "Anomaly detection methods can detect outliers, but what are the properties of an outlier? In this paper, we propose ZEBRA, a novel framework for generating explanations of an outlier based on the analysis of feature rarity in an interpretable feature space. The contributions of our work include: (a) a modular model-agnostic framework for explanations of outliers; (b) a statistical explanation method based on a rarity score and weighted aggregation functions; (c) multimodal explanations combining visual, textual, and numeric explanations. ZEBRA simplifies the mapping of low-level features to high-level concepts to generate multimodal and human-readable explanations of outliers.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Madeira_ZEBRA_Explaining_Rare_Cases_Through_Outlying_Interpretable_Concepts_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Madeira_ZEBRA_Explaining_Rare_Cases_Through_Outlying_Interpretable_Concepts_CVPRW_2023_paper.pdf"
    },
    {
        "title": "The Manifold Hypothesis for Gradient-Based Explanations",
        "author": "Sebastian Bordt, Uddeshya Upadhyay, Zeynep Akata, Ulrike von Luxburg",
        "abstract": "When do gradient-based explanation algorithms provide perceptually-aligned explanations? We propose a criterion: the feature attributions need to be aligned with the tangent space of the data manifold. To provide evidence for this hypothesis, we introduce a framework based on variational autoencoders that allows to estimate and generate image manifolds. Through experiments across a range of different datasets -- MNIST, EMNIST, CIFAR10, X-ray pneumonia and Diabetic Retinopathy detection -- we demonstrate that the more a feature attribution is aligned with the tangent space of the data, the more perceptually-aligned it tends to be. We then show that the attributions provided by popular post-hoc methods such as Integrated Gradients and SmoothGrad are more strongly aligned with the data manifold than the raw gradient. Adversarial training also improves the alignment of model gradients with the data manifold. As a consequence, we suggest that explanation algorithms should actively strive to align their explanations with the data manifold. An extended version of this paper is available at https://arxiv.org/abs/2206.07387.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Bordt_The_Manifold_Hypothesis_for_Gradient-Based_Explanations_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Bordt_The_Manifold_Hypothesis_for_Gradient-Based_Explanations_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Seg-XRes-CAM: Explaining Spatially Local Regions in Image Segmentation",
        "author": "Syed Nouman Hasany, Caroline Petitjean, Fabrice M\u00e9riaudeau",
        "abstract": "While many post-hoc model interpretability techniques exist for image classification, image segmentation has not received the same attention. An extension of Grad-CAM, Seg-Grad-CAM was proposed as a local interpretability technique for image segmentation. In this paper, we highlight that by virtue of its design, Seg-Grad-CAM does not utilize spatial information when it comes to generating explanations for regions within a segmentation map. Taking inspiration from HiResCAM, we propose Seg-XRes-CAM in order to solve this problem. We verify the utility of our proposed method by visually comparing explanations generated from Seg-Grad-CAM and Seg-XRes-CAM against a model-agnostic, perturbation-based method, RISE. The code is available at https://github.com/Nouman97/Seg_XRes_CAM.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Hasany_Seg-XRes-CAM_Explaining_Spatially_Local_Regions_in_Image_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Hasany_Seg-XRes-CAM_Explaining_Spatially_Local_Regions_in_Image_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CAVLI - Using Image Associations To Produce Local Concept-Based Explanations",
        "author": "Pushkar Shukla, Sushil Bharati, Matthew Turk",
        "abstract": "While explainability is becoming increasingly crucial in computer vision and machine learning, producing explanations that are able to link decisions made by deep neural networks to concepts that are easily understood by humans still remains a challenge. To address this challenge, we propose a framework that produces local concept based explanations for the classification decisions made by a deep neural network. Our framework is based on the intuition that if there is a high overlap between the regions of the image that the model associates the most with the concept and the regions of the image that are useful for decision-making then the decision is highly dependent on the concept. Our proposed CAVLI framework combines a global approach (TCAV) with a local approach (LIME). To test the effectiveness of our approach, we conducted experiments on both the ImageNet and CelebA datasets. These experiments demonstrated that our framework can produce explanations that are easy for humans to understand. By providing local concept-based explanations, our framework has the potential to improve the transparency and interpretability of deep neural networks in a variety of applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Shukla_CAVLI_-_Using_Image_Associations_To_Produce_Local_Concept-Based_Explanations_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Shukla_CAVLI_-_Using_Image_Associations_To_Produce_Local_Concept-Based_Explanations_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ODSmoothGrad: Generating Saliency Maps for Object Detectors",
        "author": "Chul Gwon, Steven C. Howell",
        "abstract": "Techniques for generating saliency maps continue to be used for explainability of deep learning models, with efforts primarily applied to the image classification task. Such techniques, however, can also be applied to object detectors, not only with the classification scores, but also for the bounding box parameters, which are regressed values for which the relevant pixels contributing to these parameters can be identified. In this paper, we present ODSmoothGrad, a tool for generating saliency maps for the classification and the bounding box parameters in object detectors. Given the noisiness of saliency maps, we also apply the SmoothGrad algorithm to visually enhance the pixels of interest. We demonstrate these capabilities on one-stage and two-stage object detectors, with comparisons using classifier-based techniques.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Gwon_ODSmoothGrad_Generating_Saliency_Maps_for_Object_Detectors_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Gwon_ODSmoothGrad_Generating_Saliency_Maps_for_Object_Detectors_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Sanity Checks for Patch Visualisation in Prototype-Based Image Classification",
        "author": "Romain Xu-Darme, Georges Qu\u00e9not, Zakaria Chihani, Marie-Christine Rousset",
        "abstract": "In this work, we perform an analysis of the visualisation methods implemented in ProtoPNet and ProtoTree, two self-explaining visual classifiers based on prototypes. We show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour, which can create a false sense of bias in the model. We also demonstrate quantitatively that this issue can be mitigated by using other saliency methods that provide more faithful image patches.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Xu-Darme_Sanity_Checks_for_Patch_Visualisation_in_Prototype-Based_Image_Classification_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Xu-Darme_Sanity_Checks_for_Patch_Visualisation_in_Prototype-Based_Image_Classification_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Disentangling Neuron Representations With Concept Vectors",
        "author": "Laura O'Mahony, Vincent Andrearczyk, Henning M\u00fcller, Mara Graziani",
        "abstract": "Mechanistic interpretability aims to understand how models store representations by breaking down neural networks into interpretable units. However, the occurrence of polysemantic neurons, or neurons that respond to multiple unrelated features, makes interpreting individual neurons challenging. This has led to the search for meaningful vectors, known as concept vectors, in activation space instead of individual neurons. The main contribution of this paper is a method to disentangle polysemantic neurons into concept vectors encapsulating distinct features. Our method can search for fine-grained concepts according to the user's desired level of concept separation. The analysis shows that polysemantic neurons can be disentangled into directions consisting of linear combinations of neurons. Our evaluations show that the concept vectors found encode coherent, human-understandable features.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/OMahony_Disentangling_Neuron_Representations_With_Concept_Vectors_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/OMahony_Disentangling_Neuron_Representations_With_Concept_Vectors_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Hierarchical Explanations for Video Action Recognition",
        "author": "Sadaf Gulshad, Teng Long, Nanne van Noord",
        "abstract": "To interpret deep neural networks, one main approach is to dissect the visual input and find the prototypical parts responsible for the classification. However, existing methods often ignore the hierarchical relationship between these prototypes, and thus can not explain semantic concepts at both higher level (e.g., water sports) and lower level (e.g., swimming). In this paper inspired by human cognition system, we leverage hierarchal information to deal with uncertainty. To this end, we propose HIerarchical Prototypical Explainer (HIPE) to build hierarchical relations between prototypes and classes. The faithfulness of our method is verified by reducing accuracy-explainability trade-off on UCF-101 while providing multi-level explanations.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Gulshad_Hierarchical_Explanations_for_Video_Action_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Gulshad_Hierarchical_Explanations_for_Video_Action_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robustness of Visual Explanations to Common Data Augmentation Methods",
        "author": "Lenka T\u011btkov\u00e1, Lars Kai Hansen",
        "abstract": "As the use of deep neural networks continues to grow, understanding their behaviour has become more crucial than ever. Post-hoc explainability methods are a potential solution, but their reliability is being called into question. Our research investigates the response of post-hoc visual explanations to naturally occurring transformations, often referred to as augmentations. We anticipate explanations to be invariant under certain transformations, such as changes to the colour map while responding in an equivariant manner to transformations like translation, object scaling, and rotation. We have found remarkable differences in robustness depending on the type of transformation, with some explainability methods (such as LRP composites and Guided Backprop) being more stable than others. We also explore the role of training with data augmentation. We provide evidence that explanations are typically less robust to augmentation than classification performance, regardless of whether data augmentation is used in training or not.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Tetkova_Robustness_of_Visual_Explanations_to_Common_Data_Augmentation_Methods_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Tetkova_Robustness_of_Visual_Explanations_to_Common_Data_Augmentation_Methods_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Shared Interest...Sometimes: Understanding the Alignment Between Human Perception, Vision Architectures, and Saliency Map Techniques",
        "author": "Katelyn Morrison, Ankita Mehra, Adam Perer",
        "abstract": "Empirical studies have shown that attention-based architectures outperform traditional convolutional neural networks (CNN) in terms of accuracy and robustness. As a result, attention-based architectures are increasingly used in high-stakes domains such as radiology and wildlife conservation to aid in decision-making. However, understanding how attention-based architectures compare to CNNs regarding alignment with human perception is still under-explored. Previous studies exploring how vision architectures align with human perception evaluate a single architecture with multiple explainability techniques or multiple architectures with a single explainability technique. Through an empirical analysis, we investigate how two attention-based architectures and two CNNs for two saliency map techniques align with the ground truth for human perception on 100 images from an interpretability benchmark dataset. Using the Shared Interest metrics, we found that CNNs align more with human perception when using the XRAI saliency map technique. However, we found the opposite for Grad-CAM. We discuss the implications of our analysis for human-centered explainable AI and introduce directions for future work.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Morrison_Shared_Interest...Sometimes_Understanding_the_Alignment_Between_Human_Perception_Vision_Architectures_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Morrison_Shared_Interest...Sometimes_Understanding_the_Alignment_Between_Human_Perception_Vision_Architectures_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Towards Evaluating Explanations of Vision Transformers for Medical Imaging",
        "author": "Piotr Komorowski, Hubert Baniecki, Przemyslaw Biecek",
        "abstract": "As deep learning models increasingly find applications in critical domains such as medical imaging, the need for transparent and trustworthy decision-making becomes paramount. Many explainability methods provide insights into how these models make predictions by attributing importance to input features. As Vision Transformer (ViT) becomes a promising alternative to convolutional neural networks for image classification, its interpretability remains an open research question. This paper investigates the performance of various interpretation methods on a ViT applied to classify chest X-ray images. We introduce the notion of evaluating faithfulness, sensitivity, and complexity of ViT explanations. The obtained results indicate that Layerwise relevance propagation for transformers outperforms Local interpretable model-agnostic explanations and Attention visualization, providing a more accurate and reliable representation of what a ViT has actually learned. Our findings provide insights into the applicability of ViT explanations in medical imaging and highlight the importance of using appropriate evaluation criteria for comparing them.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Komorowski_Towards_Evaluating_Explanations_of_Vision_Transformers_for_Medical_Imaging_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Komorowski_Towards_Evaluating_Explanations_of_Vision_Transformers_for_Medical_Imaging_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Ante-Hoc Generation of Task-Agnostic Interpretation Maps",
        "author": "Akash Guna R. T., Raul Benitez, Sikha O. K.",
        "abstract": "Existing explainability approaches for convolutional neural networks (CNNs) are mainly applied after training (post-hoc) which is generally unreliable. Ante-hoc explainers trained simultaneously with the CNN are more reliable. However, current ante-hoc explanation methods mainly generate inexplicit concept-based explanations tailored to specific tasks. To address these limitations, we propose a task-agnostic ante-hoc framework that can generate interpretation maps to visually explain any CNN. Our framework simultaneously trains the CNN and a weighting network - an explanation generation module. The generated maps are self-explanatory, eliminating the need for manual identification of concepts. We demonstrate that our method can interpret tasks such as classification, facial landmark detection, and image captioning. We show that our framework is explicit, faithful, and stable through experiments. To the best of our knowledge, this is the first ante-hoc CNN explanation strategy that produces visual explanations generic to CNNs for different tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/T._Ante-Hoc_Generation_of_Task-Agnostic_Interpretation_Maps_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/T._Ante-Hoc_Generation_of_Task-Agnostic_Interpretation_Maps_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Localized Shortcut Removal",
        "author": "Nicolas M. M\u00fcller, Jochen Jacobs, Jennifer Williams, Konstantin B\u00f6ttinger",
        "abstract": "Machine learning is a data-driven field, and the quality of the underlying datasets plays a crucial role in learning success. However, high performance on held-out test data does not necessarily indicate that a model generalizes or learns anything meaningful. This is often due to the existence of machine learning shortcuts - features in the data that are predictive but unrelated to the problem at hand. To address this issue for datasets where the shortcuts are smaller and more localized than true features, we propose a novel approach to detect and remove them. We use an adversarially trained lens to detect and eliminate highly predictive but semantically unconnected clues in images. In our experiments on both synthetic and real-world data, we show that our proposed approach reliably identifies and neutralizes such shortcuts without causing degradation of model performance on clean data. We believe that our approach can lead to more meaningful and generalizable machine learning models, especially in scenarios where the quality of the underlying datasets is crucial.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Muller_Localized_Shortcut_Removal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Muller_Localized_Shortcut_Removal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Confusion Matrix for Evaluating Feature Attribution Methods",
        "author": "Anna Arias-Duart, Ettore Mariotti, Dario Garcia-Gasulla, Jose Maria Alonso-Moral",
        "abstract": "The increasing use of deep learning models in critical areas of computer vision and the consequent need for insights into model behaviour have led to the development of numerous feature attribution methods. However, these attributions must be both meaningful and plausible to end-users, which is not always the case. Recent research has emphasized the importance of faithfulness in attributions, as plausibility without faithfulness can result in misleading explanations and incorrect decisions. In this work, we propose a novel approach to evaluate the faithfulness of feature attribution methods by constructing an 'Attribution Confusion Matrix', which allows us to leverage a wide range of existing metrics from the traditional confusion matrix. This approach effectively introduces multiple evaluation measures for faithfulness in feature attribution methods in a unified and consistent framework. We demonstrate the effectiveness of our approach on various datasets, attribution methods, and models, emphasizing the importance of faithfulness in generating plausible and reliable explanations while also illustrating the distinct behaviour of different feature attribution methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Arias-Duart_A_Confusion_Matrix_for_Evaluating_Feature_Attribution_Methods_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Arias-Duart_A_Confusion_Matrix_for_Evaluating_Feature_Attribution_Methods_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Analyzing Results of Depth Estimation Models With Monocular Criteria",
        "author": "Jonas Theiner, Nils Nommensen, Jim Rhotert, Matthias Springstein, Eric M\u00fcller-Budack, Ralph Ewerth",
        "abstract": "Monocular depth estimation is an essential but ill-posed (computer) vision task. While human visual perception of depth relies on several monocular depth clues, such as occlusion of objects, relative height, usual object size, linear perspective, deep learning models have to implicitly learn these cues from labeled training data to determine depth. In this paper, we investigate whether monocular depth criteria from human vision are violated for certain image instances given a model's predictions. We consider the task of depth estimation as a ranking problem, i.e., for a given pair of points, we estimate which point is nearer to the camera. In particular, we model four monocular depth criteria to automatically predict a subset of point pairs and infer their depth relation. Our experiments show that the implemented depth criteria achieve comparable performance to deep learning models. This allows the investigation of models with regard to the plausibility of predictions by finding image instances where the prediction is incorrect according to modeled human visual perception.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Theiner_Analyzing_Results_of_Depth_Estimation_Models_With_Monocular_Criteria_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Theiner_Analyzing_Results_of_Depth_Estimation_Models_With_Monocular_Criteria_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Vision DiffMask: Faithful Interpretation of Vision Transformers With Differentiable Patch Masking",
        "author": "Angelos Nalmpantis, Apostolos Panagiotopoulos, John Gkountouras, Konstantinos Papakostas, Wilker Aziz",
        "abstract": "The lack of interpretability of the Vision Transformer may hinder its use in critical real-world applications despite its effectiveness. To overcome this issue, we propose a post-hoc interpretability method called VISION DIFFMASK, which uses the activations of the model's hidden layers to predict the relevant parts of the input that contribute to its final predictions. Our approach uses a gating mechanism to identify the minimal subset of the original input that preserves the predicted distribution over classes. We demonstrate the faithfulness of our method, by introducing a faithfulness task, and comparing it to other state-of-the-art attribution methods on CIFAR-10 and ImageNet-1K, achieving compelling results.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Nalmpantis_Vision_DiffMask_Faithful_Interpretation_of_Vision_Transformers_With_Differentiable_Patch_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Nalmpantis_Vision_DiffMask_Faithful_Interpretation_of_Vision_Transformers_With_Differentiable_Patch_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Text2Concept: Concept Activation Vectors Directly From Text",
        "author": "Mazda Moayeri, Keivan Rezaei, Maziar Sanjabi, Soheil Feizi",
        "abstract": "Concept activation vectors (CAVs) enable interpretability of a model with respect to human concepts, though CAV generation requires the costly step of curating positive and negative examples for each concept one wishes to encode. To alleviate this bottleneck, we present Text2Concept, an efficient method for obtaining CAVs directly from text. Text2Concept extends the multi-modal accessibility of a CLIP model's feature space to that of an arbitrary off-the-shelf vision model, with only the small extra step of training linear layers on existing data to map the feature spaces to one another. We validate our method qualitatively, by sorting images by similarity to embedded concepts, and quantitatively, by showing surprisingly strong zero-shot classification (enabled via Text2Concept) performance for off-the-shelf vision encoders. Finally, we demonstrate two new interpretability applications of Text2Concept CAVs: building concept bottleneck models with no concept supervision, and diagnosing distribution shifts in terms of human concepts.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Moayeri_Text2Concept_Concept_Activation_Vectors_Directly_From_Text_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Moayeri_Text2Concept_Concept_Activation_Vectors_Directly_From_Text_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Coherent Concept-Based Explanations in Medical Image and Its Application to Skin Lesion Diagnosis",
        "author": "Cristiano Patr\u00edcio, Jo\u00e3o C. Neves, Luis F. Teixeira",
        "abstract": "Early detection of melanoma is crucial for preventing severe complications and increasing the chances of successful treatment. Existing deep learning approaches for melanoma skin lesion diagnosis are deemed black-box models, as they omit the rationale behind the model prediction, compromising the trustworthiness and acceptability of these diagnostic methods. Attempts to provide concept-based explanations are based on post-hoc approaches, which depend on an additional model to derive interpretations. In this paper, we propose an inherently interpretable framework to improve the interpretability of concept-based models by incorporating a hard attention mechanism and a coherence loss term to assure the visual coherence of concept activations by the concept encoder, without requiring the supervision of additional annotations. The proposed framework explains its decision in terms of human-interpretable concepts and their respective contribution to the final prediction, as well as a visual interpretation of the locations where the concept is present in the image. Experiments on skin image datasets demonstrate that our method outperforms existing black-box and concept-based models for skin lesion classification.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Patricio_Coherent_Concept-Based_Explanations_in_Medical_Image_and_Its_Application_to_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Patricio_Coherent_Concept-Based_Explanations_in_Medical_Image_and_Its_Application_to_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation",
        "author": "Alexander Koenig, Maximilian Schambach, Johannes Otterbach",
        "abstract": "Self-supervised pre-training strategies have recently shown impressive results for training general-purpose feature extraction backbones in computer vision. In combination with the Vision Transformer architecture, the DINO self-distillation technique has interesting emerging properties, such as unsupervised clustering in the latent space and semantic correspondences of the produced features without using explicit human-annotated labels. The STEGO method for unsupervised semantic segmentation contrastively distills feature correspondences of a DINO-pre-trained Vision Transformer and recently set a new state of the art. However, the detailed workings of STEGO have yet to be disentangled, preventing its usage in safety-critical applications. This paper provides a deeper understanding of the STEGO architecture and training strategy by conducting studies that uncover the working mechanisms behind STEGO, reproduce and extend its experimental validation, and investigate the ability of STEGO to transfer to different datasets. Results demonstrate that the STEGO architecture can be interpreted as a semantics-preserving dimensionality reduction technique.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Koenig_Uncovering_the_Inner_Workings_of_STEGO_for_Safe_Unsupervised_Semantic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Koenig_Uncovering_the_Inner_Workings_of_STEGO_for_Safe_Unsupervised_Semantic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Novel Benchmark for Refinement of Noisy Localization Labels in Autolabeled Datasets for Object Detection",
        "author": "Andreas B\u00e4r, Jonas Uhrig, Jeethesh Pai Umesh, Marius Cordts, Tim Fingscheidt",
        "abstract": "Autolabeling approaches are attractive w.r.t. time and cost as they allow fast annotation without human intervention. However, can we really trust the label quality of autolabeling? And further, which potential consequences arise from resulting label noise? In this work, we address these questions for localization, a subtask of object detection, by investigating the effects on a state-of-the-art deep neural network (DNN) for object detection and the widely used Pascal VOC 2012 dataset. Our contributions are threefold: First, we propose a method to inject noise into localization labels, enabling us to simulate localization label errors of autolabeling methods. Afterwards, we train a state-of-the-art object detection DNN with these noisy labels. Second, we propose a refinement network which takes a noisy localization label and its respective image as input and performs a localization refinement. Third, we again train a state-of-the-art object detection DNN, however, this time with refined localization labels. Our insights are: Training a state-of-the-art DNN for object detection on noisy localization labels leads to a severe performance drop. Our proposed localization label refinement network is able to refine the noisy localization labels. We are able to retain the performance to some extent by retraining the state-of-the-art DNN for object detection on the refined localization labels. Our study motivates a new challenging task \"refinement of noisy localization labels\" and sets a first benchmark for Pascal VOC 2012. Code is available at https://github.com/ifnspaml/LocalizationLabelNoise.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Bar_A_Novel_Benchmark_for_Localization_Label_Errors_and_Their_Refinement_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Bar_A_Novel_Benchmark_for_Localization_Label_Errors_and_Their_Refinement_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Maximum Entropy Information Bottleneck for Uncertainty-Aware Stochastic Embedding",
        "author": "Sungtae An, Nataraj Jammalamadaka, Eunji Chong",
        "abstract": "Stochastic embedding has several advantages over deterministic embedding, such as the capability of associating uncertainty with the resulting embedding and robustness to noisy data. This is especially useful when the input data has ambiguity (e.g., blurriness or corruption) which often happens with in-the-wild settings. Many existing methods for stochastic embedding are limited by the assumption that the embedding follows a standard normal distribution under the variational information bottleneck principle. We present a different variational approach to stochastic embedding in which maximum entropy acts as the bottleneck, which we call Maximum Entropy Information Bottleneck or MEIB. We show that models trained with the MEIB objective outperform existing methods in terms of regularization, perturbation robustness, probabilistic contrastive learning, and risk-controlled recognition performance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/An_Maximum_Entropy_Information_Bottleneck_for_Uncertainty-Aware_Stochastic_Embedding_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/An_Maximum_Entropy_Information_Bottleneck_for_Uncertainty-Aware_Stochastic_Embedding_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Interpretable Model-Agnostic Plausibility Verification for 2D Object Detectors Using Domain-Invariant Concept Bottleneck Models",
        "author": "Mert Keser, Gesina Schwalbe, Azarm Nowzad, Alois Knoll",
        "abstract": "Despite the unchallenged performance, deep neural network (DNN) based object detectors (OD) for computer vision have inherent, hard-to-verify limitations like brittleness, opacity, and unknown behavior on corner cases. Therefore, operation-time safety measures like monitors will be inevitable--even mandatory--for use in safetycritical applications like automated driving (AD). This paper presents an approach for plausibilization of OD detections using a small model-agnostic, robust, interpretable, and domain-invariant image classification model. The safety requirements of interpretability and robustness are achieved by using a small concept bottleneck model (CBM), a DNN intercepted by interpretable intermediate outputs. The domain-invariance is necessary for robustness against common domain shifts, and for cheap adaptation to diverse AD settings. While vanilla CBMs are here shown to fail in case of domain shifts like natural perturbations, we substantially improve the CBM via combination with trainable color-invariance filters developed for domain adaptation. Furthermore, the monitor that utilizes CBMs with trainable color-invarince filters is successfully applied in an AD OD setting for detection of hallucinated objects with zero-shot domain adaptation, and to false positive detection with few-shot adaptation, proving this to be a promising approach for error monitoring.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Keser_Interpretable_Model-Agnostic_Plausibility_Verification_for_2D_Object_Detectors_Using_Domain-Invariant_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Keser_Interpretable_Model-Agnostic_Plausibility_Verification_for_2D_Object_Detectors_Using_Domain-Invariant_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Revealing Hidden Context Bias in Segmentation and Object Detection Through Concept-Specific Explanations",
        "author": "Maximilian Dreyer, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin",
        "abstract": "Applying traditional post-hoc attribution methods to segmentation or object detection predictors offers only limited insights, as the obtained feature attribution maps at input level typically resemble the models' predicted segmentation mask or bounding box. In this work, we address the need for more informative explanations for these predictors by proposing the post-hoc eXplainable Artificial Intelligence method L-CRP to generate explanations that automatically identify and visualize relevant concepts learned, recognized and used by the model during inference as well as precisely locate them in input space. Our method therefore goes beyond singular input-level attribution maps and, as an approach based on the Concept Relevance Propagation technique, is efficiently applicable to state-of-the-art black-box architectures in segmentation and object detection, such as DeepLabV3+ and YOLOv6, among others. We verify the faithfulness of our proposed technique by quantitatively comparing different concept attribution methods, and discuss the effect on explanation complexity on popular datasets such as CityScapes, Pascal VOC and MS COCO 2017. The ability to precisely locate and communicate concepts is used to reveal and verify the use of background features, thereby highlighting possible biases of the model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Dreyer_Revealing_Hidden_Context_Bias_in_Segmentation_and_Object_Detection_Through_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Dreyer_Revealing_Hidden_Context_Bias_in_Segmentation_and_Object_Detection_Through_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Optimizing Explanations by Network Canonization and Hyperparameter Search",
        "author": "Frederik Pahde, Galip \u00dcmit Yolcu, Alexander Binder, Wojciech Samek, Sebastian Lapuschkin",
        "abstract": "Explainable AI (XAI) is slowly becoming a key component for many AI applications. Rule-based and modified backpropagation XAI approaches however often face challenges when being applied to modern model architectures including innovative layer building blocks, which is caused by two reasons. Firstly, the high flexibility of rule-based XAI methods leads to numerous potential parameterizations. Secondly, many XAI methods break the implementation-invariance axiom because they struggle with certain model components, e.g., BatchNorm layers. The latter can be addressed with model canonization, which is the process of re-structuring the model to disregard problematic components without changing the underlying function. While model canonization is straightforward for simple architectures (e.g., VGG, ResNet), it can be challenging for more complex and highly interconnected models (e.g., DenseNet). Moreover, there is only little quantifiable evidence that model canonization is beneficial for XAI. In this work, we propose canonizations for currently relevant model blocks applicable to popular deep neural network architectures, including VGG, ResNet, EfficientNet, DenseNets, as well as Relation Networks. We further suggest a XAI evaluation framework with which we quantify and compare the effects of model canonization for various XAI methods in image classification tasks on the Pascal VOC and ILSVRC2017 datasets, as well as for Visual Question Answering using CLEVR-XAI. Moreover, addressing the former issue outlined above, we demonstrate how our evaluation framework can be applied to perform hyperparameter search for XAI methods to optimize the quality of explanations. Code is available on https://github.com/frederikpahde/xai-canonization",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Pahde_Optimizing_Explanations_by_Network_Canonization_and_Hyperparameter_Search_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Pahde_Optimizing_Explanations_by_Network_Canonization_and_Hyperparameter_Search_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Category Differences Matter: A Broad Analysis of Inter-Category Error in Semantic Segmentation",
        "author": "Jingxing Zhou, J\u00fcrgen Beyerer",
        "abstract": "In current evaluation schemes of semantic segmentation, metrics are calculated in such a way that all predicted classes should equally be identical to their ground truth, paying less attention to the various manifestations of the false predictions within the object category. In this work, we propose the Critical Error Rate (CER) as a supplement to the current evaluation metrics, focusing on the error rate, which reflects predictions that fall outside of the category from the ground truth. We conduct a series of experiments evaluating the behavior of different network architectures in various evaluation setups, including domain shift, the introduction of novel classes, and a mixture of these. We demonstrate the essential criteria for network generalization with those experiments. Furthermore, we ablate the impact of utilizing various class taxonomies for the evaluation of out-of-category error.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Zhou_Category_Differences_Matter_A_Broad_Analysis_of_Inter-Category_Error_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Zhou_Category_Differences_Matter_A_Broad_Analysis_of_Inter-Category_Error_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "RL-CAM: Visual Explanations for Convolutional Networks Using Reinforcement Learning",
        "author": "Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Sahand Ghorbanpour, Vineet Gundecha, Antonio Guillen, Ricardo Luna, Avisek Naug",
        "abstract": "Convolutional Neural Networks (CNNs) are state-of-the-art models for computer vision tasks such as image classification, object detection, and segmentation. However, these models suffer from their inability to explain decisions, particularly in fields like healthcare and security, where interpretability is critical. Previous research has developed various methods for interpreting CNNs, including visualization-based approaches (e.g., saliency maps) that aim to reveal the underlying features used by the model to make predictions. In this work, we propose a novel approach that uses reinforcement learning to generate a visual explanation for CNNs. Our method considers the black-box CNN model and relies solely on the probability distribution of the model's output to localize the features contributing to a particular prediction. The proposed reinforcement learning algorithm has an agent with two actions, a forward action that explores the input image and identifies the most sensitive region to generate a localization mask and a reverse action that fine-tunes the localization mask. We evaluate the performance of our approach using multiple image segmentation metrics and compare it with existing visualization-based methods. The experimental results demonstrate that our proposed method outperforms the existing techniques, producing more accurate localization masks of regions of interest in the input images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Sarkar_RL-CAM_Visual_Explanations_for_Convolutional_Networks_Using_Reinforcement_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Sarkar_RL-CAM_Visual_Explanations_for_Convolutional_Networks_Using_Reinforcement_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Beyond AUROC & Co. for Evaluating Out-of-Distribution Detection Performance",
        "author": "Galadrielle Humblot-Renaux, Sergio Escalera, Thomas B. Moeslund",
        "abstract": "While there has been a growing research interest in developing out-of-distribution (OOD) detection methods, there has been comparably little discussion around how these methods should be evaluated. Given their relevance for safe(r) AI, it is important to examine whether the basis for comparing OOD detection methods is consistent with practical needs. In this work, we take a closer look at the go-to metrics for evaluating OOD detection, and question the approach of exclusively reducing OOD detection to a binary classification task with little consideration for the detection threshold. We illustrate the limitations of current metrics (AUROC & its friends) and propose a new metric - Area Under the Threshold Curve (AUTC), which explicitly penalizes poor separation between ID and OOD samples. Scripts and data are available at https://github.com/glhr/beyond-auroc",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Humblot-Renaux_Beyond_AUROC__Co._for_Evaluating_Out-of-Distribution_Detection_Performance_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Humblot-Renaux_Beyond_AUROC__Co._for_Evaluating_Out-of-Distribution_Detection_Performance_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Investigating CLIP Performance for Meta-Data Generation in AD Datasets",
        "author": "Sujan Sai Gannamaneni, Arwin Sadaghiani, Rohil Prakash Rao, Michael Mock, Maram Akila",
        "abstract": "Using Machine Learning (ML) models for safety-critical perception tasks in Autonomous Driving (AD) or other domains requires a thorough evaluation of the model performance and the data coverage w.r.t. the intended Operational Design Domain (ODD). However, obtaining the needed per-image semantic meta-data along the relevant dimensions of the ODD for real-world image datasets is non-trivial. Recent advances in self-supervised foundation models, specifically CLIP, suggest that such meta-data could be obtained for real-world images in an automated fashion using zero-shot classification. While CLIP was already reported to achieve promising performance on tasks such as the recognition of gender or age on facial images, we investigate to which extent less prominent and more fine-grained observables, e.g., presence of accessories such as spectacles or the shirt- or hair-color, can be determined. We provide an analysis of CLIP for generating fine-grained meta-data on three datasets from the AD domain, one of synthetic origin including ground truth, the others being Cityscapes and Railsem19. We also compare with a standard facial dataset where more elaborate attribute annotations are present. To improve the quality of generated meta-data, we additionally extend the ensemble approach of CLIP by a simple noise-suppressing technique.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/html/Gannamaneni_Investigating_CLIP_Performance_for_Meta-Data_Generation_in_AD_Datasets_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Gannamaneni_Investigating_CLIP_Performance_for_Meta-Data_Generation_in_AD_Datasets_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Flow Cytometry With Event-Based Vision and Spiking Neuromorphic Hardware",
        "author": "Steven Abreu, Muhammed Gouda, Alessio Lugnan, Peter Bienstman",
        "abstract": "Imaging flow cytometry systems play a critical role in the identification and characterization of large populations of cells or micro-particles. Such systems typically leverage deep artificial neural networks to classify samples. Here we show that an event-based camera and neuromorphic processor can be used in a flow cytometry setup to solve a binary particle classification task with less memory usage, and promising improvements in latency and energy scaling. To reduce the complexity of the spiking neural network, we combine the event-based camera with a free-space optical setup which acts as a non-linear high-dimensional feature map that is computed at the speed of light before the event-based camera receives the signal. We demonstrate, for the first time, a spiking neural network running on neuromorphic hardware for a fully event-based flow cytometry pipeline with 98.45% testing accuracy. Our best artificial neural network on frames of the same data reaches only 97.51%, establishing a neuromorphic advantage also in classification accuracy. We further show that our system will scale favorably to more complex classification tasks. We pave the way for real-time classification with throughput of up to 1,000 samples per second and open up new possibilities for online and on-chip learning in flow cytometry applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Abreu_Flow_Cytometry_With_Event-Based_Vision_and_Spiking_Neuromorphic_Hardware_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Abreu_Flow_Cytometry_With_Event-Based_Vision_and_Spiking_Neuromorphic_Hardware_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Live Demonstration: Scamp-7",
        "author": "Laurie Bose, Piotr Dudek, Stephen J. Carey, Jianing Chen",
        "abstract": "We propose to demonstrate the next generation of the SCAMP vision sensor, SCAMP-7, whose architecture embeds programmable processing cores into each pixel. This IC allows various computer vision tasks to be performed \"on-sensor\" at high speeds while avoiding the slow and power hungry transfer of image data off the device. Through in-pixel computations over the image array, visual information is reduced to small amount of highly-informative 'event' data (e.g. detection results, coordinates of objects of interest, motion vectors, etc.). This methodology reduces power consumption associated with data movements and shortens the pipeline between visual event and image information output, resulting in typical latency of a single frame time, while processing thousands frames per second.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Bose_Live_Demonstration_Scamp-7_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Bose_Live_Demonstration_Scamp-7_CVPRW_2023_paper.pdf"
    },
    {
        "title": "HUGNet: Hemi-Spherical Update Graph Neural Network Applied to Low-Latency Event-Based Optical Flow",
        "author": "Thomas Dalgaty, Thomas Mesquida, Damien Joubert, Amos Sironi, Pascal Vivet, Christoph Posch",
        "abstract": "Event camera pixels asynchronously output binary events corresponding to local light intensity changes in time. While encoding visual information in this fashion increases sparsity and the temporal detail of motion with respect to frame-based cameras, there is not yet an established machine learning method capable of exploiting these features to increase efficiency, reduce latency and, ultimately, perform optimally in event-based tasks. Graph neural networks are a promising avenue for such a method, but current solutions are too slow to be compatible with the continuous streaming nature of event-data. In this study, we propose a hemi-spherical update event-graph neural network that significantly reduces the complexity and latency of graph updating and event-level prediction. We compare our approach to existing graph neural network methods, as well as to dense-frame convolutional neural networks, on optical flow estimation tasks. Relative to the previous state of the art in event-graphs, we reduce event-graph update latency by more than four orders of magnitude and reduce the number of neural network calculations per second by 70x while predicting optical flow more accurately.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Dalgaty_HUGNet_Hemi-Spherical_Update_Graph_Neural_Network_Applied_to_Low-Latency_Event-Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Dalgaty_HUGNet_Hemi-Spherical_Update_Graph_Neural_Network_Applied_to_Low-Latency_Event-Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Event-IMU Fusion Strategies for Faster-Than-IMU Estimation Throughput",
        "author": "William Chamorro, Joan Sol\u00e0, Juan Andrade-Cetto",
        "abstract": "This study presents new methods for integrating event data and IMU readings to achieve ultra-fast camera pose estimates. The conventional predict-with-IMU-correct-with-vision approach is no longer optimal because events can be generated much more rapidly than IMU data. Therefore, two novel fusion schemes are proposed, which combine constant velocity and constant acceleration prediction models with ultra-fast (10 kHz) event-based updates and slower 1 kHz IMU updates. The first scheme uses IMU data as instantaneous measurements of acceleration and angular rate, while the second scheme considers these measurements as the average within the IMU sampling time. To provide a basis for comparison, the traditional method that predicts motion using IMU data and updates the estimates with event-feature matching over a 1ms time window is also implemented. All models are designed as Kalman filter variants, which act as the tracker module of a PTAM system in a human-made indoor scenario and are subjected to stress experiments to evaluate their capabilities. The models are also compared against an event-only estimator and a frame-based visual-inertial approach. The findings demonstrate superior performance at a throughput that is 100 times faster than the state-of-the-art.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Chamorro_Event-IMU_Fusion_Strategies_for_Faster-Than-IMU_Estimation_Throughput_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Chamorro_Event-IMU_Fusion_Strategies_for_Faster-Than-IMU_Estimation_Throughput_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Entropy Coding-Based Lossless Compression of Asynchronous Event Sequences",
        "author": "Ionut Schiopu, Radu Ciprian Bilcu",
        "abstract": "The event sensor acquires large amounts of data as events are triggered at microsecond time resolution. In this paper, a novel entropy coding-based method is proposed for encoding asynchronous event sequences. The proposed method employs the event coding framework, where: (i) the input sequence is rearranged as a set of same-timestamp subsequences, where each subsequence is represented of a set of data structures (DSs); and (ii) each DS is encoded by a specific version of the triple threshold partition (TTP) algorithm, where a bitstream collects the binary representation of a set of data elements. A first contribution consists in improving the low-complexity algorithm, LLC-ARES, by modifying the TTP algorithm to employ entropy coding-based techniques to efficient encode the set of data elements. An adaptive Markov model encodes each data element by modelling its symbol probability distribution. Six different types of data elements are distinguished, each having a different support symbol alphabet. Another contribution consists in exploring novel prediction strategies, for the unsorted spatial dimension, and parameter initialization, for the new error distributions. The experimental evaluation demonstrates that the proposed method achieves an improved average coding performance of 28.03%, 35.27%, and 64.54% compared with the state-of-the-art data compression codecs Bzip2, LZMA, and ZLIB, respectively, and 21.4% compared with LLC-ARES.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Schiopu_Entropy_Coding-Based_Lossless_Compression_of_Asynchronous_Event_Sequences_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Schiopu_Entropy_Coding-Based_Lossless_Compression_of_Asynchronous_Event_Sequences_CVPRW_2023_paper.pdf"
    },
    {
        "title": "M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset",
        "author": "Kenneth Chaney, Fernando Cladera, Ziyun Wang, Anthony Bisulco, M. Ani Hsieh, Christopher Korpela, Vijay Kumar, Camillo J. Taylor, Kostas Daniilidis",
        "abstract": "We present M3ED, the first multi-sensor event camera dataset focused on high-speed dynamic motions in robotics applications. M3ED provides high-quality synchronized and labeled data from multiple platforms, including ground vehicles, legged robots, and aerial robots, operating in challenging conditions such as driving along off-road trails, navigating through dense forests, and executing aggressive flight maneuvers. Our dataset also covers demanding operational scenarios for event cameras, such as scenes with high egomotion and multiple independently moving objects. The sensor suite used to collect M3ED includes high-resolution stereo event cameras (1280x720), grayscale imagers, an RGB imager, a high-quality IMU, a 64-beam LiDAR, and RTK localization. This dataset aims to accelerate the development of event-based algorithms and methods for edge cases encountered by autonomous systems in dynamic environments.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Chaney_M3ED_Multi-Robot_Multi-Sensor_Multi-Environment_Event_Dataset_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Chaney_M3ED_Multi-Robot_Multi-Sensor_Multi-Environment_Event_Dataset_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Density Invariant Contrast Maximization for Neuromorphic Earth Observations",
        "author": "Sami Arja, Alexandre Marcireau, Richard L. Balthazor, Matthew G. McHarg, Saeed Afshar, Gregory Cohen",
        "abstract": "Contrast maximization (CMax) techniques are widely used in event-based vision systems to estimate the motion parameters of the camera and generate high-contrast images. However, these techniques are noise-intolerance and suffer from the multiple extrema problem which arises when the scene contains more noisy events than structure, causing the contrast to be higher at multiple locations. This makes the task of estimating the camera motion extremely challenging, which is a problem for neuromorphic earth observation, because, without a proper estimation of the motion parameters, it is not possible to generate a map with high contrast, causing important details to be lost. Similar methods that use CMax addressed this problem by changing or augmenting the objective function to enable it to converge to the correct motion parameters. Our proposed solution overcomes the multiple extrema and noise-intolerance problems by correcting the warped event before calculating the contrast and offers the following advantages: it does not depend on the event data, it does not require a prior about the camera motion and keeps the rest of the CMax pipeline unchanged. This is to ensure that the contrast is only high around the correct motion parameters. Our approach enables the creation of better motion-compensated maps through an analytical compensation technique using a novel dataset from the International Space Station (ISS). Code is available at https://github.com/neuromorphicsystems/event_warping",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Arja_Density_Invariant_Contrast_Maximization_for_Neuromorphic_Earth_Observations_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Arja_Density_Invariant_Contrast_Maximization_for_Neuromorphic_Earth_Observations_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Neuromorphic Optical Flow and Real-Time Implementation With Event Cameras",
        "author": "Yannick Schnider, Stanis\u0142aw Wo\u017aniak, Mathias Gehrig, Jules Lecomte, Axel von Arnim, Luca Benini, Davide Scaramuzza, Angeliki Pantazi",
        "abstract": "Optical flow provides information on relative motion that is an important component in many computer vision pipelines. Neural networks provide high accuracy optical flow, yet their complexity is often prohibitive for application at the edge or in robots, where efficiency and latency play crucial role. To address this challenge, we build on the latest developments in event-based vision and spiking neural networks. We propose a new network architecture, inspired by Timelens, that improves the state-of-the-art self-supervised optical flow accuracy when operated both in spiking and non-spiking mode. To implement a real-time pipeline with a physical event camera, we propose a methodology for principled model simplification based on activity and latency analysis. We demonstrate high speed optical flow prediction with almost two orders of magnitude reduced complexity while maintaining the accuracy, opening the path for real-time deployments.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Schnider_Neuromorphic_Optical_Flow_and_Real-Time_Implementation_With_Event_Cameras_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Schnider_Neuromorphic_Optical_Flow_and_Real-Time_Implementation_With_Event_Cameras_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Low-Latency Monocular Depth Estimation Using Event Timing on Neuromorphic Hardware",
        "author": "Stefano Chiavazza, Svea Marie Meyer, Yulia Sandamirskaya",
        "abstract": "Depth estimation is an important task in many robotic applications. It is necessary to understand and navigate an environment and to interact with objects. Different active and passive sensing solutions can be used for depth estimation, with different tradeoffs in accuracy, range, latency, dealing with challenging light conditions, power efficiency and price. Event-based dynamic vision sensors (DVS) are particularly well-suited for situations in which low latency and low power vision are needed, e.g. on a fast mobile robot. In this work, we present an event-based depth estimation method with a single DVS using a novel depth from motion algorithm targeting neuromorphic hardware. The system first computes the optical flow on the neuromorphic chip and then computes the depth by combining optical flow with the camera velocity. The method assumes only translational motion and it successfully reconstructs the depth from the measured flow. The method can achieve low-latency depth estimation (<0.5ms) while maintaining a small network size, allowing for better scalability. We tested the algorithm on Intel's neuromorphic research chip Loihi 2.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Chiavazza_Low-Latency_Monocular_Depth_Estimation_Using_Event_Timing_on_Neuromorphic_Hardware_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Chiavazza_Low-Latency_Monocular_Depth_Estimation_Using_Event_Timing_on_Neuromorphic_Hardware_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PEDRo: An Event-Based Dataset for Person Detection in Robotics",
        "author": "Chiara Boretti, Philippe Bich, Fabio Pareschi, Luciano Prono, Riccardo Rovatti, Gianluca Setti",
        "abstract": "Event-based cameras are devices based on neuromorphic sensors that are gaining popularity in different fields, including robotics. They are suitable for tasks where high-speed, low-latency, low-power operations are required. Person detection is one of these, to allow mobile robots to monitor areas and navigate in crowded environments. Most of the available event-based datasets that contain annotated human figures and collected with a moving camera are designed for autonomous driving tasks. Yet, robotic tasks are certainly not limited to the recognition of pedestrians walking on sidewalks, which makes the above datasets of limited utility. To address this impasse, we introduce a new dataset called PEDRo, which is fully manually labeled. This dataset has been specifically developed for person detection and it counts a total number of 43259 bounding boxes included in 119 recordings. A moving DAVIS346 event-based camera has been used to collect events in a large variety of indoor and outdoor scenarios with various lighting and meteorological conditions (such as sunny, rainy and snowy). To the best of our knowledge, this is now the largest available dataset for event-based person detection, which has been recorded with a moving camera and manually labeled.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Boretti_PEDRo_An_Event-Based_Dataset_for_Person_Detection_in_Robotics_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Boretti_PEDRo_An_Event-Based_Dataset_for_Person_Detection_in_Robotics_CVPRW_2023_paper.pdf"
    },
    {
        "title": "EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-Based Video Reconstruction",
        "author": "Burak Ercan, Onur Eker, Aykut Erdem, Erkut Erdem",
        "abstract": "Event cameras are a new type of vision sensor that incorporates asynchronous and independent pixels, offering advantages over traditional frame-based cameras such as high dynamic range and minimal motion blur. However, their output is not easily understandable by humans, making the reconstruction of intensity images from event streams a fundamental task in event-based vision. While recent deep learning-based methods have shown promise in video reconstruction from events, this problem is not completely solved yet. To facilitate comparison between different approaches, standardized evaluation protocols and diverse test datasets are essential. This paper proposes a unified evaluation methodology and introduces an open-source framework called EVREAL to comprehensively benchmark and analyze various event-based video reconstruction methods from the literature. Using EVREAL, we give a detailed analysis of the state-of-the-art methods for event-based video reconstruction, and provide valuable insights into the performance of these methods under varying settings, challenging scenarios, and downstream tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Ercan_EVREAL_Towards_a_Comprehensive_Benchmark_and_Analysis_Suite_for_Event-Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Ercan_EVREAL_Towards_a_Comprehensive_Benchmark_and_Analysis_Suite_for_Event-Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision",
        "author": "Sami Barchid, Jos\u00e9 Mennesson, Chaabane Dj\u00e9raba",
        "abstract": "This paper proposes a self-supervised representation learning (SSRL) framework for event-based vision, which leverages various lightweight convolutional neural networks (CNNs) including 2D-, 3D-, and Spiking CNNs. The method uses a joint embedding architecture to maximize the agreement between features extracted from different views of the same event sequence. Popular event data augmentation techniques are employed to design an efficient augmentation policy for event-based SSRL, and we provide novel data augmentation methods to enhance the pretraining pipeline. Given the novelty of SSRL for event-based vision, we elaborate standard evaluation protocols and use them to evaluate our approach. Our study demonstrates that pretrained CNNs acquire effective and transferable features, enabling them to achieve competitive performance in object or action recognition across various commonly used event-based datasets, even in a low-data regime. This paper also conducts an experimental analysis of the extracted features regarding the Uniformity-Tolerance tradeoff to assess their quality, and measure the similarity of representations using linear Center Kernel Alignement. These quantitative measurements reinforce our observations from the performance benchmarks and show substantial differences between the learned representations of all types of CNNs despite being optimized with the same approach.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Barchid_Exploring_Joint_Embedding_Architectures_and_Data_Augmentations_for_Self-Supervised_Representation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Barchid_Exploring_Joint_Embedding_Architectures_and_Data_Augmentations_for_Self-Supervised_Representation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Live Demonstration: Real-Time Event-Based Speed Detection Using Spiking Neural Networks",
        "author": "Arjun Roy, Manish Nagaraj, Chamika Mihiranga Liyanagedera, Kaushik Roy",
        "abstract": "Event cameras are emerging as an ideal vision sensor for high-speed applications due to their low latency and power consumption. DOTIE, a recent work in literature, has proposed a method to detect objects through spatial and temporal isolation of events with a spiking neural network. In this work, we implement DOTIE to detect a disk moving in a circular motion and identify the speed of rotation. We further validate the claim that spiking architectures can efficiently handle events by implementing DOTIE on Intel Loihi, a neuromorphic hardware suitable for spiking neural networks, and reveal a 14x reduction in energy consumption compared to the CPU implementation of DOTIE.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Roy_Live_Demonstration_Real-Time_Event-Based_Speed_Detection_Using_Spiking_Neural_Networks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Roy_Live_Demonstration_Real-Time_Event-Based_Speed_Detection_Using_Spiking_Neural_Networks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Fast Trajectory End-Point Prediction With Event Cameras for Reactive Robot Control",
        "author": "Marco Monforte, Luna Gava, Massimiliano Iacono, Arren Glover, Chiara Bartolozzi",
        "abstract": "Prediction can be crucial for tasks with short time constraints if a robot has limited speed and power. A low-latency, high-frequency perception system can reduce the time needed to converge on the expectation of the future state of the world, giving the robot additional time to act - or to choose a safer action. In this paper, we exploit event cameras for asynchronous motion-driven sampling, inherent data compression, and sub-millisecond latency to reduce the convergence time of a data-driven trajectory prediction algorithm. As a use-case, we use a Panda robotic arm to intercept a ball bouncing on a table. To predict the interception point as early as possible, and cope with the intrinsic variability of trajectory length - that cannot be defined a-priori for event cameras - we adopt a Stateful Long Short-Term Memory, that asynchronously updates the prediction for each incoming point of the trajectory and does not require a predefined, fixed length input. We adopt a sim-to-real methodology in which the network is first trained on simulated data and then fine-tuned on real trajectories. Experimental results demonstrate that the dense spatial sampling performed by event cameras significantly increases the number of intercepted trajectories compared to a fixed temporal sampling typical of traditional \"frame-based\" cameras. Results motivate further exploration of the use of event cameras for prediction in higher-complexity robotic tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Monforte_Fast_Trajectory_End-Point_Prediction_With_Event_Cameras_for_Reactive_Robot_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Monforte_Fast_Trajectory_End-Point_Prediction_With_Event_Cameras_for_Reactive_Robot_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Shining Light on the DVS Pixel: A Tutorial and Discussion About Biasing and Optimization",
        "author": "Rui Gra\u00e7a, Brian McReynolds, Tobi Delbruck",
        "abstract": "The operation of the DVS event camera is controlled by the user through adjusting different bias parameters. These biases affect the response of the camera by controlling - among other parameters - the bandwidth, sensitivity, and maximum firing rate of the pixels. Besides determining the response of the camera to input signals, biases significantly impact its noise performance. Bias optimization is a multivariate process depending on the task and the scene, to which the user's knowledge about pixel design and non-idealities can be of great importance. In this paper, we go step-by-step along the signal pathway of the DVS pixel, shining light on its low-level operation and non-idealities, comparing pixel level measurements with array level measurements, and discussing and how biasing and illumination affect the pixel's behavior. With the results and discussion presented, we aim to help DVS users achieve more hardware-aware camera utilization and modelling.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Graca_Shining_Light_on_the_DVS_Pixel_A_Tutorial_and_Discussion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Graca_Shining_Light_on_the_DVS_Pixel_A_Tutorial_and_Discussion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Frugal Event Data: How Small Is Too Small? A Human Performance Assessment With Shrinking Data",
        "author": "Am\u00e9lie Gruel, Luc\u00eda Trillo Carreras, Marina Bueno Garc\u00eda, Ewa Kupczyk, Jean Martinet",
        "abstract": "When designing embedded computer vision systems with limited computational budget, one often needs to take care of the size of input data. In recent years, however, event cameras have shown increasingly large sensor sizes. How small can event data be, while preserving sufficient information for the task at hand? We present in this paper a study to assess and compare human performance in a gesture classification task using event data. Original event data from IBM's DVS128 Gesture dataset is downscaled with several spatial and temporal methods, and the classification performance on 4 classes is measured with human participants. The contributions of this paper are 3-fold: (1) we establish a size threshold under which the human performance falls behind the chance level, (2) we compare several spatial and temporal event downscaling methods and show that all methods give unequal data quality, and (3) we highlight some unexpected discrepancies in a comparison between human vs machine performance. To the best of our knowledge, this is the first human perception study with event data.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Gruel_Frugal_Event_Data_How_Small_Is_Too_Small_A_Human_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Gruel_Frugal_Event_Data_How_Small_Is_Too_Small_A_Human_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Within-Camera Multilayer Perceptron DVS Denoising",
        "author": "Antonio Rios-Navarro, Shasha Guo, Abarajithan Gnaneswaran, Keerthivasan Vijayakumar, Alejandro Linares-Barranco, Thea Aarrestad, Ryan Kastner, Tobi Delbruck",
        "abstract": "In-camera event denoising can dramatically reduce the data rate of event cameras by filtering out noise at the source. A lightweight multilayer perceptron denoising filter (MLPF) providing state-of-the-art low-cost denoising accuracy processes a small neighborhood of pixels from the timestamp image around each event to discriminate signal and noise events. This paper proposes two digital logic implementations of the MLPF denoiser and quantifies their resource cost, power, and latency. The hardware MLPF quantizes the weights and hidden unit activations to 4 bits and has about 1k weights with about 40% sparsity. The Area-Under-Curve Receiver Operating Characteristic accuracy is nearly indistinguishable from that of the floating point network. The FPGA MLPF processes each event in 10 clock cycles. In FPGA, it uses 3.5k flip flops and 11.5k LUTs. Our ASIC implementation in 65nm digital technology for a 346x260 pixel camera occupies an area of 4.3mm^2 and consumes 4nJ of energy per event at event rates up to 25MHz. The MLPF can be easily integrated into an event camera using an FPGA or as an ASIC directly on the camera chip or in the same package. This denoising could dramatically reduce the energy consumed by the communication and host processor and open new areas of always-on event camera application under scavenged and battery power.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Rios-Navarro_Within-Camera_Multilayer_Perceptron_DVS_Denoising_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Rios-Navarro_Within-Camera_Multilayer_Perceptron_DVS_Denoising_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Neuromorphic Event-Based Facial Expression Recognition",
        "author": "Lorenzo Berlincioni, Luca Cultrera, Chiara Albisani, Lisa Cresti, Andrea Leonardo, Sara Picchioni, Federico Becattini, Alberto Del Bimbo",
        "abstract": "Recently, event cameras have shown large applicability in several computer vision fields especially concerning tasks that require high temporal resolution. In this work, we investigate the usage of such kind of data for emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based Facial Expression Recognition. NEFER is composed of paired RGB and event videos representing human faces labeled with the respective emotions and also annotated with face bounding boxes and facial landmarks. We detail the data acquisition process as well as providing a baseline method for RGB and event data. The collected data captures subtle micro-expressions, which are hard to spot with RGB data, yet emerge in the event domain. We report a double recognition accuracy for the event-based approach, proving the effectiveness of a neuromorphic approach for analyzing fast and hardly detectable expressions and the emotions they conceal.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Berlincioni_Neuromorphic_Event-Based_Facial_Expression_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Berlincioni_Neuromorphic_Event-Based_Facial_Expression_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Sparse-E2VID: A Sparse Convolutional Model for Event-Based Video Reconstruction Trained With Real Event Noise",
        "author": "Pablo Rodrigo Gantier Cadena, Yeqiang Qian, Chunxiang Wang, Ming Yang",
        "abstract": "Event cameras are image sensors inspired by biology and offer several advantages over traditional frame-based cameras. However, most algorithms for reconstructing images from event camera data do not exploit the sparsity of events, resulting in inefficient zero-filled data. Given that event cameras typically have a sparse index of 90% or higher, this is particularly wasteful. In this paper, we propose a sparse model, Sparse-E2VID, that efficiently reconstructs event-based images, reducing inference time by 30%. Our model takes advantage of the sparsity of event data, making it more computationally efficient, and scales better at higher resolutions. Additionally, by using data augmentation and real noise from an event camera, our model reconstructs nearly noise-free images. In summary, our proposed model efficiently and accurately reconstructs images from event camera data by exploiting the sparsity of events. This has the potential to greatly improve the performance of event-based applications, particularly at higher resolutions. Some results can be seen in the following video: https://youtu.be/sFH9zp6kuWE",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Cadena_Sparse-E2VID_A_Sparse_Convolutional_Model_for_Event-Based_Video_Reconstruction_Trained_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Cadena_Sparse-E2VID_A_Sparse_Convolutional_Model_for_Event-Based_Video_Reconstruction_Trained_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PDAVIS: Bio-Inspired Polarization Event Camera",
        "author": "Germain Haessig, Damien Joubert, Justin Haque, Moritz B. Milde, Tobi Delbruck, Viktor Gruev",
        "abstract": "The stomatopod (mantis shrimp) visual system has recently provided a blueprint for the design of paradigm-shifting polarization and multispectral imaging sensors, enabling solutions to challenging medical and remote sensing problems. However, these bioinspired sensors lack the HDR and asynchronous polarization vision capabilities of the stomatopod visual system, limiting temporal resolution to about 12ms and dynamic range to 72dB. Here we present a novel stomatopod-inspired polarization camera which mimics the sustained and transient biological visual pathways to save power and sample data beyond the maximum Nyquist frame rate. This bio-inspired sensor simultaneously captures both synchronous intensity frames and asynchronous polarization brightness change information with sub-millisecond latencies over a million-fold range of illumination. Our PDAVIS camera is comprised of 346x260 pixels, organized in 2-by-2 macropixels, which filter the incoming light with four linear polarization filters offset by 45deg. Polarization information is reconstructed using both low cost and latency event-based algorithms and more accurate but slower deep neural networks. Our sensor is used to image HDR polarization scenes which vary at high speeds and to observe dynamical properties of single collagen fibers in bovine tendon under rapid cyclical loads.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Haessig_PDAVIS_Bio-Inspired_Polarization_Event_Camera_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Haessig_PDAVIS_Bio-Inspired_Polarization_Event_Camera_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Predictive Coding Light: Learning Compact Visual Codes by Combining Excitatory and Inhibitory Spike Timing-Dependent Plasticity",
        "author": "Antony W. N'dri, Thomas Barbier, C\u00e9line Teuli\u00e8re, Jochen Triesch",
        "abstract": "It is widely believed that the brain uses predictive coding schemes to represent sensory inputs in an efficient manner. However, it is still debated how networks of spiking neurons can learn such coding schemes in an unsupervised fashion. Here we present a hierarchical spiking neural network architecture that learns an efficient encoding of visual input from an event-based vision sensor by combining excitatory and inhibitory spike timing-dependent plasticity (STDP). The network develops receptive fields and exhibits surround suppression effects reminiscent of biological findings. We show that inhibitory STDP which aims to suppress predictable (and therefore redundant) spikes in neurons strongly reduces neural activity (and therefore energy costs) with only moderate reductions in coding fidelity.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Ndri_Predictive_Coding_Light_Learning_Compact_Visual_Codes_by_Combining_Excitatory_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Ndri_Predictive_Coding_Light_Learning_Compact_Visual_Codes_by_Combining_Excitatory_CVPRW_2023_paper.pdf"
    },
    {
        "title": "How Many Events Make an Object? Improving Single-Frame Object Detection on the 1 Mpx Dataset",
        "author": "Alexander Kugele, Thomas Pfeil, Michael Pfeiffer, Elisabetta Chicca",
        "abstract": "Event cameras are promising novel vision sensors with higher dynamic range and higher temporal resolution compared to frame-based cameras. In contrast to images, single-frame detectors without memory perform poorly on event data. We analyze the distribution of event counts in the 2D bounding boxes in the 1 Mpx Dataset to find that the distribution is skewed towards few events, rendering it impossible to detect objects based only on current information. Memory layers like LSTM can alleviate this problem, but increase training time and inference costs. To bring the advantages of single-frame detectors to event camera data, we propose a data filtering mechanism and a novel bounding box memory. The filtering mechanism excludes labels with low event count during training, which improves performance on unfiltered test data. The bounding box memory memorizes bounding boxes until an event threshold is reached, which improves performance, has a low memory and latency footprint, and can be integrated into any object detector without retraining. Improvements are shown on a simulated dataset based on moving MNIST digits, as well as the 1 Mpx Dataset, the largest event camera object detection dataset to date, illustrating that our method scales to large datasets and works in a complex real-world setting.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Kugele_How_Many_Events_Make_an_Object_Improving_Single-Frame_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Kugele_How_Many_Events_Make_an_Object_Improving_Single-Frame_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Live Demonstration: ANN vs SNN vs Hybrid Architectures for Event-Based Real-Time Gesture Recognition and Optical Flow Estimation",
        "author": "Adarsh Kumar Kosta, Marco Paul E. Apolinario, Kaushik Roy",
        "abstract": "Spiking Neural Networks (SNNs) have recently emerged as a promising solution to handle asynchronous data from event-based cameras. Their inherent recurrence allows temporal information in events to be effectively captured unlike widely used non-spiking artificial neural networks (so-called ANNs). However, SNNs are not suitable to run on GPUs and still require specialized neuromorphic hardware to process events efficiently. Hybrid SNN-ANN architectures aim to obtain the best of both worlds with initial SNN layers capturing input temporal information followed by standard ANN layers for ease of training and deployment on GPUs. In this work, we implement ANN, SNN, and hybrid architectures for real-time gesture recognition and optical flow estimation on standard GPUs. We compare different architectures in terms of prediction accuracy, number of parameters, latency, and computational power when executing them in real time on a standard laptop. Our implementation suggests that the hybrid architecture offers the best trade-off in terms of accuracy, compute efficiency, and latency on a readily available GPU platform.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Kosta_Live_Demonstration_ANN_vs_SNN_vs_Hybrid_Architectures_for_Event-Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Kosta_Live_Demonstration_ANN_vs_SNN_vs_Hybrid_Architectures_for_Event-Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MoveEnet: Online High-Frequency Human Pose Estimation With an Event Camera",
        "author": "Gaurvi Goyal, Franco Di Pietro, Nicolo Carissimi, Arren Glover, Chiara Bartolozzi",
        "abstract": "Human Pose Estimation (HPE) is crucial as a building block for tasks that are based on the accurate understanding of human position, pose and movements. Therefore, accuracy and efficiency in this block echo throughout a system, making it important to find efficient methods, that run at fast rates for online applications. The state of the art for mainstream sensors has made considerable advances, but event camera based HPE is still in its infancy. Event cameras boast high rates of data capture in a compact data structure, with advantages like high dynamic range and low power consumption. In this work, we present a system for a high frequency estimation of 2D, single-person Human Pose with event cameras. We provide an online system, that can be paired directly with an event camera to obtain high accuracy in real time. For quantitative results, we present our results on two large scale datasets, DHP19 and event-Human 3.6m. The system is robust to variance in the resolution of the camera and can run at up to 100Hz and an accuracy 89%.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Goyal_MoveEnet_Online_High-Frequency_Human_Pose_Estimation_With_an_Event_Camera_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Goyal_MoveEnet_Online_High-Frequency_Human_Pose_Estimation_With_an_Event_Camera_CVPRW_2023_paper.pdf"
    },
    {
        "title": "X-Maps: Direct Depth Lookup for Event-Based Structured Light Systems",
        "author": "Wieland Morgenstern, Niklas Gard, Simon Baumann, Anna Hilsmann, Peter Eisert",
        "abstract": "We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras. These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach. Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search. Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process. Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy. Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events. This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial. We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results. Additional results and code are available on the X-maps project page: https://fraunhoferhhi.github.io/X-maps/",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Morgenstern_X-Maps_Direct_Depth_Lookup_for_Event-Based_Structured_Light_Systems_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Morgenstern_X-Maps_Direct_Depth_Lookup_for_Event-Based_Structured_Light_Systems_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Interpolation-Based Event Visual Data Filtering Algorithms",
        "author": "Marcin Kowalczyk, Tomasz Kryjak",
        "abstract": "The field of neuromorphic vision is developing rapidly, and event cameras are finding their way into more and more applications. However, the data stream from these sensors is characterised by significant noise. In this paper, we propose a method for event data that is capable of removing approximately 99% of noise while preserving the majority of the valid signal. We have proposed four algorithms based on the matrix of infinite impulse response (IIR) filters method. We compared them on several event datasets that were further modified by adding artificially generated noise and noise recorded with dynamic vision sensor. The proposed methods use about 30KB of memory for a sensor with a resolution of 1280 x 720 and is therefore well suited for implementation in embedded devices.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Kowalczyk_Interpolation-Based_Event_Visual_Data_Filtering_Algorithms_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Kowalczyk_Interpolation-Based_Event_Visual_Data_Filtering_Algorithms_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Event-Based Blur Kernel Estimation for Blind Motion Deblurring",
        "author": "Takuya Nakabayashi, Kunihiro Hasegawa, Masakazu Matsugu, Hideo Saito",
        "abstract": "Motion blur can significantly reduce the quality of images, and researchers have developed various algorithms to address this issue. One common approach to deblurring is to use deconvolution to cancel out the blur effect, but this method is limited by the difficulty of accurately estimating blur kernels from blurred images. This is because the motion causing the blur is often complex and nonlinear. In this paper, a new method for estimating blur kernels is proposed. This method uses an event camera, which captures high-temporal-resolution data on pixel luminance changes, along with a conventional camera to capture the input blurred image. By analyzing the event data stream, the proposed method estimates the 2D motion of the blurred image at short intervals during the exposure time, and integrates this information to estimate a variety of complex blur motions. With the estimated blur kernel, the input blurred image can be deblurred using deconvolution. The proposed method does not rely on machine learning and therefore can restore blurry images without depending on the quality and quantity of training data. Experimental results show that the proposed method can estimate blur kernels even for images blurred by complex camera motions, outperforming conventional methods. Overall, this paper presents a promising approach to motion deblurring that could have practical applications in a range of fields.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Nakabayashi_Event-Based_Blur_Kernel_Estimation_for_Blind_Motion_Deblurring_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Nakabayashi_Event-Based_Blur_Kernel_Estimation_for_Blind_Motion_Deblurring_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Live Demonstration: Event-Based Visual Microphone",
        "author": "Ryogo Niwa, Tatsuki Fushimi, Kenta Yamamoto, Yoichi Ochiai",
        "abstract": "Non-contact measurement devices, such as laser Doppler vibrometers(LDV) and high-speed cameras, have been developed to measure various forms of vibrations. LDVs are expensive, and high-speed cameras have a significant trade off between resolutions and sampling frequency. Herein, we propose a non-contact, simple, and inexpensive approach to measure vibration using an event camera that records only changes in brightness. We demonstrated the reconstruction of audible sounds using the event camera; to the best of our knowledge, this study could be considered the first to successfully apply this method. This study addresses the challenges in measuring and reconstructing vibrations without requiring additional lighting and using a cost-effective method that can recover audible sounds and human voice. This method provides a new avenue for scientists and engineers to develop cost-effective and non-contact methods for measuring and reconstructing vibrations and sounds.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Niwa_Live_Demonstration_Event-Based_Visual_Microphone_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Niwa_Live_Demonstration_Event-Based_Visual_Microphone_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Live Demo: E2P-Events to Polarization Reconstruction From PDAVIS Events",
        "author": "Tobi Delbruck, Zuowen Wang, Haiyang Mei, Germain Haessig, Damien Joubert, Justin Haque, Yingkai Chen, Moritz B. Milde, Viktor Gruev",
        "abstract": "This demonstration shows live operation of of PDAVIS polarization event camera reconstruction by the E2P DNN reported in the main CVPR conference paper Deep Polarization Reconstruction with PDAVIS Events (paper 9149). Demo code: github.com/SensorsINI/e2p",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Delbruck_Live_Demo_E2P-Events_to_Polarization_Reconstruction_From_PDAVIS_Events_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Delbruck_Live_Demo_E2P-Events_to_Polarization_Reconstruction_From_PDAVIS_Events_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Live Demonstration: Integrating Event Based Hand Tracking Into TouchFree Interactions",
        "author": "Ryan Page",
        "abstract": "Hand tracking is becoming ever more prominent as an intuitive way to interact with digital content. There are however technical challenges at the sensing level, these include environmental robustness, power and system latency to name three. Event cameras have the potential to solve these, with their asynchronous readout, low power and high dynamic range. To explore the potential of event cameras, Ultraleap have developed a prototype stereo camera using two Prophesee IMX636ES sensors. To go from event data to hand positions the event data is aggregated into event frames. This is then consumed by a hand tracking model which outputs 28 joint positions for each hand with respect to the camera. This data is used by Ultraleap's TouchFree application to determine a users interaction with a 2D display. This is brought together in the Ball Pit demo shown in the supplementary material, where the user is using their hands to remove balls from a box.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Page_Live_Demonstration_Integrating_Event_Based_Hand_Tracking_Into_TouchFree_Interactions_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Page_Live_Demonstration_Integrating_Event_Based_Hand_Tracking_Into_TouchFree_Interactions_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Asynchronous Events-Based Panoptic Segmentation Using Graph Mixer Neural Network",
        "author": "Sanket Kachole, Yusra Alkendi, Fariborz Baghaei Naeini, Dimitrios Makris, Yahya Zweiri",
        "abstract": "In the context of robotic grasping, object segmentation encounters several difficulties when faced with dynamic conditions such as real-time operation, occlusion, low lighting, motion blur, and object size variability. In response to these challenges, we propose the Graph Mixer Neural Network that includes a novel collaborative contextual mixing layer, applied to 3D event graphs formed on asynchronous events. The proposed layer is designed to spread spatiotemporal correlation within an event graph at four nearest neighbor levels parallelly. We evaluate the effectiveness of our proposed method on the Event-based Segmentation (ESD) Dataset, which includes five unique image degradation challenges, including occlusion, blur, brightness, trajectory, scale variance, and segmentation of known and unknown objects. The results show that our proposed approach outperforms state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. Code available at: https://github.com/sanket0707/GNN-Mixer.git",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Kachole_Asynchronous_Events-Based_Panoptic_Segmentation_Using_Graph_Mixer_Neural_Network_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Kachole_Asynchronous_Events-Based_Panoptic_Segmentation_Using_Graph_Mixer_Neural_Network_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Live Demonstration: Tangentially Elongated Gaussian Belief Propagation for Event-Based Incremental Optical Flow Estimation",
        "author": "Yusuke Sekikawa, Jun Nagata",
        "abstract": "Optical flow estimation is a fundamental functionality in computer vision. An event-based camera, which asynchronously detects sparse intensity changes, is an ideal device for realizing low-latency estimation of the optical flow owing to its low-latency sensing mechanism. We developed an efficient full-flow estimation called Tangentially elongated Gaussian belief propagation (TEGBP). TEGBP formulates the full flow estimation as the marginalization of probability using a message-passing based on the BP. The formulation permits event-by-event asynchronous incremental updates of the full flow; i.e., given a normal-flow observation, it updates its belief about full flow by asynchronous local communication. This paper presents a \\texttt OpenMP  based real-time full-flow estimation demo by taking advantage of the asynchronous formulation. Specifically, we parallelize the individual sequence of the message exchange evoked by a single normal-flow observation. Beliefs at each node are updated on an event-by-event basis manner in parallel, realizing the real-time procession on CPUs.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Sekikawa_Live_Demonstration_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Sekikawa_Live_Demonstration_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_CVPRW_2023_paper.pdf"
    },
    {
        "title": "End-to-End Neuromorphic Lip-Reading",
        "author": "Hugo Bulzomi, Marcel Schweiker, Am\u00e9lie Gruel, Jean Martinet",
        "abstract": "Human speech perception is intrinsically a multi-modal task since speech production requires the speaker to move the lips, producing visual cues in addition to auditory information. Lip reading consists in visually interpreting the movements of the lips to understand speech, without the use of sound. It is an important task since it can either complement an audio-based speech recognition system or replace it when sound is not available. We introduce in this paper a neuromorphic model for lip reading, that uses events produced by an event-based sensor capturing lips motion as input, and that classifies short event sequences in word categories based on a SNN architecture. Experimental results show that the proposed model successfully leverages various advantages of neuromorphic approaches such as energy efficiency and low latency, which are central features in real-time embedded scenarios. To the best of our knowledge, it is the first proposal of an end-to-end neuromorphic lip reading model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Bulzomi_End-to-End_Neuromorphic_Lip-Reading_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Bulzomi_End-to-End_Neuromorphic_Lip-Reading_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Shape Awareness and Interpretability in Deep Networks Using Geometric Moments",
        "author": "Rajhans Singh, Ankita Shukla, Pavan Turaga",
        "abstract": "Deep networks for image classification often rely more on texture information than object shape. While efforts have been made to make deep-models shape-aware, it is often difficult to make such models simple, interpretable, or rooted in known mathematical definitions of shape. This paper presents a deep-learning model inspired by geometric moments, a classically well understood approach to measure shape-related properties. The proposed method consists of a trainable network for generating coordinate bases and affine parameters for making the features geometrically invariant yet in a task-specific manner. The proposed model improves the final feature's interpretation. We demonstrate the effectiveness of our method on standard image classification datasets. The proposed model achieves higher classification performance compared to the baseline and standard ResNet models while substantially improving interpretability.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DLGC/html/Singh_Improving_Shape_Awareness_and_Interpretability_in_Deep_Networks_Using_Geometric_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DLGC/papers/Singh_Improving_Shape_Awareness_and_Interpretability_in_Deep_Networks_Using_Geometric_CVPRW_2023_paper.pdf"
    },
    {
        "title": "GenSim: Unsupervised Generic Garment Simulator",
        "author": "Lokender Tiwari, Brojeshwar Bhowmick, Sanjana Sinha",
        "abstract": "In this paper, we propose a novel generic garment simulator to drape a template 3D garment of arbitrary type, size, and topology onto an arbitrary 3D body shape and pose. Existing learning-based methods for 3D garment simulation methods train a single model for each garment type, with a fixed topology. Most of them use supervised learning, which requires huge training data that is expensive to acquire. Our method circumvents the above-mentioned limitations by proposing GenSim, a generic unsupervised method for garment simulation, that can generalize to garments of different sizes, topologies, body shapes, and poses, using a single trained model. Our proposed GenSim consists of (1) a novel body-motion-aware as-rigid-as-possible (ARAP) garment deformation module that initially deforms the template garment considering the underlying body as an obstacle and (2) a Physics Enforcing Network (PEN) that adds the corrections to the ARAP deformed garment to make it physically plausible. PEN uses multiple types of garments of arbitrary topology for training using physics-aware unsupervised losses. Experimental results show that our method significantly outperforms the existing state-of-the-art methods on the challenging CLOTH3D dataset and the VTO dataset. Unlike the unsupervised method PBNS, GenSim generalizes well on unseen garments with varying shapes, sizes, types, and topologies draped on different body shapes and poses.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DLGC/html/Tiwari_GenSim_Unsupervised_Generic_Garment_Simulator_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DLGC/papers/Tiwari_GenSim_Unsupervised_Generic_Garment_Simulator_CVPRW_2023_paper.pdf"
    },
    {
        "title": "GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot Learning",
        "author": "Tejas Anvekar, Dena Bazazian",
        "abstract": "In the realm of 3D-computer vision applications, point cloud few-shot learning plays a critical role. However, it poses an arduous challenge due to the sparsity, irregularity, and unordered nature of the data. Current methods rely on complex local geometric extraction techniques such as convolution, graph, and attention mechanisms, along with extensive data-driven pre-training tasks. These approaches contradict the fundamental goal of few-shot learning, which is to facilitate efficient learning. To address this issue, we propose GPr-Net (Geometric Prototypical Network), a lightweight and computationally efficient geometric prototypical network that captures the intrinsic topology of point clouds and achieves superior performance. Our proposed method, IGI++ (Intrinsic Geometry Interpreter++) employs vector-based hand-crafted intrinsic geometry interpreters and Laplace vectors to extract and evaluate point cloud morphology, resulting in improved representations for FSL (Few-Shot Learning). Additionally, Laplace vectors enable the extraction of valuable features from point clouds with fewer points. To tackle the distribution drift challenge in few-shot metric learning, we leverage hyperbolic space and demonstrate that our approach handles intra and inter-class variance better than existing point cloud few-shot learning methods. Experimental results on the ModelNet40 dataset show that GPr-Net outperforms state-of-the-art methods in few-shot learning on point clouds, achieving utmost computational efficiency that is 170x better than all existing works. The code is publicly available at https://github.com/TejasAnvekar/GPr-Net.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DLGC/html/Anvekar_GPr-Net_Geometric_Prototypical_Network_for_Point_Cloud_Few-Shot_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DLGC/papers/Anvekar_GPr-Net_Geometric_Prototypical_Network_for_Point_Cloud_Few-Shot_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Learning To See in Nighttime Driving Scenes With Inter-Frequency Priors",
        "author": "Zhentao Fan, Xianhao Wu, Xiang Chen, Yufeng Li",
        "abstract": "Currently, image-to-image translation methods have achieved significant performance with the help of deep CNNs and GANs, but most existing models are not suitable for autonomous driving scenarios due to their interpretability. In this paper, we develop a high-quality image translator (i.e. night -> day), N2D-LPNet, to facilitate nighttime driving scene dperception. Instead of pursuing a complex network structure, we first attempt to explore the inter-frequency relation knowledge to simplify image translation process. Specifically, the lightweight Laplace pyramid is introduced as the backbone architecture to decompose the feature maps of nighttime image into high- and low-frequency components. Considering the similar morphological properties of different frequency components, we design a flexible inter-frequency guiding strategy which utilizes each lower frequency information to refine the higher frequency feature in a progressive manner. Benefiting from the advantage of constraint from inter-frequency priors, our method can process the nighttime image better under the practical driving system while still persevere competitive results. We also discuss the potential value of N2D-LPNet for other high-level vision tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/html/Fan_Learning_To_See_in_Nighttime_Driving_Scenes_With_Inter-Frequency_Priors_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/papers/Fan_Learning_To_See_in_Nighttime_Driving_Scenes_With_Inter-Frequency_Priors_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NeRT: Implicit Neural Representations for Unsupervised Atmospheric Turbulence Mitigation",
        "author": "Weiyun Jiang, Vivek Boominathan, Ashok Veeraraghavan",
        "abstract": "The atmospheric turbulence mitigation problem has emerged as a challenging inverse problem in the communities of computer vision and optics. However, current methods either rely heavily on the quality of the training dataset or fail to generalize over various scenarios, such as static scenes, dynamic scenes, and text reconstructions. We propose a novel implicit neural representation for unsupervised atmospheric turbulence mitigation (NeRT). NeRT leverages the implicit neural representations and the physically correct tilt-then-blur turbulence model to reconstruct the clean and undistorted image, given only dozens of distorted images. Further, we show that NeRT outperforms the state-of-the-art through various qualitative and quantitative evaluations. Lastly, we incorporate NeRT into continuously captured video sequences and demonstrate 48 times speedup.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/html/Jiang_NeRT_Implicit_Neural_Representations_for_Unsupervised_Atmospheric_Turbulence_Mitigation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/papers/Jiang_NeRT_Implicit_Neural_Representations_for_Unsupervised_Atmospheric_Turbulence_Mitigation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Temporally Averaged Regression for Semi-Supervised Low-Light Image Enhancement",
        "author": "Sunhyeok Lee, Donggon Jang, Dae-Shik Kim",
        "abstract": "Constructing annotated paired datasets for low-light image enhancement is complex and time-consuming, and existing deep learning models often generate noisy outputs or misinterpret shadows. To effectively learn intricate relationships between features in image space with limited labels, we introduce a deep learning model with a backbone structure that incorporates both spatial and layer-wise dependencies. The proposed model features a baseline image-enhancing network with spatial dependencies and an optimized layer attention mechanism to learn feature sparsity and importance. We present a progressive supervised loss function for improvement. Furthermore, we propose a novel Multi-Consistency Regularization (MCR) loss and integrate it within a Multi-Consistency Mean Teacher (MCMT) framework, which enforces agreement on high-level features and incorporates intermediate features for better understanding of the entire image. By combining the MCR loss with the progressive supervised loss, student network parameters can be updated in a single step. Our approach achieves significant performance improvements using fewer labeled data and unlabeled low-light images within our semi-supervised framework. Qualitative evaluations demonstrate the effectiveness of our method in leveraging comprehensive dependencies and unlabeled data for low-light image enhancement.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/html/Lee_Temporally_Averaged_Regression_for_Semi-Supervised_Low-Light_Image_Enhancement_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/papers/Lee_Temporally_Averaged_Regression_for_Semi-Supervised_Low-Light_Image_Enhancement_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MM-BSN: Self-Supervised Image Denoising for Real-World With Multi-Mask Based on Blind-Spot Network",
        "author": "Dan Zhang, Fangfang Zhou, Yuwen Jiang, Zhengming Fu",
        "abstract": "Recent advances in deep learning have been pushing image denoising techniques to a new level. In self-supervised image denoising, blind-spot network (BSN) is one of the most common methods. However, most of the existing BSN algorithms use a dot-based central mask, which is recognized as inefficient for images with large-scale spatially correlated noise. In this paper, we give the definition of large-noise and propose a multi-mask strategy using multiple convolutional kernels masked in different shapes to further break the noise spatial correlation. Furthermore, we propose a novel self-supervised image denoising method that combines the multi-mask strategy with BSN (MM-BSN). We show that different masks can cause significant performance differences, and the proposed MM-BSN can efficiently fuse the features extracted by multi-masked layers, while recovering the texture structures destroyed by multi-masking and information transmission. Our MM-BSN can be used to address the problem of large-noise denoising, which cannot be efficiently handled by other BSN methods. Extensive experiments on public real-world datasets demonstrate that the proposed MM-BSN achieves state-of-the-art performance among self-supervised and even unpaired image denoising methods for sRGB images denoising, without any labelling effort or prior knowledge. Code can be found in https://github.com/dannie125/MM-BSN.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/html/Zhang_MM-BSN_Self-Supervised_Image_Denoising_for_Real-World_With_Multi-Mask_Based_on_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/papers/Zhang_MM-BSN_Self-Supervised_Image_Denoising_for_Real-World_With_Multi-Mask_Based_on_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Dilated Convolutional Transformer for High-Quality Image Deraining",
        "author": "Yufeng Li, Jiyang Lu, Hongming Chen, Xianhao Wu, Xiang Chen",
        "abstract": "Convolutional neural networks (CNNs) and Transformers have achieved significant success in image signal processing. However, little effort has been made to effectively combine the properties of these two architectures to satisfy image deraining. In this paper, we propose an effective deraining method, dilated convolutional Transformer (DCT), which can enlarge the receptive fields of the network to aggregate global information. The fundamental building unit of our approach is the dilformer block containing multi-dilconv sparse attention (MDSA) and multi-dilconv feed-forward network (MDFN). The MDSA calculates the multi-scale query to generate accurate similarity map so that rich multi-scale information can be better utilized for the high-quality image reconstruction. In addition, we adopt ReLU to replace the original softmax to enforce sparsity in the Transformer for better feature aggregation. The MDFN is further established to better integrate the rain information of different scales in the feature transformation. Extensive experiments on the benchmarks show the favorable performance against state-of-the-art approaches.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/html/Li_Dilated_Convolutional_Transformer_for_High-Quality_Image_Deraining_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/papers/Li_Dilated_Convolutional_Transformer_for_High-Quality_Image_Deraining_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FLIGHT Mode On: A Feather-Light Network for Low-Light Image Enhancement",
        "author": "Mustafa Ozcan, Hamza Ergezer, Mustafa Ayazo\u011flu",
        "abstract": "Low-light image enhancement (LLIE) is an ill-posed inverse problem due to the lack of knowledge of the desired image which is obtained under ideal illumination conditions. Low-light conditions give rise to two main issues: a suppressed image histogram and inconsistent relative color distributions with low signal-to-noise ratio. In order to address these problems, we propose a novel approach named FLIGHT-Net using a sequence of neural architecture blocks. The first block regulates illumination conditions through pixel-wise scene dependent illumination adjustment. The output image is produced in the output of the second block, which includes channel attention and denoising sub-blocks. Our highly efficient neural network architecture delivers state-of-the-art performance with only 25K parameters. The method's code, pretrained models and resulting images will be publicly available",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/html/Ozcan_FLIGHT_Mode_On_A_Feather-Light_Network_for_Low-Light_Image_Enhancement_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/UG2/papers/Ozcan_FLIGHT_Mode_On_A_Feather-Light_Network_for_Low-Light_Image_Enhancement_CVPRW_2023_paper.pdf"
    },
    {
        "title": "RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods",
        "author": "Maciej Sypetkowski, Morteza Rezanejad, Saber Saberian, Oren Kraus, John Urbanik, James Taylor, Ben Mabey, Mason Victors, Jason Yosinski, Alborz Rezazadeh Sereshkeh, Imran Haque, Berton Earnshaw",
        "abstract": "High-throughput screening techniques are commonly used to obtain large quantities of data in many fields of biology. It is well known that artifacts arising from variability in the technical execution of different experimental batches within such screens confound these observations, and can lead to invalid biological conclusions. It is, therefore, necessary to account for these batch effects when analyzing outcomes. In this paper, we describe RxRx1, a biological dataset designed specifically for the systematic study of batch effect correction methods. The dataset consists of 125,510 high-resolution fluorescence microscopy images of human cells under 1,138 genetic perturbations in 51 experimental batches across 4 cell types. Visual inspection of the images clearly demonstrates significant batch effects. We also propose a classification task designed to evaluate the effectiveness of experimental batch correction methods on these images and examine the performance of a number of correction methods on this task. Our goal in releasing RxRx1 is to encourage the development of effective experimental batch correction methods that generalize well to unseen experimental batches. The dataset can be downloaded at https://rxrx.ai.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Sypetkowski_RxRx1_A_Dataset_for_Evaluating_Experimental_Batch_Correction_Methods_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Sypetkowski_RxRx1_A_Dataset_for_Evaluating_Experimental_Batch_Correction_Methods_CVPRW_2023_paper.pdf"
    },
    {
        "title": "An Ensemble Method With Edge Awareness for Abnormally Shaped Nuclei Segmentation",
        "author": "Yue Han, Yang Lei, Viktor Shkolnikov, Daisy Xin, Alicia Auduong, Steven Barcelo, Jan Allebach, Edward J. Delp",
        "abstract": "Abnormalities in biological cell nuclei shapes are correlated with cell cycle stages, disease states, and various external stimuli. There have been many deep learning approaches that are being used for nuclei segmentation and analysis. In recent years, transformers have performed better than CNN methods on many computer vision tasks. One problem with many deep learning nuclei segmentation methods is acquiring large amounts of annotated nuclei data, which is generally expensive to obtain. In this paper, we propose a Transformer and CNN hybrid ensemble processing method with edge awareness for accurately segmenting abnormally shaped nuclei. We call this method Hybrid Edge Mask R-CNN (HER-CNN), which uses Mask R-CNNs with the ResNet and the Swin-Transformer to segment abnormally shaped nuclei. We add an edge awareness loss to the mask prediction step of the Mask R-CNN to better distinguish the edge difference between the abnormally shaped nuclei and typical oval nuclei. We describe an ensemble processing strategy to combine or fuse individual segmentations from the CNN and the Transformer. We introduce the use of synthetic ground truth image generation to supplement the annotated training images due to the limited amount of data. Our proposed method is compared with other segmentation methods for segmenting abnormally shaped nuclei. We also include ablation studies to show the effectiveness of the edge awareness loss and the use of synthetic ground truth images.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Han_An_Ensemble_Method_With_Edge_Awareness_for_Abnormally_Shaped_Nuclei_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Han_An_Ensemble_Method_With_Edge_Awareness_for_Abnormally_Shaped_Nuclei_CVPRW_2023_paper.pdf"
    },
    {
        "title": "New Bayesian Focal Loss Targeting Aleatoric Uncertainty Estimate: Pollen Image Recognition",
        "author": "Natalia Khanzhina, Maxim Kashirin, Andrey Filchenkov",
        "abstract": "In biological image recognition, different species might look similar resulting in a small margin, which causes errors in labeling images. Pollen grain image classification heavily suffers from both problems preventing from building well-calibrated recognition models. In this research, we aim to filter out aleatoric uncertainty caused by noisy labeling and similar shape of pollen species. To estimate aleatoric uncertainty, we propose a new Bayesian Focal Softmax loss function. It uses the softmax activation, which is more convenient for a single-label tasks compared to the original Focal loss based on the logistic function. The proposed loss function better estimates aleatoric uncertainty increasing the overall model performance. For evaluation, we used two datasets, POLLEN13L-det containing 13 classes of allergic pollen and POLLEN20L-det containing additional honey plant pollen species. We achieved the state-of-the-art results for both of them by applying the proposed loss function on RetinaNet. It improved the mAP and significantly reduced the variance compared to the regular Focal loss with softmax and provided much better aleatoric uncertainty estimate compared the Bayesian Focal loss with sigmoid activation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Khanzhina_New_Bayesian_Focal_Loss_Targeting_Aleatoric_Uncertainty_Estimate_Pollen_Image_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Khanzhina_New_Bayesian_Focal_Loss_Targeting_Aleatoric_Uncertainty_Estimate_Pollen_Image_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Theia: Bleed-Through Estimation With Convolutional Neural Networks",
        "author": "Najib Ishaq, Nathan Hotaling, Nicholas Schaub",
        "abstract": "Microscopy is ubiquitous in biological research, and with high content screening there is a need to analyze images at scale. High content screening often uses multichannel, epifluorescence microscopy (multiplexing), and fluorescent images often exhibit channel mixing, or bleed-through effects, which need to be corrected before subsequent analysis (e.g. segmentation, feature extraction, etc). In this paper we present Theia, an algorithm for bleed-through correction that requires little to no a priori information about the source or content of the images (i.e. number of channels). Theia uses a novel neural network architecture inspired by Siamese Networks and Least Absolute Shrinkage and Selection Operator (LASSO) regression to learn convolutional filters that remove bleed-through. We use metrics for quantifying bleed-through, and show Theia exhibits good capacity for removing bleed-through on both synthetic and real fluorescent images. Theia was benchmarked to demonstrate scalability across diverse datasets with varying degrees of bleed-through and numbers of channels. Since Theia learns a set of convolutional kernels using popular neural network frameworks, it can make use of GPU acceleration when scaling to large datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Ishaq_Theia_Bleed-Through_Estimation_With_Convolutional_Neural_Networks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Ishaq_Theia_Bleed-Through_Estimation_With_Convolutional_Neural_Networks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Giga-SSL: Self-Supervised Learning for Gigapixel Images",
        "author": "Tristan Lazard, Marvin Lerousseau, Etienne Decenci\u00e8re, Thomas Walter",
        "abstract": "Whole slide images (WSI) are microscopy images of stained tissue slides routinely prepared for diagnosis and treatment selection in medical practice. WSI are very large (gigapixel size) and complex (made of up to millions of cells). The current state-of-the-art (SoTA) approach to classify WSI subdivides them into tiles, encodes them by pre-trained networks and applies Multiple Instance Learning (MIL) to train for specific downstream tasks. However, annotated datasets are often small, typically a few hundred to a few thousand WSI, which may cause overfitting and underperforming models. Conversely, the number of unannotated WSI is ever increasing, with datasets of tens of thousands (soon to be millions) of images available. While it has been previously proposed to use these unannotated data to identify suitable tile representations by self-supervised learning (SSL), downstream classification tasks still require full supervision because parts of the MIL architecture is not trained during tile level SSL pre-training. Here, we propose a strategy of slide level SSL to leverage the large number of WSI without annotations to infer powerful slide representations. Applying our method to The Cancer-Genome Atlas, one of the most widely used data resources in cancer research (16 TB image data), we are able to downsize the dataset to 23 MB without any loss in predictive power: we show that a linear classifier trained on top of these embeddings maintains or improves previous SoTA performances on various benchmark WSI classification tasks. Finally, we observe that training a classifier on these representations with tiny datasets (e.g. 50 slides) improved performances over SoTA by an average of +6.3 AUC points over all downstream tasks. Altogether, our Giga-SSL representations of whole slide images are agnostic of downstream classification tasks and are well-suited for small datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Lazard_Giga-SSL_Self-Supervised_Learning_for_Gigapixel_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Lazard_Giga-SSL_Self-Supervised_Learning_for_Gigapixel_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy",
        "author": "Wolfgang M. Pernice, Michael Doron, Alex Quach, Aditya Pratapa, Sultan Kenjeyev, Nicholas De Veaux, Michio Hirano, Juan C. Caicedo",
        "abstract": "Real-world deployment of computer vision systems, including in the discovery processes of biomedical research, requires causal representations that are invariant to contextual nuisances and generalize to new data. Leveraging the internal replicate structure of two novel single-cell fluorescent microscopy datasets, we propose generally applicable tests to assess the extent to which models learn causal representations across increasingly challenging levels of OOD-generalization. We show that despite seemingly strong performance as assessed by other established metrics, both naive and contemporary baselines designed to ward against confounding, collapse to random on these tests. We introduce a new method, Interventional Style Transfer (IST), that substantially improves OOD generalization by generating interventional training distributions in which spurious correlations between biological causes and nuisances are mitigated. We publish our code and datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Pernice_Out_of_Distribution_Generalization_via_Interventional_Style_Transfer_in_Single-Cell_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Pernice_Out_of_Distribution_Generalization_via_Interventional_Style_Transfer_in_Single-Cell_CVPRW_2023_paper.pdf"
    },
    {
        "title": "One-Shot and Partially-Supervised Cell Image Segmentation Using Small Visual Prompt",
        "author": "Sota Kato, Kazuhiro Hotta",
        "abstract": "Semantic segmentation of microscopic cell images using deep learning is an important technique, however, it requires a large number of images and ground truth labels for training. To address the above problem, we consider an efficient learning framework with as little data as possible, and we propose two types of learning strategies: One-shot segmentation which can learn with only one training sample, and Partially-supervised segmentation which assigns annotations to only a part of images. Furthermore, we introduce novel segmentation methods using the small prompt images inspired by prompt learning in recent studies. Our proposed methods use a pre-trained model based on only cell images and teach the information of the prompt pairs to the target image to be segmented by the attention mechanism, which allows for efficient learning while reducing the burden of annotation costs. Through experiments conducted on three types of microscopic cell image datasets, we confirmed that the proposed method improved the Dice score coefficient (DSC) in comparison with the conventional methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Kato_One-Shot_and_Partially-Supervised_Cell_Image_Segmentation_Using_Small_Visual_Prompt_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Kato_One-Shot_and_Partially-Supervised_Cell_Image_Segmentation_Using_Small_Visual_Prompt_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Fast Local Thickness",
        "author": "Vedrana Andersen Dahl, Anders Bjorholm Dahl",
        "abstract": "We propose a fast algorithm for the computation of local thickness in 2D and 3D. Compared to the conventional algorithm, our fast algorithm yields local thickness in just a fraction of the time. In our algorithm, we first compute the distance field of the object and then iteratively dilate the selected parts of the distance field. In every iteration, we employ small structuring elements, which makes our approach fast. Our algorithm is implemented in Python and is freely available as a pip-installable module. Besides giving a detailed description of our method, we test our implementation on 2D images and 3D volumes. In 2D, we compute the ground truth using the conventional local thickness methods, where the distance field is dilated with increasingly larger circular structuring elements. We use this as a reference to evaluate the quality of our results. In 3D, we have no ground truth since it would be too time-consuming to compute. Instead, we compare our results with the golden standard method provided by BoneJ. In both 2D and 3D, we compare with another Python-based approach from PoreSpy. Our algorithm performs equally well or better than other approaches, but significantly faster.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Dahl_Fast_Local_Thickness_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Dahl_Fast_Local_Thickness_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Super-Resolution Training Paradigm Based on Low-Resolution Data Only To Surpass the Technical Limits of STEM and STM Microscopy",
        "author": "Bj\u00f6rn M\u00f6ller, Jan Pirklbauer, Marvin Klingner, Peer Kasten, Markus Etzkorn, Tim J. Seifert, Uta Schlickum, Tim Fingscheidt",
        "abstract": "Modern microscopes can image at atomic resolutions but often reach technical limitations for high-resolution images captured at the smallest nanoscale. Prior works have applied super-resolution (SR) by deep neural networks employing high-resolution images as targets in supervised training. However, in practice, it may be impossible to obtain these high-resolution images at the smallest atomic scales. Approaching this problem, we consider a new super-resolution training paradigm based on low-resolution (LR) microscope images only, to surpass the highest physically captured resolution available for training. As a solution, we propose a novel multi-scale training method for SR based on LR data only, which simultaneously supervises SR at multiple resolutions, allowing the SR to generalize beyond the LR training data. We physically captured low- and high-resolution images for evaluation, thereby incorporating real microscope degradation to deliver a proof of concept. Our experiments on periodic atomic structure in STEM and STM microscopy images show that our proposed multi-scale training method enables deep neural network image SR even up to 360% of the highest physically recorded resolution. Code and data is available on github.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Moller_A_Super-Resolution_Training_Paradigm_Based_on_Low-Resolution_Data_Only_To_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Moller_A_Super-Resolution_Training_Paradigm_Based_on_Low-Resolution_Data_Only_To_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Learning To Correct Sloppy Annotations in Electron Microscopy Volumes",
        "author": "Minghao Chen, Mukesh Bangalore Renuka, Lu Mi, Jeff Lichtman, Nir Shavit, Yaron Meirovitch",
        "abstract": "Connectomics deals with the problem of reconstructing neural circuitry from electron microscopy images at the synaptic level. Automatically reconstructing circuits from these volumes requires high fidelity 3-D instance segmentation, which yet appears to be a daunting task for current computer vision algorithms. Hence, to date, most datasets are not reconstructed by fully-automated methods. Even after painstaking proofreading, these methods still produce numerous small errors. In this paper, we propose an approach to accelerate manual reconstructions by learning to correct imperfect manual annotations. To achieve this, we designed a novel solution for the canonical problem of marker-based 2-D instance segmentation, reporting a new state-of-the-art for region-growing algorithms demonstrated on challenging electron microscopy image stacks. We use our marker-based instance segmentation algorithm to learn to correct all \"sloppy\" object annotations by reducing and expanding all annotations. Our correction algorithm results in high quality morphological reconstruction (near ground truth quality), while significantly cutting annotation time ( 8x) for several examples in connectomics. We demonstrate the accuracy of our approach on public connectomics benchmarks and on a set of large-scale neuron reconstruction problems, including on a new octopus dataset that cannot be automatically segmented at scale by existing algorithms.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Chen_Learning_To_Correct_Sloppy_Annotations_in_Electron_Microscopy_Volumes_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVMI/papers/Chen_Learning_To_Correct_Sloppy_Annotations_in_Electron_Microscopy_Volumes_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Synthetic Data for Defect Segmentation on Complex Metal Surfaces",
        "author": "Juraj Fulir, Lovro Bosnar, Hans Hagen, Petra Gospodneti\u0107",
        "abstract": "Metal defect segmentation poses a great challenge for automated inspection systems due to the complex light reflection from the surface and lack of training data. In this work we introduce a real and synthetic defect segmentation dataset pair for multi-view inspection of a metal clutch part to overcome data shortage. Model pre-training on our synthetic dataset was compared to similar inspection datasets in the literature. Two techniques are presented to increase model training efficiency and prediction coverage in darker areas of the image. Results were collected over three popular segmentation architectures to confirm superior effectiveness of synthetic data and unveil various challenges of multi-view inspection.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Fulir_Synthetic_Data_for_Defect_Segmentation_on_Complex_Metal_Surfaces_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Fulir_Synthetic_Data_for_Defect_Segmentation_on_Complex_Metal_Surfaces_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Towards Sim-to-Real Industrial Parts Classification With Synthetic Dataset",
        "author": "Xiaomeng Zhu, Talha Bilal, P\u00e4r M\u00e5rtensson, Lars Hanson, M\u00e5rten Bj\u00f6rkman, Atsuto Maki",
        "abstract": "This paper is about effectively utilizing synthetic data for training deep neural networks for industrial parts classification, in particular, by taking into account the domain gap against real-world images. To this end, we introduce a synthetic dataset that may serve as a preliminary testbed for the Sim-to-Real challenge; it contains 17 objects of six industrial use cases, including isolated and assembled parts. A few subsets of objects exhibit large similarities in shape and albedo for reflecting challenging cases of industrial parts. All the sample images come with and without random backgrounds and post-processing for evaluating the importance of domain randomization. We call it Synthetic Industrial Parts dataset (SIP-17). We study the usefulness of SIP-17 through benchmarking the performance of five state-of-the-art deep network models, supervised and self-supervised, trained only on the synthetic data while testing them on real data. By analyzing the results, we deduce some insights on the feasibility and challenges of using synthetic data for industrial parts classification and for further developing larger-scale synthetic datasets. Our dataset and code are publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Zhu_Towards_Sim-to-Real_Industrial_Parts_Classification_With_Synthetic_Dataset_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Zhu_Towards_Sim-to-Real_Industrial_Parts_Classification_With_Synthetic_Dataset_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Leveraging Multi-View Data for Improved Detection Performance: An Industrial Use Case",
        "author": "Faranak Shamsafar, Sunil Jaiswal, Benjamin Kelkel, Kireeti Bodduna, Klaus Illgner-Fehns",
        "abstract": "Printed circuit boards (PCBs) are essential components of electronic devices, and ensuring their quality is crucial in their production. However, the vast variety of components and PCBs manufactured by different companies makes it challenging to adapt to production lines with speed demands. To address this challenge, we present a multi-view object detection framework that offers a fast and precise solution. We introduce a novel multi-view dataset with semi-automatic ground-truth data, which results in significant labeling resource savings. Labeling PCB boards for object detection is a challenging task due to the high density of components and the small size of the objects, which makes it difficult to identify and label them accurately. By training an object detector model with multi-view data, we achieve improved performance over single-view images. To further enhance the accuracy, we develop a multi-view inference method that aggregates results from different viewpoints. Our experiments demonstrate a 15% improvement in mAP for detecting components that range in size from 0.5 to 27.0 mm.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Shamsafar_Leveraging_Multi-View_Data_for_Improved_Detection_Performance_An_Industrial_Use_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Shamsafar_Leveraging_Multi-View_Data_for_Improved_Detection_Performance_An_Industrial_Use_CVPRW_2023_paper.pdf"
    },
    {
        "title": "What Makes a Good Data Augmentation for Few-Shot Unsupervised Image Anomaly Detection?",
        "author": "Lingrui Zhang, Shuheng Zhang, Guoyang Xie, Jiaqi Liu, Hua Yan, Jinbao Wang, Feng Zheng, Yaochu Jin",
        "abstract": "Data augmentation is a promising technique for unsupervised anomaly detection in industrial applications, where the availability of positive samples is often limited due to factors such as commercial competition and sample collection difficulties. In this paper, how to effectively select and apply data augmentation methods for unsupervised anomaly detection is studied. The impact of various data augmentation methods on different anomaly detection algorithms is systematically investigated through experiments. The experimental results show that the performance of different industrial image anomaly detection (termed as IAD) algorithms is not significantly affected by the specific data augmentation method employed and that combining multiple data augmentation methods does not necessarily yield further improvements in the accuracy of anomaly detection, although it can achieve excellent results on specific methods. These findings provide useful guidance on selecting appropriate data augmentation methods for different requirements in IAD.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Zhang_What_Makes_a_Good_Data_Augmentation_for_Few-Shot_Unsupervised_Image_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Zhang_What_Makes_a_Good_Data_Augmentation_for_Few-Shot_Unsupervised_Image_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Diversified and Multi-Class Controllable Industrial Defect Synthesis for Data Augmentation and Transfer",
        "author": "Jing Wei, Fei Shen, Chengkan Lv, Zhengtao Zhang, Feng Zhang, Huabin Yang",
        "abstract": "Data augmentation is crucial to solve few-sample issues in industrial inspection based on deep learning. However, current industrial data augmentation methods have not yet demonstrated on-par ability in the synthesis of complex defects with pixel-level annotations. This paper proposes a new defect synthesis framework to fill the gap. Firstly, DCDGANc (Diversified and multi-class Controllable Defect Generation Adversarial Networks based on constant source images) is proposed to employ class labels to construct source inputs to control the category and random codes to generate diversified styles of defects. DCDGANc can generate defect content images with pure backgrounds, which avoids the influence of non-defect information and makes it easy to obtain binary masks by segmentation. Secondly, the Poisson blending is improved to avoid content loss when blending generated defect contents to the normal backgrounds. Finally, the complete defect samples and accurate pixel-level annotations are obtained by fine image processing. Experiments are conducted to verify the effectiveness of our work in wood, fabric, metal, and marble. The results show that our methods yield significant improvement in the segmentation performance of industrial products. Moreover, our work enables zero-shot inspection by facilitating defect transfer between datasets with different backgrounds but similar defects, which can greatly reduce the cost of data collection in industrial inspection.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Wei_Diversified_and_Multi-Class_Controllable_Industrial_Defect_Synthesis_for_Data_Augmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Wei_Diversified_and_Multi-Class_Controllable_Industrial_Defect_Synthesis_for_Data_Augmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Glass Wool Defect Detection Using an Improved YOLOv5",
        "author": "Yizhou Jin, Yu Lu, Gang Zhou, Qingjie Liu, Yunhong Wang",
        "abstract": "Glass wool defect detection is a key part of product quality assessment in the glass wool production process, yet few studies have been reported in this area. We propose a glass wool defect dataset named GWD, and also use the YOLOv5s model embedded in the GSConv and the CBAM modules for both Gap and Glueless defects in this dataset. The experimental results show that the performance of the improved YOLOv5s on the GWD dataset is superior to other compared methods and achieves a relatively good level on other publicly available datasets. Compared to the vanilla YOLOv5s, the mAP50 increased by 3.7% to 84.1%, the recall increased by 4.2% to 84.4%, and the number of parameters decreased by 0.42 MB to 6.27 MB of the improved YOLOv5s model on the GWD dataset. Speed-wisely, the improved YOLOv5s achieves a 97 FPS on RTX 2080Ti, thus making it practical to be applied in the industry of glass wool defect detection. The research on the GWD dataset is likely to contribute to breakthroughs in research on other datasets of the same type as well. The GWD dataset can be obtained by contacting us via email.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Jin_Glass_Wool_Defect_Detection_Using_an_Improved_YOLOv5_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Jin_Glass_Wool_Defect_Detection_Using_an_Improved_YOLOv5_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Assigned MURA Defect Generation Based on Diffusion Model",
        "author": "Weizhi Liu, Chang Liu, Qiang Liu, Dahai Yu",
        "abstract": "In this paper, we propose a novel method for assigned MURA generation using a diffusion model. MURA is a well-known problem in the display industry, which is difficult to be inspected because it is characterized by low contrast, blurry contours, blocky uneven brightness, and irregular shape patterns, and most defects have no rules to follow. Especially, for data-driven deep learning, the shortage of MURA samples collecting from the pipeline of manufactory is the first challenging problem, because the MURA sample happens with a low probability and in various ways. To relieve the problem, our proposed approach employs a diffusion model that generates MURA defect images using a few samples, which allows us to assign the position and class of MURA in the image. Specifically, our method leverages the diffusion process to estimate the visibility of MURA, which is then used to enhance the flexibility of the MURA detection process. We evaluate the performance of our method through MURA inspection. The results demonstrate the effectiveness of our proposed approach in addressing the MURA detection problem.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Liu_Assigned_MURA_Defect_Generation_Based_on_Diffusion_Model_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Liu_Assigned_MURA_Defect_Generation_Based_on_Diffusion_Model_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Unsupervised Automatic Defect Inspection Based on Image Matching and Local One-Class Classification",
        "author": "Chengkan Lv, Zhengtao Zhang, Fei Shen, Feng Zhang",
        "abstract": "In this paper, an unsupervised defect inspection method based on anomaly detection is proposed to inspect various kinds of surface defects in the field of industrial production. This method consists of two modules: (i) An image matching module is utilized to align the input image with a pre-specified template image. Specifically, all objects to be detected will be adjusted to the same position and angle. The aligned images can reduce the difficulty of the training stage, facilitating the subsequent feature extraction and anomaly localization. (ii) After the image matching procedure, an anomaly localization module is trained to learn a mapping that concentrates normal samples in feature space. In particular, each local image region is assigned a feature center by adopting a feature map as the mapping target. Therefore, the compactness of the features extracted from the same region can be improved, which is beneficial to detect potential anomalous targets. Moreover, various artificial defective images are synthesized during the training stage to further improve the discriminatory ability of the anomaly localization module. A series of experiments are conducted on MAD dataset and the industrial production line. The experimental results verify the efficiency and versatility of the proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Lv_Unsupervised_Automatic_Defect_Inspection_Based_on_Image_Matching_and_Local_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Lv_Unsupervised_Automatic_Defect_Inspection_Based_on_Image_Matching_and_Local_CVPRW_2023_paper.pdf"
    },
    {
        "title": "XDNet: A Few-Shot Meta-Learning Approach for Cross-Domain Visual Inspection",
        "author": "Xian Yeow Lee, Lasitha Vidyaratne, Mahbubul Alam, Ahmed Farahat, Dipanjan Ghosh, Teresa Gonzalez Diaz, Chetan Gupta",
        "abstract": "Automated visual inspection has the potential to improve the efficiency and accuracy of inspection tasks across various industries. Deep learning models have been at the forefront of many automated visual inspection technologies. In this work, we focus on a specific instance of a visual inspection problem: the defect detection and classification problem. Training a deep learning model from scratch to detect defects is challenging due to the scarcity of labeled images with defects. Moreover, it is progressively more challenging to adapt a deep learning model across different domains using limited labeled data. We propose a cross-domain meta-learning framework, XDNet, to solve the defect classification problem using a few labeled samples. XDNet is inspired by recent advancements in pre-trained backbone models as general feature extractors and meta-learning frameworks, which adapt across different domains using non-parametric classifiers under limited computational resources. We demonstrate the efficacy of XDNet using a benchmark anomaly detection dataset which we re-formulate as a defect detection and classification problem. Experimental results suggest that XDNet performs significantly better ( 17%) than the existing state-of-the-art and baseline models. Additionally, we perform an ablation study to identify the important components that contribute to the improved performance of the proposed framework. Finally, we conduct a data domain-specific analysis to understand the potential strengths and drawbacks of XDNet on different types of defects.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Lee_XDNet_A_Few-Shot_Meta-Learning_Approach_for_Cross-Domain_Visual_Inspection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Lee_XDNet_A_Few-Shot_Meta-Learning_Approach_for_Cross-Domain_Visual_Inspection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Towards Automated Polyp Segmentation Using Weakly- and Semi-Supervised Learning and Deformable Transformers",
        "author": "Guangyu Ren, Michalis Lazarou, Jing Yuan, Tania Stathaki",
        "abstract": "Polyp segmentation is a crucial step towards the computer-aided diagnosis of colorectal cancer. However, most of the polyp segmentation methods require pixel-wise annotated datasets. Annotated datasets are tedious and time-consuming to produce, especially for physicians who must dedicate their time to their patients. To this end, we propose a novel weakly- and semi-supervised learning polyp segmentation framework that can be trained using only weakly annotated images along with unlabeled images making it very cost-efficient to use. More specifically our contributions are: 1) a novel weakly annotated polyp dataset, 2) a novel sparse foreground loss that suppresses false positives and improves weakly-supervised training, 3) a deformable transformer encoder neck for feature enhancement by fusing information across levels and flexible spatial locations. Extensive experimental results demonstrate the merits of our ideas on five challenging datasets outperforming some state-of-the-art fully supervised models. Also, our framework can be utilized to fine-tune models trained on natural image segmentation datasets drastically improving their performance for polyp segmentation and impressively demonstrating superior performance to fully supervised fine-tuning",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Ren_Towards_Automated_Polyp_Segmentation_Using_Weakly-_and_Semi-Supervised_Learning_and_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Ren_Towards_Automated_Polyp_Segmentation_Using_Weakly-_and_Semi-Supervised_Learning_and_CVPRW_2023_paper.pdf"
    },
    {
        "title": "How Do Label Errors Affect Thin Crack Detection by DNNs",
        "author": "Liang Xu, Han Zou, Takayuki Okatani",
        "abstract": "Numerous studies have been conducted on detecting cracks in images of roads, surfaces of concrete and other materials, and so on. Accurate annotation of cracks is crucial for supervised learning, but identifying cracks in images, particularly thin cracks, can be challenging, making accurate annotation difficult. However, little is known about how annotation errors in training data affect the accuracy of detectors trained on them. This study attempts to address this gap by synthesizing annotation errors and analyzing their effects, which, to the authors' knowledge, has not been done before in the literature. This is made possible by employing an annotation method that labels cracks as curves with a single-pixel width alongside appropriate training and evaluation methods. We synthesized various types of annotation errors, including under and over-annotation errors caused by crack-like image structures and polyline approximation of crack curves, to reduce annotation costs. The experimental results reveal several important findings, such as that under-annotation is more harmful than over-annotation and that polyline approximation has a modest impact on detection accuracy.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Xu_How_Do_Label_Errors_Affect_Thin_Crack_Detection_by_DNNs_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Xu_How_Do_Label_Errors_Affect_Thin_Crack_Detection_by_DNNs_CVPRW_2023_paper.pdf"
    },
    {
        "title": "N-Pad: Neighboring Pixel-Based Industrial Anomaly Detection",
        "author": "JunKyu Jang, Eugene Hwang, Sung-Hyuk Park",
        "abstract": "Identifying defects in the images of industrial products has been an important task to enhance quality control and reduce maintenance costs. In recent studies, industrial anomaly detection models were developed using pre-trained networks to learn nominal representations. To employ the relative positional information of each pixel, we present N-pad, a novel method for anomaly detection and segmentation in a one-class learning setting that includes the neighborhood of the target pixel for model training and evaluation. Within the model architecture, pixel-wise nominal distributions are estimated by using the features of neighboring pixels with the target pixel to allow possible marginal misalignment. Moreover, the centroids from clusters of nominal features are identified as a representative nominal set. Accordingly, anomaly scores are inferred based on the Mahalanobis distances and Euclidean distances between the target pixel and the estimated distributions or the centroid set, respectively. Thus, we have achieved state-of-the-art performance in MVTec-AD with AUROC of 99.37 for anomaly detection and 98.75 for anomaly segmentation, reducing the error by 34% compared to the next best performing model. Experiments in various settings further validate our model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Jang_N-Pad_Neighboring_Pixel-Based_Industrial_Anomaly_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Jang_N-Pad_Neighboring_Pixel-Based_Industrial_Anomaly_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Parcel3D: Shape Reconstruction From Single RGB Images for Applications in Transportation Logistics",
        "author": "Alexander Naumann, Felix Hertlein, Laura D\u00f6rr, Kai Furmans",
        "abstract": "We focus on enabling damage and tampering detection in logistics and tackle the problem of 3D shape reconstruction of potentially damaged parcels. As input we utilize single RGB images, which corresponds to use-cases where only simple handheld devices are available, e.g. for postmen during delivery or clients on delivery. We present a novel synthetic dataset, named Parcel3D, that is based on the Google Scanned Objects (GSO) dataset and consists of more than 13,000 images of parcels with full 3D annotations. The dataset contains intact, i.e. cuboid-shaped, parcels and damaged parcels, which were generated in simulations. We work towards detecting mishandling of parcels by presenting a novel architecture called CubeRefine R-CNN, which combines estimating a 3D bounding box with an iterative mesh refinement. We benchmark our approach on Parcel3D and an existing dataset of cuboid-shaped parcels in real-world scenarios. Our results show, that while training on Parcel3D enables transfer to the real world, enabling reliable deployment in real-world scenarios is still challenging. CubeRefine R-CNN yields competitive performance in terms of Mesh AP and is the only model that directly enables deformation assessment by 3D mesh comparison and tampering detection by comparing viewpoint invariant parcel side surface representations. Dataset and code are available at https://a-nau.github.io/parcel3d.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Naumann_Parcel3D_Shape_Reconstruction_From_Single_RGB_Images_for_Applications_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Naumann_Parcel3D_Shape_Reconstruction_From_Single_RGB_Images_for_Applications_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ES3Net: Accurate and Efficient Edge-Based Self-Supervised Stereo Matching Network",
        "author": "I-Sheng Fang, Hsiao-Chieh Wen, Chia-Lun Hsu, Po-Chung Jen, Ping-Yang Chen, Yong-Sheng Chen",
        "abstract": "Efficient and accurate depth estimation is crucial for real-world embedded vision applications, such as autonomous driving, 3D reconstruction, and drone navigation. Stereo matching is considered more accurate than monocular depth estimation due to the presence of a reference image, but its computational inefficiency poses a challenge for its deployment on edge devices. Moreover, it is difficult to acquire ground-truth depths for supervised training of stereo matching networks. To address these challenges, we propose Edge-based Self-Supervised Stereo matching Network (ES3Net), which efficiently estimates accurate depths without ground-truth depths for training. We introduce dual disparity to transform an efficient supervised stereo matching network into a self-supervised learning framework. Comprehensive experimental results demonstrate that ES3Net has comparable accuracy with stereo methods while outperforming monocular methods in inference time, approaching state-of-the-art performance. More specifically, our method improves over 40% in terms of RMSElog, compared to monocular methods while having 1500 times fewer parameters and running four times faster on NVIDIA Jetson TX2. The efficient and reliable estimation of depths on edge devices using ES3Net lays a good foundation for safe drone navigation.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EVW/html/Fang_ES3Net_Accurate_and_Efficient_Edge-Based_Self-Supervised_Stereo_Matching_Network_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EVW/papers/Fang_ES3Net_Accurate_and_Efficient_Edge-Based_Self-Supervised_Stereo_Matching_Network_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Fully-Binarized Distance Computation Based On-Device Few-Shot Learning for XR Applications",
        "author": "Vivek Parmar, Sandeep Kaur Kingra, Syed Shakib Sarwar, Ziyun Li, Barbara De Salvo, Manan Suri",
        "abstract": "Low-Power Edge-AI capabilities are essential for on-device extended reality (XR) applications to support the vision of Metaverse. A critical requirement for emerging AI applications is personalization and adaptability without requiring retraining. Few-shot learning using embedding-based computations present an attractive method for the same. However, quantization-based optimizations to map such computations are yet to be explored. In this work, we present a fully binarized distance computing (BinDC) framework to perform distance computations for few-shot learning using only accumulation and logic operations (XOR/XNOR). The proposed method leads to marginal loss in accuracy of  4% (for 4-bits). This leads to savings in memory ( 8x), energy ( 2.5-3x), power ( 2x) and latency ( 1.1-1.5x) compared to a floating-point cosine distance computation when using CPU-based computations performed on an embedded platform. We further demonstrate realizations utilizing RRAM (resistive random access memory) based IMC (in-memory computing) to further improve EDP (energy delay product) ( 1000x) in comparison to the embedded CPU-based realization.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EVW/html/Parmar_Fully-Binarized_Distance_Computation_Based_On-Device_Few-Shot_Learning_for_XR_Applications_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EVW/papers/Parmar_Fully-Binarized_Distance_Computation_Based_On-Device_Few-Shot_Learning_for_XR_Applications_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Hardware-Aware Pruning for FPGA Deep Learning Accelerators",
        "author": "Jef Plochaet, Toon Goedem\u00e9",
        "abstract": "Pruning has been widely used for deep neural network optimization and compression. In this paper we propose a pruning method to accelerate FPGA implementations of neural networks with the goal of lowering the inference time as much as possible, while taking into account hardware-specific constraints. We use the normalized L2-norm as a measure of filter importance and iteratively prune the network with a predefined pruning stepsize. We extend this method to also prune around residual connections using the maximum normalized L2-norm as a representation of importance for a group of connected channels. We introduce a hardware-aware pruning method for FPGA deep learning accelerators, adaptively pruning the neural network based on the size of the systolic array used to calculate the convolutions. We validate our methods by pruning a polyp segmentation model on two different datasets. This results in almost halving the inference time with minimal loss of accuracy on both datasets. We prove that our two contributions yield an extra 30% increase in processing speed compared to classical L2 pruning.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EVW/html/Plochaet_Hardware-Aware_Pruning_for_FPGA_Deep_Learning_Accelerators_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EVW/papers/Plochaet_Hardware-Aware_Pruning_for_FPGA_Deep_Learning_Accelerators_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Uncertainty in Real-Time Semantic Segmentation on Embedded Systems",
        "author": "Ethan Goan, Clinton Fookes",
        "abstract": "Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present when applied on embedded real-time systems. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful epistemic uncertainty estimates on embedded hardware in real-time for multiple models and datasets whilst maintaining predictive performance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/EVW/html/Goan_Uncertainty_in_Real-Time_Semantic_Segmentation_on_Embedded_Systems_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/EVW/papers/Goan_Uncertainty_in_Real-Time_Semantic_Segmentation_on_Embedded_Systems_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ETAD: Training Action Detection End to End on a Laptop",
        "author": "Shuming Liu, Mengmeng Xu, Chen Zhao, Xu Zhao, Bernard Ghanem",
        "abstract": "Temporal action detection (TAD) with end-to-end training often suffers from the pain of huge demand for computing resources due to long video duration. In this work, we propose an efficient temporal action detector (ETAD) that can train directly from video frames with extremely low GPU memory consumption. Our main idea is to minimize and balance the heavy computation among features and gradients in each training iteration. We propose to sequentially forward the snippet frame through the video encoder, and backward only a small necessary portion of gradients to update the encoder. To further alleviate the computational redundancy in training, we propose to dynamically sample only a small subset of proposals during training. Various sampling strategies and ratios are studied for both the encoder and detector. ETAD achieves state-of-the-art performance on TAD benchmarks with remarkable efficiency. On ActivityNet-1.3, training ETAD in 18 hours can reach 38.25% average mAP with only 1.3 GB memory per video under end-to-end training. Code is available at https://github.com/sming256/ETAD.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Liu_ETAD_Training_Action_Detection_End_to_End_on_a_Laptop_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Liu_ETAD_Training_Action_Detection_End_to_End_on_a_Laptop_CVPRW_2023_paper.pdf"
    },
    {
        "title": "STAR: Sparse Thresholded Activation Under Partial-Regularization for Activation Sparsity Exploration",
        "author": "Zeqi Zhu, Arash Pourtaherian, Luc Waeijen, Egor Bondarev, Orlando Moreira",
        "abstract": "Brain-inspired event-driven processors execute deep neural networks (DNNs) in a sparsity-aware manner. Specifically, if more zeros are induced in the activation maps, less computation will be performed in the succeeding convolution layer. However, inducing activation sparsity in DNNs remains a challenge. To address this, we propose a training approach STAR (Sparse Thresholded Activation under partial-Regularization), which combines activation regularization with thresholding, to overcome the barrier of a single threshold- or regularization-based method in sparsity improvement. More precisely, we employ the sparse penalty on the near-zero activations to fit the activation learning behavior in accuracy recovery, followed by thresholding to further suppress activations. Experimental results with SOTA networks (ResNet50/MobileNetV2, SSD, YOLOX and DeepLabV3+) on various datasets (ImageNet, KITTI, VOC2007 and CityScapes) show that STAR can reduce on average 54% more activations compared to ReLU suppression. It outperforms the state-of-the-art by a significant margin of 35% in activation suppression without compromising accuracy loss. Additionally, a case study for a commercially-available event-driven hardware architecture, Neuronflow, demonstrates that the boosted activation sparsity in ResNet50 can be efficiently translated into latency reduction by up to 2.78x, FPS improvement by up to 2.80x, and energy savings by up to 2.09x. STAR elevates event-driven processors as a superior alternative to GPUs for Edge computing.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Zhu_STAR_Sparse_Thresholded_Activation_Under_Partial-Regularization_for_Activation_Sparsity_Exploration_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Zhu_STAR_Sparse_Thresholded_Activation_Under_Partial-Regularization_for_Activation_Sparsity_Exploration_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Data-Free Model Pruning at Initialization via Expanders",
        "author": "James Stewart, Umberto Michieli, Mete Ozay",
        "abstract": "In light of the enormous computational resources required to store and train modern deep learning models, significant research has focused on model compression. When deploying compressed networks on remote devices prior to training them, a compression scheme cannot use any training data or derived information (e.g., gradients). This leaves only the structure of the network to work with, and existing literature on how graph structure affects network performance is scarce. Recently, expander graphs have been put forward as a tool for sparsifying neural architectures. Unfortunately, however, existing models can rarely outperform a naive random baseline. In this work, we propose a stronger model for generating expanders, which we then use to sparsify a variety of mainstream CNN architectures. We demonstrate that accuracy is an increasing function of expansion in a sparse model, and both analyse and elucidate its superior performance over alternative models.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Stewart_Data-Free_Model_Pruning_at_Initialization_via_Expanders_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Stewart_Data-Free_Model_Pruning_at_Initialization_via_Expanders_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Accelerable Lottery Tickets With the Mixed-Precision Quantization",
        "author": "Zhangheng Li, Yu Gong, Zhenyu Zhang, Xingyun Xue, Tianlong Chen, Yi Liang, Bo Yuan, Zhangyang Wang",
        "abstract": "In recent years, the lottery tickets hypothesis has gained widespread popularity as a means of network compression. However, the practical application of lottery tickets for hardware acceleration is difficult due to their element-wise unstructured sparsity nature. In this paper, we argue that network pruning can be seen as a special case of network quantization, and relax the hard network pruning with mixed-precision quantization in an unstructured manner, which makes it possible for real hardware acceleration. We successfully validate the wide existence of quantized lottery tickets, namely MPQ-tickets, that can match or even surpass the performance of corresponding full-precision dense networks on various representative benchmarks. Also, we demonstrate that MPQ-tickets have much higher flexibility than vanilla lottery tickets, and largely benefit from pruning when compared to QNNs. Moreover, the MPQ-tickets achieve up to 8x hardware acceleration of inference speed and 14x less memory consumption than full-precision models.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Li_Accelerable_Lottery_Tickets_With_the_Mixed-Precision_Quantization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Li_Accelerable_Lottery_Tickets_With_the_Mixed-Precision_Quantization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models",
        "author": "Phuoc-Hoan Charles Le, Xinlin Li",
        "abstract": "With the increasing popularity and the increasing size of vision transformers (ViTs), there has been an increasing interest in making them more efficient and less computationally costly for deployment on edge devices with limited computing resources. Binarization can be used to help reduce the size of ViT models and their computational cost significantly, using popcount operations when the weights and the activations are in binary. However, ViTs suffer a larger performance drop when directly applying convolutional neural network (CNN) binarization methods or existing binarization methods to binarize ViTs compared to CNNs on datasets with a large number of classes such as ImageNet-1k. With extensive analysis, we find that binary vanilla ViTs such as DeiT miss out on a lot of key architectural properties that CNNs have that allow binary CNNs to have much higher representational capability than binary vanilla ViT. Therefore, we propose BinaryViT, in which inspired by the CNN architecture, we include operations from the CNN architecture into a pure ViT architecture to enrich the representational capability of a binary ViT without introducing convolutions. These include an average pooling layer instead of a token pooling layer, a block that contains multiple average pooling branches, an affine transformation right before the addition of each main residual connection, and a pyramid structure. Experimental results on the ImageNet-1k dataset show the effectiveness of these operations that allow a fully-binary pure ViT model to be competitive with previous state-of-the-art binary (SOTA) CNN models.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Le_BinaryViT_Pushing_Binary_Vision_Transformers_Towards_Convolutional_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Le_BinaryViT_Pushing_Binary_Vision_Transformers_Towards_Convolutional_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Token Merging for Fast Stable Diffusion",
        "author": "Daniel Bolya, Judy Hoffman",
        "abstract": "The landscape of image generation has been forever changed by open vocabulary diffusion models. However, at their core these models use transformers, which makes generation slow. Better implementations to increase the throughput of these transformers have emerged, but they still evaluate the entire model. In this paper, we instead speed up diffusion models by exploiting natural redundancy in generated images by merging redundant tokens. After making some diffusion-specific improvements to Token Merging (ToMe), our ToMe for Stable Diffusion can reduce the number of tokens in an existing Stable Diffusion model by up to 60% while still producing high quality images without any extra training. In the process, we speed up image generation by up to 2x and reduce memory consumption by up to 5.6x. Furthermore, this speed-up stacks with efficient implementations such as xFormers, minimally impacting quality while being up to 5.4x faster for large images. Code is available at https://github.com/dbolya/tomesd.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Bolya_Token_Merging_for_Fast_Stable_Diffusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bolya_Token_Merging_for_Fast_Stable_Diffusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeCAtt: Efficient Vision Transformers With Decorrelated Attention Heads",
        "author": "Mayukh Bhattacharyya, Soumitri Chattopadhyay, Sayan Nag",
        "abstract": "The advent of Vision Transformers (ViT) has led to significant performance gains across various computer vision tasks over the last few years, surpassing the de facto standard CNN architectures. However, most of the prominent variations of Vision Transformers are resource-intensive architectures with huge parameter sizes. They are known to be data-hungry and overfit quickly on comparatively smaller datasets. Consequently, this holds back their widespread usage across low-resource settings, which brings forth the need to develop resource-efficient vision transformers. To this end, we introduce a regularization loss that prioritizes efficient utilization of model parameters by decorrelating the heads of a multi-headed attention block in a vision transformer. This forces the heads to learn distinct features rather than focus on the same ones. Using this loss provides a consistent performance improvement over a wide range of varying scenarios of models and datasets as we show in our experiments, which proves its superior effectiveness.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Bhattacharyya_DeCAtt_Efficient_Vision_Transformers_With_Decorrelated_Attention_Heads_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bhattacharyya_DeCAtt_Efficient_Vision_Transformers_With_Decorrelated_Attention_Heads_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations",
        "author": "Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang, Andrei Kulik, Matthias Grundmann",
        "abstract": "The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without INT8 quantization for a 512 x 512 image with 20 iterations) on GPU equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Chen_Speed_Is_All_You_Need_On-Device_Acceleration_of_Large_Diffusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Chen_Speed_Is_All_You_Need_On-Device_Acceleration_of_Large_Diffusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Dynamic Inference Acceleration of 3D Point Cloud Deep Neural Networks Using Point Density and Entropy",
        "author": "Gyudo Park, SooHyeok Kang, Wencan Cheng, Jong Hwan Ko",
        "abstract": "This paper introduces a density- and entropy-adaptive inference acceleration method for 3D point cloud based deep neural networks. Based on the entropy of each input frame, the method first determines the number of points to be inferred. Then we apply a novel density calculation method to sample the points in the order of density of each point. Experiments on two representative 3D scene flow estimation models with the KITTI dataset show that the proposed scheme reduces inference latency by 32% each within 0.01m of the estimation error.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Park_Dynamic_Inference_Acceleration_of_3D_Point_Cloud_Deep_Neural_Networks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Park_Dynamic_Inference_Acceleration_of_3D_Point_Cloud_Deep_Neural_Networks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeepGEMM: Accelerated Ultra Low-Precision Inference on CPU Architectures Using Lookup Tables",
        "author": "Darshan C. Ganji, Saad Ashfaq, Ehsan Saboori, Sudhakar Sah, Saptarshi Mitra, MohammadHossein AskariHemmat, Alexander Hoffman, Ahmed Hassanien, Mathieu L\u00e9onardon",
        "abstract": "A lot of recent progress has been made in ultra low-bit quantization, promising significant improvements in latency, memory footprint and energy consumption on edge devices. Quantization methods such as Learned Step Size Quantization can achieve model accuracy that is comparable to full-precision floating-point baselines even with sub-byte quantization. However, it is extremely challenging to deploy these ultra low-bit quantized models on mainstream CPU devices because commodity SIMD (Single Instruction, Multiple Data) hardware typically supports no less than 8-bit precision. To overcome this limitation, we propose DeepGEMM, a lookup table based approach for the execution of ultra low-precision convolutional neural networks on SIMD hardware. The proposed method precomputes all possible products of weights and activations, stores them in a lookup table, and efficiently accesses them at inference time to avoid costly multiply-accumulate operations. Our 2-bit implementation outperforms corresponding 8-bit integer kernels in the QNNPACK framework by up to 1.74x on x86 platforms.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Ganji_DeepGEMM_Accelerated_Ultra_Low-Precision_Inference_on_CPU_Architectures_Using_Lookup_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Ganji_DeepGEMM_Accelerated_Ultra_Low-Precision_Inference_on_CPU_Architectures_Using_Lookup_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Making Models Shallow Again: Jointly Learning To Reduce Non-Linearity and Depth for Latency-Efficient Private Inference",
        "author": "Souvik Kundu, Yuke Zhang, Dake Chen, Peter A. Beerel",
        "abstract": "Large number of ReLU and MAC operations of Deep neural networks make them ill-suited for latency and compute-efficient private inference. In this paper, we present a model optimization method that allows a model to learn to be shallow. In particular, we leverage the ReLU sensitivity of a convolutional block to remove a ReLU layer and merge its succeeding and preceding convolution layers to a shallow block. Unlike existing ReLU reduction methods, our joint reduction method can yield models with improved reduction of both ReLUs and linear operations by up to 1.73x and 1.47x, respectively, evaluated with ResNet18 on CIFAR-100 without any significant accuracy-drop.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Kundu_Making_Models_Shallow_Again_Jointly_Learning_To_Reduce_Non-Linearity_and_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Kundu_Making_Models_Shallow_Again_Jointly_Learning_To_Reduce_Non-Linearity_and_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Localized Latent Updates for Fine-Tuning Vision-Language Models",
        "author": "Moritz Ibing, Isaak Lim, Leif Kobbelt",
        "abstract": "Although massive pre-trained vision-language models like CLIP show impressive generalization capabilities for many tasks, still it often remains necessary to fine-tune them for improved performance on specific datasets. When doing so, it is desirable that updating the model is fast and that the model does not lose its capabilities on data outside of the dataset, as is often the case with classical fine-tuning approaches. In this work we suggest a lightweight adapter that only updates the models predictions close to seen datapoints. We demonstrate the effectiveness and speed of this relatively simple approach in the context of few-shot learning, where our results both on classes seen and unseen during training are comparable with or improve on the state of the art.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Ibing_Localized_Latent_Updates_for_Fine-Tuning_Vision-Language_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Ibing_Localized_Latent_Updates_for_Fine-Tuning_Vision-Language_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Revisiting Class Imbalance for End-to-End Semi-Supervised Object Detection",
        "author": "Purbayan Kar, Vishal Chudasama, Naoyuki Onoe, Pankaj Wasnik",
        "abstract": "Semi-supervised object detection (SSOD) has made significant progress with the development of pseudo-label-based end-to-end methods. However, many of these methods face challenges due to class imbalance, which hinders the effectiveness of the pseudo-label generator. Furthermore, in the literature, it has been observed that low-quality pseudo-labels severely limit the performance of SSOD. In this paper, we examine the root causes of low-quality pseudo-labels and present novel learning mechanisms to improve the label generation quality. To cope with high false-negative and low precision rates, we introduce an adaptive thresholding mechanism that helps the proposed network to filter out optimal bounding boxes. We further introduce a Jitter-Bagging module to provide accurate information on localization to help refine the bounding boxes. Additionally, two new losses are introduced using the background and foreground scores predicted by the teacher and student networks to improvise the pseudo-label recall rate. Furthermore, our method applies strict supervision to the teacher network by feeding strong & weak augmented data to generate robust pseudo-labels so that it can detect small and complex objects. Finally, the extensive experiments show that the proposed network outperforms state-of-the-art methods on MS-COCO and Pascal VOC datasets and allows the baseline network to achieve 100% supervised performance with much less (i.e., 20%) labeled data.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Kar_Revisiting_Class_Imbalance_for_End-to-End_Semi-Supervised_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Kar_Revisiting_Class_Imbalance_for_End-to-End_Semi-Supervised_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Pre-Training Auto-Generated Volumetric Shapes for 3D Medical Image Segmentation",
        "author": "Ryu Tadokoro, Ryosuke Yamada, Hirokatsu Kataoka",
        "abstract": "In 3D medical image segmentation, data collection and annotation costs require significant human efforts. Moreover, obtaining training data is challenging due to privacy constraints. Consequently, achieving efficient learning with limited data is an urgent 3D medical image segmentation issue. One approach to address this problem is using pre-trained models, which have been widely researched. Recently, self-supervised learning for 3D medical images has gained popularity, but the data available for such learning is also scarce, limiting the number of pre-training datasets. In recent years, formula-driven supervised learning has garnered attention. It can achieve high pre-training effects using only easily accessible synthetic data, making it a promising alternative for pre-training datasets. Inspired by this approach, we propose the Auto-generated Volumetric Shapes Database (AVS-DB) for data-scarce 3D medical image segmentation tasks. AVS-DB is automatically generated from a combination of dozens of 3D models based on polygons and shape similarity ratio variations. Our experiments show that AVS-DB pre-trained models significantly outperform models trained from scratch and achieve comparable or better performance than existing self-supervised learning methods we compared. AVS-DB can potentially enhance 3D medical image segmentation models and address limited data availability challenges.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Tadokoro_Pre-Training_Auto-Generated_Volumetric_Shapes_for_3D_Medical_Image_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Tadokoro_Pre-Training_Auto-Generated_Volumetric_Shapes_for_3D_Medical_Image_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Content-Adaptive Downsampling in Convolutional Neural Networks",
        "author": "Robin Hesse, Simone Schaub-Meyer, Stefan Roth",
        "abstract": "Many convolutional neural networks (CNNs) rely on progressive downsampling of their feature maps to increase the network's receptive field and decrease computational cost. However, this comes at the price of losing granularity in the feature maps, limiting the ability to correctly understand images or recover fine detail in dense prediction tasks. To address this, common practice is to replace the last few downsampling operations in a CNN with dilated convolutions, allowing to retain the feature map resolution without reducing the receptive field, albeit increasing the computational cost. This allows to trade off predictive performance against cost, depending on the output feature resolution. By either regularly downsampling or not downsampling the entire feature map, existing work implicitly treats all regions of the input image and subsequent feature maps as equally important, which generally does not hold. We propose an adaptive downsampling scheme that generalizes the above idea by allowing to process informative regions at a higher resolution than less informative ones. In a variety of experiments, we demonstrate the versatility of our adaptive downsampling strategy and empirically show that it improves the cost-accuracy trade-off of various established CNNs.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Hesse_Content-Adaptive_Downsampling_in_Convolutional_Neural_Networks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Hesse_Content-Adaptive_Downsampling_in_Convolutional_Neural_Networks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Vision Transformers With Mixed-Resolution Tokenization",
        "author": "Tomer Ronen, Omer Levy, Avram Golbert",
        "abstract": "Vision Transformer models process input images by dividing them into a spatially regular grid of equal-size patches. Conversely, Transformers were originally introduced over natural language sequences, where each token represents a subword - a chunk of raw data of arbitrary size. In this work, we apply this approach to Vision Transformers by introducing a novel image tokenization scheme, replacing the standard uniform grid with a mixed-resolution sequence of tokens, where each token represents a patch of arbitrary size. Using the Quadtree algorithm and a novel saliency scorer, we construct a patch mosaic where low-saliency areas of the image are processed in low resolution, routing more of the model's capacity to important image regions. Using the same architecture as vanilla Vision Transformers, our Quadformer models achieve substantial accuracy gains on image classification when controlling for the computational budget. Code and models are publicly available at https://github.com/TomerRonen34/mixed-resolution-vit.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Ronen_Vision_Transformers_With_Mixed-Resolution_Tokenization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Ronen_Vision_Transformers_With_Mixed-Resolution_Tokenization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CFDP: Common Frequency Domain Pruning",
        "author": "Samir Khaki, Weihan Luo",
        "abstract": "As the saying goes, sometimes less is more - and when it comes to neural networks, that couldn't be more true. Enter pruning, the art of selectively trimming away unnecessary parts of a network to create a more streamlined, efficient architecture. In this paper, we introduce a novel end-to-end pipeline for model pruning via the frequency domain. This work aims to shed light on the interoperability of intermediate model outputs and their significance beyond the spatial domain. Our method, dubbed Common Frequency Domain Pruning (CFDP) aims to extrapolate common frequency characteristics defined over the feature maps to rank the individual channels of a layer based on their level of importance in learning the representation. By harnessing the power of CFDP, we have achieved state-of-the-art results on CIFAR-10 with GoogLeNet reaching an accuracy of 95.25%, that is, +0.2% from the original model. We also outperform all benchmarks and match the original model's performance on ImageNet, using only 55% of the trainable parameters and 60% of the FLOPs. In addition to notable performances, models produced via CFDP exhibit robustness to a variety of configurations including pruning from untrained neural architectures, and resistance to adversarial attacks. The implementation code can be found at https://github.com/Skhaki18/CFDP.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Khaki_CFDP_Common_Frequency_Domain_Pruning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Khaki_CFDP_Common_Frequency_Domain_Pruning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Similar Class Style Augmentation for Efficient Cross-Domain Few-Shot Learning",
        "author": "Manogna Sreenivas, Soma Biswas",
        "abstract": "Cross-Domain Few-Shot Learning (CD-FSL) aims to recognize new classes from unseen domains, given limited training samples. Majority of the state-of-the-art approaches for this task introduce new task-specific additional parameters for adapting to the novel task, which involves changing the trained model architecture, in addition to increasing the number of model parameters. The first contribution of this work is to revisit the existing approaches like modifying the Batch Normalization affine parameters and the scale hyperparameter in cosine similarity based softmax loss for adapting the trained model to the new tasks, without changing the model architecture. Secondly, to aid the model learning with few examples per class, we propose to augment the data of each class with the styles of the semantically similar classes. Extensive evaluation on the challenging Meta-Dataset shows that this simple framework is very effective for the CD-FSL task. We also show that the Similar-class Style Augmentation module can be seamlessly integrated with existing approaches to further improve their performance, thus establishing the state-of-the-art in this challenging area.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Sreenivas_Similar_Class_Style_Augmentation_for_Efficient_Cross-Domain_Few-Shot_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Sreenivas_Similar_Class_Style_Augmentation_for_Efficient_Cross-Domain_Few-Shot_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Quantized Proximal Averaging Networks for Compressed Image Recovery",
        "author": "Nareddy Kartheek Kumar Reddy, Mani Madhoolika Bulusu, Praveen Kumar Pokala, Chandra Sekhar Seelamantula",
        "abstract": "We solve the analysis sparse coding problem considering a combination of convex and non-convex sparsity promoting penalties. The multi-penalty formulation results in an iterative algorithm involving proximal-averaging. We then unfold the iterative algorithm into a trainable network that facilitates learning the sparsity prior. We also consider quantization of the network weights. Quantization makes neural networks efficient both in terms of memory and computation during inference, and also renders them compatible for low-precision hardware deployment. Our learning algorithm is based on a variant of the ADAM optimizer in which the quantizer is part of the forward pass and the gradients of the loss function are evaluated corresponding to the quantized weights while doing a book-keeping of the high-precision weights. We demonstrate applications to compressed image recovery and magnetic resonance image reconstruction. The proposed approach offers superior reconstruction accuracy and quality than state-of-the-art unfolding techniques and the performance degradation is minimal even when the weights are subjected to extreme quantization.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Reddy_Quantized_Proximal_Averaging_Networks_for_Compressed_Image_Recovery_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Reddy_Quantized_Proximal_Averaging_Networks_for_Compressed_Image_Recovery_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Envisioning a Next Generation Extended Reality Conferencing System With Efficient Photorealistic Human Rendering",
        "author": "Chuanyue Shen, Letian Zhang, Zhangsihao Yang, Masood Mortazavi, Xiyun Song, Liang Peng, Heather Yu",
        "abstract": "Meeting online is becoming the new normal. Creating an immersive experience for online meetings is a necessity towards more diverse and seamless environments. Efficient photorealistic rendering of human 3D dynamics is the core of immersive meetings. Current popular applications achieve real-time conferencing but fall short in delivering photorealistic human dynamics, either due to limited 2D space or the use of avatars that lack realistic interactions between participants. Recent advances in neural rendering, such as the Neural Radiance Field (NeRF), offer the potential for greater realism in metaverse meetings. However, the slow rendering speed of NeRF poses challenges for real-time conferencing. We envision a pipeline for a future extended reality metaverse conferencing system that leverages monocular video acquisition and free-viewpoint synthesis to enhance data and hardware efficiency. Towards an immersive conferencing experience, we explore an accelerated NeRF-based free-viewpoint synthesis algorithm for rendering photorealistic human dynamics more efficiently. We show that our algorithm achieves comparable rendering quality while performing training and inference 44.5% and 213% faster than state-of-the-art methods, respectively. Our exploration provides a design basis for constructing metaverse conferencing systems that can handle complex application scenarios, including dynamic scene relighting with customized themes and multi-user conferencing that harmonizes real-world people into an extended world.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Shen_Envisioning_a_Next_Generation_Extended_Reality_Conferencing_System_With_Efficient_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Shen_Envisioning_a_Next_Generation_Extended_Reality_Conferencing_System_With_Efficient_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Phase-Field Models for Lightweight Graph Convolutional Networks",
        "author": "Hichem Sahbi",
        "abstract": "In this paper, we design lightweight graph convolutional networks (GCNs) using a particular class of regularizers, dubbed as phase-field models (PFMs). PFMs exhibit a bi-phase behavior using a particular ultra-local term that allows training both the topology and the weight parameters of GCNs as a part of a single \"end-to-end\" optimization problem. Our proposed solution also relies on a reparametrization that pushes the mask of the topology towards binary values leading to effective topology selection and high generalization while implementing any targeted pruning rate. Both masks and weights share the same set of latent variables and this further enhances the generalization power of the resulting lightweight GCNs. Extensive experiments conducted on the challenging task of skeleton-based recognition show the outperformance of PFMs against other staple regularizers as well as related lightweight design methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Sahbi_Phase-Field_Models_for_Lightweight_Graph_Convolutional_Networks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Sahbi_Phase-Field_Models_for_Lightweight_Graph_Convolutional_Networks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "AdaMTL: Adaptive Input-Dependent Inference for Efficient Multi-Task Learning",
        "author": "Marina Neseem, Ahmed Agiza, Sherief Reda",
        "abstract": "Modern Augmented reality applications require performing multiple tasks on each input frame simultaneously. Multi-task learning (MTL) represents an effective approach where multiple tasks share an encoder to extract representative features from the input frame, followed by task-specific decoders to generate predictions for each task. Generally, the shared encoder in MTL models needs to have a large representational capacity in order to generalize well to various tasks and input data, which has a negative effect on the inference latency. In this paper, we argue that due to the large variations in the complexity of the input frames, some computations might be unnecessary for the output. Therefore, we introduce AdaMTL, an adaptive framework that learns task-aware inference policies for the MTL models in an input-dependent manner. Specifically, we attach a task-aware lightweight policy network to the shared encoder and co-train it alongside the MTL model to recognize unnecessary computations. During runtime, our task-aware policy network decides which parts of the model to activate depending on the input frame and the target computational complexity. Extensive experiments on the PASCAL dataset demonstrate that AdaMTL reduces the computational complexity by 43% while improving the accuracy by 1.32% compared to single-task models. Combined with SOTA MTL methodologies, AdaMTL boosts the accuracy by 7.8% while improving the efficiency by 3.1X. When deployed on Vuzix M4000 smart glasses, AdaMTL reduces the inference latency and the energy consumption by up to 21.8% and 37.5%, respectively, compared to the static MTL model. Our code is publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Neseem_AdaMTL_Adaptive_Input-Dependent_Inference_for_Efficient_Multi-Task_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Neseem_AdaMTL_Adaptive_Input-Dependent_Inference_for_Efficient_Multi-Task_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MARRS: Modern Backbones Assisted Co-Training for Rapid and Robust Semi-Supervised Domain Adaptation",
        "author": "Saurabh Kumar Jain, Sukhendu Das",
        "abstract": "Semi-Supervised Domain Adaptation (SSDA) aims to develop domain invariant models from scarcely labeled target domain in addition to the fully labeled source domain. Current SSDA works are often applied in conjunction with ResNet34 backbone, which makes them overlook the advantages of utilizing other backbones. Hence, in this paper, we investigate the impact of employing different modern backbones in SSDA and propose a novel solution named Modern Backbones Assisted Co-training for Rapid and Robust Semi-Supervised Domain Adaptation (MARRS), that uses discriminative features of two modern backbones for training linear classifiers using the well established co-training framework. To induce diversity among classifiers for effective co-training, we propose a novel module that produces diversity at three levels, namely image, backbone, and feature distribution levels. Experiments reveal that MARRS not only achieves state-of-the-art performance across all popular SSDA datasets, but also drastically cuts the computation time compared to other SSDA approaches, making MARRS a rapid and robust solution for SSDA. We also provide extensive ablation experiments to verify our framework's vitality and primary design choices.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Jain_MARRS_Modern_Backbones_Assisted_Co-Training_for_Rapid_and_Robust_Semi-Supervised_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Jain_MARRS_Modern_Backbones_Assisted_Co-Training_for_Rapid_and_Robust_Semi-Supervised_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DynaShare: Task and Instance Conditioned Parameter Sharing for Multi-Task Learning",
        "author": "Elahe Rahimian, Golara Javadi, Frederick Tung, Gabriel Oliveira",
        "abstract": "Multi-task networks rely on effective parameter sharing to achieve robust generalization across tasks. In this paper, we present a novel parameter sharing method for multi-task learning that conditions parameter sharing on both the task and the intermediate feature representations at inference time. In contrast to traditional parameter sharing approaches, which fix or learn a deterministic sharing pattern during training and apply the same pattern to all examples during inference, we propose to dynamically decide which parts of the network to activate based on both the task and the input instance. Our approach learns a hierarchical gating policy consisting of a task-specific policy for coarse layer selection and gating units for individual input instances, which work together to determine the execution path at inference time. Experiments on the NYU v2, Cityscapes and MIMIC-III datasets demonstrate the potential of the proposed approach and its applicability across problem domains.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Rahimian_DynaShare_Task_and_Instance_Conditioned_Parameter_Sharing_for_Multi-Task_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Rahimian_DynaShare_Task_and_Instance_Conditioned_Parameter_Sharing_for_Multi-Task_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Recursions Are All You Need: Towards Efficient Deep Unfolding Networks",
        "author": "Rawwad Alhejaili, Motaz Alfarraj, Hamzah Luqman, Ali Al-Shaikhi",
        "abstract": "The use of deep unfolding networks in compressive sensing (CS) has seen wide success as they provide both simplicity and interpretability. However, since most deep unfolding networks are iterative, this incurs significant redundancies in the network. In this work, we propose a novel recursion-based framework to enhance the efficiency of deep unfolding models. First, recursions are used to effectively eliminate the redundancies in deep unfolding networks. Secondly, we randomize the number of recursions during training to decrease the overall training time. Finally, to effectively utilize the power of recursions, we introduce a learnable unit to modulate the features of the model based on both the total number of iterations and the current iteration index. To evaluate the proposed framework, we apply it to both ISTA-Net+ and COAST. Extensive testing shows that our proposed framework allows the network to cut down as much as 75% of its learnable parameters while mostly maintaining its performance, and at the same time, it cuts around 21% and 42% from the training time for ISTA-Net+ and COAST respectively. Moreover, when presented with a limited training dataset, the recursive models match or even outperform their respective non-recursive baseline. Codes and pretrained models are available at https://github.com/Rawwad-Alhejaili/Recursions-Are-All-You-Need.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Alhejaili_Recursions_Are_All_You_Need_Towards_Efficient_Deep_Unfolding_Networks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Alhejaili_Recursions_Are_All_You_Need_Towards_Efficient_Deep_Unfolding_Networks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "BlazeStyleGAN: A Real-Time On-Device StyleGAN",
        "author": "Haolin Jia, Qifei Wang, Omer Tov, Yang Zhao, Fei Deng, Lu Wang, Chuo-Ling Chang, Tingbo Hou, Matthias Grundmann",
        "abstract": "StyleGAN models have been widely adopted for generating and editing face images. Yet, few work investigated running StyleGAN models on mobile devices. In this work, we introduce BlazeStyleGAN --- to the best of our knowledge, the first StyleGAN model that can run in real-time on smartphones. We design an efficient synthesis network with the auxiliary head to convert features to RGB at each level of the generator, and only keep the last one at inference. We also improve the distillation strategy with a multi-scale perceptual loss using the auxiliary heads, and an adversarial loss for the student generator and discriminator. With these optimizations, BlazeStyleGAN can achieve real-time performance on high-end mobile GPUs. Experimental results demonstrate that BlazeStyleGAN generates high-quality face images and even mitigates some artifacts from the teacher model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Rethinking Dilated Convolution for Real-Time Semantic Segmentation",
        "author": "Roland Gao",
        "abstract": "The field-of-view is an important metric when designing a model for semantic segmentation. To obtain a large field-of-view, previous approaches generally choose to rapidly downsample the resolution, usually with average poolings or stride 2 convolutions. We take a different approach by using dilated convolutions with large dilation rates throughout the backbone, allowing the backbone to easily tune its field-of-view by adjusting its dilation rates, and show that it's competitive with existing approaches. To effectively use the dilated convolution, we show a simple upper bound on the dilation rate in order to not leave gaps in between the covolutional weights, and design an SE-ResNeXt inspired block structure that uses two parallel 3 x 3 convolutions with different dilation rates to preserve the local details. Manually tuning the dilation rates for every block can be difficult, so we also introduce a differentiable neural architecture search method that uses gradient descent to optimize the dilation rates. In addition, we propose a lightweight decoder that restores local information better than common alternatives. To demonstrate the effectiveness of our approach, our model RegSeg achieves competitive results on real-time Cityscapes and CamVid datasets. Using a T4 GPU with mixed precision, RegSeg achieves 78.3 mIOU on Cityscapes test set at 37 FPS, and 80.9 mIOU on CamVid test set at 112 FPS, both without ImageNet pretraining.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Gao_Rethinking_Dilated_Convolution_for_Real-Time_Semantic_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Gao_Rethinking_Dilated_Convolution_for_Real-Time_Semantic_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MIMMO: Multi-Input Massive Multi-Output Neural Network",
        "author": "Martin Ferianc, Miguel Rodrigues",
        "abstract": "Neural networks (NNs) have achieved superhuman accuracy in multiple tasks, but NNs predictions' certainty is often debatable, especially if confronted with out of training distribution data. Averaging predictions of an ensemble of NNs can recalibrate the certainty of the predictions, but an ensemble is computationally expensive to deploy in practice. Recently, a new hardware-efficient multi-input multi-output (MIMO) NN was proposed to fit an ensemble of independent NNs into a single NN. In this work, we propose the addition of early-exits to the MIMO architecture with inferred depth-wise weightings to produce multiple predictions for the same input, giving a more diverse ensemble. We denote this combination as MIMMO: a multi-input, massive multi-output NN and we show that it can achieve better accuracy and calibration compared to the MIMO NN, simultaneously fit more NNs and be similarly hardware efficient as MIMO or the early-exit ensemble.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Ferianc_MIMMO_Multi-Input_Massive_Multi-Output_Neural_Network_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Ferianc_MIMMO_Multi-Input_Massive_Multi-Output_Neural_Network_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Dataset Efficient Training With Model Ensembling",
        "author": "Yeonju Ro, Cong Xu, Agnieszka Ciborowska, Suparna Bhattacharya, Frankie Li, Martin Foltin",
        "abstract": "We propose a dataset-efficient deep learning training method by ensembling different models trained on different subsets. The ensembling method leverages the difficulty level of data samples to select subsets that are representative and diverse. The approach involves building a common base model with a random subset of data and then allotting different subsets to different models in an ensemble. The models are trained with their own subsets and then merged into a single model. We then propose an iterative multi-phase ensemble training that aggregates models in the ensemble more frequently and prevents divergence. The experiments on ResNet18 and ImageNet show that ensembling outperforms the no-ensemble case and achieves 64.8% accuracy with only 30% of dataset, saving 20 hours of training time in a single V100 GPU training experiment.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Ro_Dataset_Efficient_Training_With_Model_Ensembling_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Ro_Dataset_Efficient_Training_With_Model_Ensembling_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Language Models Are Causal Knowledge Extractors for Zero-Shot Video Question Answering",
        "author": "Hung-Ting Su, Yulei Niu, Xudong Lin, Winston H. Hsu, Shih-Fu Chang",
        "abstract": "Causal Video Question Answering (CVidQA) queries not only association or temporal relations but also causal relations in a video. Existing question synthesis methods pre-trained question generation (QG) systems on reading comprehension datasets with text descriptions as inputs. However, QG models only learn to ask association questions (e.g., \"what is someone doing...\") and result in inferior performance due to the poor transfer of association knowledge to CVidQA, which focuses on causal questions like \"why is someone doing ...\". Observing this, we proposed to exploit causal knowledge to generate question-answer pairs, and proposed a novel framework, Causal Knowledge Extraction from Language Models (CaKE-LM), leveraging causal commonsense knowledge from language models to tackle CVidQA. To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another (e.g., \"score a goal\" triggers \"soccer player kicking ball\") by prompting LM with the action (soccer player kicking ball) to retrieve the intention (to score a goal). CaKE-LM significantly outperforms conventional methods by 4% to 6% of zero-shot CVidQA accuracy on NExT-QA and Causal-VidQA datasets. We also conduct comprehensive analyses and provide key findings for future research.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Su_Language_Models_Are_Causal_Knowledge_Extractors_for_Zero-Shot_Video_Question_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Su_Language_Models_Are_Causal_Knowledge_Extractors_for_Zero-Shot_Video_Question_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Mutual Exclusive Modulator for Long-Tailed Recognition",
        "author": "Haixu Long, Xiaolin Zhang, Yanbin Liu, Zongtai Luo, Jianbo Liu",
        "abstract": "The long-tailed recognition (LTR) is the task of learning high-performance classifiers given extremely imbalanced training samples between categories. Most of the existing works address the problem by either enhancing the features of tail classes or re-balancing the classifiers to reduce the inductive bias. In this paper, we try to look into the root cause of the LTR task, i.e., training samples for each class are greatly imbalanced, and propose a straightforward solution. We split the categories into three groups, i.e., many, medium and few, according to the number of training images. The three groups of categories are separately predicted to reduce the difficulty for classification. This idea naturally arises a new problem of how to assign a given sample to the right class groups? We introduce a mutual exclusive modulator which can estimate the probability of an image belonging to each group. Particularly, the modulator consists of a light-weight module and learned with a mutual exclusive objective. Hence, the output probabilities of the modulator encode the data volume clues of the training dataset. They are further utilized as prior information to guide the prediction of the classifier. We conduct extensive experiments on multiple datasets, e.g., ImageNet-LT, PlaceLT and iNaturalist 2018 to evaluate the proposed approach. Our method achieves competitive performance compared to the state-of-the-art benchmarks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Long_Mutual_Exclusive_Modulator_for_Long-Tailed_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Long_Mutual_Exclusive_Modulator_for_Long-Tailed_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Zero-Shot Object Classification With Large-Scale Knowledge Graph",
        "author": "Kohei Shiba, Yusuke Mukuta, Tatsuya Harada",
        "abstract": "Zero-shot learning is used to predict unseen categories and can solve problems such as dealing with unseen categories that were not anticipated at the time of training and the lack of labeled datasets. One method for zero-shot object classification is to use a knowledge graph, which is a set of explicit knowledge. Because recognition is limited to the categories contained in the knowledge graph, and the relationships among categories are expected to be quantitatively and qualitatively richer depending on the graph size, it is desirable to handle a large-scale knowledge graph that contains as many categories as possible.We used a knowledge graph that contains approximately seven times as many categories as the knowledge graphs used mainly in existing research to enable the classification of a larger number of categories and to achieve more accurate recognition.When using a large-scale knowledge graph, the number of noisy nodes and edges is expected to increase.Therefore, we propose a method to extract useful information from the entire graph using positional relationships between categories and types of edges in the knowledge graph. We classify images that were earlier unclassifiable in existing research and show that the proposed data extraction method improves performance compared to using the entire graph.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Shiba_Zero-Shot_Object_Classification_With_Large-Scale_Knowledge_Graph_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Shiba_Zero-Shot_Object_Classification_With_Large-Scale_Knowledge_Graph_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Data-Efficient Fossil Segmentation via Model Editing",
        "author": "Indu Panigrahi, Ryan Manzuk, Adam Maloof, Ruth Fong",
        "abstract": "Most computer vision research focuses on datasets containing thousands of images of commonplace objects. However, many high-impact datasets, such as those in medicine and the geosciences, contain fine-grain objects that require domain-expert knowledge to recognize and are time-consuming to collect and annotate. As a result, these datasets contain few labeled images, and current machine vision models cannot train intensively on them. Originally introduced to correct large-language models, model-editing techniques in machine learning have been shown to improve model performance using only small amounts of data and additional training. Using a Mask R-CNN to segment ancient reef fossils in rock sample images, we present a two-part paradigm to improve fossil segmentation with few labeled images: we first identify model weaknesses using image perturbations and then mitigate those weaknesses using model editing. Specifically, we apply domain-informed image perturbations to expose the Mask R-CNN's inability to distinguish between different classes of fossils and its inconsistency in segmenting fossils with different textures. To address these shortcomings, we extend an existing model-editing method for correcting systematic mistakes in image classification to image segmentation with no additional labeled data needed and show its effectiveness in decreasing confusion between different kinds of fossils. We also highlight the best settings for model editing in our situation: making a single edit using all relevant pixels in one image (vs. using multiple images, multiple edits, or fewer pixels). Though we focus on fossil segmentation, our approach may be useful in other similar fine-grain segmentation problems where data is limited.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Panigrahi_Improving_Data-Efficient_Fossil_Segmentation_via_Model_Editing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Panigrahi_Improving_Data-Efficient_Fossil_Segmentation_via_Model_Editing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Neural Transformation Network To Generate Diverse Views for Contrastive Learning",
        "author": "Taekyung Kim, Debasmit Das, Seokeon Choi, Minki Jeong, Seunghan Yang, Sungrack Yun, Changick Kim",
        "abstract": "Recent unsupervised representation learning methods rely heavily on various transformations to generate distinctive views of given samples. Transformations for these views are generally defined manually, requiring significant human effort to design detailed configurations and validate practical efficacy. Furthermore, the diversity of these views is quite limited in scope causing the network to be invariant to only a small set of data transformations. To address these problems, we introduce a neural transformation network that learns to generate diverse views. Our proposed framework consists of an encoder-decoder network architecture that encodes semantic information and then randomly stylizes it with style amplification. However, such generative processes tend to cause degradation compared to the original images, which can harm the quality of the learned representation. To remedy this issue and generate more diverse styles, we use a linear augmentation between the generated view and the original image. Finally, we apply geometric transformations to aid in contrastive learning of representations. We evaluate the learned representations on various downstream vision tasks. Results show highly competitive recognition performance compared to the state-of-the-art methods that use learned views or hand-crafted views for representation learning.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Kim_Neural_Transformation_Network_To_Generate_Diverse_Views_for_Contrastive_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Kim_Neural_Transformation_Network_To_Generate_Diverse_Views_for_Contrastive_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "LSFSL: Leveraging Shape Information in Few-Shot Learning",
        "author": "Deepan Chakravarthi Padmanabhan, Shruthi Gowda, Elahe Arani, Bahram Zonooz",
        "abstract": "Few-shot learning (FSL) techniques seek to learn the underlying patterns in data using fewer samples, analogous to how humans learn from limited experience. In this limited-data scenario, the challenges associated with deep neural networks, such as shortcut learning and texture bias behaviors, are further exacerbated. Moreover, the significance of addressing shortcut learning is not yet fully explored in the few-shot setup. To address these issues, we propose LSFSL, which enforces the model to learn more generalizable features utilizing the implicit prior information present in the data. Through comprehensive analyses, we demonstrate that LSFSL-trained models are less vulnerable to alteration in color schemes, statistical correlations, and adversarial perturbations leveraging the global semantics in the data. Our findings highlight the potential of incorporating relevant priors in few-shot approaches to increase robustness and generalization.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Padmanabhan_LSFSL_Leveraging_Shape_Information_in_Few-Shot_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Padmanabhan_LSFSL_Leveraging_Shape_Information_in_Few-Shot_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "HNSSL: Hard Negative-Based Self-Supervised Learning",
        "author": "Wentao Zhu, Jingya Liu, Yufang Huang",
        "abstract": "Recently, learning from vast unlabeled data, especially self-supervised learning, has been emerging and attracting widespread attention. Self-supervised learning followed by supervised fine-tuning on a few labeled examples can significantly improve label efficiency and outperform standard supervised training using fully annotated data. In this work, we present a novel hard negative-based self-supervised deep learning paradigm, named HNSSL. Specifically, we design a student-teacher network to generate a multi-view of the data for self-supervised learning and integrate an online hard negative pair mining into the training. Then we derive a new triplet-type loss considering both positive sample pairs and online mined hard negative sample pairs. Extensive experiments demonstrate the effectiveness of the proposed method and its components on ILSVRC-2012 based on the same backbone network. Specifically, for the linear evaluation task, the proposed HNSSL with a ResNet-50 encoder achieves the top-1 accuracy of 77.1%, which outperforms its previous counterparts by 2.8%. For the semi-supervised learning task, HNSSL with a ResNet-50 encoder obtains the top-1 accuracy of 73.4%, which outperforms the previous ResNet-50 encoder-based semi-supervised learning results by 4.6% using only 10% labels. For the task of transfer learning with linear evaluation, HNSSL with a ResNet-50 encoder achieves the best accuracy on six of seven widely used transfer learning datasets, which averagely outperforms previous ResNet-50 encoder-based transfer learning results by 2.5%.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Zhu_HNSSL_Hard_Negative-Based_Self-Supervised_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Zhu_HNSSL_Hard_Negative-Based_Self-Supervised_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Cross-Domain Detection With Self-Supervised Learning",
        "author": "Kai Li, Curtis Wigington, Chris Tensmeyer, Vlad I. Morariu, Handong Zhao, Varun Manjunatha, Nikolaos Barmpalios, Yun Fu",
        "abstract": "Cross-Domain Detection (XDD) aims to train a domain-adaptive object detector using unlabeled images from a target domain and labeled images from a source domain. Existing approaches achieve this either by aligning the feature maps or the region proposals from the two domains, or by transferring the style of source images to that of target images. In this paper, rather than proposing another method following the existing lines, we introduce a new framework complementary to existing methods. Our framework unifies some popular Self-Supervised Learning (SSL) techniques (e.g., rotation angle prediction, strong/weak data augmentation, mean teacher modeling) and adapts them to the XDD task. Our basic idea is to leverage the unsupervised nature of these SSL techniques and apply them simultaneously across domains (source and target) and models (student and teacher). These SSL techniques can thus serve as shared bridges that facilitate knowledge transfer between domains. More importantly, as these techniques are independently applied in each domain, they are complementary to existing domain alignment techniques that relies on interactions between domains (e.g., adversarial alignment). We perform extensive analyses on these SSL techniques and show that they significantly improve the performance of existing methods. In addition, we reach comparable or even better performance than the state-of-the-art methods when integrating our framework with an old well-established method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Li_Improving_Cross-Domain_Detection_With_Self-Supervised_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Li_Improving_Cross-Domain_Detection_With_Self-Supervised_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Self-Supervised 3D Human Pose Estimation From a Single Image",
        "author": "Jose Sosa, David Hogg",
        "abstract": "We propose a new self-supervised method for predicting 3D human body pose from a single image. The prediction network is trained from a dataset of unlabelled images depicting people in typical poses and a set of unpaired 2D poses. By minimising the need for annotated data, the method has the potential for rapid application to pose estimation of other articulated structures (e.g. animals). The self-supervision comes from an earlier idea exploiting consistency between predicted pose under 3D rotation. Our method is a substantial advance on state-of-the-art self-supervised methods in training a mapping directly from images, without limb articulation constraints or any 3D empirical pose prior. We compare performance with state-of-the-art self-supervised methods using benchmark datasets that provide images and ground-truth 3D pose (Human3.6M, MPI-INF-3DHP). Despite the reduced requirement for annotated data, we show that the method outperforms on Human3.6M and matches performance on MPI-INF-3DHP. Qualitative results on a dataset of human hands show the potential for rapidly learning to predict 3D pose for articulated structures other than the human body.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Sosa_Self-Supervised_3D_Human_Pose_Estimation_From_a_Single_Image_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Sosa_Self-Supervised_3D_Human_Pose_Estimation_From_a_Single_Image_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Zero-Shot Action Recognition With Transformer-Based Video Semantic Embedding",
        "author": "Keval Doshi, Yasin Yilmaz",
        "abstract": "While video action recognition has been an active area of research for several years, zero-shot action recognition has only recently started gaining traction. In this work, we propose a novel end-to-end trained transformer model which is capable of capturing long range spatiotemporal dependencies efficiently, contrary to existing approaches which use 3D-CNNs. Moreover, to address a common ambiguity in the existing works about classes that can be considered as previously unseen, we propose a new experimentation setup that satisfies the zero-shot learning premise for action recognition by avoiding overlap between the training and testing classes. The proposed approach significantly outperforms the state of the arts in zero-shot action recognition in terms of the the top-1 accuracy on UCF-101, HMDB-51 and ActivityNet datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Doshi_Zero-Shot_Action_Recognition_With_Transformer-Based_Video_Semantic_Embedding_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Doshi_Zero-Shot_Action_Recognition_With_Transformer-Based_Video_Semantic_Embedding_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Posture-Based Infant Action Recognition in the Wild With Very Limited Data",
        "author": "Xiaofei Huang, Lingfei Luan, Elaheh Hatamimajoumerd, Michael Wan, Pooria Daneshvar Kakhaki, Rita Obeid, Sarah Ostadabbas",
        "abstract": "Automatic detection of infant actions from home videos could aid medical and behavioral specialists in the early detection of motor impairments in infancy. However, most computer vision approaches for action recognition are centered around adult subjects, following datasets and benchmarks in the field. In this work, we present a data-efficient pipeline for infant action recognition based on the idea of modeling an action as a time sequence consisting of two different stable postures with a transition period between them. The postures are detected frame-wise from the estimated 2D and 3D infant body poses and the action sequence is segmented based on the posture-driven low-dimensional features of each frame. To spur further research in the field, we also created and release the first-of-its-kind infant action dataset---InfAct---consisting of 200 fully annotated home videos representing a wide range of common infant actions, intended as a public benchmark. Among the ten more common classes of infant actions, our action recognition model achieved 78.0% accuracy when tested on InfAct, highlighting the promise of video-based infant action recognition as a viable monitoring tool for infant motor development.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Huang_Posture-Based_Infant_Action_Recognition_in_the_Wild_With_Very_Limited_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Huang_Posture-Based_Infant_Action_Recognition_in_the_Wild_With_Very_Limited_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SimDE: A Simple Domain Expansion Approach for Single-Source Domain Generalization",
        "author": "Qinwei Xu, Ruipeng Zhang, Yi-Yan Wu, Ya Zhang, Ning Liu, Yanfeng Wang",
        "abstract": "Single domain generalization challenges model generalizability to unseen target domains when only one source domain is provided for training. To tackle this problem, domain expansion is adopted to learn domain-invariant information by exposing the model to more domain variations, which is still under-explored in previous work. In this paper, we propose a new and simplified objective for learning the desirable domain expansions by generating unconfident samples through the combination of entropy maximization and cross-entropy minimization. We further devise a novel framework that trains a pair of generators from different views by switching the guidance from the dual classifiers. In this way, the resulting method called Simple Domain Expansion (SimDE) can learn diverse domain expansions effectively and efficiently. Extensive experiments on prevalent single domain generalization benchmarks demonstrate the superiority of our method by offering improved results over the state-of-the-arts methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Xu_SimDE_A_Simple_Domain_Expansion_Approach_for_Single-Source_Domain_Generalization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Xu_SimDE_A_Simple_Domain_Expansion_Approach_for_Single-Source_Domain_Generalization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MEnsA: Mix-Up Ensemble Average for Unsupervised Multi Target Domain Adaptation on 3D Point Clouds",
        "author": "Ashish Sinha, Jonghyun Choi",
        "abstract": "Unsupervised domain adaptation (UDA) addresses the problem of distribution shift between the unlabelled target domain and labelled source domain. While the single target domain adaptation (STDA) is well studied in the literature for both 2D and 3D vision tasks, multi-target domain adaptation (MTDA) is barely explored for 3D data despite its wide real-world applications such as autonomous driving systems for various geographical and climatic conditions. We establish an MTDA baseline for 3D point cloud data by proposing to mix the feature representations from all domains together to achieve better domain adaptation performance by an ensemble average, which we call Mixup Ensemble Average or MEnsA. With the mixed representation, we use a domain classifier to improve at distinguishing the feature representations of source domain from those of target domains in a shared latent space. In empirical validations on the challenging PointDA-10 dataset, we showcase a clear benefit of our simple method over previous unsupervised STDA and MTDA methods by large margins (up to 17.10% and 4.76% on averaged over all domain shifts).",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Sinha_MEnsA_Mix-Up_Ensemble_Average_for_Unsupervised_Multi_Target_Domain_Adaptation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Sinha_MEnsA_Mix-Up_Ensemble_Average_for_Unsupervised_Multi_Target_Domain_Adaptation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Self-Supervised Video Similarity Learning",
        "author": "Giorgos Kordopatis-Zilos, Giorgos Tolias, Christos Tzelepis, Ioannis Kompatsiaris, Ioannis Patras, Symeon Papadopoulos",
        "abstract": "We introduce S^2VS, a video similarity learning approach with self-supervision. Self-Supervised Learning (SSL) is typically used to train deep models on a proxy task so as to have strong transferability on target tasks after fine-tuning. Here, in contrast to prior work, SSL is used to perform video similarity learning and address multiple retrieval and detection tasks at once with no use of labeled data. This is achieved by learning via instance-discrimination with task-tailored augmentations and the widely used InfoNCE loss together with an additional loss operating jointly on self-similarity and hard-negative similarity. We benchmark our method on tasks where video relevance is defined with varying granularity, ranging from video copies to videos depicting the same incident or event. We learn a single universal model that achieves state-of-the-art performance on all tasks, surpassing previously proposed methods that use labeled data. The code and pretrained models are publicly available at: https://github.com/gkordo/s2vs",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Kordopatis-Zilos_Self-Supervised_Video_Similarity_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Kordopatis-Zilos_Self-Supervised_Video_Similarity_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Contrast, Stylize and Adapt: Unsupervised Contrastive Learning Framework for Domain Adaptive Semantic Segmentation",
        "author": "Tianyu Li, Subhankar Roy, Huayi Zhou, Hongtao Lu, St\u00e9phane Lathuili\u00e8re",
        "abstract": "To overcome the domain gap between synthetic and real-world datasets, unsupervised domain adaptation methods have been proposed for semantic segmentation. Majority of the previous approaches have attempted to reduce the gap either at the pixel or feature level, disregarding the fact that the two components interact positively. To address this, we present CONtrastive FEaTure and pIxel alignment (CONFETI) for bridging the domain gap at both the pixel and feature levels using a unique contrastive formulation. We introduce well-estimated prototypes by including category-wise cross-domain information to link the two alignments: the pixel-level alignment is achieved using the jointly trained style transfer module with the prototypical semantic consistency, while the feature-level alignment is enforced to cross-domain features with the pixel-to-prototype contrast. Our extensive experiments demonstrate that our method outperforms existing state-of-the-art methods using DeepLabV2. Our code has been made publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Li_Contrast_Stylize_and_Adapt_Unsupervised_Contrastive_Learning_Framework_for_Domain_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Li_Contrast_Stylize_and_Adapt_Unsupervised_Contrastive_Learning_Framework_for_Domain_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Incorporating Visual Grounding in GCN for Zero-Shot Learning of Human Object Interaction Actions",
        "author": "Chinmaya Devaraj, Cornelia Ferm\u00fcller, Yiannis Aloimonos",
        "abstract": "GCN-based zero-shot learning approaches commonly use fixed input graphs representing external knowledge that usually comes from language. However, such input graphs fail to incorporate the visual domain nuances. We introduce a method to ground the external knowledge graph visually. The method is demonstrated on a novel concept of grouping actions according to a shared notion and shown to be of superior performance in zero-shot action recognition on two challenging human manipulation action datasets, the EPIC Kitchens dataset, and the Charades dataset. We further show that visually grounding the knowledge graph enhances the performance of GCNs when an adversarial attack corrupts the input graph.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Devaraj_Incorporating_Visual_Grounding_in_GCN_for_Zero-Shot_Learning_of_Human_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Devaraj_Incorporating_Visual_Grounding_in_GCN_for_Zero-Shot_Learning_of_Human_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NamedMask: Distilling Segmenters From Complementary Foundation Models",
        "author": "Gyungin Shin, Weidi Xie, Samuel Albanie",
        "abstract": "The goal of this work is to segment and name regions of images without access to pixel-level labels during training. To tackle this task, we construct segmenters by distilling the complementary strengths of two foundation models. The first, CLIP, exhibits the ability to assign names to image content but lacks an accessible representation of object structure. The second, DINO, captures the spatial extent of objects but has no knowledge of object names. Our method, termed NamedMask, begins by using CLIP to construct category-specific archives of images. These images are pseudo-labelled with a category-agnostic salient object detector bootstrapped from DINO, then refined by category-specific segmenters using the CLIP archive labels. Thanks to the high quality of the refined masks, we show that a standard segmentation architecture trained on these archives with appropriate data augmentation achieves impressive semantic segmentation abilities for both single-object and multi-object images. As a result, our proposed NamedMask performs favourably against a range of prior work on five benchmarks including the VOC2012, COCO and large-scale ImageNet-S datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Shin_NamedMask_Distilling_Segmenters_From_Complementary_Foundation_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Shin_NamedMask_Distilling_Segmenters_From_Complementary_Foundation_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "In Defense of Structural Symbolic Representation for Video Event-Relation Prediction",
        "author": "Andrew Lu, Xudong Lin, Yulei Niu, Shih-Fu Chang",
        "abstract": "Understanding event relationships in videos requires a model to understand the underlying structures of events (i.e. the event type, the associated argument roles, and corresponding entities) and factual knowledge for reasoning. Structural symbolic representation (SSR) based methods directly take event types and associated argument roles/entities as inputs to perform reasoning. However, the state-of-the-art video event-relation prediction system shows the necessity of using continuous feature vectors from input videos; existing methods based solely on SSR inputs fail completely, even when given oracle event types and argument roles. In this paper, we conduct an extensive empirical analysis to answer the following questions: 1) why SSR-based method failed; 2) how to understand the evaluation setting of video event relation prediction properly; 3) how to uncover the potential of SSR-based methods. We first identify suboptimal training settings as causing the failure of previous SSR-based video event prediction models. Then through qualitative and quantitative analysis, we show how evaluation that takes only video as inputs is currently unfeasible, as well as the reliance on oracle event information to obtain an accurate evaluation. Based on these findings, we propose to further contextualize the SSR-based model to an Event-Sequence Model and equip it with more factual knowledge through a simple yet effective way of reformulating external visual commonsense knowledge bases into an event-relation prediction pretraining dataset. The resultant new state-of-the-art model eventually establishes a 25% Macro-accuracy performance boost.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Lu_In_Defense_of_Structural_Symbolic_Representation_for_Video_Event-Relation_Prediction_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Lu_In_Defense_of_Structural_Symbolic_Representation_for_Video_Event-Relation_Prediction_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Stream-Based Active Distillation for Scalable Model Deployment",
        "author": "Dani Manjah, Davide Cacciarelli, Baptiste Standaert, Mohamed Benkedadra, Gauthier Rotsart de Hertaing, Beno\u00eet Macq, St\u00e9phane Galland, Christophe De Vleeschouwer",
        "abstract": "This paper proposes a scalable technique for developing lightweight yet powerful models for object detection in videos using self-training with knowledge distillation. This approach involves training a compact student model using pseudo-labels generated by a computationally complex but generic teacher model, which can help to reduce the need for massive amounts of data and computational power. However, model-based annotations in large-scale applications may propagate errors or biases. To address these issues, our paper introduces Stream-Based Active Distillation (SBAD) to endow pre-trained students with effective and efficient fine-tuning methods that are robust to teacher imperfections. The proposed pipeline: (i) adapts a pre-trained student model to a specific use case, based on a set of frames whose pseudo-labels are predicted by the teacher, and (ii) selects on-the-fly, along a streamed video, the images that should be considered to fine-tune the student model. Various selection strategies are compared, demonstrating: 1) the effectiveness of implementing distillation with pseudo-labels, and 2) the importance of selecting images for which the pre-trained student detects with a high confidence.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Manjah_Stream-Based_Active_Distillation_for_Scalable_Model_Deployment_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Manjah_Stream-Based_Active_Distillation_for_Scalable_Model_Deployment_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Automatic Target Recognition in Low Data Regime Using Semi-Supervised Learning and Generative Data Augmentation",
        "author": "Fadoua Khmaissia, Hichem Frigui",
        "abstract": "We propose a new strategy to improve Automatic Target Recognition (ATR) from infrared (IR) images by leveraging semi-supervised learning and generative data augmentation. Our approach is twofold: first, we use an automatic detector's outputs to augment the existing labeled and unlabeled data. Second, we introduce a confidence-guided data generative augmentation technique that focuses on learning from the most challenging regions of the feature space, to generate synthetic data which can be used as extra unlabeled data. We evaluate the proposed approach on a public dataset with IR imagery of civilian and military vehicles. We show that yields substantial percentage improvements in ATR performance relative to both the baseline fully supervised model trained using the existing data only, and a semi-supervised model trained without generative data augmentation. For instance, for the most challenging data partition, our method achieves a relative increase of 29.51% over the baseline fully supervised model and a relative improvement of 2.59% over the semi-supervised model. These results demonstrate the effectiveness of our approach in low-data regimes, where labeled data is limited or expensive to obtain.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Khmaissia_Improving_Automatic_Target_Recognition_in_Low_Data_Regime_Using_Semi-Supervised_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Khmaissia_Improving_Automatic_Target_Recognition_in_Low_Data_Regime_Using_Semi-Supervised_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Leveraging Triplet Loss for Unsupervised Action Segmentation",
        "author": "Elena Bueno-Benito, Biel Tura Vecino, Mariella Dimiccoli",
        "abstract": "In this paper, we propose a novel fully unsupervised framework that learns action representations suitable for the action segmentation task from the single input video itself, without requiring any training data. Our method is a deep metric learning approach rooted in a shallow network with a triplet loss operating on similarity distributions and a novel triplet selection strategy that effectively models temporal and semantic priors to discover actions in the new representational space. Under these circumstances, we successfully recover temporal boundaries in the learned action representations with higher quality compared with existing unsupervised approaches. The proposed method is evaluated on two widely used benchmark datasets for the action segmentation task and it achieves competitive performance by applying a generic clustering algorithm on the learned representations.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Bueno-Benito_Leveraging_Triplet_Loss_for_Unsupervised_Action_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Bueno-Benito_Leveraging_Triplet_Loss_for_Unsupervised_Action_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "What Affects Learned Equivariance in Deep Image Recognition Models?",
        "author": "Robert-Jan Bruintjes, Tomasz Motyka, Jan van Gemert",
        "abstract": "Equivariance w.r.t. geometric transformations in neural networks improves data efficiency, parameter efficiency and robustness to out-of-domain perspective shifts. When equivariance is not designed into a neural network, the network can still learn equivariant functions from the data. We quantify this learned equivariance, by proposing an improved measure for equivariance. We find evidence for a correlation between learned translation equivariance and validation accuracy on ImageNet. We therefore investigate what can increase the learned equivariance in neural networks, and find that data augmentation, reduced model capacity and inductive bias in the form of convolutions induce higher learned equivariance in neural networks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Bruintjes_What_Affects_Learned_Equivariance_in_Deep_Image_Recognition_Models_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Bruintjes_What_Affects_Learned_Equivariance_in_Deep_Image_Recognition_Models_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Reliable Student: Addressing Noise in Semi-Supervised 3D Object Detection",
        "author": "Farzad Nozarian, Shashank Agarwal, Farzaneh Rezaeianaran, Danish Shahzad, Atanas Poibrenski, Christian M\u00fcller, Philipp Slusallek",
        "abstract": "Semi-supervised 3D object detection can benefit from the promising pseudo-labeling technique when labeled data is limited. However, recent approaches have overlooked the impact of noisy pseudo-labels during training, despite efforts to enhance pseudo-label quality through confidence-based filtering. In this paper, we examine the impact of noisy pseudo-labels on IoU-based target assignment and propose the Reliable Student framework, which incorporates two complementary approaches to mitigate errors. First, it involves a class-aware target assignment strategy that reduces false negative assignments in difficult classes. Second, it includes a reliability weighting strategy that suppresses false positive assignment errors while also addressing remaining false negatives from the first step. The reliability weights are determined by querying the teacher network for confidence scores of the student-generated proposals. Our work surpasses the previous state-of-the-art on KITTI 3D object detection benchmark on point clouds in the semi-supervised setting. On 1% labeled data, our approach achieves a 6.2% AP improvement for the pedestrian class, despite having only 37 labeled samples available. The improvements become significant for the 2% setting, achieving 6.0% AP and 5.7% AP improvements for the pedestrian and cyclist classes, respectively. Our code will be released at https://github.com/fnozarian/ReliableStudent",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Nozarian_Reliable_Student_Addressing_Noise_in_Semi-Supervised_3D_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Nozarian_Reliable_Student_Addressing_Noise_in_Semi-Supervised_3D_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "OWL (Observe, Watch, Listen): Audiovisual Temporal Context for Localizing Actions in Egocentric Videos",
        "author": "Merey Ramazanova, Victor Escorcia, Fabian Caba, Chen Zhao, Bernard Ghanem",
        "abstract": "Egocentric videos capture sequences of human activities from a first-person perspective and can provide rich multi-modal signals. However, most current localization methods use third-person videos and only incorporate visual information. In this work, we take a deep look into the effectiveness of audiovisual context in detecting actions in egocentric videos and introduce a simple-yet-effective approach via Observing, Watching, and Listening (OWL). OWL leverages audiovisual information and context for egocentric Temporal Action Localization (TAL). We validate our approach in two large-scale datasets, EPIC-KITCHENS and HOMAGE. Extensive experiments demonstrate the relevance of the audiovisual temporal context. Namely, we boost the localization performance (mAP) over visual-only models by +2.23% and +3.35% in the above datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Ramazanova_OWL_Observe_Watch_Listen_Audiovisual_Temporal_Context_for_Localizing_Actions_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Ramazanova_OWL_Observe_Watch_Listen_Audiovisual_Temporal_Context_for_Localizing_Actions_CVPRW_2023_paper.pdf"
    },
    {
        "title": "An Effective Crop-Paste Pipeline for Few-Shot Object Detection",
        "author": "Shaobo Lin, Kun Wang, Xingyu Zeng, Rui Zhao",
        "abstract": "Few-shot object detection (FSOD) aims to expand an object detector for novel categories given only a few instances for training. However, detecting novel categories with only a few samples usually leads to the problem of misclassification. In FSOD, we notice the false positive (FP) of novel categories is prominent, in which the base categories are often recognized as novel ones. To address this issue, a novel data augmentation pipeline that Crops the Novel instances and Pastes them on the selected Base images, called CNPB, is proposed. There are two key questions to be answered: (1) How to select useful base images? and (2) How to combine novel and base data? We design a multi-step selection strategy to find useful base data. Specifically, we first discover the base images which contain the FP of novel categories and select a certain amount of samples from them for the base and novel categories balance. Then the bad cases, such as the base images that have unlabeled ground truth or easily confused base instances, are removed by using CLIP. Finally, the same category strategy is adopted, in which a novel instance with category n is pasted on the base image with the FP of n. During combination, a novel instance is cropped and randomly down-sized, and thus pasted at the assigned optimal location from the randomly generated candidates in a selected base image. Our method is simple yet effective and can be easy to plug into existing FSOD methods, demonstrating significant potential for use. Extensive experiments on PASCAL VOC and MS COCO validate the effectiveness of our method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Lin_An_Effective_Crop-Paste_Pipeline_for_Few-Shot_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Lin_An_Effective_Crop-Paste_Pipeline_for_Few-Shot_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Impact of Pseudo Depth on Open World Object Segmentation With Minimal User Guidance",
        "author": "Robin Sch\u00f6n, Katja Ludwig, Rainer Lienhart",
        "abstract": "Pseudo depth maps are depth map predicitions which are used as ground truth during training. In this paper we leverage pseudo depth maps in order to segment objects of classes that have never been seen during training. This renders our object segmentation task an open world task. The pseudo depth maps are generated using pretrained networks, which have either been trained with the full intention to generalize to downstream tasks (LeRes and MiDaS), or which have been trained in an unsupervised fashion on video sequences (MonodepthV2). In order to tell our network which object to segment, we provide the network with a single click on the object's surface on the pseudo depth map of the image as input. We test our approach on two different scenarios: One without the RGB image and one where the RGB image is part of the input. Our results demonstrate a considerably better generalization performance from seen to unseen object types when depth is used. On the Semantic Boundaries Dataset we achieve an improvement from 61.57 to 69.79 IoU score on unseen classes, when only using half of the training classes during training and performing the segmentation on depth maps only.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Schon_Impact_of_Pseudo_Depth_on_Open_World_Object_Segmentation_With_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Schon_Impact_of_Pseudo_Depth_on_Open_World_Object_Segmentation_With_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Zero-Shot Unsupervised Transfer Instance Segmentation",
        "author": "Gyungin Shin, Samuel Albanie, Weidi Xie",
        "abstract": "Segmentation is a core computer vision competency, with applications spanning a broad range of scientifically and economically valuable domains. To date, however, the prohibitive cost of annotation has limited the deployment of flexible segmentation models. In this work, we propose Zero-shot Unsupervised Transfer Instance Segmentation (ZUTIS), a framework that aims to meet this challenge. The key strengths of ZUTIS are: (i) no requirement for instance-level or pixel-level annotations; (ii) an ability of zero-shot transfer, i.e., no assumption on access to a target data distribution; (iii) a unified framework for semantic and instance segmentations with solid performance on both tasks compared to state-or-the art unsupervised methods. While comparing to previous work, we show ZUTIS achieves a gain of 2.2 mask AP on COCO-20K and 14.5 mIoU on ImageNet-S with 919 categories for instance and semantic segmentations, respectively. Code will be made publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Shin_Zero-Shot_Unsupervised_Transfer_Instance_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Shin_Zero-Shot_Unsupervised_Transfer_Instance_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Asynchronous Federated Continual Learning",
        "author": "Donald Shenaj, Marco Toldo, Alberto Rigon, Pietro Zanuttigh",
        "abstract": "The standard class-incremental continual learning setting assumes a set of tasks seen one after the other in a fixed and predefined order. This is not very realistic in federated learning environments where each client works independently in an asynchronous manner getting data for the different tasks in time-frames and orders totally uncorrelated with the other ones. We introduce a novel federated learning setting (AFCL) where the continual learning of multiple tasks happens at each client with different orderings and in asynchronous time slots. We tackle this novel task using prototype-based learning, a representation loss, fractal pre-training, and a modified aggregation policy. Our approach, called FedSpace, effectively tackles this task as shown by the results on the CIFAR-100 dataset using 3 different federated splits with 50, 100, and 500 clients, respectively. The code and federated splits are available at https://github.com/LTTM/FedSpace.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Shenaj_Asynchronous_Federated_Continual_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/papers/Shenaj_Asynchronous_Federated_Continual_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Mixed Quantization Enabled Federated Learning To Tackle Gradient Inversion Attacks",
        "author": "Pretom Roy Ovi, Emon Dey, Nirmalya Roy, Aryya Gangopadhyay",
        "abstract": "Federated Learning (FL) enables collaborative model building among a large number of participants without the need for explicit data sharing. But this approach shows vulnerabilities when gradient inversion attacks are applied to it. FL models are at higher risk in the event of a gradient inversion attacks, which has a higher success rate in retrieving sensitive data from the model gradients, due to the presence of communication in their inherent architecture. The most alarming thing about this gradient inversion attack is that it can be performed in such a covert way that it does not hamper the training performance while the attackers backtrack from the gradients to get information about the raw data. Some of the common existing approaches proposed to prevent data reconstruction in the context of FL are adding noise with differential privacy, homomorphic encryption, and gradient pruning. These approaches suffer from some major drawbacks, including a tedious key generation process during encryption with an increasing number of clients, a significant performance drop, and difficulty in selecting a suitable pruning ratio. As a countermeasure, we propose a mixed quantization enabled FL scheme, and we empirically show that issues addressed above can be resolved. In addition, our approach can ensure more robustness as different layers of the deep model are quantized with different precisions and quantization modes. We empirically proved the validity of our defense method against both the iteration based and recursion based gradient inversion attacks and evaluated the performance of our proposed FL framework on three benchmark datasets and found out that our approach outperformed the baseline defense mechanisms.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Ovi_Mixed_Quantization_Enabled_Federated_Learning_To_Tackle_Gradient_Inversion_Attacks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/papers/Ovi_Mixed_Quantization_Enabled_Federated_Learning_To_Tackle_Gradient_Inversion_Attacks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "OpenFed: A Comprehensive and Versatile Open-Source Federated Learning Framework",
        "author": "Dengsheng Chen, Vince Junkai Tan, Zhilin Lu, Enhua Wu, Jie Hu",
        "abstract": "Recent developments in Artificial Intelligence techniques have enabled their successful application across a spectrum of commercial and industrial settings. However, these techniques require large volumes of data to be aggregated in a centralized manner, forestalling their applicability to scenarios wherein the data is sensitive or the cost of data transmission is prohibitive. Federated Learning alleviates these problems by decentralizing model training, thereby removing the need for data transfer and aggregation. To advance the adoption of Federated Learning, more research and development needs to be conducted to address some important open questions. In this work, we propose OpenFed, an open-source software framework for end-to-end Federated Learning. OpenFed reduces the barrier to entry for both researchers and downstream users of Federated Learning by the targeted removal of existing pain points. For researchers, OpenFed provides a framework wherein new methods can be easily implemented and fairly evaluated against an extensive suite of benchmarks. For downstream users, OpenFed allows Federated Learning to be plugged and play within different subject-matter contexts, removing the need for deep expertise in Federated Learning.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Chen_OpenFed_A_Comprehensive_and_Versatile_Open-Source_Federated_Learning_Framework_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/papers/Chen_OpenFed_A_Comprehensive_and_Versatile_Open-Source_Federated_Learning_Framework_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data",
        "author": "Huancheng Chen, Haris Vikalo",
        "abstract": "Federated learning (FL) is a privacy-promoting framework that enables potentially large number of clients to collaboratively train machine learning models. In an FL system, a server coordinates the collaboration by collecting and aggregating clients' model updates while the clients' data remains local and private. A major challenge in federated learning arises when the local data is non-iid -- the setting in which performance of the learned global model may deteriorate significantly compared to the scenario where the data is identically distributed across the clients. In this paper we propose FedDPMS (Federated Differentially Private Means Sharing), an FL algorithm in which clients augment local datasets with data synthesized using differentially private information collected and communicated by a trusted server. In particular, the server matches the pairs of clients having complementary local datasets and facilitates differentially-private sharing of the means of latent data representations; the clients then deploy variational auto-encoders to enrich their datasets and thus ameliorate the effects of non-iid data distribution. Our experiments on deep image classification tasks demonstrate that FedDPMS outperforms competing state-of-the-art FL methods specifically developed to address the challenge of federated learning on non-iid data.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Chen_Federated_Learning_in_Non-IID_Settings_Aided_by_Differentially_Private_Synthetic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/papers/Chen_Federated_Learning_in_Non-IID_Settings_Aided_by_Differentially_Private_Synthetic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TimelyFL: Heterogeneity-Aware Asynchronous Federated Learning With Adaptive Partial Training",
        "author": "Tuo Zhang, Lei Gao, Sunwoo Lee, Mi Zhang, Salman Avestimehr",
        "abstract": "In cross-device Federated Learning (FL) environments, scaling synchronous FL methods is challenging as stragglers hinder the training process. Moreover, the availability of each client to join the training is highly variable over time due to system heterogeneities and intermittent connectivity. Recent asynchronous FL methods (e.g., FedBuff) have been proposed to overcome these issues by allowing slower users to continue their work on local training based on stale models and to contribute to aggregation when ready. However, we show empirically that this method can lead to a substantial drop in training accuracy as well as a slower convergence rate. The primary reason is that fast-speed devices contribute to many more rounds of aggregation while others join more intermittently or not at all, and with stale model updates. To overcome this barrier, we propose TimelyFL, a heterogeneity-aware asynchronous FL framework with adaptive partial training. During the training, TimelyFL adjusts the local training workload based on the real-time resource capabilities of each client, aiming to allow more available clients to join in the global update without staleness. We demonstrate the performance benefits of TimelyFL by conducting extensive experiments on various datasets (e.g., CIFAR-10, Google Speech, and Reddit) and models (e.g., ResNet20, VGG11, and ALBERT). In comparison with the state-of-the-art (i.e., FedBuff), our evaluations reveal that TimelyFL improves participation rate by 21.13%, harvests 1.28x - 2.89x more efficiency on convergence rate, and provides a 6.25% increment on test accuracy.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Zhang_TimelyFL_Heterogeneity-Aware_Asynchronous_Federated_Learning_With_Adaptive_Partial_Training_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/papers/Zhang_TimelyFL_Heterogeneity-Aware_Asynchronous_Federated_Learning_With_Adaptive_Partial_Training_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Many-Task Federated Learning: A New Problem Setting and a Simple Baseline",
        "author": "Ruisi Cai, Xiaohan Chen, Shiwei Liu, Jayanth Srinivasa, Myungjin Lee, Ramana Kompella, Zhangyang Wang",
        "abstract": "Federated Learning (FL) was originally proposed to effectively exploit more data that are distributed at local clients even though the local data follow non-i.i.d. distributions. The fundamental intuition is that, the more data we can use the better model we are likely to obtain in spite of the increased difficulty of learning due to the non-i.i.d. data distribution, or data heterogeneity. With this intuition, we strive to further scale up FL to cover more clients to participate and increase the effective coverage of more user data, by enabling FL to handle collaboration between clients that perform different yet related task types, i.e., enabling a new level of heterogeneity: task heterogeneity, which can be entangled with data heterogeneity and lead to more intractable clients. However, solving such compound heterogeneities from both data and task levels raises major challenges, against the current global, static, and identical federated aggregation ways across clients. To tackle this new and challenging FL setting, we propose an intuitive clustering-based training baseline to tackle the significant data and task heterogeneities. Specifically, each agent dynamically infers its \"proximity\" with others by comparing their layer-wise weight updates sent to the server, and then flexibly determines how to aggregate weights with selected similar clients. We construct new testbeds to examine our novel problem setting and algorithm on two benchmark datasets in multi-task learning: NYU Depth and PASCAL-Context datasets. Extensive experiments demonstrate that our proposed method shows superiority over plain FL algorithms such as FedAvg and FedProx in the 5-task setting on Pascal-Context and even enables jointly federated learning over the combined set of PASCAL-Context and NYU Depth (9 tasks, 2 data domains). Codes are available at: https://github.com/VITA-Group/MaT-FL.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Cai_Many-Task_Federated_Learning_A_New_Problem_Setting_and_a_Simple_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FedVision/papers/Cai_Many-Task_Federated_Learning_A_New_Problem_Setting_and_a_Simple_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Visualizing Skiers' Trajectories in Monocular Videos",
        "author": "Matteo Dunnhofer, Luca Sordi, Christian Micheloni",
        "abstract": "Trajectories are fundamental to winning in alpine skiing. Tools enabling the analysis of such curves can enhance the training activity and enrich broadcasting content. In this paper, we propose SkiTraVis, an algorithm to visualize the sequence of points traversed by a skier during its performance. SkiTraVis works on monocular videos and constitutes a pipeline of a visual tracker to model the skier's motion and of a frame correspondence module to estimate the camera's motion. The separation of the two motions enables the visualization of the trajectory according to the moving camera's perspective. We performed experiments on videos of real-world professional competitions to quantify the visualization error, the computational efficiency, as well as the applicability. Overall, the results achieved demonstrate the potential of our solution for broadcasting media enhancement and coach assistance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Dunnhofer_Visualizing_Skiers_Trajectories_in_Monocular_Videos_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Dunnhofer_Visualizing_Skiers_Trajectories_in_Monocular_Videos_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Homography Based Player Identification in Live Sports",
        "author": "Yash Pandya, Kaustav Nandy, Shivam Agarwal",
        "abstract": "Modern live sports broadcasts display a wide variety of graphic visualizations identifying key players in a particular play. Traditionally, these graphics are created with extensive manual annotation for post-match analysis and take a significant amount of time to be produced. To create such visualizations in near real-time, automatic on-screen player identification and localization is essential. However, it is a challenging vision problem, especially for sports such as American football where the players wear elaborate protective equipment. In this work, we propose a novel approach which uses sensor data streams captured by wearables to automatically identify and locate on-screen players with low latency and high accuracy. The approach estimates a field registration homography using on-field player positions from RFID sensors, which is then used to identify and locate individual players on-screen. Experiments using American football data show that the method outperforms a deep learning based state-of-the-art(SOTA) vision-only field registration model both in terms of accuracy of the homography and also success rate of correct homography computation. On a dataset of over 150 replay clips, the proposed method correctly estimated the homography for approximately 25% additional clips as compared to the SOTA method. We demonstrate the efficacy of our method by applying it to the problem of rendering visualizations around key players within a few minutes of the live play. The player identification accuracy for these key players was over 96% across all clips, with an end-to-end latency of less than 1 minute.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Pandya_Homography_Based_Player_Identification_in_Live_Sports_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Pandya_Homography_Based_Player_Identification_in_Live_Sports_CVPRW_2023_paper.pdf"
    },
    {
        "title": "VARS: Video Assistant Referee System for Automated Soccer Decision Making From Multiple Views",
        "author": "Jan Held, Anthony Cioppa, Silvio Giancola, Abdullah Hamdi, Bernard Ghanem, Marc Van Droogenbroeck",
        "abstract": "The Video Assistant Referee (VAR) has revolutionized association football, enabling referees to review incidents on the pitch, make informed decisions, and ensure fairness. However, due to the lack of referees in many countries and the high cost of the VAR infrastructure, only professional leagues can benefit from it. In this paper, we propose a Video Assistant Referee System (VARS) that can automate soccer decision-making. VARS leverages the latest findings in multi-view video analysis, to provide real-time feedback to the referee, and help them make informed decisions that can impact the outcome of a game. To validate VARS, we introduce SoccerNet-MVFoul, a novel video dataset of soccer fouls from multiple camera views, annotated with extensive foul descriptions by a professional soccer referee, and we benchmark our VARS to automatically recognize the characteristics of these fouls. We believe that VARS has the potential to revolutionize soccer refereeing and take the game to new heights of fairness and accuracy across all levels of professional and amateur federations.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Held_VARS_Video_Assistant_Referee_System_for_Automated_Soccer_Decision_Making_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Held_VARS_Video_Assistant_Referee_System_for_Automated_Soccer_Decision_Making_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Towards Active Learning for Action Spotting in Association Football Videos",
        "author": "Silvio Giancola, Anthony Cioppa, Julia Georgieva, Johsan Billingham, Andreas Serner, Kerry Peek, Bernard Ghanem, Marc Van Droogenbroeck",
        "abstract": "Association football is a complex and dynamic sport, with numerous actions occurring simultaneously in each game. Analyzing football videos is challenging and requires identifying subtle and diverse spatio-temporal patterns. Despite recent advances in computer vision, current algorithms still face significant challenges when learning from limited annotated data, lowering their performance in detecting these patterns. In this paper, we propose an active learning framework that selects the most informative video samples to be annotated next, thus drastically reducing the annotation effort and accelerating the training of action spotting models to reach the highest accuracy at a faster pace. Our approach leverages the notion of uncertainty sampling to select the most challenging video clips to train on next, hastening the learning process of the algorithm. We demonstrate that our proposed active learning framework effectively reduces the required training data for accurate action spotting in football videos. We achieve similar performances for action spotting with NetVLAD++ on SoccerNet-v2, using only one-third of the dataset, indicating significant capabilities for reducing annotation time and improving data efficiency. We further validate our approach on two new datasets that focus on temporally localizing actions of headers and passes, proving its effectiveness across different action semantics in football. We believe our active learning framework for action spotting would support further applications of action spotting algorithms and accelerate annotation campaigns in the sports domain.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Giancola_Towards_Active_Learning_for_Action_Spotting_in_Association_Football_Videos_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Giancola_Towards_Active_Learning_for_Action_Spotting_in_Association_Football_Videos_CVPRW_2023_paper.pdf"
    },
    {
        "title": "NeighborTrack: Single Object Tracking by Bipartite Matching With Neighbor Tracklets and Its Applications to Sports",
        "author": "Yu-Hsi Chen, Chien-Yao Wang, Cheng-Yun Yang, Hung-Shuo Chang, Youn-Long Lin, Yung-Yu Chuang, Hong-Yuan Mark Liao",
        "abstract": "We propose a post-processor, called NeighborTrack, that leverages neighbor information of the tracking target to validate and improve single-object tracking (SOT) results. It requires no additional data or retraining. Instead, it uses the confidence score predicted by the backbone SOT network to automatically derive neighbor information and then uses this information to improve the tracking results. When tracking an occluded target, its appearance features are untrustworthy. However, a general siamese network often cannot tell whether the tracked object is occluded by reading the confidence score alone, because it could be misled by neighbors with high confidence scores. Our proposed NeighborTrack takes advantage of unoccluded neighbors' information to reconfirm the tracking target and reduces false tracking when the target is occluded. For the VOT challenge dataset commonly used in short-term object tracking, we improve three famous SOT networks, Ocean, TransT, and OSTrack, by an average of 1.92% EAO and 2.11% robustness. For the mid- and long-term tracking experiments based on OSTrack, we achieve state-of-the-art 72.25% AUC on LaSOT and 75.7% AO on GOT-10K. Most of the tracking examples we have used are related to sports.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Chen_NeighborTrack_Single_Object_Tracking_by_Bipartite_Matching_With_Neighbor_Tracklets_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Chen_NeighborTrack_Single_Object_Tracking_by_Bipartite_Matching_With_Neighbor_Tracklets_CVPRW_2023_paper.pdf"
    },
    {
        "title": "One-Shot Skeleton-Based Action Recognition on Strength and Conditioning Exercises",
        "author": "Michael Deyzel, Rensu P. Theart",
        "abstract": "There is a need in the sports and fitness industry for a practical system that can identify and understand human physical activity to enable intelligent workout feedback and virtual coaching. Such a system should be able to classify an athlete's actions from only limited examples since it is not feasible to collect a large quantity of human data for every action of interest. In this paper, we present SU-EMD, a novel dataset of skeleton motion sequences of seven common strength and conditioning exercises as captured by both a markerless and marker-based motion capture system. We then formulate the one-shot skeleton action recognition problem as a deep metric learning problem. We use the state-of-the-art graph convolutional network (GCN) to project dissimilar actions further away and similar actions closer together in the learned metric space. By training on NTU RGB+D 120, the metric GCN achieves a one-shot performance of 87.4% on all seven never-before-seen actions. In addition, an ablation study reveals the effect of different losses, embedding sizes and augmentations. Our results show that one-shot metric learning method can be used as a means to classify sports actions in a virtual coaching system where users cannot provide many expert examples for the enrolment of new actions.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Deyzel_One-Shot_Skeleton-Based_Action_Recognition_on_Strength_and_Conditioning_Exercises_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Deyzel_One-Shot_Skeleton-Based_Action_Recognition_on_Strength_and_Conditioning_Exercises_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SoccerNet-Caption: Dense Video Captioning for Soccer Broadcasts Commentaries",
        "author": "Hassan Mkhallati, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck",
        "abstract": "Soccer is more than just a game - it is a passion that transcends borders and unites people worldwide. From the roar of the crowds to the excitement of the commentators, every moment of a soccer match is a thrill. Yet, with so many games happening simultaneously, fans cannot watch them all live. Notifications for main actions can help, but lack the engagement of live commentary, leaving fans feeling disconnected. To fulfill this need, we propose in this paper a novel task of dense video captioning focusing on the generation of textual commentaries anchored with single timestamps. To support this task, we additionally present a challenging dataset consisting of almost 37k timestamped commentaries across 715.9 hours of soccer broadcast videos. Additionally, we propose a first benchmark and baseline for this task, highlighting the difficulty of temporally anchoring commentaries yet showing the capacity to generate meaningful commentaries. By providing broadcasters with a tool to summarize the content of their video with the same level of engagement as a live game, our method could help satisfy the needs of the numerous fans who follow their team but cannot necessarily watch the live game. We believe our method has the potential to enhance the accessibility and understanding of soccer content for a wider audience, bringing the excitement of the game to more people.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Mkhallati_SoccerNet-Caption_Dense_Video_Captioning_for_Soccer_Broadcasts_Commentaries_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Mkhallati_SoccerNet-Caption_Dense_Video_Captioning_for_Soccer_Broadcasts_Commentaries_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SPARTAN: Self-Supervised Spatiotemporal Transformers Approach to Group Activity Recognition",
        "author": "Naga VS Raviteja Chappa, Pha Nguyen, Alexander H. Nelson, Han-Seok Seo, Xin Li, Page Daniel Dobbs, Khoa Luu",
        "abstract": "In this paper, we propose a new, simple, and effective Self-supervised Spatio-temporal Transformers (SPARTAN) approach to Group Activity Recognition (GAR) using unlabeled video data. Given a video, we create local and global Spatio-temporal views with varying spatial patch sizes and frame rates. The proposed self-supervised objective aims to match the features of these contrasting views representing the same video to be consistent with the variations in spatiotemporal domains. To the best of our knowledge, the proposed mechanism is one of the first works to alleviate the weakly supervised setting of GAR using the encoders in video transformers. Furthermore, using the advantage of transformer models, our proposed approach supports long-term relationship modeling along spatio-temporal dimensions. The proposed SPARTAN approach performs well on two group activity recognition benchmarks, including NBA and Volleyball datasets, by surpassing the state-of-the-art results by a significant margin in terms of MCA and MPCA metrics.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Chappa_SPARTAN_Self-Supervised_Spatiotemporal_Transformers_Approach_to_Group_Activity_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Chappa_SPARTAN_Self-Supervised_Spatiotemporal_Transformers_Approach_to_Group_Activity_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "All Keypoints You Need: Detecting Arbitrary Keypoints on the Body of Triple, High, and Long Jump Athletes",
        "author": "Katja Ludwig, Julian Lorenz, Robin Sch\u00f6n, Rainer Lienhart",
        "abstract": "Performance analyses based on videos are commonly used by coaches of athletes in various sports disciplines. In individual sports, these analyses mainly comprise the body posture. This paper focuses on the disciplines of triple, high, and long jump, which require fine-grained locations of the athlete's body. Typical human pose estimation datasets provide only a very limited set of keypoints, which is not sufficient in this case. Therefore, we propose a method to detect arbitrary keypoints on the whole body of the athlete by leveraging the limited set of annotated keypoints and auto-generated segmentation masks of body parts. Evaluations show that our model is capable of detecting keypoints on the head, torso, hands, feet, arms, and legs, including also bent elbows and knees. We analyze and compare different techniques to encode desired keypoints as the model's input and their embedding for the Transformer backbone.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Ludwig_All_Keypoints_You_Need_Detecting_Arbitrary_Keypoints_on_the_Body_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Ludwig_All_Keypoints_You_Need_Detecting_Arbitrary_Keypoints_on_the_Body_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Combining Physics and Deep Learning Models To Simulate the Flight of a Golf Ball",
        "author": "William McNally, Jacob Lambeth, Dustin Brekke",
        "abstract": "We introduce a new golf ball flight model powered by deep learning. Our method combines a physics model with a deep learning model by inserting a neural network directly into the differential equations governing the projectile motion of the golf ball. The role of the neural network is to estimate the aerodynamic coefficients based on the state of the golf ball at each time step. The entire model was made end-to-end differentiable, permitting us to train the neural network using only measured launch conditions and landing positions. However, in experiments we find that using additional loss terms, such as the max height error, improves the accuracy of the predicted landing positions. The key to our approach is that we automatically learn the relationship between the aerodynamic coefficients and the state of the golf ball directly from the data as opposed to manually defining a model that imposes a bias. As a result, we are able to reduce the mean landing position error by 28% compared to a published model that learns the coefficients by fitting polynomials to the spin ratio. Our method is also computationally efficient, with a processing time of 35 ms for a single shot using a CPU.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/McNally_Combining_Physics_and_Deep_Learning_Models_To_Simulate_the_Flight_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/McNally_Combining_Physics_and_Deep_Learning_Models_To_Simulate_the_Flight_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Self-Supervised Video Interaction Classification Using Image Representation of Skeleton Data",
        "author": "Farzaneh Askari, Ruixi Jiang, Zhiwei Li, Jiatong Niu, Yuyan Shi, James J. Clark",
        "abstract": "Recognizing interactions from sports games broadcast videos is an application of Interaction Recognition from Videos (IRV), that offers many challenges due to complex interactions that are often recorded from a suboptimal view point. Annotating large scale sports specific datasets are expensive and time-consuming. Therefore, in this study, we propose to demonstrate the effectiveness of applying Self-Supervised Learning (SSL) methods for building useful representations from human skeleton pose data (pose for short) without requiring costly annotations for a large scale dataset. Given the numerous well established image-based SSL methods, we demonstrate how to adapt them for sequences of pose through data transformation and a series of pose-based augmentations. We specifically adapt the Relational Reasoning SSL (Relational-SSL for short) [27] and achieve 68.18 +- 0% and 76.62 +- 2.7% in linear evaluation and finetuning protocols, respectively, for the downstream task of IRV from sports broadcast videos. Lastly, we run ablation studies on different components of the method, including the effect of using estimated pose (versus ground truth) on the performance of the downstream task.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Askari_Self-Supervised_Video_Interaction_Classification_Using_Image_Representation_of_Skeleton_Data_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Askari_Self-Supervised_Video_Interaction_Classification_Using_Image_Representation_of_Skeleton_Data_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Monocular 3D Human Pose Estimation for Sports Broadcasts Using Partial Sports Field Registration",
        "author": "Tobias Baumgartner, Stefanie Klatt",
        "abstract": "The filming of sporting events projects and flattens the movement of athletes in the world onto a 2D broadcast image. The pixel locations of joints in these images can be detected with high validity. Recovering the actual 3D movement of the limbs (kinematics) of the athletes requires lifting these 2D pixel locations back into a third dimension, implying a certain scene geometry. The well-known line markings of sports fields allow for the calibration of the camera and for determining the actual geometry of the scene. Close-up shots of athletes are required to extract detailed kinematics, which in turn obfuscates the pertinent field markers for camera calibration. We suggest partial sports field registration, which determines a set of scene-consistent camera calibrations up to a single degree of freedom. Through joint optimization of 3D pose estimation and camera calibration, we demonstrate the successful extraction of 3D running kinematics on a 400m track. In this work, we combine advances in 2D human pose estimation and camera calibration via partial sports field registration to demonstrate an avenue for collecting valid large-scale kinematic datasets. We generate a synthetic dataset of more than 10k images in Unreal Engine 5 with different viewpoints, running styles, and body types, to show the limitations of existing monocular 3D HPE methods. Synthetic data and code are available at https://github.com/tobibaum/PartialSportsFieldReg_3DHPE.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Baumgartner_Monocular_3D_Human_Pose_Estimation_for_Sports_Broadcasts_Using_Partial_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Baumgartner_Monocular_3D_Human_Pose_Estimation_for_Sports_Broadcasts_Using_Partial_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Scale-Invariant Trajectory Simplification Method for Efficient Data Collection in Videos",
        "author": "Yang Liu, Luiz G. Hafemann",
        "abstract": "Training data is a critical requirement for machine learning tasks, and labeled training data can be expensive to acquire, often requiring manual or semi-automated data collection pipelines. For tracking applications, the data collection involves drawing bounding boxes around the classes of interest on each frame, and associate detections of the same \"instance\" over frames. In a semi-automated data collection pipeline, this can be achieved by running a baseline detection and tracking algorithm, and relying on manual correction to add/remove/change bounding boxes on each frame, as well as resolving errors in the associations over frames (track switches). In this paper, we propose a data correction pipeline to generate ground-truth data more efficiently in this semi-automated scenario. Our method simplifies the trajectories from the tracking systems and let the annotator verify and correct the objects in the sampled keyframes. Once the objects in the keyframes are corrected, the bounding boxes in the other frames are obtained by interpolation. Our method achieves substantial reduction in the number of frames requiring manual correction. In the MOT dataset, it reduces the number of frames by 30x while maintaining a HOTA score of 89.61%. Moreover, it reduces the number of frames by a factor of 10x while achieving a HOTA score of 79.24% in the SoccerNet dataset, and 85.79% in the DanceTrack dataset. The project code and data are publicly released at https://github.com/foreverYoungGitHub/trajectory-simplify-benchmark.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Liu_A_Scale-Invariant_Trajectory_Simplification_Method_for_Efficient_Data_Collection_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Liu_A_Scale-Invariant_Trajectory_Simplification_Method_for_Efficient_Data_Collection_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TemPose: A New Skeleton-Based Transformer Model Designed for Fine-Grained Motion Recognition in Badminton",
        "author": "Magnus Ibh, Stella Grasshof, Dan Witzner, Pascal Madeleine",
        "abstract": "This paper presents TemPose, a novel skeleton-based transformer model designed for fine-grained motion recognition to improve understanding of the detailed player actions in badminton. The model utilizes multiple temporal and interaction layers to capture variable-length multi-person human actions while minimizing reliance on non-human visual context. TemPose is evaluated on two fine-grained badminton datasets, where it significantly outperforms other baseline models by incorporating additional input streams, such as the shuttlecock position, into the temporal transformer layers of the model. Additionally, TemPose demonstrates great versatility by achieving competitive results compared to other state-of-the-art skeleton-based models on the large-scale action recognition benchmark NTU RGB+D. Experiments are conducted to explore how different model parameter configurations affect TemPose's performance. Additionally, a qualitative analysis of the temporal attention maps suggests that the model learns to prioritize frames of specific poses relevant to different actions while formulating an intuition of each individual's importance in the sequences. Overall, TemPose is an intuitive and versatile architecture that has the potential to be further developed and incorporated into other methods for managing human motion in sports with state-of-the-art results.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Ibh_TemPose_A_New_Skeleton-Based_Transformer_Model_Designed_for_Fine-Grained_Motion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Ibh_TemPose_A_New_Skeleton-Based_Transformer_Model_Designed_for_Fine-Grained_Motion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Human Spine Motion Capture Using Perforated Kinesiology Tape",
        "author": "Hendrik Hachmann, Bodo Rosenhahn",
        "abstract": "In this work, we present a marker-based multi-view spine tracking method that is specifically adjusted to the requirements for movements in sports. A maximal focus is on the accurate detection of markers and fast usage of the system. For this task, we take advantage of the prior knowledge of the arrangement of dots in perforated kinesiology tape. We detect the tape and its dots using a Mask R-CNN and a blob detector. Here, we can focus on detection only while skipping any image-based feature encoding or matching. We conduct a reasoning in 3D by a linear program and Markov random fields, in which the structure of the kinesiology tape is modeled and the shape of the spine is optimized. In comparison to state-of-the-art systems, we demonstrate that our system achieves high precision and marker density, is robust against occlusions, and capable of capturing fast movements.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Hachmann_Human_Spine_Motion_Capture_Using_Perforated_Kinesiology_Tape_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Hachmann_Human_Spine_Motion_Capture_Using_Perforated_Kinesiology_Tape_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SportsPose - A Dynamic 3D Sports Pose Dataset",
        "author": "Christian Keilstrup Ingwersen, Christian M\u00f8ller Mikkelstrup, Janus N\u00f8rtoft Jensen, Morten Rieger Hannemose, Anders Bjorholm Dahl",
        "abstract": "Accurate 3D human pose estimation is essential for sports analytics, coaching, and injury prevention. However, existing datasets for monocular pose estimation do not adequately capture the challenging and dynamic nature of sports movements. In response, we introduce SportsPose, a large-scale 3D human pose dataset consisting of highly dynamic sports movements. With more than 176,000 3D poses from 24 different subjects performing 5 different sports activities, SportsPose provides a diverse and comprehensive set of 3D poses that reflect the complex and dynamic nature of sports movements. Contrary to other markerless datasets we have quantitatively evaluated the precision of SportsPose by comparing our poses with a commercial marker-based system and achieve a mean error of 34.5 mm across all evaluation sequences. This is comparable to the error reported on the commonly used 3DPW dataset. We further introduce a new metric, local movement, which describes the movement of the wrist and ankle joints in relation to the body. With this, we show that SportsPose contains more movement than the Human3.6M and 3DPW datasets in these extremum joints, indicating that our movements are more dynamic. The dataset with accompanying code can be downloaded from our website 1. We hope that SportsPose will allow researchers and practitioners to develop and evaluate more effective models for the analysis of sports performance and injury prevention. With its realistic and diverse dataset, SportsPose provides a valuable resource for advancing the state-of-the-art in pose estimation in sports.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Ingwersen_SportsPose_-_A_Dynamic_3D_Sports_Pose_Dataset_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Ingwersen_SportsPose_-_A_Dynamic_3D_Sports_Pose_Dataset_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Real-Time Multi-Class Helmet Violation Detection Using Few-Shot Data Sampling Technique and YOLOv8",
        "author": "Armstrong Aboah, Bin Wang, Ulas Bagci, Yaw Adu-Gyamfi",
        "abstract": "Traffic safety is a major global concern. Helmet usage is a key factor in preventing head injuries and fatalities caused by motorcycle accidents. However, helmet usage violations continue to be a significant problem. To identify such violations, automatic helmet detection systems have been proposed and implemented using computer vision techniques. Real-time implementation of such systems is crucial for traffic surveillance and enforcement, however, most of these systems are not real-time. This study proposes a robust real-time helmet violation detection system. The proposed system utilizes a unique data processing strategy, referred to as few-shot data sampling, to develop a robust model with fewer annotations, and a single-stage object detection model, YOLOv8 (You Only Look Once Version 8), for detecting helmet violations in real-time from video frames. Our proposed method won 7th place in the 2023 AI City Challenge, Track 5, with an mAP score of 0.5861 on experimental validation data. The experimental results demonstrate the effectiveness, efficiency, and robustness of the proposed system.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Aboah_Real-Time_Multi-Class_Helmet_Violation_Detection_Using_Few-Shot_Data_Sampling_Technique_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Aboah_Real-Time_Multi-Class_Helmet_Violation_Detection_Using_Few-Shot_Data_Sampling_Technique_CVPRW_2023_paper.pdf"
    },
    {
        "title": "AdaptCD: An Adaptive Target Region-Based Commodity Detection System",
        "author": "Zeliang Ma, Delong Liu, Zhe Cui, Yanyun Zhao",
        "abstract": "With the rapid development of computer vision, the detection and counting of goods through computer vision techniques has become practicable. The AICity competition has focused its attention on the automatic recognition and counting of commodities, and has significantly propelled the advancement of this field through the organization of competitive events. Minimizing false positives and false negatives is critical to the success of this task. An Adaptive Target Region-based Commodity Detection System has been designed in this study to accurately identify the trajectory and category of goods. To alleviate the difference between training and testing data, two data augmentation methods are utilized, and various data synthesis methods are also designed to meet the training needs of different network models in the framework. Additional Adaptive Algorithms are designed to solve the problem of camera movement during product shooting. An Effective Fusion Algorithm is proposed for Dual Detectors to complement their advantages and minimize detection errors. To maximize the efficiency of the well-trained commodity classifier, an innovative Multi-layer Perception Fusion Module(MPFM) is devised to enhance the commodity classification capabilities, thereby generating more dependable features for tracking purposes. The system has been validated in Multi-Class Product Counting & Recognition for Automated Retail Checkout (2023 AI CITY Challenge Task4) competition, where our results achieve F1 Score of 0.9787 in Task4 testA, ranking second in 2023 AI CITY Challenge Task4. The code will be released at: https://github.com/mzl163/AICITY23_Task4",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Ma_AdaptCD_An_Adaptive_Target_Region-Based_Commodity_Detection_System_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Ma_AdaptCD_An_Adaptive_Target_Region-Based_Commodity_Detection_System_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Action Probability Calibration for Efficient Naturalistic Driving Action Localization",
        "author": "Rongchang Li, Cong Wu, Linze Li, Zhongwei Shen, Tianyang Xu, Xiao-jun Wu, Xi Li, Jiwen Lu, Josef Kittler",
        "abstract": "The task of naturalistic driving action localization carries significant safety implications, as it involves detecting and identifying possible distracting driving behaviors in untrimmed videos. Previous studies have demonstrated that action localization using a local snippet followed by probability-based post-processing, without any training cost or redundant structure, can outperform existing learning-based paradigms. However, the action probability is computed at the snippet-level, the input information near the boundaries is attenuated, and the snippet size is limited, which does not support the generation of more precise action boundaries. To tackle these challenges, we introduce an action probability calibration module that expands snippet-level action probability to the frame-level, based on a preset snippet position reliability, without incurring additional costs for probability prediction. The frame-level action probability and reliability enable the use of various snippet sizes and equal treatment for information of different temporal points. Additionally, based on the calibrated probability, we further design a category-customized filtering mechanism to eliminate the redundant action candidates. Our method ranks 2nd on the public leaderboard, and the code is available at https://github.com/RongchangLi/AICity2023_DrivingAction.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Li_Action_Probability_Calibration_for_Efficient_Naturalistic_Driving_Action_Localization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Li_Action_Probability_Calibration_for_Efficient_Naturalistic_Driving_Action_Localization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Comprehensive Visual Features and Pseudo Labeling for Robust Natural Language-Based Vehicle Retrieval",
        "author": "Bach Hoang Ngo, Dat Thanh Nguyen, Nhat-Tuong Do-Tran, Phuc Pham Huy Thien, Minh-Hung An, Tuan-Ngoc Nguyen, Loi Nguyen Hoang, Vinh Dinh Nguyen, Vinh Dinh",
        "abstract": "Vehicle retrieval has become crucial for public transportation and intelligent transportation systems due to the exponential development of large-scale transportation videos. Vehicle re-identification and vehicle tracking are the main components of most vision-based vehicle recovery systems. Unfortunately, the limited amount of information provided by traffic video feeds limits the vision-based vehicle retrieval algorithm's efficacy. Therefore, this article proposes a contrastive cross-modal vehicle retrieval approach to maximize the complementarity of natural language and visual representations. An efficient method to fuse multiple input image features is also proposed to extract comprehensive information from various vehicles along with pseudo labeling and efficient post-processing techniques to enhance retrieval accuracy. The proposed method achieved the 3rd ranking of Mean Reciprocal Rank (MRR) score of 0.4795 on the test set for the Challenge Track 2: Tracked-Vehicle Retrieval by Natural Language Descriptions 2023. Source code for the proposed approaches is openly accessible at https://github.com/anminhhung/AI-City-2023-Track2.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Ngo_Comprehensive_Visual_Features_and_Pseudo_Labeling_for_Robust_Natural_Language-Based_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Ngo_Comprehensive_Visual_Features_and_Pseudo_Labeling_for_Robust_Natural_Language-Based_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Enhancing Multi-Camera People Tracking With Anchor-Guided Clustering and Spatio-Temporal Consistency ID Re-Assignment",
        "author": "Hsiang-Wei Huang, Cheng-Yen Yang, Zhongyu Jiang, Pyong-Kun Kim, Kyoungoh Lee, Kwangju Kim, Samartha Ramkumar, Chaitanya Mullapudi, In-Su Jang, Chung-I Huang, Jenq-Neng Hwang",
        "abstract": "Multi-camera multiple people tracking has become an increasingly important area of research due to the growing demand for accurate and efficient indoor people tracking systems, particularly in settings such as retail, healthcare centers, and transit hubs. We proposed a novel multi-camera multiple people tracking method that uses anchor-guided clustering for cross-camera re-identification and spatio-temporal consistency for geometry-based cross-camera ID reassigning. Our approach aims to improve the accuracy of tracking by identifying key features that are unique to every individual and utilizing the overlap of views between cameras to predict accurate trajectories without needing the actual camera parameters. The method has demonstrated robustness and effectiveness in handling both synthetic and real-world data. The proposed method is evaluated on CVPR AI City Challenge 2023 dataset, achieving IDF1 of 95.36% with the first-place ranking in the challenge. The code is available at: https://github.com/ipl-uw/AIC23_Track1_UWIPL_ETRI.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Huang_Enhancing_Multi-Camera_People_Tracking_With_Anchor-Guided_Clustering_and_Spatio-Temporal_Consistency_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Huang_Enhancing_Multi-Camera_People_Tracking_With_Anchor-Guided_Clustering_and_Spatio-Temporal_Consistency_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Transformer-Based Fusion of 2D-Pose and Spatio-Temporal Embeddings for Distracted Driver Action Recognition",
        "author": "Erkut Akdag, Zeqi Zhu, Egor Bondarev, Peter H.N. de With",
        "abstract": "Classification and localization of driving actions over time is important for advanced driver-assistance systems and naturalistic driving studies. Temporal localization is challenging because it requires robustness, reliability, and accuracy. In this study, we aim to improve the temporal localization and classification accuracy performance by adapting video action recognition and 2D human-pose estimation networks to one model. Therefore, we design a transformer-based fusion architecture to effectively combine 2D-pose features and spatio-temporal features. The model uses 2D-pose features as the positional embedding of the transformer architecture and spatio-temporal features as the main input to the encoder of the transformer. The proposed solution is generic and independent of the camera numbers and positions, giving frame-based class probabilities as output. Finally, the post-processing step combines information from different camera views to obtain final predictions and eliminate false positives. The model performs well on the A2 test set of the 2023 NVIDIA AI City Challenge for naturalistic driving action recognition, achieving the overlap score of the organizer-defined distracted driver behaviour metric of 0.5079.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Akdag_Transformer-Based_Fusion_of_2D-Pose_and_Spatio-Temporal_Embeddings_for_Distracted_Driver_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Akdag_Transformer-Based_Fusion_of_2D-Pose_and_Spatio-Temporal_Embeddings_for_Distracted_Driver_CVPRW_2023_paper.pdf"
    },
    {
        "title": "The 7th AI City Challenge",
        "author": "Milind Naphade, Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching Chang, Yue Yao, Liang Zheng, Mohammed Shaiqur Rahman, Meenakshi S. Arya, Anuj Sharma, Qi Feng, Vitaly Ablavsky, Stan Sclaroff, Pranamesh Chakraborty, Sanjita Prajapati, Alice Li, Shangru Li, Krishna Kunadharaju, Shenxin Jiang, Rama Chellappa",
        "abstract": "The AI City Challenge's seventh edition emphasizes two domains at the intersection of computer vision and artificial intelligence - retail business and Intelligent Traffic Systems (ITS) - that have considerable untapped potential. The 2023 challenge had five tracks, which drew a record-breaking number of participation requests from 508 teams across 46 countries. Track 1 was a brand new track that focused on multi-target multi-camera (MTMC) people tracking, where teams trained and evaluated using both real and highly realistic synthetic data. Track 2 centered around natural-language-based vehicle track retrieval. Track 3 required teams to classify driver actions in naturalistic driving analysis. Track 4 aimed to develop an automated checkout system for retail stores using a single view camera. Track 5, another new addition, tasked teams with detecting violations of the helmet rule for motorcyclists. Two leader boards were released for submissions based on different methods: a public leader board for the contest where external private data wasn't allowed and a general leader board for all results submitted. The participating teams' top performances established strong baselines and even outperformed the state-of-the-art in the proposed challenge tracks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Naphade_The_7th_AI_City_Challenge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Naphade_The_7th_AI_City_Challenge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "M2DAR: Multi-View Multi-Scale Driver Action Recognition With Vision Transformer",
        "author": "Yunsheng Ma, Liangqi Yuan, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Zihao Li, Ziran Wang",
        "abstract": "Ensuring traffic safety and preventing accidents is a critical goal in daily driving, where the advancement of computer vision technologies can be leveraged to achieve this goal. In this paper, we present a multi-view, multi-scale framework for naturalistic driving action recognition and localization in untrimmed videos, namely M^2DAR, with a particular focus on detecting distracted driving behaviors. Our system features a weight-sharing, multi-scale Transformer-based action recognition network that learns robust hierarchical representations. Furthermore, we propose a new election algorithm consisting of aggregation, filtering, merging, and selection processes to refine the preliminary results from the action recognition module across multiple views. Extensive experiments conducted on the 7th AI City Challenge Track 3 dataset demonstrate the effectiveness of our approach, where we achieved an overlap score of 0.5921 on the A2 test set. Our source code is available at https://github.com/PurdueDigitalTwin/M2DAR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Ma_M2DAR_Multi-View_Multi-Scale_Driver_Action_Recognition_With_Vision_Transformer_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Ma_M2DAR_Multi-View_Multi-Scale_Driver_Action_Recognition_With_Vision_Transformer_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Unified Multi-Modal Structure for Retrieving Tracked Vehicles Through Natural Language Descriptions",
        "author": "Dong Xie, Linhu Liu, Shengjun Zhang, Jiang Tian",
        "abstract": "Through the development of multi-modal and contrastive learning, image and video retrieval have made immense progress over the last years. Organically fused text, image, and video knowledge brings huge potential opportunities for multi-dimension, and multi-view retrieval, especially in traffic senses. This paper proposes a novel Multi-modal Language Vehicle Retrieval (MLVR) system, for retrieving the trajectory of tracked vehicles based on natural language descriptions. The MLVR system is mainly combined with an end-to-end text-video contrastive learning model, a CLIP few-shot domain adaption method, and a semi-centralized control optimization system. Through a comprehensive understanding the knowledge from the vehicle type, color, maneuver, and surrounding environment, the MLVR forms a robust method to recognize an effective trajectory with provided natural language descriptions. Under this structure, our approach has achieved 81.79% Mean Reciprocal Rank (MRR) accuracy on the test dataset, in the 7th AI City Challenge Track 2, Tracked-Vehicle Retrieval by Natural Language Descriptions, rendering the 2nd rank on the public leaderboard.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Xie_A_Unified_Multi-Modal_Structure_for_Retrieving_Tracked_Vehicles_Through_Natural_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Xie_A_Unified_Multi-Modal_Structure_for_Retrieving_Tracked_Vehicles_Through_Natural_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robust Automatic Motorcycle Helmet Violation Detection for an Intelligent Transportation System",
        "author": "Duong Nguyen-Ngoc Tran, Long Hoang Pham, Hyung-Joon Jeon, Huy-Hung Nguyen, Hyung-Min Jeon, Tai Huu-Phuong Tran, Jae Wook Jeon",
        "abstract": "Video surveillance-based automatic detection of motorcycle helmet usage can enhance the effectiveness of educational and enforcement initiatives aimed at boosting road safety. Current detection methods, however, have room for enhancement, such as the inability to pinpoint individual motorcycles or differentiate between drivers and passengers in terms of helmet usage. This paper introduces a framework designed to detect and identify individual motorcycles while recording specific helmet usage for riders. The proposed classification approach for helmet usage demonstrates increased efficiency in comparison to previous research. Our findings highlight the exceptional accuracy of deep learning, with our method achieving a score of 0.7754 on the AI City 2023 Challenge Track 5 public leaderboard.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Tran_Robust_Automatic_Motorcycle_Helmet_Violation_Detection_for_an_Intelligent_Transportation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Tran_Robust_Automatic_Motorcycle_Helmet_Violation_Detection_for_an_Intelligent_Transportation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "An Effective Motorcycle Helmet Object Detection Framework for Intelligent Traffic Safety",
        "author": "Shun Cui, Tiantian Zhang, Hao Sun, Xuyang Zhou, Wenqing Yu, Aigong Zhen, Qihang Wu, Zhongjiang He",
        "abstract": "Detecting violations of motorcycle helmet rules is an important computer vision task that can greatly protect the lives of motorcycle drivers and passengers in traffic accidents. This abnormal event detection problem can be viewed as an image object detection task, which aims to detect the location of the motorcycle driver and passenger in the image and whether they are wearing helmets. In this paper, we propose a motorcycle helmet object detection (MHOD) framework to achieve this task. Specifically, we first utilize the object detection network with ensemble model to predict the location and category of all objects in videos which can improve the accuracy and robustness of detection model. Then for the scarcity of passenger category training data, the Passenger Recall Module (PRM) is designed via tracking refinement which greatly improves passenger category recall. Finally, we introduce the category refine module (CRM) to correct the category by combining the temporal information in the video. On the test dataset of AI City Challenge 2023 Track5, we achieve significant result compared with other teams, the proposed model ranks first on the public leaderboard of the challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Cui_An_Effective_Motorcycle_Helmet_Object_Detection_Framework_for_Intelligent_Traffic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Cui_An_Effective_Motorcycle_Helmet_Object_Detection_Framework_for_Intelligent_Traffic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Hierarchical Clustering and Refinement for Generalized Multi-Camera Person Tracking",
        "author": "Zongyi Li, Runsheng Wang, He Li, Bohao Wei, Yuxuan Shi, Hefei Ling, Jiazhong Chen, Boyuan Liu, Zhongyang Li, Hanqing Zheng",
        "abstract": "Multi-camera person tracking has gained significant attention in recent times, owing to its widespread application in surveillance scenarios. However, this task is challenging due to the viewpoint variance, heavy occlusion, and illumination changes. In order to tackle these challenges, we propose a novel Hierarchical Clustering and Refinement framework for Generalized Multi-Camera Person Tracking. Specifically, our framework comprises two main components: hierarchical clustering and hierarchical refinement. Compared with directly clustering tracklets among multiple cameras, our hierarchical clustering strategy can progressively assign tracklets to correct targets. Nevertheless, the clustering and tracking process would inevitably produce incorrect matchings. Therefore, a hierarchical refinement strategy is proposed to reduce these incorrect matchings which includes: intra-camera tracklet level refinement, appearance refinement, spatial-temporal refinement, and face refinement. Extensive experiments show the effectiveness of our method, which achieve 92% IDF1 in 2023 AI CITY CHALLENGE track1, ranking 5th on the leaderboard.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Li_Hierarchical_Clustering_and_Refinement_for_Generalized_Multi-Camera_Person_Tracking_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Li_Hierarchical_Clustering_and_Refinement_for_Generalized_Multi-Camera_Person_Tracking_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PRB-FPN+: Video Analytics for Enforcing Motorcycle Helmet Laws",
        "author": "Bor-Shiun Wang, Ping-Yang Chen, Yi-Kuan Hsieh, Jun-Wei Hsieh, Ming-Ching Chang, JiaXin He, Shin-You Teng, HaoYuan Yue, Yu-Chee Tseng",
        "abstract": "Road safety is of utmost importance, and helmet compliance for motorcyclists plays a crucial role in mitigating severe injuries and fatalities. Monitoring and enforcing helmet usage remains a challenge due to the sheer volume of motorcyclists and limited resources for enforcement. Real-time object detection technology offers a promising solution for monitoring helmet usage by effectively identifying motorcyclists and their adherence to helmet rules. However, accurately discerning helmeted motorcyclists and determining driver and passenger positions in complex real-world scenarios remains a challenge. In this paper, we present a novel two-step approach to address these challenges. First, we introduce the PRB-FPN+, a state-of-the-art detector that excels in object localization. We also explore the benefits of deep supervision by incorporating auxiliary heads within the network, leading to enhanced performance of our deep learning architectures. Second, we employ an advanced tracker to refine the tracking of drivers and passengers. Comprehensive experimental results demonstrate that our PRB-FPN+ outperforms current state-of-the-art methods, approaching the highest performance levels. Our proposed system achieved a rank of 8 on the Leaderboard in the AI City Challenge 2023 Track 5. This streamlined approach aims to provide a more reliable and accurate solution for monitoring and enforcing helmet rules among motorcyclists in challenging real-world environments.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Wang_PRB-FPN_Video_Analytics_for_Enforcing_Motorcycle_Helmet_Laws_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Wang_PRB-FPN_Video_Analytics_for_Enforcing_Motorcycle_Helmet_Laws_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CheckSORT: Refined Synthetic Data Combination and Optimized SORT for Automatic Retail Checkout",
        "author": "Ziqiang Shi, Zhongling Liu, Liu Liu, Rujie Liu, Takuma Yamamoto, Xiaoyu Mi, Daisuke Uchida",
        "abstract": "In this paper, we propose a method called CheckSORT for automatic retail checkout. We demonstrate CheckSORT on the multi-class product counting and recognition task in Track 4 of AI CITY CHALLENGE 2023. This task aims to count and identify products as they move along a retail checkout white tray, which is challenging due to occlusion, similar appearance, or blur. Based on the constraints and training data provided by the sponsor, we propose two new ideas to solve this task. The first idea is to design a controllable synthetic training data generation paradigm to bridge the gap between training data and real test videos as much as possible. The second innovation is to improve the efficiency of existing SORT tracking algorithms by proposing decomposed Kalman filter and dynamic tracklet feature sequence. Our experiments resulted in state-of-the-art (when compared with DeepSORT and StrongSORT) F1-scores of 70.3% and 62.1% on the TestA data of AI CITY CHALLENGE 2022 and 2023 respectively in the estimation of the time (in seconds) for the product to appear on the tray. Training and testing code will be available soon on github.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Shi_CheckSORT_Refined_Synthetic_Data_Combination_and_Optimized_SORT_for_Automatic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Shi_CheckSORT_Refined_Synthetic_Data_Combination_and_Optimized_SORT_for_Automatic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Helmet Rule Violation Detection for Motorcyclists Using a Custom Tracking Framework and Advanced Object Detection Techniques",
        "author": "Viet Hung Duong, Quang Huy Tran, Huu Si Phuc Nguyen, Duc Quyen Nguyen, Tien Cuong Nguyen",
        "abstract": "The use of helmets by motorcyclists is an effective way to reduce the risk of head injuries and fatalities in case of accidents. However, many countries still face the challenge of enforcing the helmet rule and ensuring compliance among riders. In this paper, we propose a novel framework that can differentiate between the driver and passengers and detect helmet rule violations for each rider empowered by computer vision and deep learning techniques. In the real-world scenario, there are many small and obstacle objects in each frame, which is a significant challenge, even with state-of-the-art detectors. To address this challenge, we employ an additional head detection module and a custom tracking algorithm that leverage auxiliary information such as moving direction, to improve detection performance on small and obstacle objects. This solution results in a significant improvement of 16% on mAP. Our complete framework achieves a final score of 69.97% on the 2023 AI City Challenge - Track 5 and ranks third among the competing teams.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Duong_Helmet_Rule_Violation_Detection_for_Motorcyclists_Using_a_Custom_Tracking_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Duong_Helmet_Rule_Violation_Detection_for_Motorcyclists_Using_a_Custom_Tracking_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Peer-to-Peer Federated Continual Learning for Naturalistic Driving Action Recognition",
        "author": "Liangqi Yuan, Yunsheng Ma, Lu Su, Ziran Wang",
        "abstract": "Naturalistic driving action recognition (NDAR) has proven to be an effective method for detecting driver distraction and reducing the risk of traffic accidents. However, the intrusive design of in-cabin cameras raises concerns about driver privacy. To address this issue, we propose a novel peer-to-peer (P2P) federated learning (FL) framework with continual learning, namely FedPC, which ensures privacy and enhances learning efficiency while reducing communication, computational, and storage overheads. Our framework focuses on addressing the clients' objectives within a serverless FL framework, with the goal of delivering personalized and accurate NDAR models. We demonstrate and evaluate the performance of FedPC on two real-world NDAR datasets, including the State Farm Distracted Driver Detection and Track 3 NDAR dataset in the 2023 AICity Challenge. The results of our experiments highlight the strong competitiveness of FedPC compared to the conventional client-to-server (C2S) FLs in terms of performance, knowledge dissemination rate, and compatibility with new clients.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Yuan_Peer-to-Peer_Federated_Continual_Learning_for_Naturalistic_Driving_Action_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Yuan_Peer-to-Peer_Federated_Continual_Learning_for_Naturalistic_Driving_Action_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Camera People Tracking With Mixture of Realistic and Synthetic Knowledge",
        "author": "Quang Qui-Vinh Nguyen, Huy Dinh-Anh Le, Truc Thi-Thanh Chau, Duc Trung Luu, Nhat Minh Chung, Synh Viet-Uyen Ha",
        "abstract": "This paper presents a solution for Track 1 of the AI City Challenge 2023, which involves Multi-Camera People Tracking in indoor scenarios. The proposed framework comprises four modules: Vehicle detection, ReID feature extraction, single-camera multi-target tracking (SCMT), single-camera matching, and multi-camera matching. A significant contribution of our approach is the introduction of ID switch detection and ID switch splitting using the Gaussian mixture model, which efficiently addresses the problem of tracklets with ID switches. Furthermore, our system performs well in matching both synthetic and real data. The proposed R-matching algorithm performs exceptionally well in real scenarios despite being trained on synthetic data. Experimental results on the public test set of 2023 AI City Challenge Track 1 demonstrate the efficacy of the proposed approach, achieving an IDF1 of 94.17% and securing 2nd position on the leaderboard. Codes will be available at https://github.com/nguyenquivinhquang/Multi-camera-People-Tracking-With-Mixture-of-Realistic-and-Synthetic-Knowledge",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Nguyen_Multi-Camera_People_Tracking_With_Mixture_of_Realistic_and_Synthetic_Knowledge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Nguyen_Multi-Camera_People_Tracking_With_Mixture_of_Realistic_and_Synthetic_Knowledge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Enhancing Retail Checkout Through Video Inpainting, YOLOv8 Detection, and DeepSort Tracking",
        "author": "Arpita Vats, David C. Anastasiu",
        "abstract": "The retail industry has witnessed a remarkable upswing in the utilization of cutting-edge artificial intelligence and computer vision techniques. Among the prominent challenges in this domain is the development of an automated checkout system that can address the multifaceted issues that arise in real-world checkout scenarios, including object occlusion, motion blur, and similarity in scanned items. In this paper, we propose a sophisticated deep learning-based framework that can effectively recognize, localize, track, and count products as they traverse in front of a camera. Our approach, which we call RetailCounter, is founded on a detect-then-track paradigm, wherein we apply tracking on the bounding box of the detected objects. Furthermore, we have incorporated an automatic identification of the detection region of interest (ROI) and efficient removal of unwanted objects from the ROI. The performance of our proposed framework is competitive, as evidenced by our F1 score of 0.8177 and the fourth-place ranking that we achieved in track 4 of the 2023 AI City Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Vats_Enhancing_Retail_Checkout_Through_Video_Inpainting_YOLOv8_Detection_and_DeepSort_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Vats_Enhancing_Retail_Checkout_Through_Video_Inpainting_YOLOv8_Detection_and_DeepSort_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Multi-Agent Motion Prediction With Heuristic Goals and Motion Refinement",
        "author": "Carlos G\u00f3mez-Hu\u00e9lamo, Marcos V. Conde, Rafael Barea, Luis M. Bergasa",
        "abstract": "Motion Prediction (MP) of multiple surrounding agents in physical environments, and accurate trajectory forecasting, is a crucial task for Autonomous Driving Stacks (ADS) and robots. Current methods for MP use end-to-end pipelines, where the input data is usually a HD map and the past trajectories of the most relevant agents; leveraging this information is a must to obtain optimal performance. In that sense, a reliable Autonomous Driving (AD) system must produce fast and accurate predictions to ensure traffic safety. In this work, we tackle Multi-Agent Motion Prediction using an end-to-end pipeline that combines Deep Learning (DL) and heuristic scene understanding. Our model uses as input the map of the scene, the past trajectories of the agents, and additional information about the scene geometry and agents e.g., type of agent, lane distribution. We design our model using powerful attention mechanisms with GNNs to enhance agents interactions, heuristic proposals as preliminary plausible information and a motion refinement module to further improve temporal consistency. We achieve SOTA results on the Argoverse 2 Motion Forecasting Benchmark reducing in millions of parameters previous methods such as GANet, and improving over LaneGCN. Our code is available at https://github.com/Cram3r95/argo2goalmp.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Conde_Improving_Multi-Agent_Motion_Prediction_With_Heuristic_Goals_and_Motion_Refinement_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Conde_Improving_Multi-Agent_Motion_Prediction_With_Heuristic_Goals_and_Motion_Refinement_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robust and Scalable Vehicle Re-Identification via Self-Supervision",
        "author": "Pirazh Khorramshahi, Vineet Shenoy, Rama Chellappa",
        "abstract": "Many state-of-the-art solutions for vehicle re-identification (re-id) mostly focus on improving the accuracy on existing re-id benchmarks using additional annotated data. To balance the demands of accuracy, availability of annotated data, and computational efficiency, we propose a simple yet effective hybrid solution empowered by self-supervised learning which is free of intricate and computationally-demanding add-on attention modules often seen in state-of-the-art approaches. Through extensive experimentation, we show our approach, termed Self-Supervised and Boosted VEhicle Re-Identification (SSBVER), is on par with state-of-the-art alternatives in terms of accuracy without introducing any additional overhead during deployment. Additionally, we show that our approach, generalizes to different backbone architectures which accommodates various resource constraints and consistently results in a significant accuracy boost. Our code is available at https://github.com/Pirazh/SSBVER.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Khorramshahi_Robust_and_Scalable_Vehicle_Re-Identification_via_Self-Supervision_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Khorramshahi_Robust_and_Scalable_Vehicle_Re-Identification_via_Self-Supervision_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Leveraging Future Trajectory Prediction for Multi-Camera People Tracking",
        "author": "Yuntae Jeon, Dai Quoc Tran, Minsoo Park, Seunghee Park",
        "abstract": "Artificial intelligence-based surveillance system, one of the essential systems for smart cities, plays a critical role in ensuring the safety and well-being of individuals. In this paper, we propose a real-time, low-computation cost Multi-Camera Multi-Target (MCMT) tracking system for people, leveraging deep-learning-based trajectory predic tion with spatial-temporal information and social informa tion. By predicting people's future trajectories, our al gorithm effectively handles object occlusion problems and maintains accurate tracking while keeping computational costs low. Our approach addresses object occlusion without relying on computationally expensive re-identification, and improves MCMT tracking performance using graph-based tracklet representation, and spectral clustering. As a re sult, our proposed approach is tested on the 2023 AI City Challenge Track 1 test dataset, automatically generated on the NVIDIA Omiverse Platform, our method achieves an IDF1 score of 0.6171 and real-time performance at 27.6 FPS. Code and pre-trained models are publicly available at https://github.com/yuntaeJ/SCIT-MCMT-Tracking.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Jeon_Leveraging_Future_Trajectory_Prediction_for_Multi-Camera_People_Tracking_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Jeon_Leveraging_Future_Trajectory_Prediction_for_Multi-Camera_People_Tracking_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ReidTrack: Reid-Only Multi-Target Multi-Camera Tracking",
        "author": "Andreas Specker, J\u00fcrgen Beyerer",
        "abstract": "Multi-target multi-camera tracking of persons in indoor scenarios such as retail stores or warehouses enables efficient placement of products and improvement of working processes. In this work, we propose the ReidTrack framework, which performs the task solely based on peoples' visual appearances. In theory, accurate person re-identification is able to solve the whole task without the need for additional and complex scene models or post-processing steps. ReidTrack is based on clustering appearance embeddings with a mechanism to avoid identity switches caused by detection bounding boxes showing the body parts of multiple individuals. With only a robust person re-identification model and the real-time detector YOLOv8 and without any auxiliary information, such as complex scene models, our approach ranks fourth concerning Track 1 of the 2023 AI City Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Specker_ReidTrack_Reid-Only_Multi-Target_Multi-Camera_Tracking_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Specker_ReidTrack_Reid-Only_Multi-Target_Multi-Camera_Tracking_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Addressing the Occlusion Problem in Multi-Camera People Tracking With Human Pose Estimation",
        "author": "Jeongho Kim, Wooksu Shin, Hancheol Park, Jongwon Baek",
        "abstract": "Multi-camera people tracking (MCPT) is a challenging task that is crucial for developing intelligent surveillance applications. In this work, we propose an MCPT system for Challenge Track 1 in the 2023 AI City Challenge. Specifically, we address the issue of occlusion, which causes significant changes in a person's appearance and makes it difficult to estimate their exact location on a global map of a given area. In this paper, we present several solutions that utilize human pose estimation for overcoming this challenge. Our experimental results demonstrate that using human pose estimation significantly improves the performance of our system. Furthermore, we achieved promising results on the official evaluation set, with an IDF1 score of 86.76%. Our code is publicly available at https://github.com/nota-github/AIC2023_Track1_Nota.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Kim_Addressing_the_Occlusion_Problem_in_Multi-Camera_People_Tracking_With_Human_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Kim_Addressing_the_Occlusion_Problem_in_Multi-Camera_People_Tracking_With_Human_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Integrating Appearance and Spatial-Temporal Information for Multi-Camera People Tracking",
        "author": "Wenjie Yang, Zhenyu Xie, Yaoming Wang, Yang Zhang, Xiao Ma, Bing Hao",
        "abstract": "Multi-Camera People Tracking (MCPT) is a crucial task in intelligent surveillance systems. However, it presents significant challenges due to issues such as heavy occlusion and variations in appearance that arise from multiple camera perspectives and congested scenarios. In this paper, we propose an effective system that integrates both appearance and spatial-temporal information to address these problems, consisting of three specially designed modules: (1) A Multi-Object Tracking (MOT) method that minimizes ID-switch errors and generates accurate trajectory appearance features for MCPT. (2) A robust intra-camera association method that leverages both appearance and spatial-temporal information. (3) An effective post-processing module comprising multi-step processing. Our proposed system is evaluated on the test set of Track1 for the 2023 AI CITY CHALLENGE, and the experimental results demonstrate its effectiveness, achieving an IDF1 score of 93.31% and ranking 3rd on the leaderboard.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Yang_Integrating_Appearance_and_Spatial-Temporal_Information_for_Multi-Camera_People_Tracking_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Yang_Integrating_Appearance_and_Spatial-Temporal_Information_for_Multi-Camera_People_Tracking_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Video Analytics for Detecting Motorcyclist Helmet Rule Violations",
        "author": "Chun-Ming Tsai, Jun-Wei Hsieh, Ming-Ching Chang, Guan-Lin He, Ping-Yang Chen, Wei-Tsung Chang, Yi-Kuan Hsieh",
        "abstract": "The use of helmets is essential for motorcyclists' safety, but non-compliance with helmet rules remains a common issue. In this study, we extend the frontier of AI video analytic technologies for detecting violations of helmet rules among motorcyclists. Our method can handle highly challenging conditions for traditional methods, including occlusions, fast vehicle movement, shadows, large viewing angles, poor illumination and weather conditions. We adopt the widely used YOLOv7 object detector and develop a first baseline using YOLOv7-E6E. We further develop two improved versions, namely YOLOv7-CBAM and YOLOv7-SimAM that better address the challenges. Experiments are performed on the 2023 AI City Challenge Track 5 contest benchmark. Evaluation on the 100 test videos of the contest demonstrates the effectiveness of our approach. The baseline YOLOv7-E6E model trained with image size 1920 achieves 0.6112 mAP. The YOLOv7-CBAM achieves 0.6389 mAP, and YOLOv7-SimAM achieves 0.6422 mAP, where both are trained with image size 1280. These models rank sixth, fifth, and fourth on the public leaderboard, respectively, which outperforms over 36 global participating teams. The code for our models is available at: https://github.com/cmtsai2023/AICITY2023_Track5_DVHRM.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Tsai_Video_Analytics_for_Detecting_Motorcyclist_Helmet_Rule_Violations_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Tsai_Video_Analytics_for_Detecting_Motorcyclist_Helmet_Rule_Violations_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Nordic Vehicle Dataset (NVD): Performance of Vehicle Detectors Using Newly Captured NVD From UAV in Different Snowy Weather Conditions.",
        "author": "Hamam Mokayed, Amirhossein Nayebiastaneh, Kanjar De, Stergios Sozos, Olle Hagner, Bj\u00f6rn Backe",
        "abstract": "Vehicle detection and recognition in drone images is a complex problem that has been used for different safety purposes. The main challenge of these images is captured at oblique angles and poses several challenges like non-uniform illumination effect, degradations, blur, occlusion, loss of visibility, etc. Additionally, weather conditions play a crucial role in causing safety concerns and add another high level of challenge to the collected data. Over the past few decades, various techniques have been employed to detect and track vehicles in different weather conditions. However, detecting vehicles in heavy snow is still in the early stages because of a lack of available data. Furthermore, there has been no research on detecting vehicles in snowy weather using real images captured by unmanned aerial vehicles (UAVs). This study aims to address this gap by providing the scientific community with data on vehicles captured by UAVs in different settings and under various snow cover conditions in the Nordic region. The data covers different adverse weather conditions like overcast with snowfall, low light and low contrast conditions with patchy snow cover, high brightness, sunlight, fresh snow, and the temperature reaching far below - 0 degrees Celsius. The study also evaluates the performance of commonly used object detection methods such as Yolo v8, Yolo v5, and fast RCNN. Additionally, data augmentation techniques are explored, and those that enhance the detectors' performance in such scenarios are proposed. The code and the dataset will be available at ( Github link will be shared after decision)",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Mokayed_Nordic_Vehicle_Dataset_NVD_Performance_of_Vehicle_Detectors_Using_Newly_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Mokayed_Nordic_Vehicle_Dataset_NVD_Performance_of_Vehicle_Detectors_Using_Newly_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Adaptive RoI With Pretrained Models for Automated Retail Checkout",
        "author": "Anudeep Dhonde, Prabhudev Guntur, Vinitha Palani",
        "abstract": "In this paper, we present a solution for automatic check\\-out in a retail store as a part of AI City Challenge 2023 Track 4. We propose a methodology which involves usage of pretrained Yolov5 models to detect person and media pipe models to detect hands of the person. This information is utilized to compute the Region of Interest (RoI) which is adaptive in nature. Afterwards, a custom trained object detection model is used detect products in the frame. We then use a tracker to track the products across video frames to avoid duplicated counting. The method is evaluated on the AI City challenge 2023 - Track 4 and gets the F1 score 0.6571 on the test A set, which places us on 6th place on the public leaderboard. The code is made public and available on GitHub.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Dhonde_Adaptive_RoI_With_Pretrained_Models_for_Automated_Retail_Checkout_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Dhonde_Adaptive_RoI_With_Pretrained_Models_for_Automated_Retail_Checkout_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeepSegmenter: Temporal Action Localization for Detecting Anomalies in Untrimmed Naturalistic Driving Videos",
        "author": "Armstrong Aboah, Ulas Bagci, Abdul Rashid Mussah, Neema Jakisa Owor, Yaw Adu-Gyamfi",
        "abstract": "Identifying unusual driving behaviors exhibited by drivers during driving is essential for understanding driver behavior and the underlying causes of crashes. Previous studies have primarily approached this problem as a classification task, assuming that naturalistic driving videos come discretized. However, both activity segmentation and classification are required for this task due to the continuous nature of naturalistic driving videos. The current study therefore departs from conventional approaches and introduces a novel methodological framework, DeepSegmenter, that simultaneously performs activity segmentation and classification in a single framework. The proposed framework consists of four major modules namely Data Module, Activity Segmentation Module, Classification Module and Postprocessing Module. Our proposed method won 8th place in the 2023 AI City Challenge, Track 3, with an activity overlap score of 0.5426 on experimental validation data. The experimental results demonstrate the effectiveness, efficiency, and robustness of the proposed system.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Aboah_DeepSegmenter_Temporal_Action_Localization_for_Detecting_Anomalies_in_Untrimmed_Naturalistic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Aboah_DeepSegmenter_Temporal_Action_Localization_for_Detecting_Anomalies_in_Untrimmed_Naturalistic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Triplet Temporal-Based Video Recognition With Multiview for Temporal Action Localization",
        "author": "Huy Duong Le, Minh Quan Vu, Manh Tung Tran, Nguyen Van Phuc",
        "abstract": "Temporal action localization (TAL) in untrimmed videos recently emerged as a crucial research topic, which has been applied in various applications such as surveillance, crowd monitoring, and driver distraction recognition. Most modern approaches in TAL divide this problem into two parts: i) feature extraction for action recognition; and ii) temporal boundary for action localization. In this study, we focus on improving the performance of the TAL task by exploiting the feature extraction effectively. Specifically, we present a temporal triplet algorithm in order to enhance temporal density-dependence information for the input video clips. Moreover, the multiview fusion framework is taken into account for enriching action representation. For the evaluation, we conduct the proposed method on the 2023 AI City Challenge Dataset. Accordingly, our method achieves competitive results and belongs to the top public leaderboard in Track 3 of the Challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Le_Triplet_Temporal-Based_Video_Recognition_With_Multiview_for_Temporal_Action_Localization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Le_Triplet_Temporal-Based_Video_Recognition_With_Multiview_for_Temporal_Action_Localization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Deep Learning-Based Automatic Checkout System Using Image Enhancement Techniques",
        "author": "Long Hoang Pham, Duong Nguyen-Ngoc Tran, Huy-Hung Nguyen, Hyung-Joon Jeon, Tai Huu-Phuong Tran, Hyung-Min Jeon, Jae Wook Jeon",
        "abstract": "The retail sector has experienced significant growth in artificial intelligence and computer vision applications, particularly with the emergence of automatic checkout (ACO) systems in stores and supermarkets. ACO systems encounter challenges such as object occlusion, motion blur, and similarity between scanned items while acquiring accurate training images for realistic checkout scenarios is difficult due to constant product updates. This paper improves existing deep learning-based ACO solutions by incorporating several image enhancement techniques in the data pre-processing step. The proposed ACO system employs a detect-and-track strategy, which involves: (1) detecting objects in areas of interest; (2) tracking objects in consecutive frames; and (3) counting objects using a track management pipeline. Several data generation techniques--including copy-and-paste, random placement, and augmentation--are employed to create diverse training data. Additionally, the proposed solution is designed as an open-ended framework that can be easily expanded to accommodate multiple tasks. The system has been evaluated on the AI City Challenge 2023 Track 4 dataset, showcasing outstanding performance by achieving a top-1 ranking on test-set A with an F1 score of 0.9792.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Pham_Improving_Deep_Learning-Based_Automatic_Checkout_System_Using_Image_Enhancement_Techniques_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Pham_Improving_Deep_Learning-Based_Automatic_Checkout_System_Using_Image_Enhancement_Techniques_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FishEye8K: A Benchmark and Dataset for Fisheye Camera Object Detection",
        "author": "Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Erkhembayar Ganbold, Jun-Wei Hsieh, Ming-Ching Chang, Ping-Yang Chen, Byambaa Dorj, Hamad Al Jassmi, Ganzorig Batnasan, Fady Alnajjar, Mohammed Abduljabbar, Fang-Pang Lin",
        "abstract": "With the advance of AI, road object detection has been a prominent topic in computer vision, mostly using perspective cameras. Fisheye lens provides omnidirectional wide coverage for using fewer cameras to monitor road intersections, however with view distortions. To our knowledge, there is no existing open dataset prepared for traffic surveillance on fisheye cameras. This paper introduces an open FishEye8K benchmark dataset for road object detection tasks, which comprises 157K bounding boxes across five classes (Pedestrian, Bike, Car, Bus, and Truck). In addition, we present benchmark results of State-of-The-Art (SoTA) models, including variations of YOLOv5, YOLOR, YOLO7, and YOLOv8. The dataset comprises 8,000 images recorded in 22 videos using 18 fisheye cameras for traffic monitoring in Hsinchu, Taiwan, at resolutions of 1080x1080 and 1280x1280. The data annotation and validation process were arduous and time-consuming, due to the ultra-wide panoramic and hemispherical fisheye camera images with large distortion and numerous road participants, particularly people riding scooters. To avoid bias, frames from a particular camera were assigned to either the training or test sets, maintaining a ratio of about 70:30 for both the number of images and bounding boxes in each class. Experimental results show that YOLOv8 and YOLOR outperform on input sizes 640x640 and 1280x1280, respectively. The dataset will be available on the GitHub link with PASCAL VOC, MS COCO, and YOLO annotation formats. The FishEye8K benchmark will provide significant contributions to the fisheye video analytics and smart city applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Gochoo_FishEye8K_A_Benchmark_and_Dataset_for_Fisheye_Camera_Object_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Gochoo_FishEye8K_A_Benchmark_and_Dataset_for_Fisheye_Camera_Object_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Attention Transformer for Naturalistic Driving Action Recognition",
        "author": "Xiaodong Dong, Ruijie Zhao, Hao Sun, Dong Wu, Jin Wang, Xuyang Zhou, Jiang Liu, Shun Cui, Zhongjiang He",
        "abstract": "To detect the start time and end time of each action in an untrimmed video in the Track 3 of AI City Challenge, this paper proposes a powerful network architecture, Multi-Attention Transformer. The previous methods extract features by setting a fixed sliding window whitch means a fixed time interval, and predict the start and end times of the action. We believe that adopting a series of fixed windows will corrupt the video feature containing contextual information. So we present a Multi-Attention transformer module which combines the local window attention and global attention to fix this problem. The method equipped with features provided by VideoMAE achieved a score of 66.34. Then use the time correction module to improve the score to 67.23 on validation set A2. Finally, we have achieved third place on Track3 A2 dataset of the AI City Challenge 2023.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Dong_Multi-Attention_Transformer_for_Naturalistic_Driving_Action_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Dong_Multi-Attention_Transformer_for_Naturalistic_Driving_Action_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Tracked-Vehicle Retrieval by Natural Language Descriptions With Multi-Contextual Adaptive Knowledge",
        "author": "Huy Dinh-Anh Le, Quang Qui-Vinh Nguyen, Duc Trung Luu, Truc Thi-Thanh Chau, Nhat Minh Chung, Synh Viet-Uyen Ha",
        "abstract": "This paper introduces our solution for Track 2 in AI City Challenge 2023. The task is tracked-vehicle retrieval by natural language descriptions with a real-world dataset of various scenarios and cameras. Our solution mainly focuses on four points: (1) To address the linguistic ambiguity in the language query, we leverage our proposed standardized version for text descriptions for the domain-adaptive training and post-processing stage. (2) Our baseline vehicle retrieval model utilizes CLIP to extract robust visual and textual feature representations to learn the unified cross-modal representations between textual and visual features. (3) Our proposed semi-supervised domain adaptive (SSDA) training method is leveraged to address the domain gap between the train and test set. (4) Finally, we propose a multi-contextual post-processing technique that prunes out the wrong results based on multi-contextual attributes information that effectively boosts the final retrieval results. Our proposed framework has yielded a competitive performance of 82.63% MRR accuracy on the test set, achieving 1st place in the competition. Codes will be available at https://github.com/zef1611/AIC23_NLRetrieval_HCMIU_CVIP",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Le_Tracked-Vehicle_Retrieval_by_Natural_Language_Descriptions_With_Multi-Contextual_Adaptive_Knowledge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Le_Tracked-Vehicle_Retrieval_by_Natural_Language_Descriptions_With_Multi-Contextual_Adaptive_Knowledge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DACNet: A Deep Automated Checkout Network With Selective Deblurring",
        "author": "Yichen Cai, Aoran Jiao",
        "abstract": "Automated checkout systems have become increasingly popular as the state-of-the-art deep learning models are efficient and accurate enough for this to become a reality. However, challenges still exist due to the differences between synthetic training data and real-life products, the blurred product images captured during the checkout process, and discontinuous detections due to product similarities or tracking misses. This paper presents a robust deep learning YOLO-based pipeline, DACNet, that counters the above challenges. During training, data augmentation involving overlaying training images onto expected backgrounds creates a more diverse and accurate training dataset. When inferencing, selective deblurring is also incorporated to enhance the clarity of the items to be recognized while maintaining efficiency. And to improve accuracy further, we introduced a retrospective checking algorithm that analyzes previous detections and corrects any inaccuracies due to flickering detections or incorrect tracking results. Together, this pipeline ensures a network that produces reliable training results and high prediction accuracies even in complex retail environments with multiple items present. The proposed method has been submitted to 2023 AI City Challenge by NVIDIA and achieved a top-3 finish on the test set A with an F1-score of 0.8254. Our code is open sourced here: https://github.com/cycv5/AICityChallenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Cai_DACNet_A_Deep_Automated_Checkout_Network_With_Selective_Deblurring_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Cai_DACNet_A_Deep_Automated_Checkout_Network_With_Selective_Deblurring_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi View Action Recognition for Distracted Driver Behavior Localization",
        "author": "Wei Zhou, Yinlong Qian, Zequn Jie, Lin Ma",
        "abstract": "This paper presents our approach for Track 3 (Naturalistic Driving Action Recognition) of the 2023 AI City Challenge, where the objective is to classify distracting driving activities in each untrimmed naturalistic driving video and localize the accurate temporal boundaries of them. Our solution relies on large model fine-tuning to train a base video recognition model on a small-scale video dataset. After that, we adopt multi-view multi-fold ensemble to produce fine-grained clip-level classification results. Given the recognition probabilities, a non-trivial clustering and removing post-processing algorithm is applied to generate final location proposals. Extensive experiments demonstrate that the proposed method achieves superior performance against other methods and rank the 1st on the Test-A2 of the challenge track.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Zhou_Multi_View_Action_Recognition_for_Distracted_Driver_Behavior_Localization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/papers/Zhou_Multi_View_Action_Recognition_for_Distracted_Driver_Behavior_Localization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TEVAD: Improved Video Anomaly Detection With Captions",
        "author": "Weiling Chen, Keng Teck Ma, Zi Jian Yew, Minhoe Hur, David Aik-Aun Khoo",
        "abstract": "Video surveillance systems are used to enhance the public safety and private assets. Automatic anomaly detection is vital in such surveillance systems to reduce the human labor and its associated costs. Previous works only consider spatial-temporal features. In many complex real-world scenarios, such visual features are unable to capture the semantic meanings required to further improve accuracy. To deal with such issues, we propose a novel framework: Text Empowered Video Anomaly Detection (TEVAD) which utilizes both visual and text features. Text features complements the visual features as they are semantically rich. Specifically, we compute text features based on the captions of the videos to capture the semantic meanings of abnormal events and thus improve the overall performance of video anomaly detection. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art results on four benchmark datasets (i.e. ShanghaiTech, UCF-Crime, XD-Violence, and UCSD-Pedestrians) and achieves improved robustness. We further analyze the captions to provide additional explainability for the anomalous videos identified by our proposed algorithm. Our codes are available at https://github.com/coranholmes/TEVAD.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Chen_TEVAD_Improved_Video_Anomaly_Detection_With_Captions_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Chen_TEVAD_Improved_Video_Anomaly_Detection_With_Captions_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Curriculum Learning for Data-Efficient Vision-Language Alignment",
        "author": "Tejas Srinivasan, Xiang Ren, Jesse Thomason",
        "abstract": "Aligning image and text encoders from scratch using contrastive learning requires large amounts of paired image-text data. We alleviate this need by aligning individually pre-trained language and vision representation models using a much smaller amount of paired data with a curriculum learning algorithm to learn fine-grained vision-language alignments. TOnICS (Training with Ontology-Informed Contrastive Sampling) initially samples minibatches whose image-text pairs contain a wide variety of objects to learn object-level vision-language alignment, and progressively samples minibatches where all image-text pairs contain the same object to learn finer-grained contextual alignment. Aligning pre-trained BERT and VinVL-OD models to each other using TOnICS outperforms CLIP on downstream zero-shot image retrieval using < 1% as much training data.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Srinivasan_Curriculum_Learning_for_Data-Efficient_Vision-Language_Alignment_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Srinivasan_Curriculum_Learning_for_Data-Efficient_Vision-Language_Alignment_CVPRW_2023_paper.pdf"
    },
    {
        "title": "BMRN: Boundary Matching and Refinement Network for Temporal Moment Localization With Natural Language",
        "author": "Muah Seol, Jonghee Kim, Jinyoung Moon",
        "abstract": "Temporal moment localization (TML) aims to retrieve the best moment in a video that matches a given sentence query. This task is challenging as it requires understanding the relationship between a video and a sentence, as well as the semantic meaning of both. TML methods using 2D temporal maps, which represent proposal features or scores on all moment proposals with the boundary of start and end times on the m and n axes, have shown performance improvements by modeling moment proposals in relation to each other. The methods, however, are limited by the coarsely pre-defined fixed boundaries of target moments, which depend on the length of training videos and the amount of memory available. To overcome this limitation, we propose a boundary matching and refinement network (BMRN) that generates 2D boundary matching and refinement maps along with a proposal feature map to obtain the final proposal score map. Our BMRN adjusts the fixed boundaries of moment proposals with predicted center and length offsets from boundary refinement maps. In addition, we introduce a length-aware proposal feature map that combines a cross-modal feature map and a similarity map between the predicted duration of the target moment and moment proposals. Our approach leads to improved TML performance on Charades-STA and ActivityNet Captions datasets, outperforming state-of-the-art methods by a large margin.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Seol_BMRN_Boundary_Matching_and_Refinement_Network_for_Temporal_Moment_Localization_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Seol_BMRN_Boundary_Matching_and_Refinement_Network_for_Temporal_Moment_Localization_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Making the V in Text-VQA Matter",
        "author": "Shamanthak Hegde, Soumya Jahagirdar, Shankar Gangisetty",
        "abstract": "Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question-answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict biased answers due to the lack of understanding of visual context. For example, in questions like \"What is written on the signboard?\", the answer predicted by the model is always \"STOP\" which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA. Specifically, we combine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Such a simple, yet effective approach increases the understanding and correlation between the image features and text present in the image, which helps in the better answering of questions. We further test the model on different datasets and compare their qualitative and quantitative results.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Hegde_Making_the_V_in_Text-VQA_Matter_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Hegde_Making_the_V_in_Text-VQA_Matter_CVPRW_2023_paper.pdf"
    },
    {
        "title": "T2V2T: Text-to-Video-to-Text Fusion for Text-to-Video Retrieval",
        "author": "Jonghee Kim, Youngwan Lee, Jinyoung Moon",
        "abstract": "Video-language transformers for text-to-video retrieval typically consist of a video encoder, a text encoder, and a joint encoder. The joint encoder can be categorized into 1) self-attention-based fusion and 2) unidirectional fusion based on cross-attention. The former approach performs self-attention on the concatenation of video and text embeddings. Although it allows complete interaction between text and video, the length of the input sequences makes it computationally intensive. Instead, unidirectional fusion employs rather efficient cross-attention to fuse video embeddings into text embeddings while ignoring text-to-video interaction. The text-to-video fusion is not well explored because of the information imbalance between text and video, which makes it difficult to determine which video patches can be used as queries in cross-attention. i.e., a text embedding corresponds to one or more patch embeddings, while a video patch embedding may not correspond to any text embeddings. In order to address this challenge, we devise a Bypass cross-attention (Bypass CA) which prevents matching between irrelevant video and text embedding pairs in the cross-attention. Using Bypass CA, we propose a novel bidirectional interaction approach, Text-to-Video-to-Text (T2V2T) fusion. The proposed T2V2T uses two unidirectional fusions with opposite directions, i.e., text-to-video fusion followed by video-to-text fusion. As a result, the proposed T2V2T fusion yields state-of-the-art results on MSR-VTT, DiDeMo, and ActivityNet Captions.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Kim_T2V2T_Text-to-Video-to-Text_Fusion_for_Text-to-Video_Retrieval_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Kim_T2V2T_Text-to-Video-to-Text_Fusion_for_Text-to-Video_Retrieval_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CLIP-Guided Vision-Language Pre-Training for Question Answering in 3D Scenes",
        "author": "Maria Parelli, Alexandros Delitzas, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, Thomas Hofmann",
        "abstract": "Training models to apply linguistic knowledge and visual concepts from 2D images to 3D world understanding is a promising direction that researchers have only recently started to explore. In this work, we design a novel 3D pre-training Vision-Language method that helps a model learn semantically meaningful and transferable 3D scene point cloud representations. We inject the representational power of the popular CLIP model into our 3D encoder by aligning the encoded 3D scene features with the corresponding 2D image and text embeddings produced by CLIP. To assess our model's 3D world reasoning capability, we evaluate it on the downstream task of 3D Visual Question Answering. Experimental quantitative and qualitative results show that our pre-training method outperforms state-of-the-art works in this task and leads to an interpretable representation of 3D scene features.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Parelli_CLIP-Guided_Vision-Language_Pre-Training_for_Question_Answering_in_3D_Scenes_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Parelli_CLIP-Guided_Vision-Language_Pre-Training_for_Question_Answering_in_3D_Scenes_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Weakly Supervised Visual Question Answer Generation",
        "author": "Charani Alampalle, Shamanthak Hegde, Soumya Jahagirdar, Shankar Gangisetty",
        "abstract": "Growing interest in conversational agents promote two-way human-computer communications involving asking and answering visual questions have become an active area of research in AI. Thus, generation of visual question-answer pair(s) becomes an important and challenging task. To address this issue, we propose a weakly-supervised visual question answer generation method that generates a relevant question-answer pairs for a given input image and associated caption. Most of the prior works are supervised and depend on the annotated question-answer datasets. In our work, we present a weakly supervised method that synthetically generates question-answer pairs procedurally from visual information and captions. The proposed method initially extracts list of answer words, then does nearest question generation that uses the caption and answer word to generate synthetic question. Next, the relevant question generator converts the nearest question to relevant language question by dependency parsing and in-order tree traversal, finally, fine-tune a ViLBERT model with the question-answer pair(s) generated at end. We perform an exhaustive experimental analysis on VQA dataset and see that our model significantly outperform SOTA methods on BLEU scores. We also show the results wrt baseline models and ablation study.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Alampalle_Weakly_Supervised_Visual_Question_Answer_Generation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Alampalle_Weakly_Supervised_Visual_Question_Answer_Generation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Language-Supervised Object Detection With Linguistic Structure Analysis",
        "author": "Arushi Rai, Adriana Kovashka",
        "abstract": "Language-supervised object detection typically uses descriptive captions from human-annotated datasets. However, in-the-wild captions take on wider styles of language. We analyze one particular ubiquitous form of language: narrative. We study the differences in linguistic structure and visual-text alignment in narrative and descriptive captions and find we can classify descriptive and narrative style captions using linguistic features such as part of speech, rhetoric structure theory, and multimodal discourse. Then, we use this to select captions from which to extract image-level labels as supervision for weakly supervised object detection. We also improve the quality of extracted labels by filtering based on proximity to verb types for both descriptive and narrative captions.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Rai_Improving_Language-Supervised_Object_Detection_With_Linguistic_Structure_Analysis_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Rai_Improving_Language-Supervised_Object_Detection_With_Linguistic_Structure_Analysis_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Visual Semantic Relatedness Dataset for Image Captioning",
        "author": "Ahmed Sabir, Francesc Moreno-Noguer, Llu\u00eds Padr\u00f3",
        "abstract": "Modern image captioning system relies heavily on extracting knowledge from images to capture the concept of a static story. In this paper, we propose a textual visual context dataset for captioning, in which the publicly available dataset COCO Captions (Lin et al., 2014) has been extended with information about the scene (such as objects in the image). Since this information has a textual form, it can be used to leverage any NLP task, such as text similarity or semantic relation methods, into captioning systems, either as an end-to-end training strategy or a post-processing based approach.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Sabir_Visual_Semantic_Relatedness_Dataset_for_Image_Captioning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/papers/Sabir_Visual_Semantic_Relatedness_Dataset_for_Image_Captioning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Underwater Moving Object Detection Using an End-to-End Encoder-Decoder Architecture and GraphSage With Aggregator and Refactoring",
        "author": "Meghna Kapoor, Suvam Patra, Badri Narayan Subudhi, Vinit Jakhetiya, Ankur Bansal",
        "abstract": "Underwater environments are greatly affected by several factors, including low visibility, high turbidity, back-scattering, dynamic background, etc., and hence pose challenges in object detection. Several algorithms consider convolutional neural networks to extract deep features and then object detection using the same. However, the dependency on the kernel's size and the network's depth results in fading relationships of latent space features and also are unable to characterize the spatial-contextual bonding of the pixels. Hence, they are unable to procure satisfactory results in complex underwater scenarios. To re-establish this relationship, we propose a unique architecture for underwater object detection where U-Net architecture is considered with the ResNet-50 backbone. Further, the latent space features from the encoder are fed to the decoder through a GraphSage model. GraphSage-based model is explored to reweight the node relationship in non-euclidean space using different aggregator functions and hence characterize the spatio-contextual bonding among the pixels. Further, we explored the dependency on different aggregator functions: mean, max, and LSTM, to evaluate the model's performance. We evaluated the proposed model on two underwater benchmark databases: F4Knowledge and underwater change detection. The performance of the proposed model is evaluated against eleven state-of-the-art techniques in terms of both visual and quantitative evaluation measures.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/html/Kapoor_Underwater_Moving_Object_Detection_Using_an_End-to-End_Encoder-Decoder_Architecture_and_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/papers/Kapoor_Underwater_Moving_Object_Detection_Using_an_End-to-End_Encoder-Decoder_Architecture_and_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Dense Multitask Learning To Reconfigure Comics",
        "author": "Deblina Bhattacharjee, Sabine S\u00fcsstrunk, Mathieu Salzmann",
        "abstract": "In this paper, we develop a MultiTask Learning (MTL) model to achieve dense predictions for comics panels to, in turn, facilitate the transfer of comics from one publication channel to another by assisting authors in the task of reconfiguring their narratives. Our MTL method can successfully identify the semantic units as well as the embedded notion of 3D in comics panels. This is a significantly challenging problem because comics comprise disparate artistic styles, illustrations, layouts, and object scales that depend on the author's creative process. Typically, dense image-based prediction techniques require a large corpus of data. Finding an automated solution for dense prediction in the comics domain, therefore, becomes more difficult with the lack of ground-truth dense annotations for the comics images. To address these challenges, we develop the following solutions- we leverage a commonly-used strategy known as unsupervised image-to-image translation, which allows us to utilize a large corpus of real-world annotations; - we utilize the results of the translations to develop our multitasking approach that is based on a vision transformer backbone and a domain transferable attention module; -we study the feasibility of integrating our MTL dense-prediction method with an existing retargeting method, thereby reconfiguring comics.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/html/Bhattacharjee_Dense_Multitask_Learning_To_Reconfigure_Comics_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/papers/Bhattacharjee_Dense_Multitask_Learning_To_Reconfigure_Comics_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Sign Language Translation from Instructional Videos",
        "author": "Laia Tarr\u00e9s, Gerard I. G\u00e1llego, Amanda Duarte, Jordi Torres, Xavier Gir\u00f3-i-Nieto",
        "abstract": "The advances in automatic sign language translation (SLT) to spoken languages have been mostly benchmarked with datasets of limited size and restricted domains. Our work advances the state of the art by providing the first baseline results on How2Sign, a large and broad dataset. We train a Transformer over I3D video features, using the reduced BLEU as a reference metric for validation, instead of the widely used BLEU score. We report a result of 8.03 on the BLEU score, and publish the first open-source implementation of its kind to promote further advances.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/html/Tarres_Sign_Language_Translation_from_Instructional_Videos_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/papers/Tarres_Sign_Language_Translation_from_Instructional_Videos_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Perception Over Time: Temporal Dynamics for Robust Image Understanding",
        "author": "Maryam Daniali, Edward Kim",
        "abstract": "While deep learning surpasses human-level performance in specific vision tasks, it is fragile and overconfident in its classification. For example, minor transformations in perspective, illumination, or object deformation in the image space can result in drastically different labeling. This is especially apparent when adversarial perturbations are present. Conversely, human visual perception is orders of magnitude more robust to input stimulus changes. Neuroscience research suggests that biological perception is a dynamic process that converges over time, even for static images and scenes. Almost all perception frameworks lack this convergence property, which makes them vulnerable to minor perturbations. Motivated by our human task results, we introduce a novel framework for incorporating temporal dynamics into static image understanding. We demonstrate a biologically plausible model that decomposes a single image into a series of coarse-to-fine images, mimicking the integration of visual information in the human brain. Our model utilizes this information \"over time\", resulting in significant improvements in its accuracy, robustness, and cost-effectiveness over standard CNNs. We explicitly quantify the adversarial robustness properties of our coarse-to-fine framework through multiple studies. Our quantitative and qualitative results convincingly demonstrate exciting and transformative improvements over standard architectures.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/html/Daniali_Perception_Over_Time_Temporal_Dynamics_for_Robust_Image_Understanding_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/papers/Daniali_Perception_Over_Time_Temporal_Dynamics_for_Robust_Image_Understanding_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Light-Weight Human Eye Fixation Solution for Smartphone Applications",
        "author": "Sudha Velusamy, Rakesh Radarapu, Anandavardhan Hegde, Narayan Kothari",
        "abstract": "A wide range of human eye fixation prediction algorithms have been presented in the research with the advent of deep learning. However, to generate better prediction outcomes, these methods are becoming increasingly complicated. In this study, we present a lightweight human eye fixation prediction network that is based on a low-complexity representation learning network and can handle a variety of real-world data. The method includes a simplified multi-level feature extraction network with an emphasize on channel and spatial attention mechanism. We investigate the effectiveness of the present technique in predicting eye fixation maps on a collection of challenging images from the SALICON and MIT1003 datasets. A comprehensive qualitative and quantitative evaluation revealed that the network could learn and capture spatial and semantic information in a scene effectively, resulting in a higher hit rate and fewer false positives in comparison with the competing solutions. The approach is implemented on Samsung Galaxy S23 with SnapDragon-SM8550 mobile platform given its short inference time of 1.4ms and low complexity model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/html/Velusamy_A_Light-Weight_Human_Eye_Fixation_Solution_for_Smartphone_Applications_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/papers/Velusamy_A_Light-Weight_Human_Eye_Fixation_Solution_for_Smartphone_Applications_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Nonverbal Communication Cue Recognition: A Pathway to More Accessible Communication",
        "author": "Zoya Shafique, Haiyan Wang, Yingli Tian",
        "abstract": "Nonverbal communication, such as body language, facial expressions, and hand gestures, is crucial to human communication as it conveys more information about emotions and attitudes than spoken words. However, individuals who are blind or have low-vision (BLV) may not have access to this method of communication, leading to asymmetry in conversations. Developing systems to recognize nonverbal communication cues (NVCs) for the BLV community would enhance communication and understanding for both parties. This paper focuses on developing a multimodal computer vision system to recognize and detect NVCs. To accomplish our objective, we are collecting a dataset focused on nonverbal communication cues. Here, we propose a baseline model for recognizing NVCs and present initial results on the Aff-Wild2 dataset. Our baseline model achieved an accuracy of 68% and a F1-Score of 64% on the Aff-Wild2 validation set, making it comparable with previous state of the art results. Furthermore, we discuss the various challenges associated with NVC recognition as well as the limitations of our current work.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/html/Shafique_Nonverbal_Communication_Cue_Recognition_A_Pathway_to_More_Accessible_Communication_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WiCV/papers/Shafique_Nonverbal_Communication_Cue_Recognition_A_Pathway_to_More_Accessible_Communication_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Leveraging TCN and Transformer for Effective Visual-Audio Fusion in Continuous Emotion Recognition",
        "author": "Weiwei Zhou, Jiada Lu, Zhaolong Xiong, Weifeng Wang",
        "abstract": "Human emotion recognition plays an important role in human-computer interaction. In this paper, we present our approach to the Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification Challenge, and Action Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Specifically, we propose a novel multi-modal fusion model that leverages Temporal Convolutional Networks (TCN) and Transformer to enhance the performance of continuous emotion recognition. Our model aims to effectively integrate visual and audio information for improved accuracy in recognizing emotions. Our model outperforms the baseline and ranks 3 in the Expression Classification challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Zhou_Leveraging_TCN_and_Transformer_for_Effective_Visual-Audio_Fusion_in_Continuous_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Zhou_Leveraging_TCN_and_Transformer_for_Effective_Visual-Audio_Fusion_in_Continuous_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Modal Information Fusion for Action Unit Detection in the Wild",
        "author": "Yuanyuan Deng, Xiaolong Liu, Liyu Meng, Wenqiang Jiang, Youqiang Dong, Chuanhe Liu",
        "abstract": "Action Unit (AU) detection is an important research branch in affective computing, which better understands human emotional intentions and responds more naturally to their needs and desires. In this paper, we present our latest progress techniques in the 5th Affective Behavior Analysis in-the-wild (ABAW) competition, including data balancing by marking, extracting features visual through models trained in face database and audio through deep networks and traditional methods, proposing model structures for mapping multimodal information to a unify multimodal vector space and fusing results from multiple models. These methods are effective on the official validation dataset of the Aff-Wild2. The final F1 in the 5th ABAW competition test dataset achieves 54.22%, 4.33% higher than the best results in the 3rd ABAW competition.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Deng_Multi-Modal_Information_Fusion_for_Action_Unit_Detection_in_the_Wild_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Deng_Multi-Modal_Information_Fusion_for_Action_Unit_Detection_in_the_Wild_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SPECTRE: Visual Speech-Informed Perceptual 3D Facial Expression Reconstruction From Videos",
        "author": "Panagiotis P. Filntisis, George Retsinas, Foivos Paraperas-Papantoniou, Athanasios Katsamanis, Anastasios Roussos, Petros Maragos",
        "abstract": "The recent state of the art on monocular 3D face reconstruction from image data has made some impressive advancements, thanks to the advent of Deep Learning. However, it has mostly focused on input coming from a single RGB image, overlooking the following important factors: a) Nowadays, the vast majority of facial image data of interest do not originate from single images but rather from videos, which contain rich dynamic information. b) Furthermore, these videos typically capture individuals in some form of verbal communication (public talks, teleconferences, audiovisual human-computer interactions, interviews, monologues/dialogues in movies, etc). When existing 3D face reconstruction methods are applied in such videos, the artifacts in the reconstruction of the shape and motion of the mouth area are often severe, since they do not match well with the speech audio. To overcome the aforementioned limitations, we present the first method for visual speech-informed perceptual reconstruction of 3D mouth expressions. We do this by proposing a \"lipreading\" loss, which guides the fitting process so that the elicited perception from the 3D reconstructed talking head resembles that of the original video footage. We demonstrate that, interestingly, the lipreading loss is better suited for 3D reconstruction of mouth movements compared to traditional landmark losses, and even direct 3D supervision. Furthermore, the devised method does not rely on any text transcriptions or corresponding audio, rendering it ideal for training in unlabeled datasets. We verify the efficiency of our method through objective evaluations on three large-scale datasets, as well as subjective evaluation with two web-based user studies. Project webpage: https://filby89.github.io/spectre/",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Filntisis_SPECTRE_Visual_Speech-Informed_Perceptual_3D_Facial_Expression_Reconstruction_From_Videos_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Filntisis_SPECTRE_Visual_Speech-Informed_Perceptual_3D_Facial_Expression_Reconstruction_From_Videos_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Facial Expression Recognition Based on Multi-Modal Features for Videos in the Wild",
        "author": "Chuanhe Liu, Xinjie Zhang, Xiaolong Liu, Tenggan Zhang, Liyu Meng, Yuchen Liu, Yuanyuan Deng, Wenqiang Jiang",
        "abstract": "This paper presents our work to the Expression Classification Challenge of the 5th Affective Behavior Analysis in-the-wild (ABAW) Competition. In our method, the multimodal features are extracted by several different pertained models, which are used to build different combinations to capture more effective emotion information. Specifically, we extracted efficient facial expression features using MAE encoder pre-trained with a large-scale face dataset. For these combinations of visual and audio modal features, we utilize two kinds of temporal encoders to explore the temporal contextual information in the data. In addition, we employ several ensemble strategies for different experimental settings to obtain the most accurate expression recognition results. Our system achieves the average F1 Score of 0.4072 on the test set of Aff-wild2 ranking 2nd, which proves the effectiveness of our method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Liu_Facial_Expression_Recognition_Based_on_Multi-Modal_Features_for_Videos_in_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Liu_Facial_Expression_Recognition_Based_on_Multi-Modal_Features_for_Videos_in_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos With Transformers",
        "author": "Jia Li, Yin Chen, Xuesong Zhang, Jiantao Nie, Ziqiang Li, Yangchen Yu, Yan Zhang, Richang Hong, Meng Wang",
        "abstract": "In this paper, we present our advanced solutions to the two sub-challenges of Affective Behavior Analysis in the wild (ABAW) 2023: the Emotional Reaction Intensity (ERI) Estimation Challenge and Expression (Expr) Classification Challenge. ABAW 2023 aims to tackle the challenge of affective behavior analysis in natural contexts, with the ultimate goal of creating intelligent machines and robots that possess the ability to comprehend human emotions, feelings, and behaviors. For the Expression Classification Challenge, we propose a streamlined approach that handles the challenges of classification effectively. However, our main contribution lies in our use of diverse models and tools to extract multimodal features such as audio and video cues from the Hume-Reaction dataset. By studying, analyzing, and combining these features, we significantly enhance the model's accuracy for sentiment prediction in a multimodal context. Furthermore, our method achieves outstanding results on the Emotional Reaction Intensity (ERI) Estimation Challenge, surpassing the baseline method by an impressive 84% increase, as measured by the Pearson Coefficient, on the validation dataset.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Li_Multimodal_Feature_Extraction_and_Fusion_for_Emotional_Reaction_Intensity_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Li_Multimodal_Feature_Extraction_and_Fusion_for_Emotional_Reaction_Intensity_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Unified Approach to Facial Affect Analysis: The MAE-Face Visual Representation",
        "author": "Bowen Ma, Wei Zhang, Feng Qiu, Yu Ding",
        "abstract": "Facial affect analysis is essential for understanding human expressions and behaviors, encompassing action unit (AU) detection, expression (EXPR) recognition, and valence-arousal (VA) estimation. The CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW) is dedicated to providing a high-quality and large-scale Aff-wild2 dataset for identifying widely used emotion representations. In this paper, we employ MAE-Face as a unified approach to develop robust visual representations for facial affect analysis. We propose multiple techniques to improve its fine-tuning performance on various downstream tasks, incorporating a two-pass pre-training process and a two-pass fine-tuning process. Our approach exhibits strong results on numerous datasets, highlighting its versatility. Moreover, the proposed model acts as a fundamental component for our final framework in the ABAW5 competition. Our submission achieves outstanding outcomes, ranking first place in the AU and EXPR tracks and second place in the VA track.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Ma_A_Unified_Approach_to_Facial_Affect_Analysis_The_MAE-Face_Visual_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Ma_A_Unified_Approach_to_Facial_Affect_Analysis_The_MAE-Face_Visual_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Large-Scale Facial Expression Recognition Using Dual-Domain Affect Fusion for Noisy Labels",
        "author": "Dexter Neo, Tsuhan Chen, Stefan Winkler",
        "abstract": "Building models for human facial expression recognition (FER) is made difficult by subjective, ambiguous and noisy annotations. This is especially true when assigning a single emotion class label to facial expressions for large in-the-wild FER datasets. Human facial expressions often contain a mixture of different mental states, which exacerbates the problem of single labels when used to categorize emotions. Dimensional models of affect - such as those using valence and arousal - provide significant advantages over categorical models in terms of representing human emotional states but have remained relatively under-explored. In this paper, we propose an approach for dual-domain affect fusion which investigates the relationships between discrete emotion classes and their continuous representations. In order to address the underlying uncertainty of the labels, we formulate a set of mixed labels via a dual-domain label fusion module to exploit these intrinsic relationships. Finally, we show the benefits of the proposed approach using AffectNet, Aff-Wild, and MorphSet, in the presence of natural and synthetic noise.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Neo_Large-Scale_Facial_Expression_Recognition_Using_Dual-Domain_Affect_Fusion_for_Noisy_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Neo_Large-Scale_Facial_Expression_Recognition_Using_Dual-Domain_Affect_Fusion_for_Noisy_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multimodal Continuous Emotion Recognition: A Technical Report for ABAW5",
        "author": "Su Zhang, Ziyuan Zhao, Cuntai Guan",
        "abstract": "We used two multimodal models for continuous valence-arousal recognition using visual, audio, and linguistic information. The first model is the same as we used in ABAW2 and ABAW3, which employs the leader-follower attention. The second model has the same architecture for spatial and temporal encoding. As for the fusion block, it employs a compact and straightforward channel attention, borrowed from the End2You toolkit. Unlike our previous attempts that use Vggish feature directly as the audio feature, this time we feed the pre-trained VGG model using logmel-spectrogram and finetune it during the training. To make full use of the data and alleviate over-fitting, cross-validation is carried out. The code is available at https://github.com/sucv/ABAW3.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Zhang_Multimodal_Continuous_Emotion_Recognition_A_Technical_Report_for_ABAW5_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Zhang_Multimodal_Continuous_Emotion_Recognition_A_Technical_Report_for_ABAW5_CVPRW_2023_paper.pdf"
    },
    {
        "title": "T-RAIN: Robust Generalization Under Weather-Aliasing Label Shift Attacks",
        "author": "Aboli Marathe, Sanjana Prabhu",
        "abstract": "In the classical supervised learning settings, classifiers are fit with the assumption of balanced label distributions and produce remarkable results on the same. In the real world, however, these assumptions often bend and in turn adversely impact model performance. Identifying bad learners in skewed target distributions is even more challenging. Thus achieving model robustness under these \"label shift\" settings is an important task in autonomous perception. In this paper, we analyze the impact of label shift on the task of multi-weather classification for autonomous vehicles. We use this information as a prior to better assess pedestrian detection in adverse weather. We model the classification performance as an indicator of robustness under 4 label shift scenarios and study the behavior of multiple classes of models. We propose t-RAIN a similarity mapping technique for synthetic data augmentation using large scale generative models and evaluate the performance on DAWN dataset. This mapping boosts model test accuracy by 2.1, 4.4, 1.9, 2.7% in no-shift, fog, snow, dust shifts respectively. We present state-of-the-art pedestrian detection results on real and synthetic weather domains with best performing 82.69 AP (snow) and 62.31 AP (fog) respectively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Marathe_T-RAIN_Robust_Generalization_Under_Weather-Aliasing_Label_Shift_Attacks_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Marathe_T-RAIN_Robust_Generalization_Under_Weather-Aliasing_Label_Shift_Attacks_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Frame Level Emotion Guided Dynamic Facial Expression Recognition With Emotion Grouping",
        "author": "Bokyeung Lee, Hyunuk Shin, Bonhwa Ku, Hanseok Ko",
        "abstract": "Facial expression recognition (FER) has received considerable attention in computer vision, with \"in-the-wild\" environments such as human-computer interaction and video understanding. Recognizing dynamic facial expressions in videos is generally considered a more practical and reliable approach than still images. However, the dynamic FER problem in videos has challenges in terms of both data acquisition and the structural aspects of the learning model. In particular, video frames that deviate from the target facial expression class can significantly degrade the performance of dynamic FER. In this paper, we present an affectivity extraction network (AEN) for dynamic FER. AEN combines features of different semantic levels and classifies both sentiment and specific emotion categories with emotion grouping. To address the challenges of dynamic FER, we propose frame-level emotion-guided loss functions and a structural aspect of the learning model. The AEN has two branches: a bottom-up branch that learns facial expressions representation at different semantic levels and outputs pseudo labels of facial expressions for each frame using a 2D FER model, and a top-down branch that learns discriminative representations by combining feature vectors of each semantic level for recognizing facial expressions at the corresponding emotion group. Additionally, the proposed frame-level emotion-guided loss functions encourage AEN to prevent the loss of emotional information and retain the emotional probability of a video clip. Experimental results on various video datasets show that the proposed AEN consistently outperforms the state-of-the-art in Ekman and sentiment FER. Representative results demonstrate the promise of the proposed AEN for dynamic FER in the video.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Lee_Frame_Level_Emotion_Guided_Dynamic_Facial_Expression_Recognition_With_Emotion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Lee_Frame_Level_Emotion_Guided_Dynamic_Facial_Expression_Recognition_With_Emotion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Ensemble Spatial and Temporal Vision Transformer for Action Units Detection",
        "author": "Ngoc Tu Vu, Van Thong Huynh, Trong Nghia Nguyen, Soo-Hyung Kim",
        "abstract": "Facial Action Units detection (FAUs) represents a fine-grained classification problem that involves identifying different units on the human face, as defined by the Facial Action Coding System. In this paper, we present a simple yet efficient Vision Transformer-based approach for addressing the task of Action Units (AU) detection in the context of Affective Behavior Analysis in-the-wild (ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to capture the temporal facial change in the video. Besides, to reduce massive size of the Vision Transformers model, we replace the ViViT feature extraction layers with the CNN backbone (Regnet). Our model outperform the baseline model of ABAW 2023 challenge, with a notable 14% difference in result. Our team has achieved a position within the top five teams in the ABAW 2023 competition, ranking slightly below the top three and four teams by a narrow margin of 0.27% and 0.43%, respectively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Vu_Ensemble_Spatial_and_Temporal_Vision_Transformer_for_Action_Units_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Vu_Ensemble_Spatial_and_Temporal_Vision_Transformer_for_Action_Units_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Modal Emotion Reaction Intensity Estimation With Temporal Augmentation",
        "author": "Feng Qiu, Bowen Ma, Wei Zhang, Yu Ding",
        "abstract": "Emotion reaction intensity (ERI) estimation aims to estimate the emotion intensities of subjects reacting to various video-based stimuli. It plays an important role in human affective behavior analysis. In this paper, we proposed a effective solution for addressing the task of ERI estimation in the fifth Affective Behavior Analysis in the wild (ABAW) competition. Based on multi-modal information, We first extract uni-modal features from images, speeches and texts, respectively and then regress the intensities of 7 emotions. To enhance the model generalization and capture context information, we employ the Temporal Augmentation module to adapt to various video samples and the Temporal SE Block to reweight temporal features adaptively. The extensive experiments conducted on large-scale dataset, Hume-Reaction, demonstrate the effectiveness of our approach. Our method achieves average pearson's correlations coefficient of 0.4160 on the validation set and obtain third place in the ERI Estimation Challenge of ABAW 2023.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Qiu_Multi-Modal_Emotion_Reaction_Intensity_Estimation_With_Temporal_Augmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Qiu_Multi-Modal_Emotion_Reaction_Intensity_Estimation_With_Temporal_Augmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Dynamic Noise Injection for Facial Expression Recognition In-the-Wild",
        "author": "SangHwa Hong, Jin-Woo Jeong",
        "abstract": "Facial expression-based emotion analysis is one of the most important artificial intelligence research fields. However, a lot of works still suffer from the low classification/regression performance caused by overfitting. Therefore, this paper proposes a new noise injection technique to alleviate this problem. Specifically, based on the ResNet-18 architecture, we dynamically add feature-level noise into the BN+ReLU unit to learn more robust features. Experiments on facial expression classification with the AffectNet dataset demonstrated the usefulness of the proposed approach.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Hong_Dynamic_Noise_Injection_for_Facial_Expression_Recognition_In-the-Wild_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Hong_Dynamic_Noise_Injection_for_Facial_Expression_Recognition_In-the-Wild_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring Expression-Related Self-Supervised Learning and Spatial Reserve Pooling for Affective Behaviour Analysis",
        "author": "Fanglei Xue, Yifan Sun, Yi Yang",
        "abstract": "Self-supervised learning (SSL) methods have gained attention for reducing dependence on labeled data. However, SSL methods are less investigated for facial expression recognition (FER), which requires expensive expression annotation, especially for large-scale video databases. In this paper, we explore an expression-related self-supervised learning (SSL) method called ContraWarping to perform expression classification in the 5th Affective Behavior Analysis in-the-wild (ABAW) competition. We also conduct a new spatial reserve pooling module to utilize all facial details for expression recognition. By evaluating on the Aff-Wild2 dataset, we demonstrate that ContraWarping outperforms existing supervised methods and other general SSL methods with only 0.7M trainable parameters and shows great application potential in the affective analysis area.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Xue_Exploring_Expression-Related_Self-Supervised_Learning_and_Spatial_Reserve_Pooling_for_Affective_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Xue_Exploring_Expression-Related_Self-Supervised_Learning_and_Spatial_Reserve_Pooling_for_Affective_CVPRW_2023_paper.pdf"
    },
    {
        "title": "EmotiEffNets for Facial Processing in Video-Based Valence-Arousal Prediction, Expression Classification and Action Unit Detection",
        "author": "Andrey V. Savchenko",
        "abstract": "In this article, the pre-trained convolutional networks from the EmotiEffNet family for frame-level feature extraction are used for downstream emotion analysis tasks from the fifth Affective Behavior Analysis in-the-wild (ABAW) competition. In particular, we propose an ensemble of a multi-layered perceptron and the LightAutoML-based classifier. The post-processing by smoothing the results for sequential frames is implemented. Experimental results for the large-scale Aff-Wild2 database demonstrate that our model is much better than the baseline facial processing using VGGFace And ResNet. For example, our macro-averaged F1-scores of facial expression recognition and action unit detection on the testing set are 11-13% greater. Moreover, the concordance correlation coefficients for valence/arousal estimation are up to 30% higher when compared to the baseline.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Savchenko_EmotiEffNets_for_Facial_Processing_in_Video-Based_Valence-Arousal_Prediction_Expression_Classification_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Savchenko_EmotiEffNets_for_Facial_Processing_in_Video-Based_Valence-Arousal_Prediction_Expression_Classification_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Integrating Holistic and Local Information To Estimate Emotional Reaction Intensity",
        "author": "Yini Fang, Liang Wu, Frederic Jumelle, Bertram Shi",
        "abstract": "Video-based Emotional Reaction Intensity (ERI) estimation measures the intensity of subjects' reactions to stimuli along several emotional dimensions from videos of the subject as they view the stimuli. We propose a multi-modal architecture for video-based ERI combining video and audio information. Video input is encoded spatially first, frame-by-frame, combining features encoding holistic aspects of the subjects' facial expressions and features encoding spatially localized aspects of their expressions. Input is then combined across time: from frame-to-frame using gated recurrent units (GRUs), then globally by a transformer. We handle variable video length with a regression token that accumulates information from all frames into a fixed-dimensional vector independent of video length. Audio information is handled similarly: spectral information extracted within each frame is integrated across time by a cascade of GRUs and a transformer with regression token. The video and audio regression tokens' outputs are merged by concatenation, then input to a final fully connected layer producing intensity estimates. Our architecture achieved excellent performance on the Hume-Reaction dataset in the ERI Esimation Challenge of the Fifth Competition on Affective Behavior Analysis in-the-Wild (ABAW5). The Pearson Correlation Coefficients between estimated and subject self-reported scores, averaged across all emotions, were 0.455 on the validation dataset and 0.4547 on the test dataset, well above the baselines. The transformer's self-attention mechanism enables our architecture to focus on the most critical video frames regardless of length. Ablation experiments establish the advantages of combining holistic/local features and of multi-modal integration. Code available at https://github.com/HKUST-NISL/ABAW5.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Fang_Integrating_Holistic_and_Local_Information_To_Estimate_Emotional_Reaction_Intensity_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Fang_Integrating_Holistic_and_Local_Information_To_Estimate_Emotional_Reaction_Intensity_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Spatial-Temporal Graph-Based AU Relationship Learning for Facial Action Unit Detection",
        "author": "Zihan Wang, Siyang Song, Cheng Luo, Yuzhi Zhou, Shiling Wu, Weicheng Xie, Linlin Shen",
        "abstract": "This paper presents our Facial Action Units (AUs) detection submission to the fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Our approach consists of three main modules: (i) a pre-trained facial representation encoder which produce a strong facial representation from each input face image in the input sequence; (ii) an AU-specific feature generator that specifically learns a set of AU features from each facial representation; and (iii) a spatio-temporal graph learning module that constructs a spatio-temporal graph representation. This graph representation describes AUs contained in all frames and predicts the occurrence of each AU based on both the modeled spatial information within the corresponding face and the learned temporal dynamics among frames. The experimental results show that our approach outperformed the baseline and the spatio-temporal graph representation learning allows our model to generate the best results among all ablated systems. Our model ranks at the 4th place in the AU recognition track at the 5th ABAW Competition. Our code is publicly available at https://github.com/wzh125/ABAW-5.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Wang_Spatial-Temporal_Graph-Based_AU_Relationship_Learning_for_Facial_Action_Unit_Detection_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Wang_Spatial-Temporal_Graph-Based_AU_Relationship_Learning_for_Facial_Action_Unit_Detection_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Dual Branch Network for Emotional Reaction Intensity Estimation",
        "author": "Jun Yu, Jichao Zhu, Wangyuan Zhu, Zhongpeng Cai, Guochen Xie, Renda Li, Gongpeng Zhao, Qiang Ling, Lei Wang, Cong Wang, Luyu Qiu, Wei Zheng",
        "abstract": "Emotional Reaction Intensity(ERI) estimation is an important task in multimodal scenarios, and has fundamental applications in medicine, safe driving and other fields. In this paper, we propose a solution to the ERI challenge of the fifth Affective Behavior Analysis in-the-wild(ABAW), a dual-branch based multi-output regression model. The spatial attention mechanism is used to better extract visual features, and the Mel-Frequency Cepstral Coefficients technology extracts acoustic features. Temporal Encoder is composed of Temporal Convolutional Network and Transformer Encoder, which is used to capture the temporal relationship between features. And a method named modality dropout is added to fusion multimodal features. Our approach for ERI challenge achieves Pearson's Correlation Coefficient of 0.4439 on the validation set and 0.4380 on the test set, which ranks second in the final leaderboard.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Yu_A_Dual_Branch_Network_for_Emotional_Reaction_Intensity_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Yu_A_Dual_Branch_Network_for_Emotional_Reaction_Intensity_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Unmasking Your Expression: Expression-Conditioned GAN for Masked Face Inpainting",
        "author": "Sridhar Sola, Darshan Gera",
        "abstract": "As face masks continue to be a part of our daily lives, the challenge of reconstructing occluded faces remains relevant. While several approaches have been proposed for removing masks from neutral facial images, few have explored the use of facial expressions as a dominant feature for reconstruction of expressive faces. To address this gap, we propose an expression-conditioned GAN (ECGAN) for reconstructing masked faces with a specified expression. Our approach leverages both the binary segmentation map of the mask and an expression label to generate high-quality images. To train our ECGAN in a supervised manner, we synthesize masked images using the RAFDB dataset to create non-masked-masked pairs of images for training. We evaluate of our approach on the RAFDB test set, demonstrating its effectiveness in generating realistic images that convincingly belong to the given expression class. This is further highlighted by comparing it to a baseline model and a state-of-the-art approach without expression-input. The code is available at https://github.com/SridharSola/ECGAN.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Sola_Unmasking_Your_Expression_Expression-Conditioned_GAN_for_Masked_Face_Inpainting_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Sola_Unmasking_Your_Expression_Expression-Conditioned_GAN_for_Masked_Face_Inpainting_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges",
        "author": "Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan Cowen, Stefanos Zafeiriou",
        "abstract": "The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part of the respective ABAW Workshop which will be held in conjunction with IEEE Computer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW Competition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at automatically analyzing affect. For this year's Competition, we feature two corpora: i) an extended version of the Aff-Wild2 database and ii) the Hume-Reaction dataset. The former database is an audiovisual one of around 600 videos of around 3M frames and is annotated with respect to: a) two continuous affect dimensions -valence (how positive/negative a person is) and arousal (how active/passive a person is)-; b) basic expressions (e.g. happiness, sadness, neutral state); and c) atomic facial muscle actions (i.e., action units). The latter dataset is an audiovisual one in which reactions of individuals to emotional stimuli have been annotated with respect to seven emotional expression intensities. Thus the 5th ABAW Competition encompasses four Challenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression Classification, iii) uni-task Action Unit Detection, and iv) Emotional Reaction Intensity Estimation. In this paper, we present these Challenges, along with their corpora, we outline the evaluation metrics, we present the baseline systems and illustrate their obtained performance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Kollias_ABAW_Valence-Arousal_Estimation_Expression_Recognition_Action_Unit_Detection__Emotional_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Kollias_ABAW_Valence-Arousal_Estimation_Expression_Recognition_Action_Unit_Detection__Emotional_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Relational Edge-Node Graph Attention Network for Classification of Micro-Expressions",
        "author": "Ankith Jain Rakesh Kumar, Bir Bhanu",
        "abstract": "Facial micro-expressions (MEs) refer to subtle, transient, and involuntary muscle movements expressing a person's true feelings. This paper presents a novel two-stream relational edge-node graph attention network-based approach to classify MEs in a video by selecting the high-intensity frames and edge-node features that can provide valuable information about the relationship between nodes and structural information in a graph structure. The paper examines the impact of different edge-node features and their relationships on the graphs. The first step involves extracting high-intensity-emotion frames from the video using optical flow. Second, node feature embeddings are calculated using the node location coordinate features and the patch size information of the optical flow across each node location. Additionally, we obtain the global and local structural similarity score using the jaccard's similarity score and radial basis function as the edge features. Third, a self-attention graph pooling layer helps to remove the nodes with lower attention scores based on the top-k selection. As the final step, the network employs a two-stream edge-node graph attention network that focuses on finding correlations among the edge and node features, such as landmark coordinates, optical flow, and global and local edge features. A three-frame graph structure is designed to obtain spatio-temporal information. For 3 and 5 expression classes, the results are compared for SMIC and CASME II databases.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Kumar_Relational_Edge-Node_Graph_Attention_Network_for_Classification_of_Micro-Expressions_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Kumar_Relational_Edge-Node_Graph_Attention_Network_for_Classification_of_Micro-Expressions_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Local Region Perception and Relationship Learning Combined With Feature Fusion for Facial Action Unit Detection",
        "author": "Jun Yu, Renda Li, Zhongpeng Cai, Gongpeng Zhao, Guochen Xie, Jichao Zhu, Wangyuan Zhu, Qiang Ling, Lei Wang, Cong Wang, Luyu Qiu, Wei Zheng",
        "abstract": "Human affective behavior analysis plays a vital role in human-computer interaction (HCI) systems. In this paper, we introduce our submission to the CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW). We propose a single-stage trained AU detection framework. Specifically, in order to effectively extract facial local region features related to AU detection, we use a local region perception module to effectively extract features of different AUs. Meanwhile, we use a graph neural network-based relational learning module to capture the relationship between AUs. In addition, considering the role of the overall feature of the target face on AU detection, we also use the feature fusion module to fuse the feature information extracted by the backbone network and the AU feature information extracted by the relationship learning module. We also adopted some sampling methods, data augmentation techniques and post-processing strategies to further improve the performance of the model. On the official test set, our method ranks third in the AU detection track. This result and subsequent ablation experiments prove the effectiveness of our proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Yu_Local_Region_Perception_and_Relationship_Learning_Combined_With_Feature_Fusion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Yu_Local_Region_Perception_and_Relationship_Learning_Combined_With_Feature_Fusion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring Large-Scale Unlabeled Faces To Enhance Facial Expression Recognition",
        "author": "Jun Yu, Zhongpeng Cai, Renda Li, Gongpeng Zhao, Guochen Xie, Jichao Zhu, Wangyuan Zhu, Qiang Ling, Lei Wang, Cong Wang, Luyu Qiu, Wei Zheng",
        "abstract": "Facial Expression Recognition (FER) is an important task in computer vision and has wide applications in many fields. In this paper, we introduce our approach to the fifth Affective Behavior Analysis in-the-wild (ABAW) Competition which will be held in CVPR20223. For facial expression recognition task, there is an urgent need to solve the problem that the limited size of FER datasets limits the generalization ability of expression recognition models, resulting in ineffective performance. To address this problem, we propose a semi-supervised learning framework that utilizes unlabeled face data to train expression recognition models effectively. Our method uses a dynamic threshold module (DTM) that can adaptively adjust the confidence threshold to fully utilize the face recognition (FR) data to generate pseudo-labels, thus improving the model's ability to model facial expressions. In the 5th ABAW Expression Classification Challenge, our method achieves good results on the Aff-Wild2 validation and test sets, demonstrating that large scale unlabeled faces can indeed improve the performance of face expression recognition.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Yu_Exploring_Large-Scale_Unlabeled_Faces_To_Enhance_Facial_Expression_Recognition_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Yu_Exploring_Large-Scale_Unlabeled_Faces_To_Enhance_Facial_Expression_Recognition_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TempT: Temporal Consistency for Test-Time Adaptation",
        "author": "Onur Cezmi Mutlu, Mohammadmahdi Honarmand, Saimourya Surabhi, Dennis P. Wall",
        "abstract": "We introduce Temporal consistency for Test-time adaptation (TempT), a novel method for test-time adaptation on videos through the use of temporal coherence of predictions across sequential frames as a self-supervision signal. TempT is an approach with broad potential applications in computer vision tasks, including facial expression recognition (FER) in videos. We evaluate TempT's performance on the AffWild2 dataset. Our approach focuses solely on the unimodal visual aspect of the data and utilizes a popular 2D CNN backbone, in contrast to larger sequential or attention-based models used in other approaches. Our preliminary experimental results demonstrate that TempT has competitive performance compared to the previous years' reported performances, and its efficacy provides a compelling proof-of-concept for its use in various real-world applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Mutlu_TempT_Temporal_Consistency_for_Test-Time_Adaptation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Mutlu_TempT_Temporal_Consistency_for_Test-Time_Adaptation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ABAW5 Challenge: A Facial Affect Recognition Approach Utilizing Transformer Encoder and Audiovisual Fusion",
        "author": "Ziyang Zhang, Liuwei An, Zishun Cui, Ao Xu, Tengteng Dong, Yueqi Jiang, Jingyi Shi, Xin Liu, Xiao Sun, Meng Wang",
        "abstract": "In this paper, we present our approach to tackling the 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). The competition comprises four sub-challenges, namely Valence-Arousal (VA) Estimation, Expression (Expr) Classification, Action Unit (AU) Detection, and Emotional Reaction Intensity (ERI) Estimation. To address theuse challenges, we leverage state-of-the-art (sota) models to extract robust audio and visual features. Subsequently, these features are fused using a Transformer Encoder for the VA, Expr, and AU sub-challenges, and TEMMA for the ERI sub-challenge. To mitigate the effect of disparate feature dimensions, we introduce an Affine Module to align the features to the same dimension. Overall, our results outperform the baseline by a substantial margin across all four sub-challenges. Specifically, for the VA Estimation sub-challenge, our method attains a mean Concordance Correlation Coefficient (CCC) of 0.5342, ranking fifth overall. For the Expression Classification subchallenge, our approach achieves an average F1 Score of 0.3337, placing fourth overall. For the AU Detection sub-challenge, our method obtains an average F1 Score of 0.4752. Lastly, for the Emotional Reaction Intensity Estimation sub-challenge, our approach yields an average Pearson's correlation coefficient of 0.3968.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Zhang_ABAW5_Challenge_A_Facial_Affect_Recognition_Approach_Utilizing_Transformer_Encoder_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Zhang_ABAW5_Challenge_A_Facial_Affect_Recognition_Approach_Utilizing_Transformer_Encoder_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Modal Facial Affective Analysis Based on Masked Autoencoder",
        "author": "Wei Zhang, Bowen Ma, Feng Qiu, Yu Ding",
        "abstract": "Human affective behavior analysis focuses on analyzing human expressions or other behaviors to enhance the understanding of human psychology. The CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW) is dedicated to providing high-quality and large-scale Aff-wild2 for the recognition of commonly used emotion representations, such as Action Units (AU), basic expression categories (EXPR), and Valence-Arousal (VA). The competition is committed to making significant strides in improving the accuracy and practicality of affective analysis research in real-world scenarios. In this paper, we introduce our submission to the CVPR 2023: ABAW5. Our approach involves several key components. First, we utilize the visual information from a Masked Autoencoder (MAE) model that has been pre-trained on a large-scale face image dataset in a self-supervised manner. Next, we finetune the MAE encoder on the image frames from the Aff-wild2 for AU, EXPR and VA tasks, which can be regarded as a static and uni-modal training. Additionally, we leverage the multi-modal and temporal information from the videos and implement a transformer-based framework to fuse the multi-modal features. Our approach achieves impressive results in the ABAW5 competition, with an average F1 score of 55.49% and 41.21% in the AU and EXPR tracks, respectively, and an average CCC of 0.6372 in the VA track. Our approach ranks first in the EXPR and AU tracks, and second in the VA track. Extensive quantitative experiments and ablation studies demonstrate the effectiveness of our proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Zhang_Multi-Modal_Facial_Affective_Analysis_Based_on_Masked_Autoencoder_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Zhang_Multi-Modal_Facial_Affective_Analysis_Based_on_Masked_Autoencoder_CVPRW_2023_paper.pdf"
    },
    {
        "title": "EVAEF: Ensemble Valence-Arousal Estimation Framework in the Wild",
        "author": "Xiaolong Liu, Lei Sun, Wenqiang Jiang, Fengyuan Zhang, Yuanyuan Deng, Zhaopei Huang, Liyu Meng, Yuchen Liu, Chuanhe Liu",
        "abstract": "This paper presents our work to the Valence-Arousal Estimation Challenge of the 5th Affective Behavior Analysis in-the-wild (ABAW) competition. We explore the problems in this VA challenge from three aspects: 1) To obtain efficient and robust feature representations, we explore the role of multiple visual and video feature extractors; 2)Based on multimodal feature representations that fuse the visual and video information, we utilize four types of temporal encoders to capture the temporal context information in the video, including the LSTM, GRU, Transformer based encoder and a combined encoder of Transformer and LSTM; 3) five model ensemble strategies are used to combine multiple results with different model settings. Our system achieves the performance in Concordance Correlation Coefficients (CCC) of 0.6193 for valence, 0.6634 for arousal, and a mean CCC of 0.6414 on the test set, which demonstrates the effectiveness of our proposed method and ranks first place in the challenge.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Liu_EVAEF_Ensemble_Valence-Arousal_Estimation_Framework_in_the_Wild_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Liu_EVAEF_Ensemble_Valence-Arousal_Estimation_Framework_in_the_Wild_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Compound Expression Recognition In-the-Wild With AU-Assisted Meta Multi-Task Learning",
        "author": "Ximan Li, Weihong Deng, Shan Li, Yong Li",
        "abstract": "Facial expression recognition (FER) has received wide attention as an essential part of affective computing. Considering its ambiguity and variety, more attention has been paid to compound expression recognition. Since emotions are generated by the contraction of muscle groups, the action units (AUs) analysis has a vital role in FER. However, AU analysis of compound expression has only been conducted in the laboratory, lacking real-world databases with manually annotated compound expressions and AUs. We construct a real-world affective faces database of compound emotions (RAF-CE), with both compound expression labels and AU labels. Our AU analysis of compound facial expressions conducted on RAF-CE reveals that AU patterns and AU frequencies are different in the lab-controlled compound expressions and the real-world ones. Based on the analysis, we propose a meta-based multi-task learning (MML) for compound FER with AU recognition utilized as an auxiliary task. To fully exploit the priori AU-emotion constraint observed in RAF-CE, an alignment loss is introduced to explicitly match the distribution of AU and FE predictions with each other. Furthermore, we adopt meta-learning to adaptively adjust task weights and improve the positive effect of the auxiliary task. The method can learn refined expression representations latent in the facial topology. Experiments prove the effectiveness of the proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Li_Compound_Expression_Recognition_In-the-Wild_With_AU-Assisted_Meta_Multi-Task_Learning_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Li_Compound_Expression_Recognition_In-the-Wild_With_AU-Assisted_Meta_Multi-Task_Learning_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Analysis of Emotion Annotation Strength Improves Generalization in Speech Emotion Recognition Models",
        "author": "Joao Palotti, Gagan Narula, Lekan Raheem, Herbert Bay",
        "abstract": "Recent advances in speech emotion recognition (SER) have relied on a mix of acted and in-the-wild research datasets. It is unclear whether annotations in these datasets are of similar strength or quality, can reliably be detected by other human annotators, and to what extent emotion classification knowledge can be transferred between acted and in-the-wild data. A well known, large in-the-wild dataset for emotion classification and sentiment analysis is the CMU-MOSEI video dataset. The raw annotations of CMU-MOSEI are \"soft labels\" on a Likert scale. Usually, experiments are performed with a simple binarization of these fine-grained labels. In this work, we re-annotated 1% of the data from two acted and two in-the-wild datasets to analyze the strength of emotion annotation per label, compare annotation accuracy between acted and in-the-wild data, and identify an appropriate threshold for CMU-MOSEI label binarization. We report a significant improvement (7% increase on weighted average F1) using the same model architecture in emotion classification by simply identifying a better threshold for CMU-MOSEI. Further, we show that emotion annotation strength of acted and in-the-wild data is similar, and that the same model architecture generalizes to the same extent when trained on acted and tested on in-the-wild data, and vice-versa.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Palotti_Analysis_of_Emotion_Annotation_Strength_Improves_Generalization_in_Speech_Emotion_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Palotti_Analysis_of_Emotion_Annotation_Strength_Improves_Generalization_in_Speech_Emotion_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Inferring Affective Experience From the Big Picture Metaphor: A Two-Dimensional Visual Breadth Model",
        "author": "Song Tong, Jingyi Duan, Xuefeng Liang, Takatsune Kumada, Kaiping Peng, Ryoichi Nakashima",
        "abstract": "This study explores the psychological significance of the commonly used visual metaphor 'seeing the big picture' and examines whether and how it leads to positive experiences in real-life situations. To elucidate this phenomenon, a two-dimensional model of visual breadth is proposed, then respectively operationalized by two computer vision approaches. Our approaches are evaluated on a collected data set with 29,216 photos. The results revealed that physical and contextual breadth are two essential visual structures that compose a 'big picture'. Furthermore, these two visual breadths interactively shape people's affective experiences. This study provides insight into the psychological implications of the 'big picture' metaphor and sheds light on its practical potential for computer vision approaches and affective computing in the wild.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Tong_Inferring_Affective_Experience_From_the_Big_Picture_Metaphor_A_Two-Dimensional_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Tong_Inferring_Affective_Experience_From_the_Big_Picture_Metaphor_A_Two-Dimensional_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Real-Time Estimation of Heart Rate in Situations Characterized by Dynamic Illumination Using Remote Photoplethysmography",
        "author": "Patrik Hansen, Marianela Garc\u00eda Lozano, Farzad Kamrani, Joel Brynielsson",
        "abstract": "Remote photoplethysmography (rPPG) is a technique that aims to remotely estimate the heart rate of an individual using an RGB camera. Although several studies use the rPPG methodology, it is usually applied in a laboratory in a controlled environment, where both the camera and the subject are static, and the illumination is ideal for the task. However, applying rPPG in a real-life scenario is much more demanding, since dynamic illumination issues arise. The work presented in this paper introduces a framework to estimate the heart rate of an individual in real-time using an RGB camera in a situation characterized by dynamic illumination. Such situations occur, for example, when either the camera or the subject is moving, and/or the face visibility is limited. The framework uses a face detection program to extract regions of interest on an individual's face. These regions are combined and constitute the input to a convolutional neural network, which is trained to estimate the heart rate in real-time. The method is evaluated on three publicly available datasets, and an in-house dataset specifically collected for the purpose of this study, that includes motions and dynamic illumination. The method shows good performance on all four datasets, outperforming other methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Hansen_Real-Time_Estimation_of_Heart_Rate_in_Situations_Characterized_by_Dynamic_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Hansen_Real-Time_Estimation_of_Heart_Rate_in_Situations_Characterized_by_Dynamic_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Contactless Respiratory Rate Monitoring for ICU Patients Based on Unsupervised Learning",
        "author": "Zimeng Liu, Bin Huang, Chun-Liang Lin, Chieh-Liang Wu, Changchen Zhao, Wen-Cheng Chao, Yu-Cheng Wu, Yadan Zheng, Zhiru Wang",
        "abstract": "Recently, the task of contactless physiological signal monitoring based on deep learning technologies has attracted a large number of scholars. However, few studies focus on the application of real-world scenarios, especially in clinical medicine scenes. In this paper, a novel video-based contactless respiratory rate measurement algorithm is developed for the Intensive Care Unit (ICU) patients. Firstly, a large-scale clinical real-world database towards ICU patient is collected in this study. Then, based on the dataset, the unsupervised learning is first introduced to extract the respiration waveform from the chest area of patients. Lastly, a respiratory rate estimator based on neural networks is proposed and trained on a periodical physiological signal simulation dataset, and utilizes the transfer learning technique to extract the respiratory rate from only a 10-second respiration waveform. We obtained an estimated respiratory rate with an MAE of 2.8 breaths/min and an STD of 3.0 breaths/min against the reference value computed from the specialized medical device. Extensive experiments demonstrate that our proposed methods achieve competitive results over the state-of-the-art (SOTA) method in the real-world scenario.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Liu_Contactless_Respiratory_Rate_Monitoring_for_ICU_Patients_Based_on_Unsupervised_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Liu_Contactless_Respiratory_Rate_Monitoring_for_ICU_Patients_Based_on_Unsupervised_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Optimizing Camera Exposure Control Settings for Remote Vital Sign Measurements in Low-Light Environments",
        "author": "Ismoil Odinaev, Jing Wei Chin, Kin Ho Luo, Zhang Ke, Richard H.Y. So, Kwan Long Wong",
        "abstract": "Remote photoplethysmography (rPPG) is an optical technique that enables both non-invasive and efficient measurement of vital signs from facial videos. However, the quality of rPPG measurements can be adversely affected by improper camera exposure control and bad lighting conditions. In this paper, we present a systematic study of camera exposure control settings, specifically gain and exposure time, in low-light environments. Our results indicate that manual adjustment of gain and exposure time can significantly improve the quality of rPPG measurements, enabling accurate vital sign measurement even in environments with illuminance levels as low as 25 lux. Furthermore, we demonstrate that the optimal brightness range for rPPG-based vital sign measurement depends on the sensitivity of the vital sign to the shape and peaks of the rPPG signal. These findings have important practical implications for the use of rPPG in healthcare and remote monitoring applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Odinaev_Optimizing_Camera_Exposure_Control_Settings_for_Remote_Vital_Sign_Measurements_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Odinaev_Optimizing_Camera_Exposure_Control_Settings_for_Remote_Vital_Sign_Measurements_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Camera-Based Recovery of Cardiovascular Signals From Unconstrained Face Videos Using an Attention Network",
        "author": "Yogesh Deshpande, Surendrabikram Thapa, Abhijit Sarkar, A. Lynn Abbott",
        "abstract": "This paper addresses the problem of recovering the shape morphology of blood volume pulse (BVP) information from a video of a person's face. Video-based remote plethysmography methods have shown promising results in estimating vital signs such as heart rate and breathing rate. However, recovering the instantaneous pulse rate signals is still a challenge for the community. This is due to the fact that most of the previous methods concentrate on capturing the temporal average of the cardiovascular signals. In contrast, we present an approach in which BVP signals are extracted with a focus on the recovery of the signal shape morphology as a generalized form for the computation of physiological metrics. We also place emphasis on allowing natural movements by the subject. Furthermore, our system is capable of extracting individual BVP instances with sufficient signal detail to facilitate candidate re-identification. These improvements have resulted in part from the incorporation of a robust skin-detection module into the overall imaging-based photoplethysmography (iPPG) framework. We present extensive experimental results using the challenging UBFC-Phys dataset and the well-known COHFACE dataset. The source code is available at https://github.com/yogeshd21/CVPM-2023-iPPG-Paper.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Deshpande_Camera-Based_Recovery_of_Cardiovascular_Signals_From_Unconstrained_Face_Videos_Using_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Deshpande_Camera-Based_Recovery_of_Cardiovascular_Signals_From_Unconstrained_Face_Videos_Using_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Temporal Encoder-Decoder Approach to Extracting Blood Volume Pulse Signal Morphology From Face Videos",
        "author": "Fulan Li, Surendrabikram Thapa, Shreyas Bhat, Abhijit Sarkar, A. Lynn Abbott",
        "abstract": "This paper considers methods for extracting blood volume pulse (BVP) representations from video of the human face. Whereas most previous systems have been concerned with estimating vital signs such as average heart rate, this paper addresses the more difficult problem of recovering BVP signal morphology. We present a new approach that is inspired by temporal encoder-decoder architectures that have been used for audio signal separation. As input, this system accepts a temporal sequence of RGB (red, green, blue) values that have been spatially averaged over a small portion of the face. The output of the system is a temporal sequence that approximates a BVP signal. In order to reduce noise in the recovered signal, a separate processing step extracts individual pulses and performs normalization and outlier removal. After these steps, individual pulse shapes have been extracted that are sufficiently distinct to support biometric authentication. Our findings demonstrate the effectiveness of our approach in extracting BVP signal morphology from facial videos, which presents exciting opportunities for further research in this area. The source code is available at https://github.com/Adleof/CVPM-2023-Temporal-Encoder-Decoder-iPPG.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Li_A_Temporal_Encoder-Decoder_Approach_to_Extracting_Blood_Volume_Pulse_Signal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Li_A_Temporal_Encoder-Decoder_Approach_to_Extracting_Blood_Volume_Pulse_Signal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Camera Based Eye State Estimation for ICU Patients: A Pilot Clinical Study",
        "author": "Haowen Wang, Weijun Huang, Jia Huang, Guowei Wang, Hongzhou Lu, Wenjin Wang",
        "abstract": "In the Intensive Care Unit (ICU), the awakening of patients from comas is indicative of recovery. This article investigates the feasibility of using conventional and deep learning-based methods for eye state estimation based on videos recorded by a CCTV-camera installed in the ICU. For handcrafted feature-based methods, HOG and RGB features are combined as the input of the SVM classifier to classify the eye state as open and closed. For deep learning-based methods, the eye and face images were used as joint input for classification. The clinical trial involved 48 ICU patients, and the accuracy of the benchmarked methods was compared. The results show that the HOG-RGB based method achieved an accuracy of 91.39%, while the deep learning-based method achieved an accuracy of 89.35%. These findings highlight the chances of using CCTV cameras to estimate the eye state of ICU patients, which can be a useful mean to provide information regarding the consciousness for clinicians to assess the patient's recovery.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Wang_Camera_Based_Eye_State_Estimation_for_ICU_Patients_A_Pilot_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Wang_Camera_Based_Eye_State_Estimation_for_ICU_Patients_A_Pilot_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Deep Learning-Based Image Enhancement for Robust Remote Photoplethysmography in Various Illumination Scenarios",
        "author": "Shutao Chen, Sui Kei Ho, Jing Wei Chin, Kin Ho Luo, Tsz Tai Chan, Richard H.Y. So, Kwan Long Wong",
        "abstract": "Remote photoplethysmography (rPPG) is a non-invasive and convenient approach for measuring human vital signs using a camera. However, accurate measurement can be challenging due to the different illumination of the surrounding environment. In this study, we present a deep learning-based image enhancement model (IEM) inspired by the Retinex theory to improve the robustness of rPPG signal extraction and heart rate (HR) estimation in different lighting conditions. We finetuned the IEM with a time-shifted negative Pearson correlation between the PPG signal ground truth and the predicted rPPG signal from a pretrained 3D CNN (PhysNet). We evaluated our method using a publicly available dataset (BH-rPPG) of various lighting scenarios and our own private dataset. Our results demonstrate that our proposed model is generalizable and significantly improves rPPG extraction and HR estimation accuracy across a range of illumination intensities.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Ho_Deep_Learning-Based_Image_Enhancement_for_Robust_Remote_Photoplethysmography_in_Various_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Ho_Deep_Learning-Based_Image_Enhancement_for_Robust_Remote_Photoplethysmography_in_Various_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Promoting Generalization in Cross-Dataset Remote Photoplethysmography",
        "author": "Nathan Vance, Jeremy Speth, Benjamin Sporrer, Patrick Flynn",
        "abstract": "Remote Photoplethysmography (rPPG), or the remote monitoring of a subject's heart rate using a camera, has seen a shift from handcrafted techniques to deep learning models. While current solutions offer substantial performance gains, we show that these models tend to learn a bias to pulse wave features inherent to the training dataset. We develop augmentations to mitigate this learned bias by expanding both the range and variability of heart rates that the model sees while training, resulting in improved model convergence when training and cross-dataset generalization at test time. Through a 3-way cross dataset analysis we demonstrate a reduction in mean absolute error from over 13 beats per minute to below 3 beats per minute. We compare our method with other recent rPPG systems, finding similar performance under a variety of evaluation parameters.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Vance_Promoting_Generalization_in_Cross-Dataset_Remote_Photoplethysmography_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Vance_Promoting_Generalization_in_Cross-Dataset_Remote_Photoplethysmography_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-View Body Image-Based Prediction of Body Mass Index and Various Body Part Sizes",
        "author": "Seunghyun Kim, Kunyoung Lee, Eui Chul Lee",
        "abstract": "This paper proposes a novel model for predicting body mass index and various body part sizes using front, side, and back body images. The model is trained on a large dataset of labeled images. The results show that the model can accurately predict body mass index and various body part sizes such as chest, waist, hip, thigh, forearm, and shoulder width. One significant advantage of the proposed model is that it can use multiple views of the body to achieve more accurate predictions, overcoming the limitations of models that only used a single image. The model also does not require complex pre-processing or feature extraction, making it straightforward to apply in practice. We also explore the impact of different environmental factors, such as clothing and posture, on the model's performance. The findings show that the model is relatively insensitive to posture but is more sensitive to clothing, emphasizing the importance of controlling for clothing when using this model. Overall, the proposed model represents a step forward in predicting body mass index and various body part sizes from images. The model's accuracy, convenience, and ability to use multiple views of the body make it a promising tool for a wide range of applications. The proposed method is expected to be utilized as a parameter for accurate sensing of various vision-based non-contact biomarkers, in addition to body mass index inference.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Kim_Multi-View_Body_Image-Based_Prediction_of_Body_Mass_Index_and_Various_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Kim_Multi-View_Body_Image-Based_Prediction_of_Body_Mass_Index_and_Various_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Full-Body Cardiovascular Sensing With Remote Photoplethysmography",
        "author": "Lu Niu, Jeremy Speth, Nathan Vance, Benjamin Sporrer, Adam Czajka, Patrick Flynn",
        "abstract": "Remote photoplethysmography (rPPG) allows for noncontact monitoring of blood volume changes from a camera by detecting minor fluctuations in reflected light. Prior applications of rPPG focused on face videos. In this paper we explored the feasibility of rPPG from non-face body regions such as the arms, legs, and hands. We collected a new dataset titled Multi-Site Physiological Monitoring (MSPM), which will be released with this paper. The dataset consists of 90 frames per second video of exposed arms, legs, and face, along with 10 synchronized PPG recordings. We performed baseline heart rate estimation experiments from non-face regions with several state-of-the-art rPPG approaches, including chrominance-based (CHROM), plane-orthogonal-to-skin (POS) and RemotePulseNet (RPNet). To our knowledge, this is the first evaluation of the fidelity of rPPG signals simultaneously obtained from multiple regions of a human body. Our experiments showed that skin pixels from arms, legs, and hands are all potential sources of the blood volume pulse. The best-performing approach, POS, achieved a mean absolute error peaking at 7.11 beats per minute from non-facial body parts compared to 1.38 beats per minute from the face. Additionally, we performed experiments on pulse transit time (PTT) from both the contact PPG and rPPG signals. We found that remote PTT is possible with moderately high frame rate video when distal locations on the body are visible. These findings and the supporting dataset should facilitate new research on non-face rPPG and monitoring blood flow dynamics over the whole body with a camera.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Niu_Full-Body_Cardiovascular_Sensing_With_Remote_Photoplethysmography_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Niu_Full-Body_Cardiovascular_Sensing_With_Remote_Photoplethysmography_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Frequency Tracker for Unsupervised Heart Rate Estimation",
        "author": "Iskander Zhalbekov, Leonid Beynenson, Alexey Trushkov, Ivan Bulychev, Wenshuai Yin",
        "abstract": "We present frequency tracking for extracting heart rate trace from blood volume pulse (BVP) signal that can be used as an alternative for commonly used approach based on the mode of the BVP signal power spectral density. Our approach is based on particle filtering framework which provides smooth heart rate estimate, it is robust to motion-induced artifacts and noise. The method could be easily tuned and can be coupled with unsupervised BVP extraction approaches without the need for training. We evaluate our method on publicly available part of LGI dataset. Proposed algorithm shows competitive results comparing to argmax approach.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Zhalbekov_Frequency_Tracker_for_Unsupervised_Heart_Rate_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Zhalbekov_Frequency_Tracker_for_Unsupervised_Heart_Rate_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Remote Mass Facial Temperature Screening in Varying Ambient Temperatures and Distances",
        "author": "Chu Chu Qiu, Jing Wei Chin, Kwan Long Wong, Tsz Tai Chan, Yu Dong He, Richard H.Y. So",
        "abstract": "Remote body temperature measurement using infrared thermography has been widely deployed worldwide to detect feverish persons, but the measurement accuracy is affected by various factors including ambient temperature and sensor-subject distance. We present a novel compensation model to address the undesirable interacting influence of ambient temperature and sensor-subject distance during remote facial temperature screening in real-world setting. We derived our model on site-data collected over 12 months and demonstrated the significant linear relationship between ambient temperature and the measured temperature from a thermal camera. In addition, the interaction between the effects of sensor-subject distance and ambient temperature on the measured temperature is significant. Our model can significantly reduce the measurement error (MAE) by 23.5% and is better than the best existing models. The model can also extend the detection distance by up to 46% with sensitivity and specificity over 90%.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Qiu_Remote_Mass_Facial_Temperature_Screening_in_Varying_Ambient_Temperatures_and_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Qiu_Remote_Mass_Facial_Temperature_Screening_in_Varying_Ambient_Temperatures_and_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Photoplethysmography Imaging Algorithm for Real-Time Monitoring of Skin Perfusion Maps",
        "author": "Uldis Rubins, Aleksejs Miscuks, Yousef Qawqzeh, Zbignevs Marcinkevics, Andris Grabovskis",
        "abstract": "Photoplethysmography imaging (PPGI) is a cost-effective and reliable contactless optical technique for assessing cutaneous microcirculation. Its applications extend beyond extracting vital signs, such as heart rate and respiratory rate, to include spatiotemporal perfusion measurements from human skin. We propose an algorithm that provides real-time estimation of perfusion maps with adequate spatial resolution (320x240 pixels) and temporal resolution (0.2 s), which can be executed on a mid-range laptop computer. The algorithm was tested for determining perfusion changes in skin regions during regional anesthesia (RA) procedures. Seven patients (aged 18-80 years) undergoing hand surgery received peripheral nerve brachial plexus blocks during RA procedures. At baseline and after the RA procedure, 4, 10, 15, and 20 minutes later, the perfusion map density showed median values of 53.7%, 57.3%, 64.3%, 70.6%, and 71.1%, respectively, indicating an increase in perfusion due to the local anesthetic. Our easy-to-implement real-time video processing algorithm demonstrates potential for skin perfusion monitoring during RA procedures.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Rubins_Photoplethysmography_Imaging_Algorithm_for_Real-Time_Monitoring_of_Skin_Perfusion_Maps_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Rubins_Photoplethysmography_Imaging_Algorithm_for_Real-Time_Monitoring_of_Skin_Perfusion_Maps_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Deep Learning-Enabled Sleep Staging From Vital Signs and Activity Measured Using a Near-Infrared Video Camera",
        "author": "Jonathan Carter, Jo\u00e3o Jorge, Bindia Venugopal, Oliver Gibson, Lionel Tarassenko",
        "abstract": "Conventional sleep monitoring is time-consuming, expensive and uncomfortable, requiring a large number of contact sensors to be attached to the patient. Video data is commonly recorded as part of a sleep laboratory assessment. If accurate sleep staging could be achieved solely from video, this would overcome many of the problems of traditional methods. In this work we use heart rate, breathing rate and activity measures, all derived from a near-infrared video camera, to perform sleep stage classification. We use a deep transfer learning approach to overcome data scarcity, by using an existing contact-sensor dataset to learn effective representations from the heart and breathing rate time series. Using a dataset of 50 healthy volunteers, we achieve an accuracy of 73.4% and a Cohen's kappa of 0.61 in four-class sleep stage classification, establishing a new state-of-the-art for video-based sleep staging.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Carter_Deep_Learning-Enabled_Sleep_Staging_From_Vital_Signs_and_Activity_Measured_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Carter_Deep_Learning-Enabled_Sleep_Staging_From_Vital_Signs_and_Activity_Measured_CVPRW_2023_paper.pdf"
    },
    {
        "title": "LSTC-rPPG: Long Short-Term Convolutional Network for Remote Photoplethysmography",
        "author": "Jun Seong Lee, Gyutae Hwang, Moonwook Ryu, Sang Jun Lee",
        "abstract": "Remote photoplethysmography (rPPG) is a non-contact technique for measuring blood pulse signals associated with cardiac activity. Although rPPG is considered an alternative to traditional contact-based photoplethysmography (PPG) because of its non-contact nature, obtaining reliable measurements remains a challenge owing to the sensitiveness of rPPG. In recent years, deep learning-based methods have improved the reliability of rPPG, but they suffer from certain limitations in utilizing long-term features such as periodic tendencies over long durations. In this paper, we propose a deep learning-based method that models long short-term spatio-temporal features and optimizes the long short-term features, ensuring reliable rPPG. The proposed method is composed of three key components: i) a deep learning architecture, denoted by LSTC-rPPG, which models long short-term spatio-temporal features and combines the features for reliable rPPG, ii) a temporal attention refinement module that mitigates temporal mismatches between the long-term and short-term features, and iii) a frequency scale invariant hybrid loss to guide long-short term features. In experiments on the UBFC-rPPG database, the proposed method demonstrated a mean absolute error of 0.7, root mean square error of 1.0, and Pearson correlation coefficient of 0.99 for heart rate estimation accuracy, outperforming contemporary state-of-the-art methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Lee_LSTC-rPPG_Long_Short-Term_Convolutional_Network_for_Remote_Photoplethysmography_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Lee_LSTC-rPPG_Long_Short-Term_Convolutional_Network_for_Remote_Photoplethysmography_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Single Image Based Infant Body Height and Weight Estimation",
        "author": "Huaijing Shu, Lirong Ren, Liping Pan, Dongmin Huang, Hongzhou Lu, Wenjin Wang",
        "abstract": "The collection of infant body data such as height and weight is a useful mean of tracking its growth and wellness. The contact-based measurements using height and weight scales are manual and cumbersome, camera-based methods were proposed to obtain features from the adult face or body for height and weight estimation. In this paper, we created a clinical dataset including 200 newborns collected at obstetrics, and benchmarked four convolutional neural networks for infant height estimation, where the MobileNet, a lightweight and efficient network, was chosen as the backbone for deep feature extraction. Moreover, we investigated different MobileNet-based variants for infant weight estimation, including the linear regression model, one-task model and multi-task model. Several sets of experiment were carried out on the newborn dataset to validate the effectiveness of the proposed methods. The results show that the Mean Absolute Error (MAE) of different models are quite similar, which are in a decent error range (average MAE for height estimation is <1.1 cm and average MAE for weight estimation is <0.28 kg). Among them, the multi-task MobileNet has better temporal stability given its lower variance of measurement in a video.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Shu_Single_Image_Based_Infant_Body_Height_and_Weight_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Shu_Single_Image_Based_Infant_Body_Height_and_Weight_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Respiratory Rate Estimation Based on Detected Mask Area in Thermal Images",
        "author": "Natalia Kowalczyk, Jacek Ruminski",
        "abstract": "The popularity of non-contact methods of measuring vital signs, particularly respiratory rate, has increased during the SARS-COV-2 pandemic. Breathing parameters can be estimated by analysis of temperature changes observed in thermal images of nostrils or mouth regions. However, wearing virus-protection face masks prevents direct detection of such face regions. In this work, we propose to use an automatic mask detection approach to select pixels within a mask region as a source of respiration information allowing efficient estimation of respiratory signals. We performed experiments with two important types of virus protection masks, i.e., FFP2 (N95) and surgical masks, for subjects while sitting, slowly walking from a short distance toward a camera, and slowly walking with moderate head movements. Experiments conducted with the adapted YOLO model have shown that detection of the mask area on the face allows for higher SNR values and reduces error in respiratory rate estimation in all analyzed scenarios. The Mean Absolute Error for respiratory rate estimation was below 1 bpm for sitting subjects for all types of masks. The error for walking subjects was 1.21 bpm for an FFP2 mask and about 2.1 bpm for a surgical mask. In the presence of head movements, while walking, the MAE was below 1.39 bpm and less than 1 bpm when only one outlier was removed.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/Kowalczyk_Respiratory_Rate_Estimation_Based_on_Detected_Mask_Area_in_Thermal_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/Kowalczyk_Respiratory_Rate_Estimation_Based_on_Detected_Mask_Area_in_Thermal_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Improving Systolic Blood Pressure Prediction From Remote Photoplethysmography Using a Stacked Ensemble Regressor",
        "author": "Lieke D. van Putten, Kate E. Bamford",
        "abstract": "Hypertension is a serious health risk, and early diagnosis is key to start treatment and avoid fatal complications. We present a stacked ensemble model to predict systolic blood pressure from remote photoplethysmogramy, which enables cuffless measurements. To train the stacked ensemble model, a large dataset with facial remote photoplethysmogramy signals and ground truth values for blood pressure was collected by trained clinicians. From over 4500 measurements 1410 were selected for training following quality control. Over 100 different features were derived from these signals, including statistical features, time domain and frequency domain features. Nine of these features were selected using a forward feature selector. We verified the accuracy of the model on a separately collected validation set. Using a multi-layer perceptron regressor, linear support vector regressor, radial support vector regressor, and ElasticNet for the base models combined with a support vector machine classifier in the stacked ensemble and a RidgeCV model for the final layer, the mean error of the model is reduced to 1.1 mmHg, mean absolute error to 9.5 mmHg and the standard deviation to 12.3 mmHg. Critically, 79% of the hypertensive patients are correctly identified as hypertensive with a prediction over 140 mmHg.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/html/van_Putten_Improving_Systolic_Blood_Pressure_Prediction_From_Remote_Photoplethysmography_Using_a_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/CVPM/papers/van_Putten_Improving_Systolic_Blood_Pressure_Prediction_From_Remote_Photoplethysmography_Using_a_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Learning Multi-Scale Representations With Single-Stream Network for Video Retrieval",
        "author": "Chia-Hui Wang, Yu-Chee Tseng, Ting-Hui Chiang, Yan-Ann Chen",
        "abstract": "With the explosive growth of video contents in the Internet, video retrieval has become an important issue that can benefit video recommendation and copyright detection. Since the key features of a video may distribute in distant regions of a lengthy video, several works have made a success by exploiting multi-stream, multi-scale architectures to learn and merge distant features. However, a multi-stream network is costly in terms of memory and computing overhead. The number of scales and these scales are handcrafted and fixed once a model is finalized. Further, being more complicated, multi-stream networks are more prone to being overfitting and lead to poorer generalization. This paper proposes a single-stream network with built-in dilated spatial and temporal learning capability. By combining with modern techniques, including Denoising Autoencoder, Squeeze-and-Excitation Attention, and Triplet Comparative Mechanism, our model achieves state-of-the-art performance in several video retrieval tasks on the FIVR-200K, CC_WEB_VIDEO, and EVVE datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Wang_Learning_Multi-Scale_Representations_With_Single-Stream_Network_for_Video_Retrieval_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Wang_Learning_Multi-Scale_Representations_With_Single-Stream_Network_for_Video_Retrieval_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ZippyPoint: Fast Interest Point Detection, Description, and Matching Through Mixed Precision Discretization",
        "author": "Menelaos Kanakis, Simon Maurer, Matteo Spallanzani, Ajad Chhatkuli, Luc Van Gool",
        "abstract": "Efficient detection and description of geometric regions in images is a prerequisite in visual systems for localization and mapping. Such systems still rely on traditional hand-crafted methods for efficient generation of lightweight descriptors, a common limitation of the more powerful neural network models that come with high compute and specific hardware requirements. In this paper, we focus on the adaptations required by detection and description neural networks to enable their use in computationally limited platforms such as robots, mobile, and augmented reality devices. To that end, we investigate and adapt network quantization techniques to accelerate inference and enable its use on compute limited platforms. In addition, we revisit common practices in descriptor quantization and propose the use of a binary descriptor normalization layer, enabling the generation of distinctive binary descriptors with a constant number of ones. ZippyPoint, our efficient quantized network with binary descriptors, improves the network runtime speed, the descriptor matching speed, and the 3D model size, by at least an order of magnitude when compared to full-precision counterparts. These improvements come at a minor performance degradation as evaluated on the tasks of homography estimation, visual localization, and map-free visual relocalization. Code and models are available at https://github.com/menelaoskanakis/ZippyPoint.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Kanakis_ZippyPoint_Fast_Interest_Point_Detection_Description_and_Matching_Through_Mixed_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Kanakis_ZippyPoint_Fast_Interest_Point_Detection_Description_and_Matching_Through_Mixed_CVPRW_2023_paper.pdf"
    },
    {
        "title": "SphereGlue: Learning Keypoint Matching on High Resolution Spherical Images",
        "author": "Christiano Gava, Vishal Mukunda, Tewodros Habtegebrial, Federico Raue, Sebastian Palacio, Andreas Dengel",
        "abstract": "Traditionally, spherical keypoint matching has been performed using greedy algorithms, such as Nearest Neighbors (NN) search. NN based algorithms often lead to erroneous or insufficient matches as they fail to leverage global keypoint neighborhood information. Inspired by a recent learned perspective matching approach we introduce SphereGlue: a Graph Neural Network based feature matching for high-resolution spherical images. The proposed model naturally handles the severe distortions resulting from geometric transformations. Rigorous evaluations demonstrate the efficacy of SphereGlue in matching both learned and handcrafted keypoints, on synthetic and real high-resolution spherical images. Moreover, SphereGlue generalizes well to previously unseen real-world and synthetic scenes. Results on camera pose estimation show that SphereGlue can directly replace state-of-the-art matching algorithms, in downstream tasks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Gava_SphereGlue_Learning_Keypoint_Matching_on_High_Resolution_Spherical_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Gava_SphereGlue_Learning_Keypoint_Matching_on_High_Resolution_Spherical_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Are Local Features All You Need for Cross-Domain Visual Place Recognition?",
        "author": "Giovanni Barbarani, Mohamad Mostafa, Hajali Bayramov, Gabriele Trivigno, Gabriele Berton, Carlo Masone, Barbara Caputo",
        "abstract": "Visual Place Recognition is a task that aims to predict the coordinates of an image (called query) based solely on visual clues. Most commonly, a retrieval approach is adopted, where the query is matched to the most similar images from a large database of geotagged photos, using learned global descriptors. Despite recent advances, recognizing the same place when the query comes from a significantly different distribution is still a major hurdle for state of the art retrieval methods. Examples are heavy illumination changes (e.g. night-time images) or substantial occlusions (e.g. transient objects). In this work we explore whether re-ranking methods based on spatial verification can tackle these challenges, following the intuition that local descriptors are inherently more robust than global features to domain shifts. To this end, we provide a new, comprehensive benchmark on current state of the art models. We also introduce two new demanding datasets with night and occluded queries, to be matched against a city-wide database. Code and datasets are available at https://github.com/gbarbarani/re-ranking-for-VPR.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Barbarani_Are_Local_Features_All_You_Need_for_Cross-Domain_Visual_Place_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Barbarani_Are_Local_Features_All_You_Need_for_Cross-Domain_Visual_Place_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Structured Epipolar Matcher for Local Feature Matching",
        "author": "Jiahao Chang, Jiahuan Yu, Tianzhu Zhang",
        "abstract": "Local feature matching is challenging due to textureless and repetitive patterns. Existing methods focus on using appearance features and global interaction and matching, while the importance of geometry priors in local feature matching has not been fully exploited. Different from these methods, in this paper, we delve into the importance of geometry prior and propose Structured Epipolar Matcher (SEM) for local feature matching, which can leverage the geometric information in an iterative matching way. The proposed model enjoys several merits. First, our proposed Structured Feature Extractor can model the relative positional relationship between pixels and high-confidence anchor points. Second, our proposed Epipolar Attention and Matching can filter out irrelevant areas by utilizing the epipolar constraint. Extensive experimental results on five standard benchmarks demonstrate the superior performance of our SEM compared to state-of-the-art methods. Project page: https://sem2023.github.io.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Chang_Structured_Epipolar_Matcher_for_Local_Feature_Matching_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Chang_Structured_Epipolar_Matcher_for_Local_Feature_Matching_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Multi-Scale Local Implicit Keypoint Descriptor for Keypoint Matching",
        "author": "JongMin Lee, Eunhyeok Park, Sungjoo Yoo",
        "abstract": "We investigate the potential of multi-scale descriptors which has been under-explored in the existing literature. At the pixel level, we propose utilizing both coarse and fine-grained descriptors and present a scale-aware method of negative sampling, which trains descriptors at different scales in a complementary manner, thereby improving their discriminative power. For sub-pixel level descriptors, we also propose adopting coordinate-based implicit modeling and learning the non-linearity of local descriptors on continuous-domain coordinates. Our experiments show that the proposed method achieves state-of-the-art performance on various tasks, i.e., image matching, relative pose estimation, and visual localization.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Lee_Multi-Scale_Local_Implicit_Keypoint_Descriptor_for_Keypoint_Matching_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Lee_Multi-Scale_Local_Implicit_Keypoint_Descriptor_for_Keypoint_Matching_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Find My Astronaut Photo: Automated Localization and Georectification of Astronaut Photography",
        "author": "Alex Stoken, Kenton Fisher",
        "abstract": "Astronaut photography from the International Space Station (ISS) forms one of the longest continuous remote sensing datasets of Earth and has facilitated a large body of research ranging from glacial surface area analysis to volcanic sediment delivery. Such studies are enabled by the geolocation and georectification of the imagery. Yet, localizing astronaut photography of Earth is a challenging and labor-intensive task, tempering the amount of research that can be performed. We present a method for automatically localizing these images named Find My Astronaut Photo, which makes this task feasible by casting the problem as a precision-oriented image similarity and matching exercise. As the ISS orbits the globe, astronauts can view and photograph most locations on Earth, so there is no precomputable database of finite landmarks for image comparison. Therefore, we iteratively generate potentially similar images from geolocated satellite imagery on-demand and rely on an image matcher to discriminately detect overall similarity between these images and an astronaut photo. We evaluate various image matching techniques to find methods which allow us to discretize and reduce our search space to a manageable size, and locate astronaut photographs with high precision and speed. Find My Astronaut Photo has successfully geolocated over 30,000 photos to date, adding critical location information that increases the downstream utility of the Gateway to Astronaut Photography of Earth (GAPE) database. We also introduce AIMS, the Astronaut Imagery Matching Subset, a new real world evaluation dataset that joins the collection of challenging image matching benchmarks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Stoken_Find_My_Astronaut_Photo_Automated_Localization_and_Georectification_of_Astronaut_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Stoken_Find_My_Astronaut_Photo_Automated_Localization_and_Georectification_of_Astronaut_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Geometry Enhanced Reference-Based Image Super-Resolution",
        "author": "Han Zou, Liang Xu, Takayuki Okatani",
        "abstract": "With the prevalence of smartphones equipped with a multi-camera system comprising multiple cameras with different field-of-view (FoVs), images captured by two or three cameras now share a portion of the FoV that are compatible with reference-based super-resolution (RefSR). In this work, we propose a novel RefSR model that utilizes geometric matching methods to enhance its performance in two aspects. First, we integrate geometric matching maps to improve feature fusion. Second, we train the matching modules equipped in the RefSR models under the supervision of accurate geometric matching maps to increase their robustness. Our experimental results demonstrate the effectiveness and state-of-the-art performance of the proposed method.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Zou_Geometry_Enhanced_Reference-Based_Image_Super-Resolution_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Zou_Geometry_Enhanced_Reference-Based_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Language Guided Local Infiltration for Interactive Image Retrieval",
        "author": "Fuxiang Huang, Lei Zhang",
        "abstract": "Interactive Image Retrieval (IIR) aims to retrieve images that are generally similar to the reference image but under the requested text modification. The existing methods usually concatenate or sum the features of image and text simply and roughly, which, however, is difficult to precisely change the local semantics of the image that the text intends to modify. To solve this problem, we propose a Language Guided Local Infiltration (LGLI) system, which fully utilizes the text information and penetrates text features into image features as much as possible. Specifically, we first propose a Language Prompt Visual Localization (LPVL) module to generate a localization mask which explicitly locates the region (semantics) intended to be modified. Then we introduce a Text Infiltration with Local Awareness (TILA) module, which is deployed in the network to precisely modify the reference image and generate image-text infiltrated representation. Extensive experiments on various benchmark databases validate that our method outperforms most state-of-the-art IIR approaches.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Huang_Language_Guided_Local_Infiltration_for_Interactive_Image_Retrieval_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Huang_Language_Guided_Local_Infiltration_for_Interactive_Image_Retrieval_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ConVol-E: Continuous Volumetric Embeddings for Human-Centric Dense Correspondence Estimation",
        "author": "Amogh Tiwari, Pranav Manu, Nakul Rathore, Astitva Srivastava, Avinash Sharma",
        "abstract": "We present Continuous Volumetric Embeddings (ConVol-E), a novel robust representation for dense correspondence-matching across RGB images of different human subjects in arbitrary poses and appearances under non-rigid deformation scenarios. Unlike existing represen-tations [8, 14], ConVol-E captures the deviation from the underlying parametric body model by choosing suitable anchor/key points on the underlying parametric body surface and then representing any point in the volume based on its euclidean relationship with the anchor points. It allows us to represent any arbitrary point around the parametric body (clothing details, hair, etc.) by an embedding vector. Subsequently, given a monocular RGB image of a person, we learn to predict per-pixel ConVol-E embedding, which carries a similar meaning across different subjects and is invariant to pose and appearance, thereby acting as a descriptor to establish robust dense correspondences across different images of humans. We empirically evaluate our proposed embedding using a novel metric and show superior performance compared to the state-of-the-art for the task of in-the-wild dense correspondence matching across different subjects, camera views, and appearance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Tiwari_ConVol-E_Continuous_Volumetric_Embeddings_for_Human-Centric_Dense_Correspondence_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Tiwari_ConVol-E_Continuous_Volumetric_Embeddings_for_Human-Centric_Dense_Correspondence_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeepRM: Deep Recurrent Matching for 6D Pose Refinement",
        "author": "Alexander Avery, Andreas Savakis",
        "abstract": "Precise 6D pose estimation of rigid objects from RGB images is a critical but challenging task in robotics, augmented reality and human-computer interaction. To address this problem, we propose DeepRM, a novel recurrent network architecture for 6D pose refinement. DeepRM leverages initial coarse pose estimates to render synthetic images of target objects. The rendered images are then matched with the observed images to predict a rigid transform for updating the previous pose estimate. This process is repeated to incrementally refine the estimate at each iteration. The DeepRM architecture incorporates LSTM units to propagate information through each refinement step, significantly improving overall performance. In contrast to current 2-stage Perspective-n-Point based solutions, DeepRM is trained end-to-end, and uses a scalable backbone that can be tuned via a single parameter for accuracy and efficiency. During training, a multi-scale optical flow head is added to predict the optical flow between the observed and synthetic images. Optical flow prediction stabilizes the training process, and enforces the learning of features that are relevant to the task of pose estimation. Our results demonstrate that DeepRM achieves state-of-the-art performance on two widely accepted challenging datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/html/Avery_DeepRM_Deep_Recurrent_Matching_for_6D_Pose_Refinement_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/papers/Avery_DeepRM_Deep_Recurrent_Matching_for_6D_Pose_Refinement_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Pretrained Pixel-Aligned Reference Network for 3D Human Reconstruction",
        "author": "Gee-Sern Hsu, Yu-Hong Lin, Chin-Cheng Chang",
        "abstract": "We propose the Pretrained Pixel-aligned Reference (PPR) network for 3D human reconstruction. The PPR network utilizes a pretrained model embedded with a reference mesh surface and full-view normals to better constrain spatial query processing, leading to improved mesh surface reconstruction. Our network consists of a dual-path encoder and a query network. The dual-path encoder extracts front-back view features from the input image through one path, and full-view reference features from a pretrained model through the other path. These features, along with additional spatial traits, are concatenated and processed by the query network to estimate the desired mesh surface. During training, we consider points on the pretrained model as well as around the ground-truth mesh surfaces, enabling the implicit function to better capture the mesh surface and overall posture. We evaluate the performance of our approach through experiments on the THuman 2.0 and RenderPeople datasets, and compare it with state-of-the-art methods.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/html/Hsu_Pretrained_Pixel-Aligned_Reference_Network_for_3D_Human_Reconstruction_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/papers/Hsu_Pretrained_Pixel-Aligned_Reference_Network_for_3D_Human_Reconstruction_CVPRW_2023_paper.pdf"
    },
    {
        "title": "KBody: Towards General, Robust, and Aligned Monocular Whole-Body Estimation",
        "author": "Nikolaos Zioulis, James F. O'Brien",
        "abstract": "KBody is a method for fitting a low-dimensional body model to an image. It follows a predict-and-optimize approach, relying on data-driven model estimates for the constraints that will be used to solve for the body's parameters. Acknowledging the importance of high quality correspondences, it leverages \"virtual joints\" to improve fitting performance, disentangles the optimization between the pose and shape parameters, and integrates asymmetric distance fields to strike a balance in terms of pose and shape capturing capacity, as well as pixel alignment. We also show that generative model inversion offers a strong appearance prior that can be used to complete partial human images and used as a building block for generalized and robust monocular body fitting. Project page: https://klothed.github.io/KBody.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/html/Zioulis_KBody_Towards_General_Robust_and_Aligned_Monocular_Whole-Body_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/papers/Zioulis_KBody_Towards_General_Robust_and_Aligned_Monocular_Whole-Body_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CherryPicker: Semantic Skeletonization and Topological Reconstruction of Cherry Trees",
        "author": "Lukas Meyer, Andreas Gilson, Oliver Scholz, Marc Stamminger",
        "abstract": "In plant phenotyping, accurate trait extraction from 3D point clouds of trees is still an open problem. For automatic modeling and trait extraction of tree organs such as blossoms and fruits, the semantically segmented point cloud of a tree and the tree skeleton are necessary. Therefore, we present CherryPicker, an automatic pipeline that reconstructs photo-metric point clouds of trees, performs semantic segmentation and extracts their topological structure in form of a skeleton. Our system combines several state-of-the-art algorithms to enable automatic processing for further usage in 3D-plant phenotyping applications. Within this pipeline, we present a method to automatically estimate the scale factor of a monocular reconstruction to overcome scale ambiguity and obtain metrically correct point clouds. Furthermore, we propose a semantic skeletonization algorithm build up on Laplacian-based contraction. We also show by weighting different tree organs semantically, our approach can effectively remove artifacts induced by occlusion and structural size variations. CherryPicker obtains high-quality topology reconstructions of cherry trees with precise details.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/html/Meyer_CherryPicker_Semantic_Skeletonization_and_Topological_Reconstruction_of_Cherry_Trees_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/papers/Meyer_CherryPicker_Semantic_Skeletonization_and_Topological_Reconstruction_of_Cherry_Trees_CVPRW_2023_paper.pdf"
    },
    {
        "title": "On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild",
        "author": "Raiyan Rahman, Christopher Indris, Tianxiao Zhang, Kaidong Li, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang",
        "abstract": "Aphid infestations can cause extensive damage to wheat and sorghum fields and spread plant viruses, resulting in significant yield losses in agriculture. To address this issue, farmers often rely on chemical pesticides, which are inefficiently applied over large areas of fields. As a result, a considerable amount of pesticide is wasted on areas without pests, while inadequate amounts are applied to areas with severe infestations. The paper focuses on the urgent need for an intelligent autonomous system that can locate and spray infestations within complex crop canopies, reducing pesticide use and environmental impact. We have collected and labeled a large aphid image dataset in the field, and propose the use of real-time semantic segmentation models to segment clusters of aphids. A multiscale dataset is generated to allow for learning the clusters at different scales. We compare the segmentation speeds and accuracy of four state-of-the-art real-time semantic segmentation models on the aphid cluster dataset, benchmarking them against nonreal-time models. The study results show the effectiveness of a real-time solution, which can reduce inefficient pesticide use and increase crop yields, paving the way towards an autonomous pest detection system.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/html/Rahman_On_the_Real-Time_Semantic_Segmentation_of_Aphid_Clusters_in_the_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/papers/Rahman_On_the_Real-Time_Semantic_Segmentation_of_Aphid_Clusters_in_the_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A Realistic Synthetic Mushroom Scenes Dataset",
        "author": "Dafni Anagnostopoulou, George Retsinas, Niki Efthymiou, Panagiotis Filntisis, Petros Maragos",
        "abstract": "In this work, we present the Realistic Synthetic Mushroom Scenes Dataset, which encompasses images depicting mushrooms in various settings in relatively cluttered scenes. The dataset is composed of 15,000 high-quality, realistic images with various useful annotations. The dataset can be leveraged to address problems associated with mushroom detection, instance segmentation, and 3D pose estimation. These tasks are of paramount importance in automating the mushroom harvesting process in mushroom farms, which is a challenging and costly procedure. Also, we proffer a three-step pipeline that can generate annotated and realistic synthetic images, commencing with a singular 3D model that can be easily applied to a range of crops beyond mushrooms (https://github.com/dafniana/Synthetic-Mushroom-Dataset).",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/html/Anagnostopoulou_A_Realistic_Synthetic_Mushroom_Scenes_Dataset_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/papers/Anagnostopoulou_A_Realistic_Synthetic_Mushroom_Scenes_Dataset_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Agronav: Autonomous Navigation Framework for Agricultural Robots and Vehicles Using Semantic Segmentation and Semantic Line Detection",
        "author": "Shivam K. Panda, Yongkyu Lee, M. Khalid Jawed",
        "abstract": "The successful implementation of vision-based navigation in agricultural fields hinges upon two critical components: 1) the accurate identification of key components within the scene, and 2) the identification of lanes through the detection of boundary lines that separate the crops from the traversable ground. We propose Agronav, an end-to-end vision-based autonomous navigation framework, which outputs the centerline from the input image by sequentially processing it through semantic segmentation and semantic line detection models. We also present Agroscapes, a pixel-level annotated dataset collected across six different crops, captured from varying heights and angles. This ensures that the framework trained on Agroscapes is generalizable across both ground and aerial robotic platforms. Codes, models and dataset will be publicly released.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/html/Panda_Agronav_Autonomous_Navigation_Framework_for_Agricultural_Robots_and_Vehicles_Using_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/papers/Panda_Agronav_Autonomous_Navigation_Framework_for_Agricultural_Robots_and_Vehicles_Using_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Mushroom Segmentation and 3D Pose Estimation From Point Clouds Using Fully Convolutional Geometric Features and Implicit Pose Encoding",
        "author": "George Retsinas, Niki Efthymiou, Petros Maragos",
        "abstract": "Modern agricultural applications rely more and more on deep learning solutions. However, training well-performing deep networks requires a large amount of annotated data that may not be available and in the case of 3D annotation may not even be feasible for human annotators. In this work, we develop a deep learning approach to segment mushrooms and estimate their pose on 3D data, in the form of point clouds, acquired by depth sensors. To circumvent the annotation problem, we create a synthetic dataset of mushroom scenes, where we are fully aware of 3D information, such as the pose of each mushroom. The proposed network has a fully convolutional backbone, that parses sparse 3D data, and predicts pose information that implicitly defines both instance segmentation and pose estimation task. We have validated the effectiveness of the proposed implicit-based approach for a synthetic test set, as well as provided qualitative results for a small set of real acquired point clouds with depth sensors.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/html/Retsinas_Mushroom_Segmentation_and_3D_Pose_Estimation_From_Point_Clouds_Using_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/papers/Retsinas_Mushroom_Segmentation_and_3D_Pose_Estimation_From_Point_Clouds_Using_CVPRW_2023_paper.pdf"
    },
    {
        "title": "MTLSegFormer: Multi-Task Learning With Transformers for Semantic Segmentation in Precision Agriculture",
        "author": "Diogo Nunes Goncalves, Jose Marcato Junior, Pedro Zamboni, Hemerson Pistori, Jonathan Li, Keiller Nogueira, Wesley Nunes Gon\u00e7alves",
        "abstract": "Multi-task learning has proven to be effective in improving the performance of correlated tasks. Most of the existing methods use a backbone to extract initial features with independent branches for each task, and the exchange of information between the branches usually occurs through the concatenation or sum of the feature maps of the branches. However, this type of information exchange does not directly consider the local characteristics of the image nor the level of importance or correlation between the tasks. In this paper, we propose a semantic segmentation method, MTLSegFormer, which combines multi-task learning and attention mechanisms. After the backbone feature extraction, two feature maps are learned for each task. The first map is proposed to learn features related to its task, while the second map is obtained by applying learned visual attention to locally re-weigh the feature maps of the other tasks. In this way, weights are assigned to local regions of the image of other tasks that have greater importance for the specific task. Finally, the two maps are combined and used to solve a task. We tested the performance in two challenging problems with correlated tasks and observed a significant improvement in accuracy, mainly in tasks with high dependence on the others.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/html/Goncalves_MTLSegFormer_Multi-Task_Learning_With_Transformers_for_Semantic_Segmentation_in_Precision_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/papers/Goncalves_MTLSegFormer_Multi-Task_Learning_With_Transformers_for_Semantic_Segmentation_in_Precision_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PeanutNeRF: 3D Radiance Field for Peanuts",
        "author": "Farah Saeed, Jin Sun, Peggy Ozias-Akins, Ye (Juliet) Chu, Changying (Charlie) Li",
        "abstract": "Accurate phenotypic analysis can help plant breeders efficiently identify and analyze suitable plant traits to enhance crop yield. While 2D images from RGB cameras are easily accessible, their trait estimation performance is limited due to occlusion and the absence of depth information. On the other hand, 3D data from LiDAR sensors are noisy and limited in their ability to capture very thin plant parts such as peanut plant pegs. To combine the merits of both the 2D and 3D data analysis, the 2D images were used to capture thin parts in peanut plants, and deep learning-based 3D reconstruction using captured 2D images was performed to obtain 3D point clouds with information about the scene from different angles. The neural radiance fields were optimized for implicit 3D representation of the plants. The trained radiance fields were queried for 3D reconstruction to achieve point clouds for a 360-degree view and frontal view of the plant. With frontal-view reconstruction and the corresponding 2D images, we used Frustum PVCNN to perform 3D detection of peanut pods. We showed the effectiveness of PeanutNeRF on peanut plants with and without foliage: it showed negligible noise and a chamfer distance of less than 0.0004 from a manually cleaned version. The pod detection showed a precision of around 0.7 at the IoU threshold of 0.5 on the validation set. This method can assist in accurate plant phenotypic studies of peanuts and other important crops.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/html/Saeed_PeanutNeRF_3D_Radiance_Field_for_Peanuts_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/papers/Saeed_PeanutNeRF_3D_Radiance_Field_for_Peanuts_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ECA-ConvNeXt: A Rice Leaf Disease Identification Model Based on ConvNeXt",
        "author": "Xiaoqi Wang, Yaojun Wang, Jingbo Zhao, Jing Niu",
        "abstract": "As an essential food crop, rice is often infested with diseases that can cause significant yield losses and seriously damage economic income and food health. Early identification and control of rice diseases is an effective way to alleviate these problems. However, manual identification and diagnosis of rice leaf diseases requires experienced specialists and is time-consuming. In our study, we propose the ECA-ConvNeXt model, based on the ConvNeXt network, which can identify six categories of typical rice leaf diseases and healthy rice leaves. We also established a rice leaf disease identification dataset that contains images of healthy and diseased rice leaves with complex backgrounds and their disease category labels. In the proposed ECA-ConvNeXt model, we incorporated the ECA (Efficient Channel Attention) module, which improved the feature extraction performance using only a few parameters. Transfer learning was applied to load pre-training weights and fine-tuning was used to reduce training costs and improve the model performance. We tested the performance of ECA-ConvNeXt on the rice leaf disease identification dataset. Experimental results show that the proposed model achieved an accuracy of 94.82%, a precision of 94.47%, a recall rate of 94.31%, and an F1-Score of 94.33% on the rice leaf disease identification dataset. These results suggest that the proposed network effectively identifies rice leaf diseases.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/html/Wang_ECA-ConvNeXt_A_Rice_Leaf_Disease_Identification_Model_Based_on_ConvNeXt_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/AgriVision/papers/Wang_ECA-ConvNeXt_A_Rice_Leaf_Disease_Identification_Model_Based_on_ConvNeXt_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ConvMLP: Hierarchical Convolutional MLPs for Vision",
        "author": "Jiachen Li, Ali Hassani, Steven Walton, Humphrey Shi",
        "abstract": "MLP-based architectures, which consist of a sequence of consecutive multi-layer perceptron blocks, have recently been found to reach comparable results to convolutional and transformer-based methods. However, most adopt spatial MLPs which take fixed dimension inputs, therefore making it difficult to apply them to downstream tasks, such as object detection and semantic segmentation. Moreover, single-stage designs further limit performance in other computer vision tasks and fully connected layers bear heavy computation. To tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for visual recognition, which is a lightweight, stage-wise, co-design of convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1 accuracy on ImageNet-1k with 9M parameters and 2.4 GMACs (15% and 19% of MLP-Mixer-B/16, respectively). Experiments on object detection and semantic segmentation further show that visual representation learned by ConvMLP can be seamlessly transferred and achieve competitive results with fewer parameters. Our code and pre-trained models are open-sourced at https://github.com/SHI-Labs/Convolutional-MLPs.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WFM/html/Li_ConvMLP_Hierarchical_Convolutional_MLPs_for_Vision_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WFM/papers/Li_ConvMLP_Hierarchical_Convolutional_MLPs_for_Vision_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation",
        "author": "Yifeng Shi, Feng Lv, Xinliang Wang, Chunlong Xia, Shaojie Li, Shujie Yang, Teng Xi, Gang Zhang",
        "abstract": "With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledge, Open-TransMind is the first open-source transportation foundation model with multi-task and multi-modal capabilities. Simultaneously, Open-TransMind can achieve state-of-the-art performance on detection, classification, and segmentation datasets of traffic scenarios. Our source code is available at https://github.com/Traffic-X/Open-TransMind.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WFM/html/Shi_Open-TransMind_A_New_Baseline_and_Benchmark_for_1st_Foundation_Model_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WFM/papers/Shi_Open-TransMind_A_New_Baseline_and_Benchmark_for_1st_Foundation_Model_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Cali-NCE: Boosting Cross-Modal Video Representation Learning With Calibrated Alignment",
        "author": "Nanxuan Zhao, Jianbo Jiao, Weidi Xie, Dahua Lin",
        "abstract": "With the large-scale video-text datasets being collected, learning general visual-textual representation has gained increasing attention. While recent methods are designed with the assumption that the alt-text description naturally conveys the meaning and context of the video in semantics (i.e. well aligned with each other), it is unlikely to be satisfied for the Internet data, which potentially harms the quality of the learned visual-textual representation. To address this challenge, we first revisit three mainstream approaches: correspondence modeling, contrastive learning and predictive coding, demonstrating that a simple co-training strategy with these methods leads to a clear improvement in performance. To further explore the complementary nature of different training strategies, we propose a simple yet effective joint training framework that factorizes the total objective into conditional ones, termed as Cali-NCE. Our method first estimates confidence scores for measuring the correspondence between video and text descriptions, and the scores are later used to calibrate the sample weightings during contrastive training. Through extensive experiments, we show that the proposed approach achieves state-of-the-art performance on multiple downstream tasks: text-to-video retrieval, video action recognition, and video retrieval. Code and models will be made publicly available.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/WFM/html/Zhao_Cali-NCE_Boosting_Cross-Modal_Video_Representation_Learning_With_Calibrated_Alignment_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/WFM/papers/Zhao_Cali-NCE_Boosting_Cross-Modal_Video_Representation_Learning_With_Calibrated_Alignment_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Flexible-Modal Face Anti-Spoofing: A Benchmark",
        "author": "Zitong Yu, Ajian Liu, Chenxu Zhao, Kevin H. M. Cheng, Xu Cheng, Guoying Zhao",
        "abstract": "Face anti-spoofing (FAS) plays a vital role in securing face recognition systems from presentation attacks. Benefitted from the maturing camera sensors, single-modal (RGB) and multi-modal (e.g., RGB+Depth) FAS has been applied in various scenarios with different configurations of sensors/modalities. Existing single- and multi-modal FAS methods usually separately train and deploy models for each possible modality scenario, which might be redundant and inefficient. Can we train a unified model, and flexibly deploy it under various modality scenarios? In this paper, we establish the first flexible-modal FAS benchmark with the principle 'train one for all'. To be specific, with trained multi-modal (RGB+Depth+IR) FAS models, both intra- and cross-dataset testings are conducted on four flexible-modal sub-protocols (RGB, RGB+Depth, RGB+IR, and RGB+Depth+IR). We also investigate prevalent deep models and feature fusion strategies for flexible-modal FAS. We hope this new benchmark will facilitate the future research of the multi-modal FAS. The protocols and codes are available at https://github.com/ZitongYu/Flex-Modal-FAS",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/html/Yu_Flexible-Modal_Face_Anti-Spoofing_A_Benchmark_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/papers/Yu_Flexible-Modal_Face_Anti-Spoofing_A_Benchmark_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Bandpass Filter Based Dual-Stream Network for Face Anti-Spoofing",
        "author": "Dingheng Zeng, Liang Gao, Hao Fang, Guohui Xiang, Yue Feng, Quan Lu",
        "abstract": "Face Attack Detection (PAD) technology is crucial for protecting facial recognition systems. At present, methods for Face Anti-spoofing (FAS) mainly focus on short-distance applications, and algorithm performance can sharply decline when facing challenges such as low resolution, pedestrian obstruction, and blurriness in long-distance scenarios. To address these issues, we propose a dual-stream architecture that combines information from the images and its bandpass filtered image to distinguish attacks. Specifically, one branch extracts detailed facial structure and texture information from the original spatial domain of images. The other branch take the Gaussian bandpass filtered image as input to learn the complementary discriminative features. The filtering process was done in frequency domain by FFT/IFFT. We proposed a cross-attention fusion module to fuse the features extracted by the two network branches. Additionally, to further improve the model's generalization ability to data quality, we use automatic correction and lion optimizer. Finally, our method achieved a result of 6.22% on the ACER metric and ranked third in the 4th Face Anti-Spoofing Challenge @CVPR2023.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/html/Lu_Bandpass_Filter_Based_Dual-Stream_Network_for_Face_Anti-Spoofing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/papers/Lu_Bandpass_Filter_Based_Dual-Stream_Network_for_Face_Anti-Spoofing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Dynamic Feature Queue for Surveillance Face Anti-Spoofing via Progressive Training",
        "author": "Keyao Wang, Mouxiao Huang, Guosheng Zhang, Haixiao Yue, Gang Zhang, Yu Qiao",
        "abstract": "In recent years, face recognition systems have faced increasingly security threats, making it essential to employ Face Anti-spoofing (FAS) to protect against various types of attacks in traditional scenarios like phone unlocking, face payment and self-service security inspection. However, further exploration is required to fully secure FAS in long-distance settings. In this paper, we propose two contributions to enhance the security of face recognition systems: Dynamic Feature Queue (DFQ) and Progressive Training Strategy (PTS). DFQ converts the conventional binary classification task into a multi-classification task. It treats live samples as a closed set and attack samples as an open set by using a dynamic queue that stores the features of spoofing samples and updates them. On the other hand, PTS targets difficult samples and iteratively adds them in batches for training. The proposed PTS divides the entire training set into blocks, trains only a small portion of the data, and gradually increases the training data with each stage while also incorporating low-scoring positive samples and high-scoring spoof samples from the test set. These two contributions complement each other by enhancing the model's ability to generalize and defend against various types of attacks, making the face recognition system more secure and reliable. Our proposed methods have achieved top performance on ACER metric with 4.73% on the SuHiFiMask dataset and won the first prize in Surveillance Face Anti-spoofing track of the Challenge@CVPR 2023.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/html/Wang_Dynamic_Feature_Queue_for_Surveillance_Face_Anti-Spoofing_via_Progressive_Training_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/papers/Wang_Dynamic_Feature_Queue_for_Surveillance_Face_Anti-Spoofing_via_Progressive_Training_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Surveillance Face Presentation Attack Detection Challenge",
        "author": "Hao Fang, Ajian Liu, Jun Wan, Sergio Escalera, Hugo Jair Escalante, Zhen Lei",
        "abstract": "Face Anti-spoofing (FAS) is essential to secure face recognition systems from various physical attacks. However, most of the studies lacked consideration of long-distance scenarios. Specifically, compared with FAS in traditional scenes such as phone unlocking, face payment, and self-service security inspection, FAS in long-distance such as station squares, parks, and self-service supermarkets are equally important, but it has not been sufficiently explored yet. In order to fill this gap in the FAS community, we collect a large-scale Surveillance High-Fidelity Mask (SuHiFiMask). SuHiFiMask contains 10,195 videos from 101 subjects of different age groups, which are collected by 7 mainstream surveillance cameras. Based on this dataset and protocol-3 for evaluating the robustness of the algorithm under quality changes, we organized a face presentation attack detection challenge in surveillance scenarios. It attracted 180 teams for the development phase with a total of 37 teams qualifying for the final round. The organization team re-verified and re-ran the submitted code and used the results as the final ranking. In this paper, we present an overview of the challenge, including an introduction to the dataset used, the definition of the protocol, the evaluation metrics, and the announcement of the competition results. Finally, we present the top-ranked algorithms and the research ideas provided by the competition for attack detection in long-range surveillance scenarios.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/html/Escalera_Surveillance_Face_Presentation_Attack_Detection_Challenge_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/papers/Escalera_Surveillance_Face_Presentation_Attack_Detection_Challenge_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results",
        "author": "Dong Wang, Jia Guo, Qiqi Shao, Haochi He, Zhian Chen, Chuanbao Xiao, Ajian Liu, Sergio Escalera, Hugo Jair Escalante, Zhen Lei, Jun Wan, Jiankang Deng",
        "abstract": "Face anti-spoofing (FAS) is an essential mechanism for safeguarding the integrity of automated face recognition systems. Despite substantial advancements, the generalization of existing approaches to real-world applications remains challenging. This limitation can be attributed to the scarcity and lack of diversity in publicly available FAS datasets, which often leads to overfitting during training or saturation during testing. In terms of quantity, the number of spoof subjects is a critical determinant. Most datasets comprise fewer than 2,000 subjects. With regard to diversity, the majority of datasets consist of spoof samples collected in controlled environments using repetitive, mechanical processes. This data collection methodology results in homogenized samples and a dearth of scenario diversity. To address these shortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, a large-scale, diverse FAS dataset collected in unconstrained settings. Our dataset encompasses 853,729 images of 321,751 spoof subjects and 529,571 images of 148,169 live subjects, representing a substantial increase in quantity. Moreover, our dataset incorporates spoof data obtained from the internet, spanning a wide array of scenarios and various commercial sensors, including 17 presentation attacks (PAs) that encompass both 2D and 3D forms. This novel data collection strategy markedly enhances FAS data diversity. Leveraging the WFAS dataset and Protocol 1 (Known-Type), we host the Wild Face Anti-Spoofing Challenge at the CVPR2023 workshop. Additionally, we meticulously evaluate representative methods using Protocol 1 and Protocol 2 (Unknown-Type). Through an in-depth examination of the challenge outcomes and benchmark baselines, we provide insightful analyses and propose potential avenues for future research.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/html/Wang_Wild_Face_Anti-Spoofing_Challenge_2023_Benchmark_and_Results_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/papers/Wang_Wild_Face_Anti-Spoofing_Challenge_2023_Benchmark_and_Results_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Adversarial Domain Generalization for Surveillance Face Anti-Spoofing",
        "author": "Yongluo Liu, Yaowen Xu, Zhaofan Zou, Zhuming Wang, Bowen Zhang, Lifang Wu, Zhizhi Guo, Zhixiang He",
        "abstract": "In traditional scenes (short-distance applications), the current Face Anti-Spoofing (FAS) methods have achieved satisfactory performance. However, in surveillance scenes (long-distance applications), those methods cannot be generalized well due to the deviation in image quality. Some methods attempt to recover lost details from low-quality images through image reconstruction, but unknown image degradation results in suboptimal performance. In this paper, we regard image quality degradation as a domain generalization problem. Specifically, we propose an end-to-end Adversarial Domain Generalization Network (ADGN) to improve the generalization of FAS. We first divide the accessible training data into multiple sub-source domains based on image quality scores. Then, a feature extractor and a domain discriminator are trained to make the extracted features from different sub-source domains undistinguishable (i.e., quality-invariant features), thus forming an adversarial learning procedure. At the same time, we have introduced the transfer learning strategy to address the problem of insufficient training data. Our method won second place in \"Track Surveillance Face Anti-spoofing\" of the 4th Face Anti-spoofing Challenge@CVPR2023. Our final submission obtains 9.21% APCER, 1.90% BPCER, and 5.56% ACER, respectively.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/html/Zou_Adversarial_Domain_Generalization_for_Surveillance_Face_Anti-Spoofing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/papers/Zou_Adversarial_Domain_Generalization_for_Surveillance_Face_Anti-Spoofing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Attack-Agnostic Deep Face Anti-Spoofing",
        "author": "Ajian Liu, Zichang Tan, Yanyan Liang, Jun Wan",
        "abstract": "The task of face anti-spoofing (FAS) is to determine whether the captured face from a face recognition system is live or fake. Current methods which are trained with existing fake faces ignore the generalization and perform poorly on unseen attacks. To tackle this problem, a novel Attack-agnostic Face Anti-spoofing framework is proposed. Different from previous methods that can be treated as a defense system, we regard face anti-spoofing as a unified framework with the attack and defense systems, and optimize the defense system against unseen attacks via adversarial training with attack system. Concretely, the attack system consists of two modules: an Adversarial learning-based Attack Pattern Generation (Adv-APG) module and a Supervised learning-based Attack Pattern Drift (Sup-APD) module. The Adv-APG module generates a series of spoofing samples via recombining a live face with known attack patterns in a generative way. The Sup-APD module pulls the generated spoofing samples in a supervised way to an unknown domain that makes the defense system ineffective. Extensive experiments are conducted by using three different defense architectures to verify that the proposed attack system can improve the performance on both seen- and unseen attacks on multiple datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/html/Liu_Attack-Agnostic_Deep_Face_Anti-Spoofing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/papers/Liu_Attack-Agnostic_Deep_Face_Anti-Spoofing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Exploring the Effectiveness of Lightweight Architectures for Face Anti-Spoofing",
        "author": "Yoanna Mart\u00ednez-D\u00edaz, Heydi M\u00e9ndez-V\u00e1zquez, Luis S. Luevano, Miguel Gonzalez-Mendoza",
        "abstract": "Detecting spoof faces is crucial in ensuring the robustness of face-based identity recognition and access control systems, as faces can be captured easily without the user's cooperation in uncontrolled environments. Several deep models have been proposed for this task, achieving high levels of accuracy but at a high computational cost. Considering the very good results obtained by lightweight deep networks on different computer vision tasks, in this work we explore the effectiveness of this kind of architectures for face anti-spoofing. Specifically, we asses the performance of three lightweight face models on two challenging benchmark databases. The conducted experiments indicate that face anti-spoofing solutions based on lightweight face models are able to achieve comparable accuracy results to those obtained by state-of-the-art very deep models, with a significantly lower computational complexity.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/html/Martinez-Diaz_Exploring_the_Effectiveness_of_Lightweight_Architectures_for_Face_Anti-Spoofing_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/FAS/papers/Martinez-Diaz_Exploring_the_Effectiveness_of_Lightweight_Architectures_for_Face_Anti-Spoofing_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Graph-CoVis: GNN-Based Multi-View Panorama Global Pose Estimation",
        "author": "Negar Nejatishahidin, Will Hutchcroft, Manjunath Narayana, Ivaylo Boyadzhiev, Yuguang Li, Naji Khosravan, Jana Ko\u0161eck\u00e1, Sing Bing Kang",
        "abstract": "In this paper, we address the problem of wide-baseline camera pose estimation from a group of 360deg panoramas under upright-camera assumption. Recent work has demonstrated the merit of deep-learning for end-to-end direct relative pose regression in 360deg panorama pairs. To exploit the benefits of multi-view logic in a learning-based framework, we introduce Graph-CoVis, which nontrivially extends CoVisPose from relative two-view to global multi-view spherical camera pose estimation. Graph-CoVis is a novel Graph Neural Network based architecture that jointly learns the co-visible structure and global motion in an end-to-end and fully-supervised approach. Using the ZInD dataset, which features real homes presenting wide-baselines, occlusion, and limited visual overlap, we show that our model performs competitively to state-of-the-art approaches",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Nejatishahidin_Graph-CoVis_GNN-Based_Multi-View_Panorama_Global_Pose_Estimation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Nejatishahidin_Graph-CoVis_GNN-Based_Multi-View_Panorama_Global_Pose_Estimation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Self-Supervised Interest Point Detection and Description for Fisheye and Perspective Images",
        "author": "Marcela Mera-Trujillo, Shivang Patel, Yu Gu, Gianfranco Doretto",
        "abstract": "Keypoint detection and matching is a fundamental task in many computer vision problems, from shape reconstruction, to structure from motion, to AR/VR applications and robotics. It is a well-studied problem with remarkable successes such as SIFT, and more recent deep learning approaches. While great robustness is exhibited by these techniques with respect to noise, illumination variation, and rigid motion transformations, less attention has been placed on image distortion sensitivity. In this work, we focus on the case when this is caused by the geometry of the cameras used for image acquisition, and consider the keypoint detection and matching problem between the hybrid scenario of a fisheye and a projective image. We build on a state-of-the-art approach and derive a self-supervised procedure that enables training an interest point detector and descriptor network. We also collected two new datasets for additional training and testing in this unexplored scenario, and we demonstrate that current approaches are suboptimal because they are designed to work in traditional projective conditions, while the proposed approach turns out to be the most effective.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Mera-Trujillo_Self-Supervised_Interest_Point_Detection_and_Description_for_Fisheye_and_Perspective_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Mera-Trujillo_Self-Supervised_Interest_Point_Detection_and_Description_for_Fisheye_and_Perspective_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Applications of Deep Learning for Top-View Omnidirectional Imaging: A Survey",
        "author": "Jingrui Yu, Ana Cecilia Perez Grassi, Gangolf Hirtz",
        "abstract": "A large field-of-view fisheye camera allows for capturing a large area with minimal numbers of cameras when they are mounted on a high position facing downwards. This topview omnidirectional setup greatly reduces the work and cost for deployment compared to traditional solutions with multiple perspective cameras. In recent years, deep learning has been widely employed for vision related tasks, including for such omnidirectional settings. In this survey, we look at the application of deep learning in combination with omnidirectional top-view cameras, including the available datasets, human and object detection, human pose estimation, activity recognition and other miscellaneous applications.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Yu_Applications_of_Deep_Learning_for_Top-View_Omnidirectional_Imaging_A_Survey_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Yu_Applications_of_Deep_Learning_for_Top-View_Omnidirectional_Imaging_A_Survey_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Human Pose Estimation in Monocular Omnidirectional Top-View Images",
        "author": "Jingrui Yu, Tobias Scheck, Roman Seidel, Yukti Adya, Dipankar Nandi, Gangolf Hirtz",
        "abstract": "Human pose estimation (HPE) with convolutional neural networks (CNNs) for indoor monitoring is one of the major challenges in computer vision. In contrast to HPE in perspective views, an indoor monitoring system can consist of an omnidirectional camera with a field of view of 180deg to detect the pose of a person with only one sensor per room. To recognize human pose, the detection of keypoints is an essential upstream step. In our work we propose a new dataset for training and evaluation of CNNs for the task of keypoint detection in omnidirectional images. The training dataset, THEODORE+, consists of 50,000 images and is created by a 3D rendering engine, where humans are randomly walking through an indoor environment. In a dynamically created 3D scene, persons move randomly with simultaneously moving omnidirectional camera to generate synthetic RGB images and 2D and 3D ground truth. For evaluation purposes, the real-world PoseFES dataset with two scenarios and 701 frames with up to eight persons per scene was captured and annotated. We propose four training paradigms to finetune or re-train two top-down models in MMPose and two bottom-up models in CenterNet on THEODORE+. Beside a qualitative evaluation we report quantitative results. Compared to a COCO pretrained baseline, we achieve significant improvements especially for top-view scenes on the PoseFES dataset. Our datasets can be found at https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/index.php.en.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Yu_Human_Pose_Estimation_in_Monocular_Omnidirectional_Top-View_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Yu_Human_Pose_Estimation_in_Monocular_Omnidirectional_Top-View_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Visual Gyroscope: Combination of Deep Learning Features and Direct Alignment for Panoramic Stabilization",
        "author": "Bruno Berenguel-Baeta, Antoine N. Andr\u00e9, Guillaume Caron, Jesus Bermudez-Cameo, Jose J. Guerrero",
        "abstract": "In this article we present a visual gyroscope based on equirectangular panoramas. We propose a new pipeline where we leverage the advantages of the combination of three different methods to obtain a fast and accurate estimation of the attitude of the camera. We quantitatively and qualitatively validate our method on two image sequences taken with a 360 dual-fisheye camera mounted on different aerial vehicles.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Berenguel-Baeta_Visual_Gyroscope_Combination_of_Deep_Learning_Features_and_Direct_Alignment_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Berenguel-Baeta_Visual_Gyroscope_Combination_of_Deep_Learning_Features_and_Direct_Alignment_CVPRW_2023_paper.pdf"
    },
    {
        "title": "FishDreamer: Towards Fisheye Semantic Completion via Unified Image Outpainting and Segmentation",
        "author": "Hao Shi, Yu Li, Kailun Yang, Jiaming Zhang, Kunyu Peng, Alina Roitberg, Yaozu Ye, Huajian Ni, Kaiwei Wang, Rainer Stiefelhagen",
        "abstract": "This paper raises the new task of Fisheye Semantic Completion (FSC), where dense texture, structure, and semantics of a fisheye image are inferred even beyond the sensor field-of-view (FoV). Fisheye cameras have larger FoV than ordinary pinhole cameras, yet its unique special imaging model naturally leads to a blind area at the edge of the image plane. This is suboptimal for safety-critical applications since important perception tasks, such as semantic segmentation, become very challenging within the blind zone. Previous works considered the out-FoV outpainting and in-FoV segmentation separately. However, we observe that these two tasks are actually closely coupled. To jointly estimate the tightly intertwined complete fisheye image and scene semantics, we introduce the new FishDreamer which relies on successful ViTs enhanced with a novel Polar-aware Cross Attention module (PCA) to leverage dense context and guide semantically-consistent content generation while considering different polar distributions. In addition to the contribution of the novel task and architecture, we also derive Cityscapes-BF and KITTI360-BF datasets to facilitate training and evaluation of this new track. Our experiments demonstrate that the proposed FishDreamer outperforms methods solving each task in isolation and surpasses alternative approaches on the Fisheye Semantic Completion. Code and datasets are publicly available at https://github.com/MasterHow/FishDreamer.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Shi_FishDreamer_Towards_Fisheye_Semantic_Completion_via_Unified_Image_Outpainting_and_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Shi_FishDreamer_Towards_Fisheye_Semantic_Completion_via_Unified_Image_Outpainting_and_CVPRW_2023_paper.pdf"
    },
    {
        "title": "ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities",
        "author": "Siddharth Ravi, Pau Climent-Perez, Th\u00e9o Morales, Carlo Huesca-Spairani, Kooshan Hashemifard, Francisco Fl\u00f3rez-Revuelta",
        "abstract": "We introduce ODIN (the OmniDirectional INdoor dataset), the first large-scale multi-modal dataset aimed at spurring research using top-view omnidirectional cameras in challenges related to human behaviour understanding. Recorded in real-life indoor environments with varying levels of occlusion, the dataset contains images of participants performing various activities of daily living. Along with omnidirectional images, additional synchronized modalities of data are provided. These include (1) RGB, infrared, and depth images from multiple RGB-D cameras, (2) egocentric videos, (3) physiological signals and accelerometer readings from a smart bracelet, and (4) 3D scans of the recording environments. To the best of our knowledge, ODIN is also the first dataset to provide camera-frame 3D human pose estimates for omnidirectional images, which are obtained using our novel pipeline. The project is open sourced and available at https://odin-dataset.github.io.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Ravi_ODIN_An_OmniDirectional_INdoor_Dataset_Capturing_Activities_of_Daily_Living_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Ravi_ODIN_An_OmniDirectional_INdoor_Dataset_Capturing_Activities_of_Daily_Living_CVPRW_2023_paper.pdf"
    },
    {
        "title": "A System for Dense Monocular Mapping With a Fisheye Camera",
        "author": "Louis Gallagher, Ganesh Sistu, Jonathan Horgan, John B. McDonald",
        "abstract": "We introduce a novel dense mapping system that uses a single monocular fisheye camera as the sole input sensor and incrementally builds a dense surfel representations of the scene's 3D geometry. We extend an existing hybrid sparse-dense monocular SLAM system, reformulating the mapping pipeline in terms of the Kannala-Brandt fisheye camera model. Each frame is processed in its original undistorted fisheye form, with no attempt to remove distortion. To estimate depth, we introduce a new version of the PackNet depth estimation neural network adapted for fisheye inputs. We reformulate PackNet's multi-view stereo selfsupervised loss in terms of the Kannala-Brandt fisheye camera model. To encourage the network to learn metric depth during training, the pose network is weakly supervised with the camera's ground-truth inter-frame velocity. To improve overall performance, we additionally provide sparse depth supervision from dataset LiDAR and SICK laser scanners. We demonstrate our system's performance on the real-world KITTI-360 benchmark dataset. Our experimental results show that our system is capable of accurate, metric camera tracking and dense surface reconstruction within local windows. Our system operates within real-time processing rates and in challenging conditions. We direct the reader to the following video where the system can be seen in operation: https://youtu.be/Y-9q_wfqocs.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Gallagher_A_System_for_Dense_Monocular_Mapping_With_a_Fisheye_Camera_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Gallagher_A_System_for_Dense_Monocular_Mapping_With_a_Fisheye_Camera_CVPRW_2023_paper.pdf"
    },
    {
        "title": "GPR-Net: Multi-View Layout Estimation via a Geometry-Aware Panorama Registration Network",
        "author": "Jheng-Wei Su, Chi-Han Peng, Peter Wonka, Hung-Kuo Chu",
        "abstract": "We present a room layout estimation framework that jointly learns wide baseline panorama registration and layout estimation given a pair of 360 panoramas. To effectively tackle the wide baseline registration problem, we introduce a novel end-to-end supervised Geometry-aware Panorama Registration Network or GPR-Net that exploits the layout geometry and computes fine-grained correspondences on the layout boundary, instead of the global pixel-space. GPR-Net consists of two main parts. The geometry transformer learns a set of 1D horizon features sampled on the panorama. These 1D feature maps encode geometric cues describing the ceiling-wall and floor-wall layout boundaries, and the correspondence and co-visibility between layout boundaries. These learned geometric cues are further used for direct regression of relative pose (translation and rotation) with a pose transformer. The final layout is then obtained by registering the two layouts using the estimated pose and taking the union of the two individual layouts derived from the estimated layout boundary maps. Experimental results indicate that our method achieves state-of-the-art performance in both panorama registration and layout estimation on a large-scale indoor panorama dataset ZInD. Our code is available online.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Su_GPR-Net_Multi-View_Layout_Estimation_via_a_Geometry-Aware_Panorama_Registration_Network_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Su_GPR-Net_Multi-View_Layout_Estimation_via_a_Geometry-Aware_Panorama_Registration_Network_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PanoPoint: Self-Supervised Feature Points Detection and Description for 360deg Panorama",
        "author": "Hengzhi Zhang, Hong Yi, Haijing Jia, Wei Wang, Makoto Odamaki",
        "abstract": "We introduce PanoPoint, the joint feature point detection and description applied to the nonlinear distortions and the multi-view geometry problems between 360deg panoramas. Our fully convolutional model operates directly in panoramas and computes pixel-level feature point locations and associated descriptors in a single forward pass rather than performing image preprocessing (e.g. panorama to Cubemap) followed by feature detection and description. To train the PanoPoint model, we propose PanoMotion, which simulates the representation between different viewpoints and generates warped panoramas. Moreover, we propose PanoMotion Adaptation, a multi-viewpoint adaptation annotation approach for boosting feature point detection repeatability instead of manual labelling. We train on the annotated synthetic dataset generated by the above method, which outperforms the traditional and other learned approaches and achieves state-of-the-art results on repeatability, localization accuracy, point correspondence precision and real-time metrics, especially for panoramas with significant viewpoint and illumination changes.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Zhang_PanoPoint_Self-Supervised_Feature_Points_Detection_and_Description_for_360deg_Panorama_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Zhang_PanoPoint_Self-Supervised_Feature_Points_Detection_and_Description_for_360deg_Panorama_CVPRW_2023_paper.pdf"
    },
    {
        "title": "TransFusion: Multi-Modal Fusion Network for Semantic Segmentation",
        "author": "Abhisek Maiti, Sander Oude Elberink, George Vosselman",
        "abstract": "The complementary properties of 2D color images and 3D point clouds can potentially improve semantic segmentation compared to using uni-modal data. Multi-modal data fusion is however challenging due to the heterogeneity, dimensionality of the data, the difficulty of aligning different modalities to the same reference frame, and the presence of modality-specific bias. In this regard, we propose a new model, TransFusion, for semantic segmentation that fuses images directly with point clouds without the need for lossy pre-processing of the point clouds. TransFusion outperforms the baseline FCN model that uses images with depth maps. Compared to the baseline, our method improved mIoU by 4% and 2% for the Vaihingen and Potsdam datasets. We demonstrate the capability of our proposed model to adequately learn the spatial and structural information resulting in better inference.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Maiti_TransFusion_Multi-Modal_Fusion_Network_for_Semantic_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/papers/Maiti_TransFusion_Multi-Modal_Fusion_Network_for_Semantic_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "DeFlow: Self-Supervised 3D Motion Estimation of Debris Flow",
        "author": "Liyuan Zhu, Yuru Jia, Shengyu Huang, Nicholas Meyer, Andreas Wieser, Konrad Schindler, Jordan Aaron",
        "abstract": "Existing work on scene flow estimation focuses on autonomous driving and mobile robotics, while automated solutions are lacking for motion in nature, such as that exhibited by debris flows. We propose DeFlow, a model for 3D motion estimation of debris flows, together with a newly captured dataset. We adopt a novel multi-level sensor fusion architecture and self-supervision to incorporate the inductive biases of the scene. We further adopt a multi-frame temporal processing module to enable flow speed estimation over time. Our model achieves state-of-the-art optical flow and depth estimation on our dataset, and fully automates the motion estimation for debris flows. Source code and dataset are available at https://github.com/prs-eth/DeFlow.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Zhu_DeFlow_Self-Supervised_3D_Motion_Estimation_of_Debris_Flow_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/papers/Zhu_DeFlow_Self-Supervised_3D_Motion_Estimation_of_Debris_Flow_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Scan2LoD3: Reconstructing Semantic 3D Building Models at LoD3 Using Ray Casting and Bayesian Networks",
        "author": "Olaf Wysocki, Yan Xia, Magdalena Wysocki, Eleonora Grilli, Ludwig Hoegner, Daniel Cremers, Uwe Stilla",
        "abstract": "Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge. Unlike mesh-based models, they require watertight geometry and object-wise semantics at the facade level. The principal challenge of such demanding semantic 3D reconstruction is reliable facade-level semantic segmentation of 3D input data. We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving facade-level semantic 3D segmentation. To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts. These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a library of facade objects. Extensive experiments on the TUM city campus datasets demonstrate the superior performance of the proposed Scan2LoD3 over the state-of-the-art methods in facade-level detection, semantic segmentation, and LoD3 building model reconstruction. We believe our method can foster the development of probability-driven semantic 3D reconstruction at LoD3 since not only the high-definition reconstruction but also reconstruction confidence becomes pivotal for various applications such as autonomous driving and urban simulations.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Wysocki_Scan2LoD3_Reconstructing_Semantic_3D_Building_Models_at_LoD3_Using_Ray_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/papers/Wysocki_Scan2LoD3_Reconstructing_Semantic_3D_Building_Models_at_LoD3_Using_Ray_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Human Vision Based 3D Point Cloud Semantic Segmentation of Large-Scale Outdoor Scenes",
        "author": "Sunghwan Yoo, Yeonjeong Jeong, Maryam Jameela, Gunho Sohn",
        "abstract": "This paper proposes EyeNet, a novel semantic segmentation network for point clouds that addresses the critical yet often overlooked parameter of coverage area size. Inspired by human peripheral vision, EyeNet overcomes the limitations of conventional networks by introducing a simple but efficient multi-contour input and a parallel processing network with connection blocks between parallel streams. The proposed approach effectively addresses the challenges of dense point clouds, as demonstrated by our ablation studies and state-of-the-art performance on Large-Scale Outdoor datasets.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Yoo_Human_Vision_Based_3D_Point_Cloud_Semantic_Segmentation_of_Large-Scale_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/papers/Yoo_Human_Vision_Based_3D_Point_Cloud_Semantic_Segmentation_of_Large-Scale_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Few-Shot Depth Completion Using Denoising Diffusion Probabilistic Model",
        "author": "Weihang Ran, Wei Yuan, Ryosuke Shibasaki",
        "abstract": "Generating dense depth maps from sparse LiDAR data is a challenging task, benefiting a lot of computer vision and photogrammetry tasks including autonomous driving, 3D point cloud generation, and aerial spatial awareness. Using RGB images as guidance to generate pixel-wise depth map is good, but these multi-modal data fusion networks always need numerous high-quality datasets like KITTI dataset to train on. Since this may be difficult in some cases, how to achieve few-shot learning with less train samples is worth discussing. So in this paper, we firstly proposed a few-shot learning paradigm for depth completion based on pre-trained denoising diffusion probabilistic model. To evaluate our model and other baselines, we constructed a smaller train set with only 12.5% samples from KITTI depth completion dataset to test their few-shot learning ability. Our model achieved the best on all metrics with a 5% improvement in RMSE compared to the second-place model.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Ran_Few-Shot_Depth_Completion_Using_Denoising_Diffusion_Probabilistic_Model_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/papers/Ran_Few-Shot_Depth_Completion_Using_Denoising_Diffusion_Probabilistic_Model_CVPRW_2023_paper.pdf"
    },
    {
        "title": "PSMNet-FusionX3: LiDAR-Guided Deep Learning Stereo Dense Matching on Aerial Images",
        "author": "Teng Wu, Bruno Vallet, Marc Pierrot-Deseilligny",
        "abstract": "Dense image matching (DIM) and LiDAR are two complementary techniques for recovering the 3D geometry of real scenes. While DIM provides dense surfaces, they are often noisy and contaminated with outliers. Conversely, LiDAR is more accurate and robust, but less dense and more expensive compared to DIM. In this work, we investigate learning-based methods to refine surfaces produced by photogrammetry with sparse LiDAR point clouds. Unlike the current state-of-the-art approaches in the computer vision community, our focus is on aerial acquisitions typical in photogrammetry. We propose a densification pipeline that adopts a PSMNet backbone with triangulated irregular network interpolation based expansion, feature enhancement in cost volume, and conditional cost volume normalization, i.e. PSMNet-FusionX3. Our method works better on low density and is less sensitive to distribution, demonstrating its effectiveness across a range of LiDAR point cloud densities and distributions, including analyses of dataset shifts. Furthermore, we have made both our aerial (image and disparity) dataset and code available for public use. Further information can be found at https://github.com/whuwuteng/PSMNet-FusionX3.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Wu_PSMNet-FusionX3_LiDAR-Guided_Deep_Learning_Stereo_Dense_Matching_on_Aerial_Images_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/papers/Wu_PSMNet-FusionX3_LiDAR-Guided_Deep_Learning_Stereo_Dense_Matching_on_Aerial_Images_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Fusion-SUNet: Spatial Layout Consistency for 3D Semantic Segmentation",
        "author": "Maryam Jameela, Gunho Sohn, Sunghwan Yoo",
        "abstract": "The paper discusses the need for a reliable and efficient computer vision system to inspect utility networks with minimal human involvement, due to the aging infrastructure of these networks. We propose a deep learning technique, Fusion-Semantic Utility Network (Fusion-SUNet), to classify the dense and irregular point clouds obtained from the airborne laser terrain mapping (ALTM) system used for data collection. The proposed network combines two networks to achieve voxel-based semantic segmentation of the point clouds at multi-resolution with object categories in three dimensions and predict two-dimensional regional labels distinguishing corridor regions from non-corridors. The network imposes spatial layout consistency on the features of the voxel-based 3D network using regional segmentation features. The authors demonstrate the effectiveness of the proposed technique by testing it on 67km^2 of utility corridor data with average density of 5pp/m2, achieving significantly better performance compared to the state-of-the-art baseline network, with an F1 score of 93% for pylon class, 99% for ground class, 99% for vegetation class, and 98% for powerline class.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Jameela_Fusion-SUNet_Spatial_Layout_Consistency_for_3D_Semantic_Segmentation_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/papers/Jameela_Fusion-SUNet_Spatial_Layout_Consistency_for_3D_Semantic_Segmentation_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Pointless Global Bundle Adjustment With Relative Motions Hessians",
        "author": "Ewelina Rupnik, Marc Pierrot-Deseilligny",
        "abstract": "Bundle adjustment (BA) is the standard way to optimise camera poses and to produce sparse representations of a scene. However, as the number of camera poses and features grows, refinement through bundle adjustment becomes inefficient. Inspired by global motion averaging methods, we propose a new bundle adjustment objective which does not rely on image features' reprojection errors yet maintains precision on par with classical BA. Our method averages over relative motions while implicitly incorporating the contribution of the structure in the adjustment. To that end, we weight the objective function by local hessian matrices - a by-product of local bundle adjustments performed on relative motions (e.g., pairs or triplets) during the pose initialisation step. Such hessians are extremely rich as they encapsulate both the features' random errors and the geometric configuration between the cameras. These pieces of information propagated to the global frame help to guide the final optimisation in a more rigorous way. We argue that this approach is an upgraded version of the motion averaging approach and demonstrate its effectiveness on both photogrammetric datasets and computer vision benchmarks.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Rupnik_Pointless_Global_Bundle_Adjustment_With_Relative_Motions_Hessians_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/PCV/papers/Rupnik_Pointless_Global_Bundle_Adjustment_With_Relative_Motions_Hessians_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Robust Monocular 3D Human Motion With Lasso-Based Differential Kinematics",
        "author": "Abed Malti",
        "abstract": "This work introduces a method to robustly reconstruct 3D human motion from the motion of 2D skeletal landmarks. We propose to use a lasso (least absolute shrinkage and selection operator) optimization framework where the l1-norm is computed over the vector of differential angular kinematics and the l2-norm is computed over the differential 2D reprojection error. The l1-norm term allows us to model sparse kinematic angular motion. The minimization of the reprojection error allows us to assume a bounded noise in both the kinematic model and the 2D landmark detection. This bound is controlled by a scale factor associated to the l2-norm data term. A posteriori verification condition is provided to check whether or not the lasso formulation has allowed us to recover the ground-truth 3D human motion. Results on publicly available data demonstrates the effectiveness of the proposed approach on state-of-the-art methods. It shows that both sparsity and bounded noise assumptions encoded in lasso formulation are robust priors to safely recover 3D human motion.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Malti_Robust_Monocular_3D_Human_Motion_With_Lasso-Based_Differential_Kinematics_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Malti_Robust_Monocular_3D_Human_Motion_With_Lasso-Based_Differential_Kinematics_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CAMM: Building Category-Agnostic and Animatable 3D Models From Monocular Videos",
        "author": "Tianshu Kuai, Akash Karthikeyan, Yash Kant, Ashkan Mirzaei, Igor Gilitschenski",
        "abstract": "Animating an object in 3D often requires an articulated structure, e.g. a kinematic chain or skeleton of the manipulated object with proper skinning weights, to obtain smooth movements and surface deformations. However, existing models that allow direct pose manipulations are either limited to specific object categories or built with specialized equipment. To reduce the work needed for creating animatable 3D models, we propose a novel reconstruction method that learns an animatable kinematic chain for any articulated object. Our method operates on monocular videos without prior knowledge of the object's shape or underlying structure. Our approach is on par with state-of-the-art 3D surface reconstruction methods on various articulated object categories while enabling direct pose manipulations by re-posing the learned kinematic chain.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Kuai_CAMM_Building_Category-Agnostic_and_Animatable_3D_Models_From_Monocular_Videos_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Kuai_CAMM_Building_Category-Agnostic_and_Animatable_3D_Models_From_Monocular_Videos_CVPRW_2023_paper.pdf"
    },
    {
        "title": "Unbiased 4D: Monocular 4D Reconstruction With a Neural Deformation Model",
        "author": "Erik C.M. Johnson, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian Theobalt",
        "abstract": "Capturing general deforming scenes is crucial for many applications in computer graphics and vision, and it is especially challenging when only a monocular RGB video of the scene is available. Competing methods assume dense point tracks over the input views, 3D templates, large-scale training datasets, or only capture small-scale deformations. In stark contrast to those, our method makes none of these assumptions while significantly outperforming the previous state of the art in challenging scenarios. Moreover, our technique includes two new--in the context of non-rigid 3D reconstruction--components, i.e., 1) A coordinate-based and implicit neural representation for non-rigid scenes, which enables an unbiased reconstruction of dynamic scenes, and 2) A novel dynamic scene flow loss, which enables the reconstruction of larger deformations. Results on our new dataset, which will be made publicly available, demonstrate the clear improvement over the state of the art in terms of surface reconstruction accuracy and robustness to large deformations.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Johnson_Unbiased_4D_Monocular_4D_Reconstruction_With_a_Neural_Deformation_Model_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Johnson_Unbiased_4D_Monocular_4D_Reconstruction_With_a_Neural_Deformation_Model_CVPRW_2023_paper.pdf"
    },
    {
        "title": "CAT-NeRF: Constancy-Aware Tx2Former for Dynamic Body Modeling",
        "author": "Haidong Zhu, Zhaoheng Zheng, Wanrong Zheng, Ram Nevatia",
        "abstract": "This paper addresses the problem of human rendering in the video with temporal appearance constancy. Reconstructing dynamic body shapes with volumetric neural rendering methods, such as NeRF, requires finding the correspondence of the points in the canonical and observation space, which demands understanding human body shape and motion. Some methods use rigid transformation, such as SE(3), which cannot precisely model each frame's unique motion and muscle movements. Others generate the transformation for each frame with a trainable network, such as neural blend weight field or translation vector field, which does not consider the appearance constancy of general body shape. In this paper, we propose CAT-NeRF for self-awareness of appearance constancy with Tx^2Former, a novel way to combine two Transformer layers, to separate appearance constancy and uniqueness. Appearance constancy models the general shape across the video, and uniqueness models the unique patterns for each frame. We further introduce a novel Covariance Loss to limit the correlation between each pair of appearance uniquenesses to ensure the frame-unique pattern is maximally captured in appearance uniqueness. We assess our method on H36M and ZJU-MoCap and show state-of-the-art performance.",
        "page": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.html",
        "pdf": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.pdf"
    }
]