[
    {
        "title": "Holistic Features are almost Sufficient for Text-to-Video Retrieval",
        "author": "Kaibin Tian, Ruixiang Zhao, Zijie Xin, Bangxiang Lan, Xirong Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning to navigate efficiently and precisely in real environments",
        "author": "Guillaume Bono, Herv\u00e9 Poirier, Leonid Antsfeld, Gianluca Monaci, Boris Chidlovskii, Christian Wolf",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Instance-level Expert Knowledge and Aggregate Discriminative Attention for Radiology Report Generation",
        "author": "Shenshen Bu, Taiji Li, Zhiming Dai, Yuedong Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning",
        "author": "Tung Le, Khai Nguyen, shanlin sun, Nhat Ho, Xiaohui Xie",
        "abstract": "In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis. Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT). Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost. Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an unsupervised shape matching framework. This unsupervised framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability measures. We also introduce an adaptive refinement process utilizing entropy regularized OT, further refining feature alignments for accurate point-to-point correspondences. Our method demonstrates superior performance in non-rigid shape matching, including near-isometric and non-isometric scenarios, and excels in downstream tasks like segmentation transfer. The empirical results on diverse datasets highlight our framework's effectiveness and generalization capabilities, setting new standards in non-rigid shape matching with efficient OT metrics and an adaptive refinement module.",
        "page": "http://arxiv.org/abs/2403.01781",
        "pdf": "http://arxiv.org/pdf/2403.01781.pdf"
    },
    {
        "title": "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection",
        "author": "Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Observation-Guided Diffusion Probabilistic Models",
        "author": "Junoh Kang, Jinyoung Choi, Sungik Choi, Bohyung Han",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Image Sculpting: Precise Object Editing with 3D Geometry Control",
        "author": "Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, Saining Xie",
        "abstract": "We present Image Sculpting, a new framework for editing 2D images by incorporating tools from 3D geometry and graphics. This approach differs markedly from existing methods, which are confined to 2D spaces and typically rely on textual instructions, leading to ambiguity and limited control. Image Sculpting converts 2D objects into 3D, enabling direct interaction with their 3D geometry. Post-editing, these objects are re-rendered into 2D, merging into the original image to produce high-fidelity results through a coarse-to-fine enhancement process. The framework supports precise, quantifiable, and physically-plausible editing options such as pose editing, rotation, translation, 3D composition, carving, and serial addition. It marks an initial step towards combining the creative freedom of generative models with the precision of graphics pipelines.",
        "page": "http://arxiv.org/abs/2401.01702",
        "pdf": "http://arxiv.org/pdf/2401.01702.pdf"
    },
    {
        "title": "Exploring the Transferability of Visual Prompting for Multimodal Large Language Models",
        "author": "Yichi Zhang, Yinpeng Dong, Siyuan Zhang, Tianzan Min, Hang Su, Jun Zhu",
        "abstract": "Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities, their performance is still inferior to specialized models on downstream tasks, which makes adaptation necessary to enhance their utility. However, fine-tuning methods require independent training for every model, leading to huge computation and memory overheads. In this paper, we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task. To achieve this, we propose Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model. We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts, including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance. We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction.",
        "page": "http://arxiv.org/abs/2404.11207",
        "pdf": "http://arxiv.org/pdf/2404.11207.pdf"
    },
    {
        "title": "Gaussian Shell Maps for Efficient 3D Human Generation",
        "author": "Rameen Abdal, Wang Yifan, Zifan Shi, Yinghao Xu, Ryan Po, Zhengfei Kuang, Qifeng Chen, Dit-Yan Yeung, Gordon Wetzstein",
        "abstract": "Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of $512 \\times 512$ pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.",
        "page": "http://arxiv.org/abs/2311.17857",
        "pdf": "http://arxiv.org/pdf/2311.17857.pdf"
    },
    {
        "title": "Active Generalized Category Discovery",
        "author": "Shijie Ma, Fei Zhu, Zhun Zhong, Xu-Yao Zhang, Cheng-Lin Liu",
        "abstract": "Generalized Category Discovery (GCD) is a pragmatic and challenging open-world task, which endeavors to cluster unlabeled samples from both novel and old classes, leveraging some labeled data of old classes. Given that knowledge learned from old classes is not fully transferable to new classes, and that novel categories are fully unlabeled, GCD inherently faces intractable problems, including imbalanced classification performance and inconsistent confidence between old and new classes, especially in the low-labeling regime. Hence, some annotations of new classes are deemed necessary. However, labeling new classes is extremely costly. To address this issue, we take the spirit of active learning and propose a new setting called Active Generalized Category Discovery (AGCD). The goal is to improve the performance of GCD by actively selecting a limited amount of valuable samples for labeling from the oracle. To solve this problem, we devise an adaptive sampling strategy, which jointly considers novelty, informativeness and diversity to adaptively select novel samples with proper uncertainty. However, owing to the varied orderings of label indices caused by the clustering of novel classes, the queried labels are not directly applicable to subsequent training. To overcome this issue, we further propose a stable label mapping algorithm that transforms ground truth labels to the label space of the classifier, thereby ensuring consistent training across different active selection stages. Our method achieves state-of-the-art performance on both generic and fine-grained datasets. Our code is available at https://github.com/mashijie1028/ActiveGCD",
        "page": "http://arxiv.org/abs/2403.04272",
        "pdf": "http://arxiv.org/pdf/2403.04272.pdf"
    },
    {
        "title": "TUMTraf V2X Cooperative Perception Dataset",
        "author": "Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, Alois Knoll",
        "abstract": "Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: https://tum-traffic-dataset.github.io/tumtraf-v2x.",
        "page": "http://arxiv.org/abs/2403.01316",
        "pdf": "http://arxiv.org/pdf/2403.01316.pdf"
    },
    {
        "title": "Unsupervised Blind Image Deblurring Based on Self-Enhancement",
        "author": "Lufei Chen, Xiangpeng Tian, Shuhua Xiong, Yinjie Lei, Chao Ren",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want",
        "author": "Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang",
        "abstract": "Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models. To fulfill the requirements, we introduce Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to suggest attentive regions and fine-tuned with constructed millions of RGBA region-text pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to open-world recognition, multimodal large language models, and conditional 2D / 3D generation. It has a strong potential to serve as a versatile tool for image-related tasks.",
        "page": "http://arxiv.org/abs/2312.03818",
        "pdf": "http://arxiv.org/pdf/2312.03818.pdf"
    },
    {
        "title": "Bridging the Synthetic-to-Authentic Gap: Distortion-Guided Unsupervised Domain Adaptation for Blind Image Quality Assessment",
        "author": "Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li",
        "abstract": "The annotation of blind image quality assessment (BIQA) is labor-intensive and time-consuming, especially for authentic images. Training on synthetic data is expected to be beneficial, but synthetically trained models often suffer from poor generalization in real domains due to domain gaps. In this work, we make a key observation that introducing more distortion types in the synthetic dataset may not improve or even be harmful to generalizing authentic image quality assessment. To solve this challenge, we propose distortion-guided unsupervised domain adaptation for BIQA (DGQA), a novel framework that leverages adaptive multi-domain selection via prior knowledge from distortion to match the data distribution between the source domains and the target domain, thereby reducing negative transfer from the outlier source domains. Extensive experiments on two cross-domain settings (synthetic distortion to authentic distortion and synthetic distortion to algorithmic distortion) have demonstrated the effectiveness of our proposed DGQA. Besides, DGQA is orthogonal to existing model-based BIQA methods, and can be used in combination with such models to improve performance with less training data.",
        "page": "http://arxiv.org/abs/2405.04167",
        "pdf": "http://arxiv.org/pdf/2405.04167.pdf"
    },
    {
        "title": "Supervised Anomaly Detection for Complex Industrial Images",
        "author": "Aimira Baitieva, David Hurych, Victor Besnier, Olivier BERNARD",
        "abstract": "Automating visual inspection in industrial production lines is essential for increasing product quality across various industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However, existing public datasets primarily consist of images without anomalies, limiting the practical application of AD methods in production settings. To address this challenge, we present (1) the Valeo Anomaly Dataset (VAD), a novel real-world industrial dataset comprising 5000 images, including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset, we introduce (2) Segmentation-based Anomaly Detector (SegAD). First, SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next, SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier, yielding the final anomaly score. Our SegAD achieves state-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC). The code and the models are publicly available.",
        "page": "http://arxiv.org/abs/2405.04953",
        "pdf": "http://arxiv.org/pdf/2405.04953.pdf"
    },
    {
        "title": "EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion",
        "author": "Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, Lu Sheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles",
        "author": "Rui Song, Chenwei Liang, Hu Cao, Zhiran Yan, Walter Zimmer, Markus Gross, Andreas Festag, Alois Knoll",
        "abstract": "Collaborative perception in automated vehicles leverages the exchange of information between agents, aiming to elevate perception results. Previous camera-based collaborative 3D perception methods typically employ 3D bounding boxes or bird's eye views as representations of the environment. However, these approaches fall short in offering a comprehensive 3D environmental prediction. To bridge this gap, we introduce the first method for collaborative 3D semantic occupancy prediction. Particularly, it improves local 3D semantic occupancy predictions by hybrid fusion of (i) semantic and occupancy task features, and (ii) compressed orthogonal attention features shared between vehicles. Additionally, due to the lack of a collaborative perception dataset designed for semantic occupancy prediction, we augment a current collaborative perception dataset to include 3D collaborative semantic occupancy labels for a more robust evaluation. The experimental findings highlight that: (i) our collaborative semantic occupancy predictions excel above the results from single vehicles by over 30%, and (ii) models anchored on semantic occupancy outpace state-of-the-art collaborative 3D detection techniques in subsequent perception applications, showcasing enhanced accuracy and enriched semantic-awareness in road environments.",
        "page": "http://arxiv.org/abs/2402.07635",
        "pdf": "http://arxiv.org/pdf/2402.07635.pdf"
    },
    {
        "title": "Fun with Flags: Robust Principal Directions via Flag Manifolds",
        "author": "Tolga Birdal, Nathan Mankovich",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CoralSCOP: Segment any COral Image on this Planet",
        "author": "Zheng Ziqiang, Liang Haixin, Binh-Son Hua, Tim, Yue Him Wong, Put ANG, Apple CHUI, Sai-Kit Yeung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "R-Cyclic Diffuser: Reductive and Cyclic Latent Diffusion for 3D Clothed Human Digitalization",
        "author": "Kennard Chan, Fayao Liu, Guosheng Lin, Chuan-Sheng Foo, Weisi Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unleashing Channel Potential: Space-Frequency Selection Convolution for SAR Object Detection",
        "author": "Ke Li, Di Wang, Zhangyuan Hu, Wenxuan Zhu, Shaofeng Li, Quan Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DPHMs: Diffusion Parametric Head Models for Depth-based Tracking",
        "author": "Jiapeng Tang, Angela Dai, Yinyu Nie, Lev Markhasin, Justus Thies, Matthias Nie\u00dfner",
        "abstract": "We introduce Diffusion Parametric Head Models (DPHMs), a generative model that enables robust volumetric head reconstruction and tracking from monocular depth sequences. While recent volumetric head models, such as NPHMs, can now excel in representing high-fidelity head geometries, tracking and reconstructing heads from real-world single-view depth sequences remains very challenging, as the fitting to partial and noisy observations is underconstrained. To tackle these challenges, we propose a latent diffusion-based prior to regularize volumetric head reconstruction and tracking. This prior-based regularizer effectively constrains the identity and expression codes to lie on the underlying latent manifold which represents plausible head shapes. To evaluate the effectiveness of the diffusion-based prior, we collect a dataset of monocular Kinect sequences consisting of various complex facial expression motions and rapid transitions. We compare our method to state-of-the-art tracking methods and demonstrate improved head identity reconstruction as well as robust expression tracking.",
        "page": "http://arxiv.org/abs/2312.01068",
        "pdf": "http://arxiv.org/pdf/2312.01068.pdf"
    },
    {
        "title": "CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation",
        "author": "Bo-Yuan Sun, Yuqi Yang, Le Zhang, Ming-Ming Cheng, Qibin Hou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis",
        "author": "Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, Matthias Nie\u00dfner",
        "abstract": "We present DiffuScene for indoor 3D scene synthesis based on a novel scene configuration denoising diffusion model. It generates 3D instance properties stored in an unordered object set and retrieves the most similar geometry for each object configuration, which is characterized as a concatenation of different attributes, including location, size, orientation, semantics, and geometry features. We introduce a diffusion network to synthesize a collection of 3D indoor objects by denoising a set of unordered object attributes. Unordered parametrization simplifies and eases the joint distribution approximation. The shape feature diffusion facilitates natural object placements, including symmetries. Our method enables many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes than state-of-the-art methods. Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.",
        "page": "http://arxiv.org/abs/2303.14207",
        "pdf": "http://arxiv.org/pdf/2303.14207.pdf"
    },
    {
        "title": "Learning from One Continuous Video Stream",
        "author": "Joao Carreira, Michael King, Viorica Patraucean, Dilara Gokay, Catalin Ionescu, Yi Yang, Daniel Zoran, Joseph Heyward, Carl Doersch, Yusuf Aytar, Dima Damen, Andrew Zisserman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ICP-Flow: LiDAR Scene Flow Estimation with ICP",
        "author": "Yancong Lin, Holger Caesar",
        "abstract": "Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference. However, these methods do not take into account that objects in autonomous driving often move rigidly. We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations. We propose ICP-Flow, a learning-free flow estimator. The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations. Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP. The complete scene flow is then recovered from the rigid transformations. We outperform state-of-the-art baselines, including supervised models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward neural network, supervised by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference. We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.4 seconds where other models fail to deliver meaningful results.",
        "page": "http://arxiv.org/abs/2402.17351",
        "pdf": "http://arxiv.org/pdf/2402.17351.pdf"
    },
    {
        "title": "Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion",
        "author": "Litu Rout, Yujia Chen, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, Wen-Sheng Chu",
        "abstract": "Sampling from the posterior distribution poses a major computational challenge in solving inverse problems using latent diffusion models. Common methods rely on Tweedie's first-order moments, which are known to induce a quality-limiting bias. Existing second-order approximations are impractical due to prohibitive computational costs, making standard reverse diffusion processes intractable for posterior sampling. This paper introduces Second-order Tweedie sampler from Surrogate Loss (STSL), a novel sampler that offers efficiency comparable to first-order Tweedie with a tractable reverse process using second-order approximation. Our theoretical results reveal that the second-order approximation is lower bounded by our surrogate loss that only requires $O(1)$ compute using the trace of the Hessian, and by the lower bound we derive a new drift term to make the reverse process tractable. Our method surpasses SoTA solvers PSLD and P2L, achieving 4X and 8X reduction in neural function evaluations, respectively, while notably enhancing sampling quality on FFHQ, ImageNet, and COCO benchmarks. In addition, we show STSL extends to text-guided image editing and addresses residual distortions present from corrupted images in leading text-guided image editing methods. To our best knowledge, this is the first work to offer an efficient second-order approximation in solving inverse problems using latent diffusion and editing real-world images with corruptions.",
        "page": "http://arxiv.org/abs/2312.00852",
        "pdf": "http://arxiv.org/pdf/2312.00852.pdf"
    },
    {
        "title": "SFOD: Spiking Fusion Object Detector",
        "author": "Yimeng Fan, Wei Zhang, Changsong Liu, Mingyang Li, Wenrui Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Tumor Micro-environment Interactions Guided Graph Learning for Survival Analysis of Human Cancers from Whole-slide Pathological Images.",
        "author": "WEI SHAO, YangYang Shi, Daoqiang Zhang, Junjie Zhou, Peng Wan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NC-TTT: A Noise Constrastive Approach for Test-Time Training",
        "author": "David OSOWIECHI, Gustavo Vargas Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Mask Grounding for Referring Image Segmentation",
        "author": "Yong Xien Chng, Henry Zheng, Yizeng Han, Xuchong QIU, Gao Huang",
        "abstract": "Referring Image Segmentation (RIS) is a challenging task that requires an algorithm to segment objects referred by free-form language expressions. Despite significant progress in recent years, most state-of-the-art (SOTA) methods still suffer from considerable language-image modality gap at the pixel and word level. These methods generally 1) rely on sentence-level language features for language-image alignment and 2) lack explicit training supervision for fine-grained visual grounding. Consequently, they exhibit weak object-level correspondence between visual and language features. Without well-grounded features, prior methods struggle to understand complex expressions that require strong reasoning over relationships among multiple objects, especially when dealing with rarely used or ambiguous clauses. To tackle this challenge, we introduce a novel Mask Grounding auxiliary task that significantly improves visual grounding within language features, by explicitly teaching the model to learn fine-grained correspondence between masked textual tokens and their matching visual objects. Mask Grounding can be directly used on prior RIS methods and consistently bring improvements. Furthermore, to holistically address the modality gap, we also design a cross-modal alignment loss and an accompanying alignment module. These additions work synergistically with Mask Grounding. With all these techniques, our comprehensive approach culminates in MagNet (Mask-grounded Network), an architecture that significantly outperforms prior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating our method's effectiveness in addressing current limitations of RIS algorithms. Our code and pre-trained weights will be released.",
        "page": "http://arxiv.org/abs/2312.12198",
        "pdf": "http://arxiv.org/pdf/2312.12198.pdf"
    },
    {
        "title": "LAA-Net: Localized Artifact Attention Network for Quality-Agnostic and Generalizable Deepfake Detection",
        "author": "Dat NGUYEN, Nesryne Mejri, Inder Pal Singh, Polina Kuleshova, Marcella Astrid, Anis Kacem, Enjie Ghorbel, Djamila Aouada",
        "abstract": "This paper introduces a novel approach for high-quality deepfake detection called Localized Artifact Attention Network (LAA-Net). Existing methods for high-quality deepfake detection are mainly based on a supervised binary classifier coupled with an implicit attention mechanism. As a result, they do not generalize well to unseen manipulations. To handle this issue, two main contributions are made. First, an explicit attention mechanism within a multi-task learning framework is proposed. By combining heatmap-based and self-consistency attention strategies, LAA-Net is forced to focus on a few small artifact-prone vulnerable regions. Second, an Enhanced Feature Pyramid Network (E-FPN) is proposed as a simple and effective mechanism for spreading discriminative low-level features into the final feature output, with the advantage of limiting redundancy. Experiments performed on several benchmarks show the superiority of our approach in terms of Area Under the Curve (AUC) and Average Precision (AP). The code is available at https://github.com/10Ring/LAA-Net.",
        "page": "http://arxiv.org/abs/2401.13856",
        "pdf": "http://arxiv.org/pdf/2401.13856.pdf"
    },
    {
        "title": "Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models",
        "author": "Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, Weidi Xie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SimDA: Simple Diffusion Adapter for Efficient Video Generation",
        "author": "Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, Yu-Gang Jiang",
        "abstract": "The recent wave of AI-generated content has witnessed the great development and success of Text-to-Image (T2I) technologies. By contrast, Text-to-Video (T2V) still falls short of expectations though attracting increasing interests. Existing works either train from scratch or adapt large T2I model to videos, both of which are computation and resource expensive. In this work, we propose a Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1B parameters of a strong T2I model, adapting it to video generation in a parameter-efficient way. In particular, we turn the T2I model for T2V by designing light-weight spatial and temporal adapters for transfer learning. Besides, we change the original spatial attention to the proposed Latent-Shift Attention (LSA) for temporal consistency. With similar model architecture, we further train a video super-resolution model to generate high-definition (1024x1024) videos. In addition to T2V generation in the wild, SimDA could also be utilized in one-shot video editing with only 2 minutes tuning. Doing so, our method could minimize the training effort with extremely few tunable parameters for model adaptation.",
        "page": "http://arxiv.org/abs/2308.09710",
        "pdf": "http://arxiv.org/pdf/2308.09710.pdf"
    },
    {
        "title": "Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching",
        "author": "Lennart Bastian, Yizheng Xie, Nassir Navab, Zorah L\u00e4hner",
        "abstract": "Non-isometric shape correspondence remains a fundamental challenge in computer vision. Traditional methods using Laplace-Beltrami operator (LBO) eigenmodes face limitations in characterizing high-frequency extrinsic shape changes like bending and creases. We propose a novel approach of combining the non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with the intrinsic ones of the LBO, creating a hybrid spectral space in which we construct functional maps. To this end, we present a theoretical framework to effectively integrate non-orthogonal basis functions into descriptor- and learning-based functional map methods. Our approach can be incorporated easily into existing functional map pipelines across varying applications and is able to handle complex deformations beyond isometries. We show extensive evaluations across various supervised and unsupervised settings and demonstrate significant improvements. Notably, our approach achieves up to 15% better mean geodesic error for non-isometric correspondence settings and up to 45% improvement in scenarios with topological noise.",
        "page": "http://arxiv.org/abs/2312.03678",
        "pdf": "http://arxiv.org/pdf/2312.03678.pdf"
    },
    {
        "title": "Dual Prior Unfolding for Snapshot Compressive Imaging",
        "author": "Jiancheng Zhang, Haijin Zeng, Jiezhang Cao, Yongyong Chen, Dengxiu Yu, Yinping Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SVDTree: Semantic Voxel Diffusion for Single Image Tree Reconstruction",
        "author": "Yuan Li, Zhihao Liu, Bedrich Benes, Xiaopeng Zhang, Jianwei Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking",
        "author": "Wei Cao, Chang Luo, Biao Zhang, Matthias Nie\u00dfner, Jiapeng Tang",
        "abstract": "We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based priors enable more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent sets instead of using global latent codes. This novel 4D representation allows us to learn local shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporally-coherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid computational overhead, we designed an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations. More detailed information can be found at https://vveicao.github.io/projects/Motion2VecSets/.",
        "page": "http://arxiv.org/abs/2401.06614",
        "pdf": "http://arxiv.org/pdf/2401.06614.pdf"
    },
    {
        "title": "MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception",
        "author": "Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, Jing Shao",
        "abstract": "It is a long-lasting goal to design an embodied system that can solve long-horizon open-world tasks in human-like ways. However, existing approaches usually struggle with compound difficulties caused by the logic-aware decomposition and context-aware execution of these tasks. To this end, we introduce MP5, an open-ended multimodal embodied system built upon the challenging Minecraft simulator, which can decompose feasible sub-objectives, design sophisticated situation-aware plans, and perform embodied action control, with frequent communication with a goal-conditioned active perception scheme. Specifically, MP5 is developed on top of recent advances in Multimodal Large Language Models (MLLMs), and the system is modulated into functional modules that can be scheduled and collaborated to ultimately solve pre-defined context- and process-dependent tasks. Extensive experiments prove that MP5 can achieve a 22% success rate on difficult process-dependent tasks and a 91% success rate on tasks that heavily depend on the context. Moreover, MP5 exhibits a remarkable ability to address many open-ended tasks that are entirely novel.",
        "page": "http://arxiv.org/abs/2312.07472",
        "pdf": "http://arxiv.org/pdf/2312.07472.pdf"
    },
    {
        "title": "UnO: Unsupervised Occupancy Fields for Perception and Forecasting",
        "author": "Ben Agro, Quinlan Sykora, Sergio Casas, Thomas Gilles, Raquel Urtasun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards High-fidelity Artistic Image Vectorization via Texture-Encapsulated Shape Parameterization",
        "author": "Ye Chen, Bingbing Ni, Jinfan Liu, Xiaoyang Huang, Xuanhong Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Simple Recipe for Language-guided Domain Generalized Segmentation",
        "author": "Mohammad Fahes, TUAN-HUNG VU, Andrei Bursuc, Patrick P\u00e9rez, Raoul de Charette",
        "abstract": "Generalization to new domains not seen during training is one of the long-standing challenges in deploying neural networks in real-world applications. Existing generalization techniques either necessitate external images for augmentation, and/or aim at learning invariant representations by imposing various alignment constraints. Large-scale pretraining has recently shown promising generalization capabilities, along with the potential of binding different modalities. For instance, the advent of vision-language models like CLIP has opened the doorway for vision models to exploit the textual modality. In this paper, we introduce a simple framework for generalizing semantic segmentation networks by employing language as the source of randomization. Our recipe comprises three key ingredients: (i) the preservation of the intrinsic CLIP robustness through minimal fine-tuning, (ii) language-driven local style augmentation, and (iii) randomization by locally mixing the source and augmented styles during training. Extensive experiments report state-of-the-art results on various generalization benchmarks. Code is accessible at https://github.com/astra-vision/FAMix .",
        "page": "http://arxiv.org/abs/2311.17922",
        "pdf": "http://arxiv.org/pdf/2311.17922.pdf"
    },
    {
        "title": "Semantic-aware SAM for Point-Prompted Instance Segmentation",
        "author": "Zhaoyang Wei, Pengfei Chen, Xuehui Yu, Guorong Li, Jianbin Jiao, Zhenjun Han",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations",
        "author": "Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang chen, Qian HE, Yongdong Zhang",
        "abstract": "The diffusion-based text-to-image model harbors immense potential in transferring reference style. However, current encoder-based approaches significantly impair the text controllability of text-to-image models while transferring styles. In this paper, we introduce DEADiff to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images. The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions. Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement. 2) A non-reconstructive learning method. The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics. We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the text-to-image model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively. Our project page is https://tianhao-qi.github.io/DEADiff/.",
        "page": "http://arxiv.org/abs/2403.06951",
        "pdf": "http://arxiv.org/pdf/2403.06951.pdf"
    },
    {
        "title": "Robust Self-calibration of Focal Lengths from the Fundamental Matrix",
        "author": "Viktor Kocur, Daniel Kyselica, Zuzana Kukelova",
        "abstract": "The problem of self-calibration of two cameras from a given fundamental matrix is one of the basic problems in geometric computer vision. Under the assumption of known principal points and square pixels, the well-known Bougnoux formula offers a means to compute the two unknown focal lengths. However, in many practical situations, the formula yields inaccurate results due to commonly occurring singularities. Moreover, the estimates are sensitive to noise in the computed fundamental matrix and to the assumed positions of the principal points. In this paper, we therefore propose an efficient and robust iterative method to estimate the focal lengths along with the principal points of the cameras given a fundamental matrix and priors for the estimated camera parameters. In addition, we study a computationally efficient check of models generated within RANSAC that improves the accuracy of the estimated models while reducing the total computational time. Extensive experiments on real and synthetic data show that our iterative method brings significant improvements in terms of the accuracy of the estimated focal lengths over the Bougnoux formula and other state-of-the-art methods, even when relying on inaccurate priors.",
        "page": "http://arxiv.org/abs/2311.16304",
        "pdf": "http://arxiv.org/pdf/2311.16304.pdf"
    },
    {
        "title": "Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval",
        "author": "Haochen Han, Qinghua Zheng, Guang Dai, Minnan Luo, Jingdong Wang",
        "abstract": "Collecting well-matched multimedia datasets is crucial for training cross-modal retrieval models. However, in real-world scenarios, massive multimodal data are harvested from the Internet, which inevitably contains Partially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data will remarkably harm the cross-modal retrieval performance. Previous efforts tend to mitigate this problem by estimating a soft correspondence to down-weight the contribution of PMPs. In this paper, we aim to address this challenge from a new perspective: the potential semantic similarity among unpaired samples makes it possible to excavate useful knowledge from mismatched pairs. To achieve this, we propose L2RM, a general framework based on Optimal Transport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to generate refined alignments by seeking a minimal-cost transport plan across different modalities. To formalize the rematching idea in OT, first, we propose a self-supervised cost function that automatically learns from explicit similarity-cost mapping relation. Second, we present to model a partial OT problem while restricting the transport among false positives to further boost refined alignments. Extensive experiments on three benchmarks demonstrate our L2RM significantly improves the robustness against PMPs for existing models. The code is available at https://github.com/hhc1997/L2RM.",
        "page": "http://arxiv.org/abs/2403.05105",
        "pdf": "http://arxiv.org/pdf/2403.05105.pdf"
    },
    {
        "title": "Zero-Shot Structure-Preserving Diffusion Model for  High Dynamic Range Tone Mapping",
        "author": "Ruoxi Zhu, Shusong Xu, Peiye Liu, Sicheng Li, Yanheng Lu, Dimin Niu, Zihao Liu, Zihao Meng, Li Zhiyong, Xinhua Chen, Yibo Fan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LLM4SGG: Large Language Models for Weakly Supervised Scene Graph Generation",
        "author": "Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Prompt3D: Random Prompt Assisted Weakly-Supervised 3D Object Detection",
        "author": "Xiaohong Zhang, Huisheng Ye, Jingwen Li, Qinyu Tang, Yuanqi Li, Yanwen Guo, Jie Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ID-Blau: Image Deblurring by Implicit Diffusion-based reBLurring AUgmentation",
        "author": "Jia-Hao Wu, Fu-Jen Tsai, Yan-Tsung Peng, Charles Tsai, Chia-Wen Lin, Yen-Yu Lin",
        "abstract": "Image deblurring aims to remove undesired blurs from an image captured in a dynamic scene. Much research has been dedicated to improving deblurring performance through model architectural designs. However, there is little work on data augmentation for image deblurring. Since continuous motion causes blurred artifacts during image exposure, we aspire to develop a groundbreaking blur augmentation method to generate diverse blurred images by simulating motion trajectories in a continuous space. This paper proposes Implicit Diffusion-based reBLurring AUgmentation (ID-Blau), utilizing a sharp image paired with a controllable blur condition map to produce a corresponding blurred image. We parameterize the blur patterns of a blurred image with their orientations and magnitudes as a pixel-wise blur condition map to simulate motion trajectories and implicitly represent them in a continuous space. By sampling diverse blur conditions, ID-Blau can generate various blurred images unseen in the training set. Experimental results demonstrate that ID-Blau can produce realistic blurred images for training and thus significantly improve performance for state-of-the-art deblurring models. The source code is available at https://github.com/plusgood-steven/ID-Blau.",
        "page": "http://arxiv.org/abs/2312.10998",
        "pdf": "http://arxiv.org/pdf/2312.10998.pdf"
    },
    {
        "title": "SPU-PMD: Self-Supervised Point Cloud Upsampling via Progressive Mesh Deformation",
        "author": "Yanzhe Liu, Rong Chen, Yushi Li, Yixi Li, Xuehou Tan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AdaShift: Learning Discriminative Self-Gated Neural Feature Activation With an Adaptive Shift Factor",
        "author": "Sudong Cai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging",
        "author": "Takahiro Shirakawa, Seiichi Uchida",
        "abstract": "Layout-aware text-to-image generation is a task to generate multi-object images that reflect layout conditions in addition to text conditions. The current layout-aware text-to-image diffusion models still have several issues, including mismatches between the text and layout conditions and quality degradation of generated images. This paper proposes a novel layout-aware text-to-image diffusion model called NoiseCollage to tackle these issues. During the denoising process, NoiseCollage independently estimates noises for individual objects and then crops and merges them into a single noise. This operation helps avoid condition mismatches; in other words, it can put the right objects in the right places. Qualitative and quantitative evaluations show that NoiseCollage outperforms several state-of-the-art models. These successful results indicate that the crop-and-merge operation of noises is a reasonable strategy to control image generation. We also show that NoiseCollage can be integrated with ControlNet to use edges, sketches, and pose skeletons as additional conditions. Experimental results show that this integration boosts the layout accuracy of ControlNet. The code is available at https://github.com/univ-esuty/noisecollage.",
        "page": "http://arxiv.org/abs/2403.03485",
        "pdf": "http://arxiv.org/pdf/2403.03485.pdf"
    },
    {
        "title": "ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis",
        "author": "Xiangjun Gao, Xiaoyu Li, Chaopeng Zhang, Qi Zhang, Yan-Pei Cao, Ying Shan, Long Quan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Realigning Confidence with Temporal Saliency Information for Point-Level Weakly-Supervised Temporal Action Localization",
        "author": "Ziying Xia, Jian Cheng, Siyu Liu, Yongxiang Hu, Shiguang Wang, Zhang Yijie, Wanli Dang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Iterative Diffusion-Based Refinement",
        "author": "Jiuming Liu, Guangming Wang, Weicai Ye, Chaokang Jiang, Jinru Han, Zhe Liu, Guofeng Zhang, Dalong Du, Hesheng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "JRDB-Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Interactions Within Social Groups",
        "author": "Simindokht Jahangard, Zhixi Cai, Shiki Wen, Hamid Rezatofighi",
        "abstract": "Understanding human social behaviour is crucial in computer vision and robotics. Micro-level observations like individual actions fall short, necessitating a comprehensive approach that considers individual behaviour, intra-group dynamics, and social group levels for a thorough understanding. To address dataset limitations, this paper introduces JRDB-Social, an extension of JRDB. Designed to fill gaps in human understanding across diverse indoor and outdoor social contexts, JRDB-Social provides annotations at three levels: individual attributes, intra-group interactions, and social group context. This dataset aims to enhance our grasp of human social dynamics for robotic applications. Utilizing the recent cutting-edge multi-modal large language models, we evaluated our benchmark to explore their capacity to decipher social human behaviour.",
        "page": "http://arxiv.org/abs/2404.04458",
        "pdf": "http://arxiv.org/pdf/2404.04458.pdf"
    },
    {
        "title": "Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide Image Classification",
        "author": "Tingting Zheng, Kui Jiang, Hongxun Yao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video",
        "author": "Huiqiang Sun, Xingyi Li, Liao Shen, Xinyi Ye, Ke Xian, Zhiguo Cao",
        "abstract": "Recent advancements in dynamic neural radiance field methods have yielded remarkable outcomes. However, these approaches rely on the assumption of sharp input images. When faced with motion blur, existing dynamic NeRF methods often struggle to generate high-quality novel views. In this paper, we propose DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views from a monocular video affected by motion blur. To account for motion blur in input images, we simultaneously capture the camera trajectory and object Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we employ a global cross-time rendering approach to ensure consistent temporal coherence across the entire scene. We curate a dataset comprising diverse dynamic scenes that are specifically tailored for our task. Experimental results on our dataset demonstrate that our method outperforms existing approaches in generating sharp novel views from motion-blurred inputs while maintaining spatial-temporal consistency of the scene.",
        "page": "http://arxiv.org/abs/2403.10103",
        "pdf": "http://arxiv.org/pdf/2403.10103.pdf"
    },
    {
        "title": "RadSimReal: Bridging the Gap Between Synthetic and Real Data in Radar Object Detection With Simulation",
        "author": "Oded Bialer, Yuval Haitman",
        "abstract": "Object detection in radar imagery with neural networks shows great potential for improving autonomous driving. However, obtaining annotated datasets from real radar images, crucial for training these networks, is challenging, especially in scenarios with long-range detection and adverse weather and lighting conditions where radar performance excels. To address this challenge, we present RadSimReal, an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental conditions, all without the need for real data collection. Remarkably, our findings demonstrate that training object detection models on RadSimReal data and subsequently evaluating them on real-world data produce performance levels comparable to models trained and tested on real data from the same dataset, and even achieves better performance when testing across different real datasets. RadSimReal offers advantages over other physical radar simulations that it does not necessitate knowledge of the radar design details, which are often not disclosed by radar suppliers, and has faster run-time. This innovative tool has the potential to advance the development of computer vision algorithms for radar-based autonomous driving applications.",
        "page": "http://arxiv.org/abs/2404.18150",
        "pdf": "http://arxiv.org/pdf/2404.18150.pdf"
    },
    {
        "title": "Atom-Level Optical Chemical Structure Recognition with Limited Supervision",
        "author": "Martijn Oldenhof, Edward De Brouwer, Adam Arany, Yves Moreau",
        "abstract": "Identifying the chemical structure from a graphical representation, or image, of a molecule is a challenging pattern recognition task that would greatly benefit drug development. Yet, existing methods for chemical structure recognition do not typically generalize well, and show diminished effectiveness when confronted with domains where data is sparse, or costly to generate, such as hand-drawn molecule images. To address this limitation, we propose a new chemical structure recognition tool that delivers state-of-the-art performance and can adapt to new domains with a limited number of data samples and supervision. Unlike previous approaches, our method provides atom-level localization, and can therefore segment the image into the different atoms and bonds. Our model is the first model to perform OCSR with atom-level entity detection with only SMILES supervision. Through rigorous and extensive benchmarking, we demonstrate the preeminence of our chemical structure recognition approach in terms of data efficiency, accuracy, and atom-level entity prediction.",
        "page": "http://arxiv.org/abs/2404.01743",
        "pdf": "http://arxiv.org/pdf/2404.01743.pdf"
    },
    {
        "title": "Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation",
        "author": "Haojie Zhang, Yongyi Su, Xun Xu, Kui Jia",
        "abstract": "The success of large language models has inspired the computer vision community to explore image segmentation foundation model that is able to zero/few-shot generalize through prompt engineering. Segment-Anything(SAM), among others, is the state-of-the-art image segmentation foundation model demonstrating strong zero/few-shot generalization. Despite the success, recent studies reveal the weakness of SAM under strong distribution shift. In particular, SAM performs awkwardly on corrupted natural images, camouflaged images, medical images, etc. Motivated by the observations, we aim to develop a self-training based strategy to adapt SAM to target distribution. Given the unique challenges of large source dataset, high computation cost and incorrect pseudo label, we propose a weakly supervised self-training architecture with anchor regularization and low-rank finetuning to improve the robustness and computation efficiency of adaptation. We validate the effectiveness on 5 types of downstream segmentation tasks including natural clean/corrupted images, medical images, camouflaged images and robotic images. Our proposed method is task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art domain adaptation methods on almost all downstream tasks with the same testing prompt inputs.",
        "page": "http://arxiv.org/abs/2312.03502",
        "pdf": "http://arxiv.org/pdf/2312.03502.pdf"
    },
    {
        "title": "Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization",
        "author": "Deng Li, Aming Wu, Yaowei Wang, Yahong Han",
        "abstract": "Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.",
        "page": "http://arxiv.org/abs/2402.18447",
        "pdf": "http://arxiv.org/pdf/2402.18447.pdf"
    },
    {
        "title": "SNI-SLAM: Semantic Neural Implicit SLAM",
        "author": "Siting Zhu, Guangming Wang, Hermann Blum, Jiuming Liu, LiangSong, Marc Pollefeys, Hesheng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GSVA: Generalized Segmentation via Multimodal Large Language Models",
        "author": "Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, Gao Huang",
        "abstract": "Generalized Referring Expression Segmentation (GRES) extends the scope of classic RES to refer to multiple objects in one expression or identify the empty targets absent in the image. GRES poses challenges in modeling the complex spatial relationships of the instances in the image and identifying non-existing referents. Multimodal Large Language Models (MLLMs) have recently shown tremendous progress in these complicated vision-language tasks. Connecting Large Language Models (LLMs) and vision models, MLLMs are proficient in understanding contexts with visual inputs. Among them, LISA, as a representative, adopts a special [SEG] token to prompt a segmentation mask decoder, e.g., SAM, to enable MLLMs in the RES task. However, existing solutions to GRES remain unsatisfactory since current segmentation MLLMs cannot correctly handle the cases where users might reference multiple subjects in a singular prompt or provide descriptions incongruent with any image target. In this paper, we propose Generalized Segmentation Vision Assistant (GSVA) to address this gap. Specifically, GSVA reuses the [SEG] token to prompt the segmentation model towards supporting multiple mask references simultaneously and innovatively learns to generate a [REJ] token to reject the null targets explicitly. Experiments validate GSVA's efficacy in resolving the GRES issue, marking a notable enhancement and setting a new record on the GRES benchmark gRefCOCO dataset. GSVA also proves effective across various classic referring segmentation and comprehension tasks.",
        "page": "http://arxiv.org/abs/2312.10103",
        "pdf": "http://arxiv.org/pdf/2312.10103.pdf"
    },
    {
        "title": "Adaptive Bidirectional Displacement for Semi-Supervised Medical Image Segmentation",
        "author": "Hanyang Chi, Jian Pang, Bingfeng Zhang, Weifeng Liu",
        "abstract": "Consistency learning is a central strategy to tackle unlabeled data in semi-supervised medical image segmentation (SSMIS), which enforces the model to produce consistent predictions under the perturbation. However, most current approaches solely focus on utilizing a specific single perturbation, which can only cope with limited cases, while employing multiple perturbations simultaneously is hard to guarantee the quality of consistency learning. In this paper, we propose an Adaptive Bidirectional Displacement (ABD) approach to solve the above challenge. Specifically, we first design a bidirectional patch displacement based on reliable prediction confidence for unlabeled data to generate new samples, which can effectively suppress uncontrollable regions and still retain the influence of input perturbations. Meanwhile, to enforce the model to learn the potentially uncontrollable content, a bidirectional displacement operation with inverse confidence is proposed for the labeled images, which generates samples with more unreliable information to facilitate model learning. Extensive experiments show that ABD achieves new state-of-the-art performances for SSMIS, significantly improving different baselines. Source code is available at https://github.com/chy-upc/ABD.",
        "page": "http://arxiv.org/abs/2405.00378",
        "pdf": "http://arxiv.org/pdf/2405.00378.pdf"
    },
    {
        "title": "Unsupervised Semantic Segmentation Through Depth-Guided Feature Correlation and Sampling",
        "author": "Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski",
        "abstract": "Traditionally, training neural networks to perform semantic segmentation required expensive human-made annotations. But more recently, advances in the field of unsupervised learning have made significant progress on this issue and towards closing the gap to supervised algorithms. To achieve this, semantic knowledge is distilled by learning to correlate randomly sampled features from images across an entire dataset. In this work, we build upon these advances by incorporating information about the structure of the scene into the training process through the use of depth information. We achieve this by (1) learning depth-feature correlation by spatially correlate the feature maps with the depth maps to induce knowledge about the structure of the scene and (2) implementing farthest-point sampling to more effectively select relevant features by utilizing 3D sampling techniques on depth information of the scene. Finally, we demonstrate the effectiveness of our technical contributions through extensive experimentation and present significant improvements in performance across multiple benchmark datasets.",
        "page": "http://arxiv.org/abs/2309.12378",
        "pdf": "http://arxiv.org/pdf/2309.12378.pdf"
    },
    {
        "title": "CPGA: Coding Priors-Guided Aggregation Network for Compressed Video Quality Enhancement",
        "author": "Qiang Zhu, Jinhua Hao, Yukang Ding, Yu Liu, Qiao Mo, Ming Sun, Chao Zhou, Shuyuan Zhu",
        "abstract": "Recently, numerous approaches have achieved notable success in compressed video quality enhancement (VQE). However, these methods usually ignore the utilization of valuable coding priors inherently embedded in compressed videos, such as motion vectors and residual frames, which carry abundant temporal and spatial information. To remedy this problem, we propose the Coding Priors-Guided Aggregation (CPGA) network to utilize temporal and spatial information from coding priors. The CPGA mainly consists of an inter-frame temporal aggregation (ITA) module and a multi-scale non-local aggregation (MNA) module. Specifically, the ITA module aggregates temporal information from consecutive frames and coding priors, while the MNA module globally captures spatial information guided by residual frames. In addition, to facilitate research in VQE task, we newly construct the Video Coding Priors (VCP) dataset, comprising 300 videos with various coding priors extracted from corresponding bitstreams. It remedies the shortage of previous datasets on the lack of coding information. Experimental results demonstrate the superiority of our method compared to existing state-of-the-art methods. The code and dataset will be released at https://github.com/CPGA/CPGA.git.",
        "page": "http://arxiv.org/abs/2403.10362",
        "pdf": "http://arxiv.org/pdf/2403.10362.pdf"
    },
    {
        "title": "X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization",
        "author": "Anna Kukleva, Fadime Sener, Edoardo Remelli, Bugra Tekin, Eric Sauser, Bernt Schiele, Shugao Ma",
        "abstract": "Lately, there has been growing interest in adapting vision-language models (VLMs) to image and third-person video classification due to their success in zero-shot recognition. However, the adaptation of these models to egocentric videos has been largely unexplored. To address this gap, we propose a simple yet effective cross-modal adaptation framework, which we call X-MIC. Using a video adapter, our pipeline learns to align frozen text embeddings to each egocentric video directly in the shared embedding space. Our novel adapter architecture retains and improves generalization of the pre-trained VLMs by disentangling learnable temporal modeling and frozen visual encoder. This results in an enhanced alignment of text embeddings to each egocentric video, leading to a significant improvement in cross-dataset generalization. We evaluate our approach on the Epic-Kitchens, Ego4D, and EGTEA datasets for fine-grained cross-dataset action generalization, demonstrating the effectiveness of our method. Code is available at https://github.com/annusha/xmic",
        "page": "http://arxiv.org/abs/2403.19811",
        "pdf": "http://arxiv.org/pdf/2403.19811.pdf"
    },
    {
        "title": "StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN",
        "author": "Jongwoo Choi, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh",
        "abstract": "We propose a method that can generate cinemagraphs automatically from a still landscape image using a pre-trained StyleGAN. Inspired by the success of recent unconditional video generation, we leverage a powerful pre-trained image generator to synthesize high-quality cinemagraphs. Unlike previous approaches that mainly utilize the latent space of a pre-trained StyleGAN, our approach utilizes its deep feature space for both GAN inversion and cinemagraph generation. Specifically, we propose multi-scale deep feature warping (MSDFW), which warps the intermediate features of a pre-trained StyleGAN at different resolutions. By using MSDFW, the generated cinemagraphs are of high resolution and exhibit plausible looping animation. We demonstrate the superiority of our method through user studies and quantitative comparisons with state-of-the-art cinemagraph generation methods and a video generation method that uses a pre-trained StyleGAN.",
        "page": "http://arxiv.org/abs/2403.14186",
        "pdf": "http://arxiv.org/pdf/2403.14186.pdf"
    },
    {
        "title": "ContextSeg: Sketch Semantic Segmentation by Querying the Context with Attention",
        "author": "Jiawei Wang, Changjian Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks",
        "author": "Xinyu Shi, Zecheng Hao, Zhaofei Yu",
        "abstract": "The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer achieves higher accuracy with fewer parameters and lower energy consumption than other spiking Vision Transformer counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN field.",
        "page": "http://arxiv.org/abs/2403.14302",
        "pdf": "http://arxiv.org/pdf/2403.14302.pdf"
    },
    {
        "title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames",
        "author": "Pinelopi Papalampidi, Skanda Koppula, Shreya Pathak, Justin Chiu, Joseph Heyward, Viorica Patraucean, Jiajun Shen, Antoine Miech, Andrew Zisserman, Aida Nematzadeh",
        "abstract": "Understanding long, real-world videos requires modeling of long-range visual dependencies. To this end, we explore video-first architectures, building on the common paradigm of transferring large-scale, image--text models to video via shallow temporal fusion. However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to poor video--language alignment in standard video datasets, and (2) higher memory consumption, bottlenecking the number of frames that can be processed. To mitigate the memory bottleneck, we systematically analyze the memory/accuracy trade-off of various efficient methods: factorized attention, parameter-efficient image-to-video adaptation, input masking, and multi-resolution patchification. Surprisingly, simply masking large portions of the video (up to 75%) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our simple approach for training long video-to-text models, which scales to 1B parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segment-based information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema).",
        "page": "http://arxiv.org/abs/2312.07395",
        "pdf": "http://arxiv.org/pdf/2312.07395.pdf"
    },
    {
        "title": "Distilling ODE Solvers of Diffusion Models into Smaller Steps",
        "author": "Sanghwan Kim, Hao Tang, Fisher Yu",
        "abstract": "Abstract Diffusion models have recently gained prominence as a novel category of generative models. Despite their success, these models face a notable drawback in terms of slow sampling speeds, requiring a high number of function evaluations (NFE) in the order of hundreds or thousands. In response, both learning-free and learning-based sampling strategies have been explored to expedite the sampling process. Learning-free sampling employs various ordinary differential equation (ODE) solvers based on the formulation of diffusion ODEs. However, it encounters challenges in faithfully tracking the true sampling trajectory, particularly for small NFE. Conversely, learning-based sampling methods, such as knowledge distillation, demand extensive additional training, limiting their practical applicability. To overcome these limitations, we introduce Distilled-ODE solvers (D-ODE solvers), a straightforward distillation approach grounded in ODE solver formulations. Our method seamlessly integrates the strengths of both learning-free and learning-based sampling. D-ODE solvers are constructed by introducing a single parameter adjustment to existing ODE solvers. Furthermore, we optimize D-ODE solvers with smaller steps using knowledge distillation from ODE solvers with larger steps across a batch of samples. Comprehensive experiments demonstrate the superior performance of D-ODE solvers compared to existing ODE solvers, including DDIM, PNDM, DPM-Solver, DEIS, and EDM, particularly in scenarios with fewer NFE. Notably, our method incurs negligible computational overhead compared to previous distillation techniques, facilitating straightforward and rapid integration with existing samplers. Qualitative analysis reveals that D-ODE solvers not only enhance image quality but also faithfully follow the target ODE trajectory.",
        "page": "http://arxiv.org/abs/2309.16421",
        "pdf": "http://arxiv.org/pdf/2309.16421.pdf"
    },
    {
        "title": "An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning",
        "author": "Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao",
        "abstract": "Heterogeneous Federated Learning (HtFL) enables collaborative learning on multiple clients with different model architectures while preserving privacy. Despite recent research progress, knowledge sharing in HtFL is still difficult due to data and model heterogeneity. To tackle this issue, we leverage the knowledge stored in pre-trained generators and propose a new upload-efficient knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL). Our FedKTL can produce client-task-related prototypical image-vector pairs via the generator's inference on the server. With these pairs, each client can transfer pre-existing knowledge from the generator to its local model through an additional supervised local task. We conduct extensive experiments on four datasets under two types of data heterogeneity with 14 kinds of models including CNNs and ViTs. Results show that our upload-efficient FedKTL surpasses seven state-of-the-art methods by up to 7.31% in accuracy. Moreover, our knowledge transfer scheme is applicable in scenarios with only one edge client. Code: https://github.com/TsingZ0/FedKTL",
        "page": "http://arxiv.org/abs/2403.15760",
        "pdf": "http://arxiv.org/pdf/2403.15760.pdf"
    },
    {
        "title": "Dr.Hair: Reconstructing Scalp-Connected Hair Strands without Pre-training via Differentiable Rendering of Line Segments",
        "author": "Yusuke Takimoto, Hikari Takehara, Hiroyuki Sato, Zihao Zhu, Bo Zheng",
        "abstract": "In the film and gaming industries, achieving a realistic hair appearance typically involves the use of strands originating from the scalp. However, reconstructing these strands from observed surface images of hair presents significant challenges. The difficulty in acquiring Ground Truth (GT) data has led state-of-the-art learning-based methods to rely on pre-training with manually prepared synthetic CG data. This process is not only labor-intensive and costly but also introduces complications due to the domain gap when compared to real-world data. In this study, we propose an optimization-based approach that eliminates the need for pre-training. Our method represents hair strands as line segments growing from the scalp and optimizes them using a novel differentiable rendering algorithm. To robustly optimize a substantial number of slender explicit geometries, we introduce 3D orientation estimation utilizing global optimization, strand initialization based on Laplace's equation, and reparameterization that leverages geometric connectivity and spatial proximity. Unlike existing optimization-based methods, our method is capable of reconstructing internal hair flow in an absolute direction. Our method exhibits robust and accurate inverse rendering, surpassing the quality of existing methods and significantly improving processing speed.",
        "page": "http://arxiv.org/abs/2403.17496",
        "pdf": "http://arxiv.org/pdf/2403.17496.pdf"
    },
    {
        "title": "MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model",
        "author": "Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, Mike Zheng Shou",
        "abstract": "This paper studies the human image animation task, which aims to generate a video of a certain reference identity following a particular motion sequence. Existing animation works typically employ the frame-warping technique to animate the reference image towards the target motion. Despite achieving reasonable results, these approaches face challenges in maintaining temporal consistency throughout the animation due to the lack of temporal modeling and poor preservation of reference identity. In this work, we introduce MagicAnimate, a diffusion-based framework that aims at enhancing temporal consistency, preserving reference image faithfully, and improving animation fidelity. To achieve this, we first develop a video diffusion model to encode temporal information. Second, to maintain the appearance coherence across frames, we introduce a novel appearance encoder to retain the intricate details of the reference image. Leveraging these two innovations, we further employ a simple video fusion technique to encourage smooth transitions for long video animation. Empirical results demonstrate the superiority of our method over baseline approaches on two benchmarks. Notably, our approach outperforms the strongest baseline by over 38% in terms of video fidelity on the challenging TikTok dancing dataset. Code and model will be made available.",
        "page": "http://arxiv.org/abs/2311.16498",
        "pdf": "http://arxiv.org/pdf/2311.16498.pdf"
    },
    {
        "title": "$V_kD:$ Improving knowledge distillation using orthogonal projections",
        "author": "Roy Miles, Ismail Elezi, Jiankang Deng",
        "abstract": "Knowledge distillation is an effective method for training small and efficient deep learning models. However, the efficacy of a single method can degenerate when transferring to other tasks, modalities, or even other architectures. To address this limitation, we propose a novel constrained feature distillation method. This method is derived from a small set of core principles, which results in two emerging components: an orthogonal projection and a task-specific normalisation. Equipped with both of these components, our transformer models can outperform all previous methods on ImageNet and reach up to a 4.4% relative improvement over the previous state-of-the-art methods. To further demonstrate the generality of our method, we apply it to object detection and image generation, whereby we obtain consistent and substantial performance improvements over state-of-the-art. Code and models are publicly available: https://github.com/roymiles/vkd",
        "page": "http://arxiv.org/abs/2403.06213",
        "pdf": "http://arxiv.org/pdf/2403.06213.pdf"
    },
    {
        "title": "GenesisTex: Adapting Image Denoising Diffusion to Texture Space",
        "author": "Chenjian Gao, Boyan Jiang, Xinghui Li, YingPeng Zhang, Qian Yu",
        "abstract": "We present GenesisTex, a novel method for synthesizing textures for 3D geometries from text descriptions. GenesisTex adapts the pretrained image diffusion model to texture space by texture space sampling. Specifically, we maintain a latent texture map for each viewpoint, which is updated with predicted noise on the rendering of the corresponding viewpoint. The sampled latent texture maps are then decoded into a final texture map. During the sampling process, we focus on both global and local consistency across multiple viewpoints: global consistency is achieved through the integration of style consistency mechanisms within the noise prediction network, and low-level consistency is achieved by dynamically aligning latent textures. Finally, we apply reference-based inpainting and img2img on denser views for texture refinement. Our approach overcomes the limitations of slow optimization in distillation-based methods and instability in inpainting-based methods. Experiments on meshes from various sources demonstrate that our method surpasses the baseline methods quantitatively and qualitatively.",
        "page": "http://arxiv.org/abs/2403.17782",
        "pdf": "http://arxiv.org/pdf/2403.17782.pdf"
    },
    {
        "title": "Exact Fusion via Feature Distribution Matching for Few-shot Image Generation",
        "author": "Yingbo Zhou, Yutong Ye, Pengyu Zhang, Xian Wei, Mingsong Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Long-Tail Class Incremental Learning via Independent Sub-prototype Construction",
        "author": "Xi Wang, Xu Yang, Jie Yin, Kun Wei, Cheng Deng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "T-VSL: Text-Guided Visual Sound Source Localization in Mixtures",
        "author": "Tanvir Mahmud, Yapeng Tian, Diana Marculescu",
        "abstract": "Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video. Existing self-supervised and weakly supervised source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures. These methods often rely on audio-visual correspondence as guidance, which can lead to substantial performance drops in complex multi-source localization scenarios. The lack of access to individual source sounds in multi-source mixtures during training exacerbates the difficulty of learning effective audio-visual correspondence for localization. To address this limitation, in this paper, we propose incorporating the text modality as an intermediate feature guide using tri-modal joint embedding models (e.g., AudioCLIP) to disentangle the semantic audio-visual source correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by predicting the class of sounding entities in mixtures. Subsequently, the textual representation of each sounding source is employed as guidance to disentangle fine-grained audio-visual source correspondence from multi-source mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables our framework to handle a flexible number of sources and exhibits promising zero-shot transferability to unseen classes during test time. Extensive experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets demonstrate significant performance improvements over state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2404.01751",
        "pdf": "http://arxiv.org/pdf/2404.01751.pdf"
    },
    {
        "title": "LidaRF: Delving into Lidar for Neural Radiance Field on Street Scenes",
        "author": "shanlin sun, Bingbing Zhuang, Ziyu Jiang, Buyu Liu, Xiaohui Xie, Manmohan Chandraker",
        "abstract": "Photorealistic simulation plays a crucial role in applications such as autonomous driving, where advances in neural radiance fields (NeRFs) may allow better scalability through the automatic creation of digital 3D assets. However, reconstruction quality suffers on street scenes due to largely collinear camera motions and sparser samplings at higher speeds. On the other hand, the application often demands rendering from camera views that deviate from the inputs to accurately simulate behaviors like lane changes. In this paper, we propose several insights that allow a better utilization of Lidar data to improve NeRF quality on street scenes. First, our framework learns a geometric scene representation from Lidar, which is fused with the implicit grid-based representation for radiance decoding, thereby supplying stronger geometric information offered by explicit point cloud. Second, we put forth a robust occlusion-aware depth supervision scheme, which allows utilizing densified Lidar points by accumulation. Third, we generate augmented training views from Lidar points for further improvement. Our insights translate to largely improved novel view synthesis under real driving scenes.",
        "page": "http://arxiv.org/abs/2405.00900",
        "pdf": "http://arxiv.org/pdf/2405.00900.pdf"
    },
    {
        "title": "PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness",
        "author": "Anh-Quan Cao, Angela Dai, Raoul de Charette",
        "abstract": "We propose the task of Panoptic Scene Completion (PSC) which extends the recently popular Semantic Scene Completion (SSC) task with instance-level information to produce a richer understanding of the 3D scene. Our PSC proposal utilizes a hybrid mask-based technique on the non-empty voxels from sparse multi-scale completions. Whereas the SSC literature overlooks uncertainty which is critical for robotics applications, we instead propose an efficient ensembling to estimate both voxel-wise and instance-wise uncertainties along PSC. This is achieved by building on a multi-input multi-output (MIMO) strategy, while improving performance and yielding better uncertainty for little additional compute. Additionally, we introduce a technique to aggregate permutation-invariant mask predictions. Our experiments demonstrate that our method surpasses all baselines in both Panoptic Scene Completion and uncertainty estimation on three large-scale autonomous driving datasets. Our code and data are available at https://astra-vision.github.io/PaSCo .",
        "page": "http://arxiv.org/abs/2312.02158",
        "pdf": "http://arxiv.org/pdf/2312.02158.pdf"
    },
    {
        "title": "Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness",
        "author": "Sibo Wang, Jie Zhang, Zheng Yuan, Shiguang Shan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Enhancing Intrinsic Features for Debiasing via Investigating Class-Discerning Common Attributes in Bias-Contrastive Pair",
        "author": "Jeonghoon Park, Chaeyeon Chung, Jaegul Choo",
        "abstract": "In the image classification task, deep neural networks frequently rely on bias attributes that are spuriously correlated with a target class in the presence of dataset bias, resulting in degraded performance when applied to data without bias attributes. The task of debiasing aims to compel classifiers to learn intrinsic attributes that inherently define a target class rather than focusing on bias attributes. While recent approaches mainly focus on emphasizing the learning of data samples without bias attributes (i.e., bias-conflicting samples) compared to samples with bias attributes (i.e., bias-aligned samples), they fall short of directly guiding models where to focus for learning intrinsic features. To address this limitation, this paper proposes a method that provides the model with explicit spatial guidance that indicates the region of intrinsic features. We first identify the intrinsic features by investigating the class-discerning common features between a bias-aligned (BA) sample and a bias-conflicting (BC) sample (i.e., bias-contrastive pair). Next, we enhance the intrinsic features in the BA sample that are relatively under-exploited for prediction compared to the BC sample. To construct the bias-contrastive pair without using bias information, we introduce a bias-negative score that distinguishes BC samples from BA samples employing a biased model. The experiments demonstrate that our method achieves state-of-the-art performance on synthetic and real-world datasets with various levels of bias severity.",
        "page": "http://arxiv.org/abs/2404.19250",
        "pdf": "http://arxiv.org/pdf/2404.19250.pdf"
    },
    {
        "title": "Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features",
        "author": "Thomas Wimmer, Peter Wonka, Maks Ovsjanikov",
        "abstract": "With the immense growth of dataset sizes and computing resources in recent years, so-called foundation models have become popular in NLP and vision tasks. In this work, we propose to explore foundation models for the task of keypoint detection on 3D shapes. A unique characteristic of keypoint detection is that it requires semantic and geometric awareness while demanding high localization accuracy. To address this problem, we propose, first, to back-project features from large pre-trained 2D vision models onto 3D shapes and employ them for this task. We show that we obtain robust 3D features that contain rich semantic information and analyze multiple candidate features stemming from different 2D foundation models. Second, we employ a keypoint candidate optimization module which aims to match the average observed distribution of keypoints on the shape and is guided by the back-projected features. The resulting approach achieves a new state of the art for few-shot keypoint detection on the KeyPointNet dataset, almost doubling the performance of the previous best methods.",
        "page": "http://arxiv.org/abs/2311.18113",
        "pdf": "http://arxiv.org/pdf/2311.18113.pdf"
    },
    {
        "title": "Self-Supervised Dual Contouring",
        "author": "Ramana Sundararaman, Roman Klokov, Maks Ovsjanikov",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PoNQ: a Neural QEM-based Mesh Representation",
        "author": "Nissim Maruani, Maks Ovsjanikov, Pierre Alliez, Mathieu Desbrun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning",
        "author": "Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking Generalizable Face Anti-spoofing via Hierarchical Prototype-guided Distribution Refinement in Hyperbolic Space",
        "author": "Chengyang Hu, Ke-Yue Zhang, Taiping Yao, Shouhong Ding, Lizhuang Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Grounding and Enhancing Grid-based Models for Neural Fields",
        "author": "Zelin Zhao, FENGLEI FAN, Wenlong Liao, Junchi Yan",
        "abstract": "Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical framework for grid-based models. This framework points out that these models' approximation and generalization behaviors are determined by grid tangent kernels (GTK), which are intrinsic properties of grid-based models. The proposed framework facilitates a consistent and systematic analysis of diverse grid-based models. Furthermore, the introduced framework motivates the development of a novel grid-based model named the Multiplicative Fourier Adaptive Grid (MulFAGrid). The numerical analysis demonstrates that MulFAGrid exhibits a lower generalization bound than its predecessors, indicating its robust generalization performance. Empirical studies reveal that MulFAGrid achieves state-of-the-art performance in various tasks, including 2D image fitting, 3D signed distance field (SDF) reconstruction, and novel view synthesis, demonstrating superior representation ability. The project website is available at https://sites.google.com/view/cvpr24-2034-submission/home.",
        "page": "http://arxiv.org/abs/2403.20002",
        "pdf": "http://arxiv.org/pdf/2403.20002.pdf"
    },
    {
        "title": "Loose Inertial Poser: Motion Capture with IMU-attached Loose-Wear Jacket",
        "author": "Chengxu Zuo, Yiming Wang, Lishuang Zhan, Shihui Guo, Xinyu Yi, Feng Xu, Yipeng Qin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild",
        "author": "Zeren Jiang, Chen Guo, Manuel Kaufmann, Tianjian Jiang, Julien Valentin, Otmar Hilliges, Jie Song",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection",
        "author": "Dinh Phat Do, Taehoon Kim, JAEMIN NA, Jiwon Kim, Keonho LEE, Kyunghwan Cho, Wonjun Hwang",
        "abstract": "Domain adaptation for object detection typically entails transferring knowledge from one visible domain to another visible domain. However, there are limited studies on adapting from the visible to the thermal domain, because the domain gap between the visible and thermal domains is much larger than expected, and traditional domain adaptation can not successfully facilitate learning in this situation. To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each domain. Specifically, we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each domain. The framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal domains during training. We validate the superiority of our method through newly designed experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST. Source code is available at https://github.com/EdwardDo69/D3T .",
        "page": "http://arxiv.org/abs/2403.09359",
        "pdf": "http://arxiv.org/pdf/2403.09359.pdf"
    },
    {
        "title": "E-GPS: Explainable Geometry Problem Solving via Top-Down Solver and Bottom-Up Generator",
        "author": "Wenjun Wu, Lingling Zhang, Jun Liu, Xi Tang, Yaxian Wang, Shaowei Wang, QianYing Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images",
        "author": "JungEun Kim, Hangyul Yoon, Geondo Park, Kyungsu Kim, Eunho Yang",
        "abstract": "4D medical images, which represent 3D images with temporal information, are crucial in clinical practice for capturing dynamic changes and monitoring long-term disease progression. However, acquiring 4D medical images poses challenges due to factors such as radiation exposure and imaging duration, necessitating a balance between achieving high temporal resolution and minimizing adverse effects. Given these circumstances, not only is data acquisition challenging, but increasing the frame rate for each dataset also proves difficult. To address this challenge, this paper proposes a simple yet effective Unsupervised Volumetric Interpolation framework, UVI-Net. This framework facilitates temporal interpolation without the need for any intermediate frames, distinguishing it from the majority of other existing unsupervised methods. Experiments on benchmark datasets demonstrate significant improvements across diverse evaluation metrics compared to unsupervised and supervised baselines. Remarkably, our approach achieves this superior performance even when trained with a dataset as small as one, highlighting its exceptional robustness and efficiency in scenarios with sparse supervision. This positions UVI-Net as a compelling alternative for 4D medical imaging, particularly in settings where data availability is limited. The source code is available at https://github.com/jungeun122333/UVI-Net.",
        "page": "http://arxiv.org/abs/2404.01464",
        "pdf": "http://arxiv.org/pdf/2404.01464.pdf"
    },
    {
        "title": "Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization",
        "author": "Guopeng Li, Ming Qian, Gui-Song Xia",
        "abstract": "This paper investigates the effective utilization of unlabeled data for large-area cross-view geo-localization (CVGL), encompassing both unsupervised and semi-supervised settings. Common approaches to CVGL rely on ground-satellite image pairs and employ label-driven supervised training. However, the cost of collecting precise cross-view image pairs hinders the deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more challenging to handle the significant imaging and spatial gaps between ground and satellite images. To this end, we propose an unsupervised framework including a cross-view projection to guide the model for retrieving initial pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by leveraging the fact that ``the perfectly paired ground-satellite image is located in a unique and identical scene\". The framework exhibits competitive performance compared with supervised works on three open-source benchmarks. Our code and models will be released on https://github.com/liguopeng0923/UCVGL.",
        "page": "http://arxiv.org/abs/2403.14198",
        "pdf": "http://arxiv.org/pdf/2403.14198.pdf"
    },
    {
        "title": "PromptCoT: Align Prompt Distribution via Adapted Chain-of-Thought",
        "author": "Junyi Yao, Yijiang Liu, Zhen Dong, Mingfei Guo, Helan Hu, Kurt Keutzer, Li Du, Daquan Zhou, Shanghang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HOISDF: Constraining 3D Hand Object Pose Estimation with Global Signed Distance Fields",
        "author": "Haozhe Qi, Chen Zhao, Mathieu Salzmann, Alexander Mathis",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding",
        "author": "Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, Cong Yao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VRP-SAM: SAM with Visual Reference Prompt",
        "author": "Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, gang zhang, Errui Ding, Jingdong Wang, Zechao Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
        "author": "Yijun Yang, Tianyi Zhou, kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi",
        "abstract": "While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often challenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world. Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dynamics. Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark's diverse tasks highlight EMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.",
        "page": "http://arxiv.org/abs/2311.16714",
        "pdf": "http://arxiv.org/pdf/2311.16714.pdf"
    },
    {
        "title": "UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory",
        "author": "Haiwen Diao, Bo Wan, Ying Zhang, Xu Jia, Huchuan Lu, Long Chen",
        "abstract": "Parameter-efficient transfer learning (PETL), i.e., fine-tuning a small portion of parameters, is an effective strategy for adapting pre-trained models to downstream domains. To further reduce the memory demand, recent PETL works focus on the more valuable memory-efficient characteristic. In this paper, we argue that the scalability, adaptability, and generalizability of state-of-the-art methods are hindered by structural dependency and pertinency on specific pre-trained backbones. To this end, we propose a new memory-efficient PETL strategy, Universal Parallel Tuning (UniPT), to mitigate these weaknesses. Specifically, we facilitate the transfer process via a lightweight and learnable parallel network, which consists of: 1) A parallel interaction module that decouples the sequential connections and processes the intermediate activations detachedly from the pre-trained network. 2) A confidence aggregation module that learns optimal strategies adaptively for integrating cross-layer features. We evaluate UniPT with different backbones (e.g., T5, VSE$\\infty$, CLIP4Clip, Clip-ViL, and MDETR) on various vision-and-language and pure NLP tasks. Extensive ablations on 18 datasets have validated that UniPT can not only dramatically reduce memory consumption and outperform the best competitor, but also achieve competitive performance over other plain PETL methods with lower training memory overhead. Our code is publicly available at: https://github.com/Paranioar/UniPT.",
        "page": "http://arxiv.org/abs/2308.14316",
        "pdf": "http://arxiv.org/pdf/2308.14316.pdf"
    },
    {
        "title": "G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images",
        "author": "Zixiong Huang, Qi Chen, Libo Sun, Yifan Yang, Naizhou Wang, Qi Wu, Mingkui Tan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rolling Shutter Correction with Intermediate Distortion Flow Estimation",
        "author": "Mingdeng Cao, Sidi Yang, Yujiu Yang, Yinqiang Zheng",
        "abstract": "This paper proposes to correct the rolling shutter (RS) distorted images by estimating the distortion flow from the global shutter (GS) to RS directly. Existing methods usually perform correction using the undistortion flow from the RS to GS. They initially predict the flow from consecutive RS frames, subsequently rescaling it as the displacement fields from the RS frame to the underlying GS image using time-dependent scaling factors. Following this, RS-aware forward warping is employed to convert the RS image into its GS counterpart. Nevertheless, this strategy is prone to two shortcomings. First, the undistortion flow estimation is rendered inaccurate by merely linear scaling the flow, due to the complex non-linear motion nature. Second, RS-aware forward warping often results in unavoidable artifacts. To address these limitations, we introduce a new framework that directly estimates the distortion flow and rectifies the RS image with the backward warping operation. More specifically, we first propose a global correlation-based flow attention mechanism to estimate the initial distortion flow and GS feature jointly, which are then refined by the following coarse-to-fine decoder layers. Additionally, a multi-distortion flow prediction strategy is integrated to mitigate the issue of inaccurate flow estimation further. Experimental results validate the effectiveness of the proposed method, which outperforms state-of-the-art approaches on various benchmarks while maintaining high efficiency. The project is available at \\url{https://github.com/ljzycmd/DFRSC}.",
        "page": "http://arxiv.org/abs/2404.06350",
        "pdf": "http://arxiv.org/pdf/2404.06350.pdf"
    },
    {
        "title": "Differentiable Point-based Inverse Rendering",
        "author": "Hoon-Gyu Chung, Seokjun Choi, Seung-Hwan Baek",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "KITRO: Refining Human Mesh by 2D Clues and Kinematic-tree Rotation",
        "author": "Fengyuan Yang, Kerui Gu, Angela Yao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LIVE: Online Large Video-Language Model for Streaming Video",
        "author": "Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer",
        "author": "Hyeongjin Nam, Daniel Jung, Gyeongsik Moon, Kyoung Mu Lee",
        "abstract": "Human-object contact serves as a strong cue to understand how humans physically interact with objects. Nevertheless, it is not widely explored to utilize human-object contact information for the joint reconstruction of 3D human and object from a single image. In this work, we present a novel joint 3D human-object reconstruction method (CONTHO) that effectively exploits contact information between humans and objects. There are two core designs in our system: 1) 3D-guided contact estimation and 2) contact-based 3D human and object refinement. First, for accurate human-object contact estimation, CONTHO initially reconstructs 3D humans and objects and utilizes them as explicit 3D guidance for contact estimation. Second, to refine the initial reconstructions of 3D human and object, we propose a novel contact-based refinement Transformer that effectively aggregates human features and object features based on the estimated human-object contact. The proposed contact-based refinement prevents the learning of erroneous correlation between human and object, which enables accurate 3D reconstruction. As a result, our CONTHO achieves state-of-the-art performance in both human-object contact estimation and joint reconstruction of 3D human and object. The code is publicly available at https://github.com/dqj5182/CONTHO_RELEASE.",
        "page": "http://arxiv.org/abs/2404.04819",
        "pdf": "http://arxiv.org/pdf/2404.04819.pdf"
    },
    {
        "title": "Event-assisted Low-Light Video Object Segmentation",
        "author": "Li Hebei, Jin Wang, Jiahui Yuan, Yue Li, Wenming Weng, Yansong Peng, Yueyi Zhang, Zhiwei Xiong, Xiaoyan Sun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Binarized Low-light Raw Video Enhancement",
        "author": "Gengchen Zhang, Yulun Zhang, Xin Yuan, Ying Fu",
        "abstract": "Recently, deep neural networks have achieved excellent performance on low-light raw video enhancement. However, they often come with high computational complexity and large memory costs, which hinder their applications on resource-limited devices. In this paper, we explore the feasibility of applying the extremely compact binary neural network (BNN) to low-light raw video enhancement. Nevertheless, there are two main issues with binarizing video enhancement models. One is how to fuse the temporal information to improve low-light denoising without complex modules. The other is how to narrow the performance gap between binary convolutions with the full precision ones. To address the first issue, we introduce a spatial-temporal shift operation, which is easy-to-binarize and effective. The temporal shift efficiently aggregates the features of neighbor frames and the spatial shift handles the misalignment caused by the large motion in videos. For the second issue, we present a distribution-aware binary convolution, which captures the distribution characteristics of real-valued input and incorporates them into plain binary convolutions to alleviate the degradation in performance. Extensive quantitative and qualitative experiments have shown our high-efficiency binarized low-light raw video enhancement method can attain a promising performance.",
        "page": "http://arxiv.org/abs/2403.19944",
        "pdf": "http://arxiv.org/pdf/2403.19944.pdf"
    },
    {
        "title": "Generating Enhanced Negatives for Training Language-Based Object Detectors",
        "author": "Shiyu Zhao, Long Zhao, Vijay Kumar BG, Yumin Suh, Dimitris N. Metaxas, Manmohan Chandraker, Samuel Schulter",
        "abstract": "The recent progress in language-based open-vocabulary object detection can be largely attributed to finding better ways of leveraging large-scale data with free-form text annotations. Training such models with a discriminative objective function has proven successful, but requires good positive and negative samples. However, the free-form nature and the open vocabulary of object descriptions make the space of negatives extremely large. Prior works randomly sample negatives or use rule-based techniques to build them. In contrast, we propose to leverage the vast knowledge built into modern generative models to automatically build negatives that are more relevant to the original data. Specifically, we use large-language-models to generate negative text descriptions, and text-to-image diffusion models to also generate corresponding negative images. Our experimental analysis confirms the relevance of the generated negative data, and its use in language-based detectors improves performance on two complex benchmarks. Code is available at \\url{https://github.com/xiaofeng94/Gen-Enhanced-Negs}.",
        "page": "http://arxiv.org/abs/2401.00094",
        "pdf": "http://arxiv.org/pdf/2401.00094.pdf"
    },
    {
        "title": "Taming Self-Training for Open-Vocabulary Object Detection",
        "author": "Shiyu Zhao, Samuel Schulter, Long Zhao, Zhixing Zhang, Vijay Kumar BG, Yumin Suh, Manmohan Chandraker, Dimitris N. Metaxas",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DITTO: Dual and Integrated Latent Topologies for Implicit 3D Reconstruction",
        "author": "Jaehyeok Shim, Kyungdon Joo",
        "abstract": "We propose a novel concept of dual and integrated latent topologies (DITTO in short) for implicit 3D reconstruction from noisy and sparse point clouds. Most existing methods predominantly focus on single latent type, such as point or grid latents. In contrast, the proposed DITTO leverages both point and grid latents (i.e., dual latent) to enhance their strengths, the stability of grid latents and the detail-rich capability of point latents. Concretely, DITTO consists of dual latent encoder and integrated implicit decoder. In the dual latent encoder, a dual latent layer, which is the key module block composing the encoder, refines both latents in parallel, maintaining their distinct shapes and enabling recursive interaction. Notably, a newly proposed dynamic sparse point transformer within the dual latent layer effectively refines point latents. Then, the integrated implicit decoder systematically combines these refined latents, achieving high-fidelity 3D reconstruction and surpassing previous state-of-the-art methods on object- and scene-level datasets, especially in thin and detailed structures.",
        "page": "http://arxiv.org/abs/2403.05005",
        "pdf": "http://arxiv.org/pdf/2403.05005.pdf"
    },
    {
        "title": "Discontinuity-preserving Normal Integration with Auxiliary Edges",
        "author": "Hyomin Kim, Yucheol Jung, Seungyong Lee",
        "abstract": "Many surface reconstruction methods incorporate normal integration, which is a process to obtain a depth map from surface gradients. In this process, the input may represent a surface with discontinuities, e.g., due to self-occlusion. To reconstruct an accurate depth map from the input normal map, hidden surface gradients occurring from the jumps must be handled. To model these jumps correctly, we design a novel discretization scheme for the domain of normal integration. Our key idea is to introduce auxiliary edges, which bridge between piecewise-smooth patches in the domain so that the magnitude of hidden jumps can be explicitly expressed. Using the auxiliary edges, we design a novel algorithm to optimize the discontinuity and the depth map from the input normal map. Our method optimizes discontinuities by using a combination of iterative re-weighted least squares and iterative filtering of the jump magnitudes on auxiliary edges to provide strong sparsity regularization. Compared to previous discontinuity-preserving normal integration methods, which model the magnitudes of jumps only implicitly, our method reconstructs subtle discontinuities accurately thanks to our explicit representation of jumps allowing for strong sparsity regularization.",
        "page": "http://arxiv.org/abs/2404.03138",
        "pdf": "http://arxiv.org/pdf/2404.03138.pdf"
    },
    {
        "title": "DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models",
        "author": "Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong",
        "abstract": "We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body's shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the generation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deformation field. This facilitates the generation of more complete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face ''Janus'' problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing methods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation.",
        "page": "http://arxiv.org/abs/2304.00916",
        "pdf": "http://arxiv.org/pdf/2304.00916.pdf"
    },
    {
        "title": "Novel Class Discovery for Ultra-Fine-Grained Visual Categorization",
        "author": "Qi Jia, Yaqi Cai, Qi Jia, Binglin Qiu, Weimin Wang, Nan Pu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing",
        "author": "Zeyinzi Jiang, Chaojie Mao, Yulin Pan, Zhen Han, Jingfeng Zhang",
        "abstract": "Image diffusion models have been utilized in various tasks, such as text-to-image generation and controllable image synthesis. Recent research has introduced tuning methods that make subtle adjustments to the original models, yielding promising results in specific adaptations of foundational generative diffusion models. Rather than modifying the main backbone of the diffusion model, we delve into the role of skip connection in U-Net and reveal that hierarchical features aggregating long-distance information across encoder and decoder make a significant impact on the content and quality of image generation. Based on the observation, we propose an efficient generative tuning framework, dubbed SCEdit, which integrates and edits Skip Connection using a lightweight tuning module named SC-Tuner. Furthermore, the proposed framework allows for straightforward extension to controllable image synthesis by injecting different conditions with Controllable SC-Tuner, simplifying and unifying the network design for multi-condition inputs. Our SCEdit substantially reduces training parameters, memory usage, and computational expense due to its lightweight tuners, with backward propagation only passing to the decoder blocks. Extensive experiments conducted on text-to-image generation and controllable image synthesis tasks demonstrate the superiority of our method in terms of efficiency and performance. Project page: \\url{https://scedit.github.io/}",
        "page": "http://arxiv.org/abs/2312.11392",
        "pdf": "http://arxiv.org/pdf/2312.11392.pdf"
    },
    {
        "title": "Active Domain Adaptation with False Negative Prediction for Object Detection",
        "author": "Yuzuru Nakamura, Yasunori Ishii, Takayoshi Yamashita",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Free3D: Consistent Novel View Synthesis without 3D Representation",
        "author": "Chuanxia Zheng, Andrea Vedaldi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Amodal Ground Truth and Completion in the Wild",
        "author": "Guanqi Zhan, Chuanxia Zheng, Weidi Xie, Andrew Zisserman",
        "abstract": "This paper studies amodal image segmentation: predicting entire object segmentation masks including both visible and invisible (occluded) parts. In previous work, the amodal segmentation ground truth on real images is usually predicted by manual annotaton and thus is subjective. In contrast, we use 3D data to establish an automatic pipeline to determine authentic ground truth amodal masks for partially occluded objects in real images. This pipeline is used to construct an amodal completion evaluation benchmark, MP3D-Amodal, consisting of a variety of object categories and labels. To better handle the amodal completion task in the wild, we explore two architecture variants: a two-stage model that first infers the occluder, followed by amodal mask completion; and a one-stage model that exploits the representation power of Stable Diffusion for amodal segmentation across many categories. Without bells and whistles, our method achieves a new state-of-the-art performance on Amodal segmentation datasets that cover a large variety of objects, including COCOA and our new MP3D-Amodal dataset. The dataset, model, and code are available at https://www.robots.ox.ac.uk/~vgg/research/amodal/.",
        "page": "http://arxiv.org/abs/2312.17247",
        "pdf": "http://arxiv.org/pdf/2312.17247.pdf"
    },
    {
        "title": "Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches",
        "author": "Qing Yu, Mikihiro Tanaka, Kent Fujiwara",
        "abstract": "To build a cross-modal latent space between 3D human motion and language, acquiring large-scale and high-quality human motion data is crucial. However, unlike the abundance of image data, the scarcity of motion data has limited the performance of existing motion-language models. To counter this, we introduce \"motion patches\", a new representation of motion sequences, and propose using Vision Transformers (ViT) as motion encoders via transfer learning, aiming to extract useful knowledge from the image domain and apply it to the motion domain. These motion patches, created by dividing and sorting skeleton joints based on body parts in motion sequences, are robust to varying skeleton structures, and can be regarded as color image patches in ViT. We find that transfer learning with pre-trained weights of ViT obtained through training with 2D image data can boost the performance of motion analysis, presenting a promising direction for addressing the issue of limited motion data. Our extensive experiments show that the proposed motion patches, used jointly with ViT, achieve state-of-the-art performance in the benchmarks of text-to-motion retrieval, and other novel challenging tasks, such as cross-skeleton recognition, zero-shot motion classification, and human interaction recognition, which are currently impeded by the lack of data.",
        "page": "http://arxiv.org/abs/2405.04771",
        "pdf": "http://arxiv.org/pdf/2405.04771.pdf"
    },
    {
        "title": "Discovering Syntactic Interaction Clues for Human-Object Interaction Detection",
        "author": "Jinguo Luo, Weihong Ren, Weibo Jiang, Xi'ai Chen, Qiang Wang, Zhi Han, Honghai LIU",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement",
        "author": "Daiwei Yu, Zhuorong Li, Lina Wei, Canghong Jin, Yun Zhang, Sixian Chan",
        "abstract": "Adversarial training (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against adversarial attacks. However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in adversarial robustness between the training and testing curves. In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm. As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT. Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers. Empirical results demonstrate that our method can simultaneously boost the standard accuracy and robust performance across multiple benchmark datasets, attack types, and architectures. In addition, we also provide a set of analyses from the perspectives of information theory to dive into our method and suggest the importance of soft labels for robust generalization.",
        "page": "http://arxiv.org/abs/2403.09101",
        "pdf": "http://arxiv.org/pdf/2403.09101.pdf"
    },
    {
        "title": "On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm",
        "author": "Peng Sun, Bei Shi, Daiwei Yu, Tao Lin",
        "abstract": "Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes, achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU (while the SOTA only achieves 21% but requires 6 hours).",
        "page": "http://arxiv.org/abs/2312.03526",
        "pdf": "http://arxiv.org/pdf/2312.03526.pdf"
    },
    {
        "title": "OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning",
        "author": "Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Cheng, Jinglun Li, Zhaoyu Chen, Wenqiang Zhang",
        "abstract": "Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker. OneTracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker. This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object. Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker. Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter-efficient finetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our OneTracker outperforms other models and achieves state-of-the-art performance.",
        "page": "http://arxiv.org/abs/2403.09634",
        "pdf": "http://arxiv.org/pdf/2403.09634.pdf"
    },
    {
        "title": "Infrared Small Target Detection with Scale and Location Sensitivity",
        "author": "Qiankun Liu, Rui Liu, Bolun Zheng, Hongkui Wang, Ying Fu",
        "abstract": "Recently, infrared small target detection (IRSTD) has been dominated by deep-learning-based methods. However, these methods mainly focus on the design of complex model structures to extract discriminative features, leaving the loss functions for IRSTD under-explored. For example, the widely used Intersection over Union (IoU) and Dice losses lack sensitivity to the scales and locations of targets, limiting the detection performance of detectors. In this paper, we focus on boosting detection performance with a more effective loss but a simpler model structure. Specifically, we first propose a novel Scale and Location Sensitive (SLS) loss to handle the limitations of existing losses: 1) for scale sensitivity, we compute a weight for the IoU loss based on target scales to help the detector distinguish targets with different scales: 2) for location sensitivity, we introduce a penalty term based on the center points of targets to help the detector localize targets more precisely. Then, we design a simple Multi-Scale Head to the plain U-Net (MSHNet). By applying SLS loss to each scale of the predictions, our MSHNet outperforms existing state-of-the-art methods by a large margin. In addition, the detection performance of existing detectors can be further improved when trained with our SLS loss, demonstrating the effectiveness and generalization of our SLS loss. The code is available at https://github.com/ying-fu/MSHNet.",
        "page": "http://arxiv.org/abs/2403.19366",
        "pdf": "http://arxiv.org/pdf/2403.19366.pdf"
    },
    {
        "title": "TutteNet: Injective 3D Deformations by Composition of 2D Mesh Deformations",
        "author": "Bo Sun, Thibault Groueix, Chen Song, Qixing Huang, Noam Aigerman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NECA: Neural Customizable Human Avatar",
        "author": "Junjin Xiao, Qing Zhang, Zhan Xu, Wei-Shi Zheng",
        "abstract": "Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at https://github.com/iSEE-Laboratory/NECA.",
        "page": "http://arxiv.org/abs/2403.10335",
        "pdf": "http://arxiv.org/pdf/2403.10335.pdf"
    },
    {
        "title": "Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Pre-training Framework",
        "author": "Vu Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, Johan Verjans",
        "abstract": "Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling zero-shot pathological recognition by comparing the query image with the textual descriptions for each disease. Due to the complex semantics of biomedical texts, current methods struggle to align medical images with key pathological findings in unstructured reports. This leads to the misalignment with the target disease's textual representation. In this paper, we introduce a novel VLP framework designed to dissect disease descriptions into their fundamental aspects, leveraging prior knowledge about the visual manifestations of pathologies. This is achieved by consulting a large language model and medical experts. Integrating a Transformer module, our approach aligns an input image with the diverse elements of a disease, generating aspect-centric image representations. By consolidating the matches from each aspect, we improve the compatibility between an image and its associated disease. Additionally, capitalizing on the aspect-oriented representations, we present a dual-head Transformer tailored to process known and unknown diseases, optimizing the comprehensive detection efficacy. Conducting experiments on seven downstream datasets, ours improves the accuracy of recent methods by up to 8.56% and 17.26% for seen and unseen categories, respectively. Our code is released at https://github.com/HieuPhan33/MAVL.",
        "page": "http://arxiv.org/abs/2403.07636",
        "pdf": "http://arxiv.org/pdf/2403.07636.pdf"
    },
    {
        "title": "3D Feature Tracking via Event Camera",
        "author": "Siqi Li, Zhou Zhikuan, Zhou Xue, Yipeng Li, Shaoyi Du, Yue Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Background Prompts to Discover Implicit Knowledge for Open Vocabulary Object Detection",
        "author": "Jiaming Li, Jiacheng Zhang, Jichang Li, Ge Li, Si Liu, Liang Lin, Guanbin Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras",
        "author": "Huajian Huang, Longwei Li, Hui Cheng, Sai-Kit Yeung",
        "abstract": "The integration of neural rendering and the SLAM system recently showed promising results in joint localization and photorealistic view reconstruction. However, existing methods, fully relying on implicit representations, are so resource-hungry that they cannot run on portable devices, which deviates from the original intention of SLAM. In this paper, we present Photo-SLAM, a novel SLAM framework with a hyper primitives map. Specifically, we simultaneously exploit explicit geometric features for localization and learn implicit photometric features to represent the texture information of the observed environment. In addition to actively densifying hyper primitives based on geometric features, we further introduce a Gaussian-Pyramid-based training method to progressively learn multi-level features, enhancing photorealistic mapping performance. The extensive experiments with monocular, stereo, and RGB-D datasets prove that our proposed system Photo-SLAM significantly outperforms current state-of-the-art SLAM systems for online photorealistic mapping, e.g., PSNR is 30% higher and rendering speed is hundreds of times faster in the Replica dataset. Moreover, the Photo-SLAM can run at real-time speed using an embedded platform such as Jetson AGX Orin, showing the potential of robotics applications.",
        "page": "http://arxiv.org/abs/2311.16728",
        "pdf": "http://arxiv.org/pdf/2311.16728.pdf"
    },
    {
        "title": "CAGE: Controllable Articulation GEneration",
        "author": "Jiayi Liu, Hou In Ivan Tam, Ali Mahdavi Amiri, Manolis Savva",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion",
        "author": "Hao Ai, Lin Wang",
        "abstract": "360 depth estimation has recently received great attention for 3D reconstruction owing to its omnidirectional field of view (FoV). Recent approaches are predominantly focused on cross-projection fusion with geometry-based re-projection: they fuse 360 images with equirectangular projection (ERP) and another projection type, e.g., cubemap projection to estimate depth with the ERP format. However, these methods suffer from 1) limited local receptive fields, making it hardly possible to capture large FoV scenes, and 2) prohibitive computational cost, caused by the complex cross-projection fusion module design. In this paper, we propose Elite360D, a novel framework that inputs the ERP image and icosahedron projection (ICOSAP) point set, which is undistorted and spatially continuous. Elite360D is superior in its capacity in learning a representation from a local-with-global perspective. With a flexible ERP image encoder, it includes an ICOSAP point encoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M parameters). Specifically, the ERP image encoder can take various perspective image-trained backbones (e.g., ResNet, Transformer) to extract local features. The point encoder extracts the global features from the ICOSAP. Then, the B2F module captures the semantic- and distance-aware dependencies between each pixel of the ERP feature and the entire ICOSAP feature set. Without specific backbone design and obvious computational cost increase, Elite360D outperforms the prior arts on several benchmark datasets.",
        "page": "http://arxiv.org/abs/2403.16376",
        "pdf": "http://arxiv.org/pdf/2403.16376.pdf"
    },
    {
        "title": "Continual Segmentation with Disentangled Objectness Learning and Class Recognition",
        "author": "Yizheng Gong, Siyue Yu, Xiaoyang Wang, Jimin Xiao",
        "abstract": "Most continual segmentation methods tackle the problem as a per-pixel classification task. However, such a paradigm is very challenging, and we find query-based segmenters with built-in objectness have inherent advantages compared with per-pixel ones, as objectness has strong transfer ability and forgetting resistance. Based on these findings, we propose CoMasTRe by disentangling continual segmentation into two stages: forgetting-resistant continual objectness learning and well-researched continual classification. CoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at the first stage and leaving recognition to the second stage. During continual learning, a simple but effective distillation is adopted to strengthen objectness. To further mitigate the forgetting of old classes, we design a multi-label class distillation strategy suited for segmentation. We assess the effectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show that our method outperforms per-pixel and query-based methods on both datasets. Code will be available at https://github.com/jordangong/CoMasTRe.",
        "page": "http://arxiv.org/abs/2403.03477",
        "pdf": "http://arxiv.org/pdf/2403.03477.pdf"
    },
    {
        "title": "Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation",
        "author": "Bingfeng Zhang, Siyue Yu, Yunchao Wei, Yao Zhao, Jimin Xiao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "The STVchrono Dataset: Towards Continuous Change Recognition in Time",
        "author": "Yanjun Sun, Yue Qiu, Mariia Khan, Fumiya Matsuzawa, Kenji Iwata",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Context-Guided Spatio-Temporal Video Grounding",
        "author": "Xin Gu, Heng Fan, Yan Huang, Tiejian Luo, Libo Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unexplored Faces of Robustness and Out-of-Distribution: Covariate Shifts in Environment and Sensor Domains",
        "author": "Eunsu Baek, Keondo Park, Ji-yoon Kim, Hyung-Sin Kim",
        "abstract": "Computer vision applications predict on digital images acquired by a camera from physical scenes through light. However, conventional robustness benchmarks rely on perturbations in digitized images, diverging from distribution shifts occurring in the image acquisition process. To bridge this gap, we introduce a new distribution shift dataset, ImageNet-ES, comprising variations in environmental and camera sensor factors by directly capturing 202k images with a real camera in a controllable testbed. With the new dataset, we evaluate out-of-distribution (OOD) detection and model robustness. We find that existing OOD detection methods do not cope with the covariate shifts in ImageNet-ES, implying that the definition and detection of OOD should be revisited to embrace real-world distribution shifts. We also observe that the model becomes more robust in both ImageNet-C and -ES by learning environment and sensor variations in addition to existing digital augmentations. Lastly, our results suggest that effective shift mitigation via camera sensor control can significantly improve performance without increasing model size. With these findings, our benchmark may aid future research on robustness, OOD, and camera sensor control for computer vision. Our code and dataset are available at https://github.com/Edw2n/ImageNet-ES.",
        "page": "http://arxiv.org/abs/2404.15882",
        "pdf": "http://arxiv.org/pdf/2404.15882.pdf"
    },
    {
        "title": "TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
        "author": "Dai Shi",
        "abstract": "Due to the depth degradation effect in residual connections, many efficient Vision Transformers models that rely on stacking layers for information exchange often fail to form sufficient information mixing, leading to unnatural visual perception. To address this issue, in this paper, we propose Aggregated Attention, a biomimetic design-based token mixer that simulates biological foveal vision and continuous eye movement while enabling each token on the feature map to have a global perception. Furthermore, we incorporate learnable tokens that interact with conventional queries and keys, which further diversifies the generation of affinity matrices beyond merely relying on the similarity between queries and keys. Our approach does not rely on stacking for information exchange, thus effectively avoiding depth degradation and achieving natural visual perception. Additionally, we propose Convolutional GLU, a channel mixer that bridges the gap between GLU and SE mechanism, which empowers each token to have channel attention based on its nearest neighbor image features, enhancing local modeling capability and model robustness. We combine aggregated attention and convolutional GLU to create a new visual backbone called TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves state-of-the-art performance across multiple model sizes. At a resolution of $224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0%, surpassing ConvNeXt-B with 69% fewer parameters. Our TransNeXt-Base achieves an ImageNet accuracy of 86.2% and an ImageNet-A accuracy of 61.6% at a resolution of $384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic segmentation mIoU of 54.7.",
        "page": "http://arxiv.org/abs/2311.17132",
        "pdf": "http://arxiv.org/pdf/2311.17132.pdf"
    },
    {
        "title": "Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse",
        "author": "Yining Wang, Junjie Sun, Chenyue Wang, Mi Zhang, Min Yang",
        "abstract": "Recent studies have noted an intriguing phenomenon termed Neural Collapse, that is, when the neural networks establish the right correlation between feature spaces and the training targets, their last-layer features, together with the classifier weights, will collapse into a stable and symmetric structure. In this paper, we extend the investigation of Neural Collapse to the biased datasets with imbalanced attributes. We observe that models will easily fall into the pitfall of shortcut learning and form a biased, non-collapsed feature space at the early period of training, which is hard to reverse and limits the generalization capability. To tackle the root cause of biased classification, we follow the recent inspiration of prime training, and propose an avoid-shortcut learning framework without additional training complexity. With well-designed shortcut primes based on Neural Collapse structure, the models are encouraged to skip the pursuit of simple shortcuts and naturally capture the intrinsic correlations. Experimental results demonstrate that our method induces better convergence properties during training, and achieves state-of-the-art generalization performance on both synthetic and real-world biased datasets.",
        "page": "http://arxiv.org/abs/2405.05587",
        "pdf": "http://arxiv.org/pdf/2405.05587.pdf"
    },
    {
        "title": "Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction",
        "author": "Jianping Jiang, xinyu zhou, Bingxuan Wang, Xiaoming Deng, Chao Xu, Boxin Shi",
        "abstract": "Reliable hand mesh reconstruction (HMR) from commonly-used color and depth sensors is challenging especially under scenarios with varied illuminations and fast motions. Event camera is a highly promising alternative for its high dynamic range and dense temporal resolution properties, but it lacks key texture appearance for hand mesh reconstruction. In this paper, we propose EvRGBHand -- the first approach for 3D hand mesh reconstruction with an event camera and an RGB camera compensating for each other. By fusing two modalities of data across time, space, and information dimensions,EvRGBHand can tackle overexposure and motion blur issues in RGB-based HMR and foreground scarcity and background overflow issues in event-based HMR. We further propose EvRGBDegrader, which allows our model to generalize effectively in challenging scenes, even when trained solely on standard scenes, thus reducing data acquisition costs. Experiments on real-world data demonstrate that EvRGBHand can effectively solve the challenging issues when using either type of camera alone via retaining the merits of both, and shows the potential of generalization to outdoor scenes and another type of event camera.",
        "page": "http://arxiv.org/abs/2403.07346",
        "pdf": "http://arxiv.org/pdf/2403.07346.pdf"
    },
    {
        "title": "Prompt-Free Diffusion: Taking \u201cText\u201d out of Text-to-Image Diffusion Models",
        "author": "Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, Humphrey Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
        "author": "Jiayi Guo, Xingqian Xu, Yifan Pu, Zanlin Ni, Chaofei Wang, Manushree Vasu, Shiji Song, Gao Huang, Humphrey Shi",
        "abstract": "Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with high fidelity and diverse contents. Despite this advancement, latent space smoothness within diffusion models remains largely unexplored. Smooth latent spaces ensure that a perturbation on an input latent corresponds to a steady change in the output image. This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing. In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations. To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth. Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step. In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model. Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks. Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models. Code is available at https://github.com/SHI-Labs/Smooth-Diffusion.",
        "page": "http://arxiv.org/abs/2312.04410",
        "pdf": "http://arxiv.org/pdf/2312.04410.pdf"
    },
    {
        "title": "Fourier Priors-Guided Diffusion for Zero-Shot Joint Low-Light Enhancement and Deblurring",
        "author": "Xiaoqian Lv, Shengping Zhang, Chenyang Wang, Yichen Zheng, Bineng Zhong, Chongyi Li, Liqiang Nie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning from Synthetic Human Group Activities",
        "author": "Che-Jui Chang, Danrui Li, Deep Patel, Parth Goel, Seonghyeon Moon, Samuel Sohn, Honglu Zhou, Sejong Yoon, Vladimir Pavlovic, Mubbasir Kapadia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MMA-Diffusion: MultiModal Attack on Diffusion Models",
        "author": "Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Tsung-Yi Ho, Xu Nan, Qiang Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning",
        "author": "Haoyu Chen, Wenbo Li, Jinjin Gu, Jingjing Ren, Haoze Sun, Xueyi Zou, Youliang Yan, Zhensong Zhang, Lei Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CoSeR: Bridging Image and Language for Cognitive Super-Resolution",
        "author": "Haoze Sun, Wenbo Li, Jianzhuang Liu, Haoyu Chen, Renjing Pei, Xueyi Zou, Youliang Yan, Yujiu Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Transferable and Principled Efficiency for Open-Vocabulary Segmentation",
        "author": "Jingxuan Xu, Wuyang Chen, Yao Zhao, Yunchao Wei",
        "abstract": "Recent success of pre-trained foundation vision-language models makes Open-Vocabulary Segmentation (OVS) possible. Despite the promising performance, this approach introduces heavy computational overheads for two challenges: 1) large model sizes of the backbone; 2) expensive costs during the fine-tuning. These challenges hinder this OVS strategy from being widely applicable and affordable in real-world scenarios. Although traditional methods such as model compression and efficient fine-tuning can address these challenges, they often rely on heuristics. This means that their solutions cannot be easily transferred and necessitate re-training on different models, which comes at a cost. In the context of efficient OVS, we target achieving performance that is comparable to or even better than prior OVS works based on large vision-language foundation models, by utilizing smaller models that incur lower training costs. The core strategy is to make our efficiency principled and thus seamlessly transferable from one OVS framework to others without further customization. Comprehensive experiments on diverse OVS benchmarks demonstrate our superior trade-off between segmentation accuracy and computation costs over previous works. Our code is available on https://github.com/Xujxyang/OpenTrans",
        "page": "http://arxiv.org/abs/2404.07448",
        "pdf": "http://arxiv.org/pdf/2404.07448.pdf"
    },
    {
        "title": "Unbiased Estimator for Distorted Conic in Camera Calibration",
        "author": "Chaehyeon Song, Jaeho Shin, Myung-Hwan Jeon, Jongwoo Lim, Ayoung Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Explaining CLIP's performance disparities on data from blind/low vision users",
        "author": "Daniela Massiceti, Camilla Longden, Agnieszka S\u0142owik, Samuel Wills, Martin Grayson, Cecily Morrison",
        "abstract": "Large multi-modal models (LMMs) hold the potential to usher in a new era of automated visual assistance for people who are blind or low vision (BLV). Yet, these models have not been systematically evaluated on data captured by BLV users. We address this by empirically assessing CLIP, a widely-used LMM likely to underpin many assistive technologies. Testing 25 CLIP variants in a zero-shot classification task, we find that their accuracy is 15 percentage points lower on average for images captured by BLV users than web-crawled images. This disparity stems from CLIP's sensitivities to 1) image content (e.g. not recognizing disability objects as well as other objects); 2) image quality (e.g. not being robust to lighting variation); and 3) text content (e.g. not recognizing objects described by tactile adjectives as well as visual ones). We delve deeper with a textual analysis of three common pre-training datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content is rarely mentioned. We then provide three examples that illustrate how the performance disparities extend to three downstream models underpinned by CLIP: OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5 images can mitigate CLIP's quality-of-service disparities for BLV users in some scenarios, which we discuss alongside a set of other possible mitigations.",
        "page": "http://arxiv.org/abs/2311.17315",
        "pdf": "http://arxiv.org/pdf/2311.17315.pdf"
    },
    {
        "title": "GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation",
        "author": "Zifan Wang, Junyu Chen, Ziqing Chen, Pengwei Xie, Rui Chen, Li Yi",
        "abstract": "This paper presents GenH2R, a framework for learning generalizable vision-based human-to-robot (H2R) handover skills. The goal is to equip robots with the ability to reliably receive objects with unseen geometry handed over by humans in various complex trajectories. We acquire such generalizability by learning H2R handover at scale with a comprehensive solution including procedural simulation assets creation, automated demonstration generation, and effective imitation learning. We leverage large-scale 3D model repositories, dexterous grasp generation methods, and curve-based 3D animation to create an H2R handover simulation environment named \\simabbns, surpassing the number of scenes in existing simulators by three orders of magnitude. We further introduce a distillation-friendly demonstration generation method that automatically generates a million high-quality demonstrations suitable for learning. Finally, we present a 4D imitation learning method augmented by a future forecasting objective to distill demonstrations into a visuo-motor handover policy. Experimental evaluations in both simulators and the real world demonstrate significant improvements (at least +10\\% success rate) over baselines in all cases. The project page is https://GenH2R.github.io/.",
        "page": "http://arxiv.org/abs/2401.00929",
        "pdf": "http://arxiv.org/pdf/2401.00929.pdf"
    },
    {
        "title": "SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream",
        "author": "Lin Zhu, Kangmin Jia, Yifan Zhao, Yunshan Qi, Lizhi Wang, Hua Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation",
        "author": "Vikas Thamizharasan, Difan Liu, Matthew Fisher, Nanxuan Zhao, Evangelos Kalogerakis, Michal Luk\u00e1\u010d",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Attention Calibration for Disentangled Text-to-Image Personalization",
        "author": "Yanbing Zhang, Mengping Yang, Qin Zhou, Zhe Wang",
        "abstract": "Recent thrilling progress in large-scale text-to-image (T2I) models has unlocked unprecedented synthesis quality of AI-generated content (AIGC) including image generation, 3D and video composition. Further, personalized techniques enable appealing customized production of a novel concept given only several images as reference. However, an intriguing problem persists: Is it possible to capture multiple, novel concepts from one single reference image? In this paper, we identify that existing approaches fail to preserve visual consistency with the reference image and eliminate cross-influence from concepts. To alleviate this, we propose an attention calibration mechanism to improve the concept-level understanding of the T2I model. Specifically, we first introduce new learnable modifiers bound with classes to capture attributes of multiple concepts. Then, the classes are separated and strengthened following the activation of the cross-attention operation, ensuring comprehensive and self-contained concepts. Additionally, we suppress the attention activation of different classes to mitigate mutual influence among concepts. Together, our proposed method, dubbed DisenDiff, can learn disentangled multiple concepts from one single image and produce novel customized images with learned concepts. We demonstrate that our method outperforms the current state of the art in both qualitative and quantitative evaluations. More importantly, our proposed techniques are compatible with LoRA and inpainting pipelines, enabling more interactive experiences.",
        "page": "http://arxiv.org/abs/2403.18551",
        "pdf": "http://arxiv.org/pdf/2403.18551.pdf"
    },
    {
        "title": "EVS-assisted joint Deblurring, Rolling-Shutter Correction and Video Frame Interpolation through Sensor Inverse Modeling",
        "author": "Rui Jiang, Fangwen Tu, Yixuan Long, Aabhaas Vaish, Bowen Zhou, Qinyi Wang, Wei Zhang, Yuntan Fang, Luis Eduardo Garc\u00eda Capel, Bo Mu, Tiejun Dai, Andreas Suess",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SIRA: Scalable Inter-frame Relation and Association for Radar Perception",
        "author": "Ryoma Yataka, Pu (Perry) Wang, Petros Boufounos, Ryuhei Takahashi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PixelRNN: In-pixel Recurrent Neural Networks for End-to-end-optimized Perception with Neural Sensors",
        "author": "Haley So, Laurie Bose, Piotr Dudek, Gordon Wetzstein",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences",
        "author": "Minyoung Hwang, Luca Weihs, Chanwoo Park, Kimin Lee, Aniruddha Kembhavi, Kiana Ehsani",
        "abstract": "Customizing robotic behaviors to be aligned with diverse human preferences is an underexplored challenge in the field of embodied AI. In this paper, we present Promptable Behaviors, a novel framework that facilitates efficient personalization of robotic agents to diverse human preferences in complex environments. We use multi-objective reinforcement learning to train a single policy adaptable to a broad spectrum of preferences. We introduce three distinct methods to infer human preferences by leveraging different types of interactions: (1) human demonstrations, (2) preference feedback on trajectory comparisons, and (3) language instructions. We evaluate the proposed method in personalized object-goal navigation and flee navigation tasks in ProcTHOR and RoboTHOR, demonstrating the ability to prompt agent behaviors to satisfy human preferences in various scenarios. Project page: https://promptable-behaviors.github.io",
        "page": "http://arxiv.org/abs/2312.09337",
        "pdf": "http://arxiv.org/pdf/2312.09337.pdf"
    },
    {
        "title": "Category-Level Multi-Part Multi-Joint 3D Shape Assembly",
        "author": "Yichen Li, Kaichun Mo, Yueqi Duan, He Wang, Jiequan Zhang, Lin Shao, Wojciech Matusik, Leonidas Guibas",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "IReNe: Instant Recoloring of Neural Radiance Fields",
        "author": "Alessio Mazzucchelli, Adrian Garcia-Garcia, Elena Garces, Fernando Rivas-Manzaneque, Francesc Moreno-Noguer, Adrian Penate-Sanchez",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SEAS: ShapE-Aligned Supervision for Person Re-Identification",
        "author": "Haidong Zhu, Pranav Budhwant, Zhaoheng Zheng, Ram Nevatia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis",
        "author": "Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, Christian Theobalt",
        "abstract": "Gestures play a key role in human communication. Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance. Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words. Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis. Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing. Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures. To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another. We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks. We urge the reader to watch our supplementary video at our website.",
        "page": "http://arxiv.org/abs/2403.17936",
        "pdf": "http://arxiv.org/pdf/2403.17936.pdf"
    },
    {
        "title": "Continuous Pose for Monocular Cameras in Neural Implicit Representation",
        "author": "Qi Ma, Danda Paudel, Ajad Chhatkuli, Luc Van Gool",
        "abstract": "In this paper, we showcase the effectiveness of optimizing monocular camera poses as a continuous function of time. The camera poses are represented using an implicit neural function which maps the given time to the corresponding camera pose. The mapped camera poses are then used for the downstream tasks where joint camera pose optimization is also required. While doing so, the network parameters -- that implicitly represent camera poses -- are optimized. We exploit the proposed method in four diverse experimental settings, namely, (1) NeRF from noisy poses; (2) NeRF from asynchronous Events; (3) Visual Simultaneous Localization and Mapping (vSLAM); and (4) vSLAM with IMUs. In all four settings, the proposed method performs significantly better than the compared baselines and the state-of-the-art methods. Additionally, using the assumption of continuous motion, changes in pose may actually live in a manifold that has lower than 6 degrees of freedom (DOF) is also realized. We call this low DOF motion representation as the \\emph{intrinsic motion} and use the approach in vSLAM settings, showing impressive camera tracking performance.",
        "page": "http://arxiv.org/abs/2311.17119",
        "pdf": "http://arxiv.org/pdf/2311.17119.pdf"
    },
    {
        "title": "In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing",
        "author": "Yiran Xu, Zhixin Shu, Cameron Smith, Seoung Wug Oh, Jia-Bin Huang",
        "abstract": "3D-aware GANs offer new capabilities for view synthesis while preserving the editing functionalities of their 2D counterparts. GAN inversion is a crucial step that seeks the latent code to reconstruct input images or videos, subsequently enabling diverse editing tasks through manipulation of this latent code. However, a model pre-trained on a particular dataset (e.g., FFHQ) often has difficulty reconstructing images with out-of-distribution (OOD) objects such as faces with heavy make-up or occluding objects. We address this issue by explicitly modeling OOD objects from the input in 3D-aware GANs. Our core idea is to represent the image using two individual neural radiance fields: one for the in-distribution content and the other for the out-of-distribution object. The final reconstruction is achieved by optimizing the composition of these two radiance fields with carefully designed regularization. We demonstrate that our explicit decomposition alleviates the inherent trade-off between reconstruction fidelity and editability. We evaluate reconstruction accuracy and editability of our method on challenging real face images and videos and showcase favorable results against other baselines.",
        "page": "http://arxiv.org/abs/2302.04871",
        "pdf": "http://arxiv.org/pdf/2302.04871.pdf"
    },
    {
        "title": "Efficient Solution of Point-Line Absolute Pose",
        "author": "Petr Hruby, Timothy Duff, Marc Pollefeys",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers",
        "author": "Jonathan F. Carter, Joao Jorge, Oliver Gibson, Lionel Tarassenko",
        "abstract": "Advances in camera-based physiological monitoring have enabled the robust, non-contact measurement of respiration and the cardiac pulse, which are known to be indicative of the sleep stage. This has led to research into camera-based sleep monitoring as a promising alternative to \"gold-standard\" polysomnography, which is cumbersome, expensive to administer, and hence unsuitable for longer-term clinical studies. In this paper, we introduce SleepVST, a transformer model which enables state-of-the-art performance in camera-based sleep stage classification (sleep staging). After pre-training on contact sensor data, SleepVST outperforms existing methods for cardio-respiratory sleep staging on the SHHS and MESA datasets, achieving total Cohen's kappa scores of 0.75 and 0.77 respectively. We then show that SleepVST can be successfully transferred to cardio-respiratory waveforms extracted from video, enabling fully contact-free sleep staging. Using a video dataset of 50 nights, we achieve a total accuracy of 78.8\\% and a Cohen's $\\kappa$ of 0.71 in four-class video-based sleep staging, setting a new state-of-the-art in the domain.",
        "page": "http://arxiv.org/abs/2404.03831",
        "pdf": "http://arxiv.org/pdf/2404.03831.pdf"
    },
    {
        "title": "Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension",
        "author": "Quan Liu, Hongzi Zhu, Zhenxi Wang, Yunsong Zhou, Shan Chang, Minyi Guo",
        "abstract": "Registration of point clouds collected from a pair of distant vehicles provides a comprehensive and accurate 3D view of the driving scenario, which is vital for driving safety related applications, yet existing literature suffers from the expensive pose label acquisition and the deficiency to generalize to new data distributions. In this paper, we propose EYOC, an unsupervised distant point cloud registration method that adapts to new point cloud distributions on the fly, requiring no global pose labels. The core idea of EYOC is to train a feature extractor in a progressive fashion, where in each round, the feature extractor, trained with near point cloud pairs, can label slightly farther point cloud pairs, enabling self-supervision on such far point cloud pairs. This process continues until the derived extractor can be used to register distant point clouds. Particularly, to enable high-fidelity correspondence label generation, we devise an effective spatial filtering scheme to select the most representative correspondences to register a point cloud pair, and then utilize the aligned point clouds to discover more correct correspondences. Experiments show that EYOC can achieve comparable performance with state-of-the-art supervised methods at a lower training cost. Moreover, it outwits supervised methods regarding generalization performance on new data distributions.",
        "page": "http://arxiv.org/abs/2403.03532",
        "pdf": "http://arxiv.org/pdf/2403.03532.pdf"
    },
    {
        "title": "LEOD: Label-Efficient Object Detection for Event Cameras",
        "author": "Ziyi Wu, Mathias Gehrig, Qing Lyu, Xudong Liu, Igor Gilitschenski",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles",
        "author": "Vanessa Skliarova, Egor Zakharov, Otmar Hilliges, Michael J. Black, Justus Thies",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
        "author": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen",
        "abstract": "We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.",
        "page": "http://arxiv.org/abs/2311.16502",
        "pdf": "http://arxiv.org/pdf/2311.16502.pdf"
    },
    {
        "title": "SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection",
        "author": "Mingxuan Liu, Tyler Hayes, Elisa Ricci, Gabriela Csurka, Riccardo Volpi",
        "abstract": "Open-vocabulary object detection (OvOD) has transformed detection into a language-guided task, empowering users to freely define their class vocabularies of interest during inference. However, our initial investigation indicates that existing OvOD detectors exhibit significant variability when dealing with vocabularies across various semantic granularities, posing a concern for real-world deployment. To this end, we introduce Semantic Hierarchy Nexus (SHiNe), a novel classifier that uses semantic knowledge from class hierarchies. It runs offline in three steps: i) it retrieves relevant super-/sub-categories from a hierarchy for each target class; ii) it integrates these categories into hierarchy-aware sentences; iii) it fuses these sentence embeddings to generate the nexus classifier vector. Our evaluation on various detection benchmarks demonstrates that SHiNe enhances robustness across diverse vocabulary granularities, achieving up to +31.9% mAP50 with ground truth hierarchies, while retaining improvements using hierarchies generated by large language models. Moreover, when applied to open-vocabulary classification on ImageNet-1k, SHiNe improves the CLIP zero-shot baseline by +2.8% accuracy. SHiNe is training-free and can be seamlessly integrated with any off-the-shelf OvOD detector, without incurring additional computational overhead during inference. The code is open source.",
        "page": "http://arxiv.org/abs/2405.10053",
        "pdf": "http://arxiv.org/pdf/2405.10053.pdf"
    },
    {
        "title": "Unified Language-driven Zero-shot Domain Adaptation",
        "author": "Senqiao Yang, Zhuotao Tian, Li Jiang, Jiaya Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation",
        "author": "Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, Jiaya Jia",
        "abstract": "The booming of 3D recognition in the 2020s began with the introduction of point cloud transformers. They quickly overwhelmed sparse CNNs and became state-of-the-art models, especially in 3D semantic segmentation. However, sparse CNNs are still valuable networks, due to their efficiency treasure, and ease of application. In this work, we reexamine the design distinctions and test the limits of what a sparse CNN can achieve. We discover that the key credit to the performance difference is adaptivity. Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap. This exploration led to the creation of Omni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal computational cost. Without any self-attention modules, OA-CNNs favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation benchmarks respectively, while maintaining at most 5x better speed than transformer counterparts. This revelation highlights the potential of pure sparse CNNs to outperform transformer-related networks.",
        "page": "http://arxiv.org/abs/2403.14418",
        "pdf": "http://arxiv.org/pdf/2403.14418.pdf"
    },
    {
        "title": "GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding",
        "author": "Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, Hengshuang Zhao, Jiaya Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Point Transformer V3: Simpler, Faster, Stronger",
        "author": "Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao",
        "abstract": "This paper is not motivated to seek innovation within the attention mechanism. Instead, it focuses on overcoming the existing trade-offs between accuracy and efficiency within the context of point cloud processing, leveraging the power of scale. Drawing inspiration from recent advances in 3D large-scale representation learning, we recognize that model performance is more influenced by scale than by intricate design. Therefore, we present Point Transformer V3 (PTv3), which prioritizes simplicity and efficiency over the accuracy of certain mechanisms that are minor to the overall performance after scaling, such as replacing the precise neighbor search by KNN with an efficient serialized neighbor mapping of point clouds organized with specific patterns. This principle enables significant scaling, expanding the receptive field from 16 to 1024 points while remaining efficient (a 3x increase in processing speed and a 10x improvement in memory efficiency compared with its predecessor, PTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that span both indoor and outdoor scenarios. Further enhanced with multi-dataset joint training, PTv3 pushes these results to a higher level.",
        "page": "http://arxiv.org/abs/2312.10035",
        "pdf": "http://arxiv.org/pdf/2312.10035.pdf"
    },
    {
        "title": "Training Vision Transformers for Semi-Supervised Semantic Segmentation",
        "author": "Xinting Hu, Li Jiang, Bernt Schiele",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SpecNeRF: Gaussian Directional Encoding for Specular Reflections",
        "author": "Li Ma, Vasu Agrawal, Haithem Turki, Changil Kim, Chen Gao, Pedro V. Sander, Michael Zollhoefer, Christian Richardt",
        "abstract": "Neural radiance fields have achieved remarkable performance in modeling the appearance of 3D scenes. However, existing approaches still struggle with the view-dependent appearance of glossy surfaces, especially under complex lighting of indoor environments. Unlike existing methods, which typically assume distant lighting like an environment map, we propose a learnable Gaussian directional encoding to better model the view-dependent effects under near-field lighting conditions. Importantly, our new directional encoding captures the spatially-varying nature of near-field lighting and emulates the behavior of prefiltered environment maps. As a result, it enables the efficient evaluation of preconvolved specular color at any 3D location with varying roughness coefficients. We further introduce a data-driven geometry prior that helps alleviate the shape radiance ambiguity in reflection modeling. We show that our Gaussian directional encoding and geometry prior significantly improve the modeling of challenging specular reflections in neural radiance fields, which helps decompose appearance into more physically meaningful components.",
        "page": "http://arxiv.org/abs/2312.13102",
        "pdf": "http://arxiv.org/pdf/2312.13102.pdf"
    },
    {
        "title": "Holodeck: Language Guided Generation of 3D Embodied AI Environments",
        "author": "Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark",
        "abstract": "3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope. To mitigate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fully automatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as \"apartment for a researcher with a cat\" and \"office of a professor who is a fan of Star Wars\". Holodeck leverages a large language model (i.e., GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents.",
        "page": "http://arxiv.org/abs/2312.09067",
        "pdf": "http://arxiv.org/pdf/2312.09067.pdf"
    },
    {
        "title": "Learning to Remove Wrinkled Transparent Film with Polarized Prior",
        "author": "Jiaqi Tang, RUIZHENG WU, Xiaogang Xu, Sixing Hu, Ying-Cong Chen",
        "abstract": "In this paper, we study a new problem, Film Removal (FR), which attempts to remove the interference of wrinkled transparent films and reconstruct the original information under films for industrial recognition systems. We first physically model the imaging of industrial materials covered by the film. Considering the specular highlight from the film can be effectively recorded by the polarized camera, we build a practical dataset with polarization information containing paired data with and without transparent film. We aim to remove interference from the film (specular highlights and other degradations) with an end-to-end framework. To locate the specular highlight, we use an angle estimation network to optimize the polarization angle with the minimized specular highlight. The image with minimized specular highlight is set as a prior for supporting the reconstruction network. Based on the prior and the polarized images, the reconstruction network can decouple all degradations from the film. Extensive experiments show that our framework achieves SOTA performance in both image reconstruction and industrial downstream tasks. Our code will be released at \\url{https://github.com/jqtangust/FilmRemoval}.",
        "page": "http://arxiv.org/abs/2403.04368",
        "pdf": "http://arxiv.org/pdf/2403.04368.pdf"
    },
    {
        "title": "Sparse Global Matching for Video Frame Interpolation with Large Motion",
        "author": "Chunxu Liu, Guozhen Zhang, Rui Zhao, Limin Wang",
        "abstract": "Large motion poses a critical challenge in Video Frame Interpolation (VFI) task. Existing methods are often constrained by limited receptive fields, resulting in sub-optimal performance when handling scenarios with large motion. In this paper, we introduce a new pipeline for VFI, which can effectively integrate global-level information to alleviate issues associated with large motion. Specifically, we first estimate a pair of initial intermediate flows using a high-resolution feature map for extracting local details. Then, we incorporate a sparse global matching branch to compensate for flow estimation, which consists of identifying flaws in initial flows and generating sparse flow compensation with a global receptive field. Finally, we adaptively merge the initial flow estimation with global flow compensation, yielding a more accurate intermediate flow. To evaluate the effectiveness of our method in handling large motion, we carefully curate a more challenging subset from commonly used benchmarks. Our method demonstrates the state-of-the-art performance on these VFI subsets with large motion.",
        "page": "http://arxiv.org/abs/2404.06913",
        "pdf": "http://arxiv.org/pdf/2404.06913.pdf"
    },
    {
        "title": "MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading",
        "author": "Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amin Fadaeinejad, Rafael M. O. Cruz, Marc-Andr\u00e9 Carbonneau",
        "abstract": "Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods. We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrinsic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects. The project website and the dataset are available on the following link: https://ubisoft-laforge.github.io/character/mosar/",
        "page": "http://arxiv.org/abs/2312.13091",
        "pdf": "http://arxiv.org/pdf/2312.13091.pdf"
    },
    {
        "title": "ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image",
        "author": "Marco Pesavento, Yuanlu Xu, Nikolaos Sarafianos, Robert Maier, Ziyan Wang, Chun-Han Yao, Marco Volino, Edmond Boyer, Adrian Hilton, Tony Tung",
        "abstract": "Recent progress in human shape learning, shows that neural implicit models are effective in generating 3D human surfaces from limited number of views, and even from a single RGB image. However, existing monocular approaches still struggle to recover fine geometric details such as face, hands or cloth wrinkles. They are also easily prone to depth ambiguities that result in distorted geometries along the camera optical axis. In this paper, we explore the benefits of incorporating depth observations in the reconstruction process by introducing ANIM, a novel method that reconstructs arbitrary 3D human shapes from single-view RGB-D images with an unprecedented level of accuracy. Our model learns geometric details from both multi-resolution pixel-aligned and voxel-aligned features to leverage depth information and enable spatial relationships, mitigating depth ambiguities. We further enhance the quality of the reconstructed shape by introducing a depth-supervision strategy, which improves the accuracy of the signed distance field estimation of points that lie on the reconstructed surface. Experiments demonstrate that ANIM outperforms state-of-the-art works that use RGB, surface normals, point cloud or RGB-D data as input. In addition, we introduce ANIM-Real, a new multi-modal dataset comprising high-quality scans paired with consumer-grade RGB-D camera, and our protocol to fine-tune ANIM, enabling high-quality reconstruction from real-world human capture.",
        "page": "http://arxiv.org/abs/2403.10357",
        "pdf": "http://arxiv.org/pdf/2403.10357.pdf"
    },
    {
        "title": "CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs",
        "author": "Haocheng Yuan, Jing Xu, Hao Pan, Adrien Bousseau, Niloy J. Mitra, Changjian Li",
        "abstract": "CAD programs are a popular way to compactly encode shapes as a sequence of operations that are easy to parametrically modify. However, without sufficient semantic comments and structure, such programs can be challenging to understand, let alone modify. We introduce the problem of semantic commenting CAD programs, wherein the goal is to segment the input program into code blocks corresponding to semantically meaningful shape parts and assign a semantic label to each block. We solve the problem by combining program parsing with visual-semantic analysis afforded by recent advances in foundational language and vision models. Specifically, by executing the input programs, we create shapes, which we use to generate conditional photorealistic images to make use of semantic annotators for such images. We then distill the information across the images and link back to the original programs to semantically comment on them. Additionally, we collected and annotated a benchmark dataset, CADTalk, consisting of 5,288 machine-made programs and 45 human-made programs with ground truth semantic comments. We extensively evaluated our approach, compared it to a GPT-based baseline, and an open-set shape segmentation baseline, and reported an 83.24% accuracy on the new CADTalk dataset. Code and data: https://enigma-li.github.io/CADTalk/.",
        "page": "http://arxiv.org/abs/2311.16703",
        "pdf": "http://arxiv.org/pdf/2311.16703.pdf"
    },
    {
        "title": "Enhancing Visual Continual Learning with Language-Guided Supervision",
        "author": "Bolin Ni, Hongbo Zhao, Chenghao Zhang, Ke Hu, Gaofeng Meng, Zhaoxiang Zhang, Shiming Xiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction",
        "author": "Yi ZHOU, Hui Zhang, Jiaqian Yu, yifan yang, Sangil Jung, Seung-In Park, ByungIn Yoo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DMR: Decomposed Multi-Modality Representations for Frames and Events Fusion in Visual Reinforcement Learning",
        "author": "Haoran Xu, Peixi Peng, Guang Tan, Yuan Li, Xinhai Xu, Yonghong Tian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation",
        "author": "Qinghe Ma, Jian Zhang, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao",
        "abstract": "Both limited annotation and domain shift are prevalent challenges in medical image segmentation. Traditional semi-supervised segmentation and unsupervised domain adaptation methods address one of these issues separately. However, the coexistence of limited annotation and domain shift is quite common, which motivates us to introduce a novel and challenging scenario: Mixed Domain Semi-supervised medical image Segmentation (MiDSS). In this scenario, we handle data from multiple medical centers, with limited annotations available for a single domain and a large amount of unlabeled data from multiple domains. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data. To tackle this issue, we employ Unified Copy-Paste (UCP) between images to construct intermediate domains, facilitating the knowledge transfer from the domain of labeled data to the domains of unlabeled data. To fully utilize the information within the intermediate domain, we propose a symmetric Guidance training strategy (SymGD), which additionally offers direct guidance to unlabeled data by merging pseudo labels from intermediate samples. Subsequently, we introduce a Training Process aware Random Amplitude MixUp (TP-RAM) to progressively incorporate style-transition components into intermediate samples. Compared with existing state-of-the-art approaches, our method achieves a notable 13.57% improvement in Dice score on Prostate dataset, as demonstrated on three public datasets. Our code is available at https://github.com/MQinghe/MiDSS .",
        "page": "http://arxiv.org/abs/2404.08951",
        "pdf": "http://arxiv.org/pdf/2404.08951.pdf"
    },
    {
        "title": "NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors",
        "author": "Yannan He, Garvita Tiwari, Tolga Birdal, Jan Lenssen, Gerard Pons-Moll",
        "abstract": "Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge. To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space. To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm. We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times. NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model. We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance. Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation.",
        "page": "http://arxiv.org/abs/2403.03122",
        "pdf": "http://arxiv.org/pdf/2403.03122.pdf"
    },
    {
        "title": "LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry",
        "author": "Weirong Chen, Le Chen, Rui Wang, Marc Pollefeys",
        "abstract": "Visual odometry estimates the motion of a moving camera based on visual input. Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability. These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas. To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation. Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty. Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes. Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end. Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks.",
        "page": "http://arxiv.org/abs/2401.01887",
        "pdf": "http://arxiv.org/pdf/2401.01887.pdf"
    },
    {
        "title": "CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise Sketch Instance Guided Attention",
        "author": "Mohammad Sadil Khan, Elona Dupont, Sk Aziz Ali, Kseniya Cherenkova, Anis Kacem, Djamila Aouada",
        "abstract": "Reverse engineering in the realm of Computer-Aided Design (CAD) has been a longstanding aspiration, though not yet entirely realized. Its primary aim is to uncover the CAD process behind a physical object given its 3D scan. We propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to recover the design history of a CAD model represented as a sequence of sketch-and-extrusion from an input point cloud. Our model learns visual-language representations by layer-wise cross-attention between point cloud and CAD language embedding. In particular, a new Sketch instance Guided Attention (SGA) module is proposed in order to reconstruct the fine-grained details of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not only reconstructs a unique full design history of the corresponding CAD model given an input point cloud but also provides multiple plausible design choices. This allows for an interactive reverse engineering scenario by providing designers with multiple next-step choices along with the design process. Extensive experiments on publicly available CAD datasets showcase the effectiveness of our approach against existing baseline models in two settings, namely, full design history recovery and conditional auto-completion from point clouds.",
        "page": "http://arxiv.org/abs/2402.17678",
        "pdf": "http://arxiv.org/pdf/2402.17678.pdf"
    },
    {
        "title": "DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing",
        "author": "Kaiwen Zhang, Yifan Zhou, Xudong XU, Bo Dai, Xingang Pan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LAENeRF: Local Appearance Editing for Neural Radiance Fields",
        "author": "Lukas Radl, Michael Steiner, Andreas Kurz, Markus Steinberger",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective",
        "author": "Yu-Bang Zheng, Xile Zhao, Junhua Zeng, Chao Li, Qibin Zhao, Heng-Chao Li, Ting-Zhu Huang",
        "abstract": "Tensor network (TN) representation is a powerful technique for computer vision and machine learning. TN structure search (TN-SS) aims to search for a customized structure to achieve a compact representation, which is a challenging NP-hard problem. Recent \"sampling-evaluation\"-based methods require sampling an extensive collection of structures and evaluating them one by one, resulting in prohibitively high computational costs. To address this issue, we propose a novel TN paradigm, named SVD-inspired TN decomposition (SVDinsTN), which allows us to efficiently solve the TN-SS problem from a regularized modeling perspective, eliminating the repeated structure evaluations. To be specific, by inserting a diagonal factor for each edge of the fully-connected TN, SVDinsTN allows us to calculate TN cores and diagonal factors simultaneously, with the factor sparsity revealing a compact TN structure. In theory, we prove a convergence guarantee for the proposed method. Experimental results demonstrate that the proposed method achieves approximately 100 to 1000 times acceleration compared to the state-of-the-art TN-SS methods while maintaining a comparable level of representation ability.",
        "page": "http://arxiv.org/abs/2305.14912",
        "pdf": "http://arxiv.org/pdf/2305.14912.pdf"
    },
    {
        "title": "DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction",
        "author": "Weiyi Lv, Yuhang Huang, NING Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AnyDoor: Zero-shot Object-level Image Customization",
        "author": "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, Hengshuang Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dispersed Structured Light for Hyperspectral 3D Imaging",
        "author": "Suhyun Shin, Seokjun Choi, Felix Heide, Seung-Hwan Baek",
        "abstract": "Hyperspectral 3D imaging aims to acquire both depth and spectral information of a scene. However, existing methods are either prohibitively expensive and bulky or compromise on spectral and depth accuracy. In this work, we present Dispersed Structured Light (DSL), a cost-effective and compact method for accurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera system by placing a sub-millimeter thick diffraction grating film front of the projector. The grating disperses structured light based on light wavelength. To utilize the dispersed structured light, we devise a model for dispersive projection image formation and a per-pixel hyperspectral 3D reconstruction method. We validate DSL by instantiating a compact experimental prototype. DSL achieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth error of 1mm. We demonstrate that DSL outperforms prior work on practical hyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D imaging for diverse application domains, including computer vision and graphics, cultural heritage, geology, and biology.",
        "page": "http://arxiv.org/abs/2311.18287",
        "pdf": "http://arxiv.org/pdf/2311.18287.pdf"
    },
    {
        "title": "Active Prompt Learning in Vision Language Models",
        "author": "Jihwan Bang, Sumyeong Ahn, Jae-Gil Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "4D-DRESS: A 4D Dataset of Real-World Human Clothing With Semantic Annotations",
        "author": "Wenbo Wang, Hsuan-I Ho, Chen Guo, Boxiang Rong, Artur Grigorev, Jie Song, Juan Jose Zarate, Otmar Hilliges",
        "abstract": "The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.",
        "page": "http://arxiv.org/abs/2404.18630",
        "pdf": "http://arxiv.org/pdf/2404.18630.pdf"
    },
    {
        "title": "CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective",
        "author": "Shunsuke Yasuki, Masato Taki",
        "abstract": "Recently, convolutional neural networks (CNNs) with large size kernels have attracted much attention in the computer vision field, following the success of the Vision Transformers. Large kernel CNNs have been reported to perform well in downstream vision tasks as well as in classification performance. The reason for the high-performance of large kernel CNNs in downstream tasks has been attributed to the large effective receptive field (ERF) produced by large size kernels, but this view has not been fully tested. We therefore revisit the performance of large kernel CNNs in downstream task, focusing on the weakly supervised object localization (WSOL) task. WSOL, a difficult downstream task that is not fully supervised, provides a new angle to explore the capabilities of the large kernel CNNs. Our study compares the modern large kernel CNNs ConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that ERF size is important for improving downstream task performance. Our analysis of the factors contributing to high performance provides a different perspective, in which the main factor is feature map improvement. Furthermore, we find that modern CNNs are robust to the CAM problems of local regions of objects being activated, which has long been discussed in WSOL. CAM is the most classic WSOL method, but because of the above-mentioned problems, it is often used as a baseline method for comparison. However, experiments on the CUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and simple data augmentation methods can achieve performance (90.99% MaxBoxAcc) comparable to the latest WSOL method, which is CNN-based and requires special training or complex post-processing. The code is available at https://github.com/snskysk/CAM-Back-Again.",
        "page": "http://arxiv.org/abs/2403.06676",
        "pdf": "http://arxiv.org/pdf/2403.06676.pdf"
    },
    {
        "title": "UniVS: Unified and Universal Video Segmentation with Prompts as Queries",
        "author": "Minghan LI, Shuai Li, Xindong Zhang, Lei Zhang",
        "abstract": "Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS tasks into prompt-guided target segmentation, eliminating the heuristic inter-frame matching process. Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios. UniVS shows a commendable balance between performance and universality on 10 challenging VS benchmarks, covering video instance, semantic, panoptic, object, and referring segmentation tasks. Code can be found at \\url{https://github.com/MinghanLi/UniVS}.",
        "page": "http://arxiv.org/abs/2402.18115",
        "pdf": "http://arxiv.org/pdf/2402.18115.pdf"
    },
    {
        "title": "FSRT: Facial Scene Representation Transformer for Face Reenactment from Factorized Appearance, Head-pose, and Facial Expression Features",
        "author": "Andre Rochow, Max Schwarz, Sven Behnke",
        "abstract": "The task of face reenactment is to transfer the head motion and facial expressions from a driving video to the appearance of a source image, which may be of a different person (cross-reenactment). Most existing methods are CNN-based and estimate optical flow from the source image to the current driving frame, which is then inpainted and refined to produce the output animation. We propose a transformer-based encoder for computing a set-latent representation of the source image(s). We then predict the output color of a query pixel using a transformer-based decoder, which is conditioned with keypoints and a facial expression vector extracted from the driving frame. Latent representations of the source person are learned in a self-supervised manner that factorize their appearance, head pose, and facial expressions. Thus, they are perfectly suited for cross-reenactment. In contrast to most related work, our method naturally extends to multiple source images and can thus adapt to person-specific facial dynamics. We also propose data augmentation and regularization schemes that are necessary to prevent overfitting and support generalizability of the learned representations. We evaluated our approach in a randomized user study. The results indicate superior performance compared to the state-of-the-art in terms of motion transfer quality and temporal consistency.",
        "page": "http://arxiv.org/abs/2404.09736",
        "pdf": "http://arxiv.org/pdf/2404.09736.pdf"
    },
    {
        "title": "SURE: SUrvey REcipes for building reliable and robust deep networks",
        "author": "Yuting Li, Yingyi Chen, Xuanlong Yu, Dexiong Chen, Xi Shen",
        "abstract": "In this paper, we revisit techniques for uncertainty estimation within deep neural networks and consolidate a suite of techniques to enhance their reliability. Our investigation reveals that an integrated application of diverse techniques--spanning model regularization, classifier and optimization--substantially improves the accuracy of uncertainty predictions in image classification tasks. The synergistic effect of these techniques culminates in our novel SURE approach. We rigorously evaluate SURE against the benchmark of failure prediction, a critical testbed for uncertainty estimation efficacy. Our results showcase a consistently better performance than models that individually deploy each technique, across various datasets and model architectures. When applied to real-world challenges, such as data corruption, label noise, and long-tailed class distribution, SURE exhibits remarkable robustness, delivering results that are superior or on par with current state-of-the-art specialized methods. Particularly on Animal-10N and Food-101N for learning with noisy labels, SURE achieves state-of-the-art performance without any task-specific adjustments. This work not only sets a new benchmark for robust uncertainty estimation but also paves the way for its application in diverse, real-world scenarios where reliability is paramount. Our code is available at \\url{https://yutingli0606.github.io/SURE/}.",
        "page": "http://arxiv.org/abs/2403.00543",
        "pdf": "http://arxiv.org/pdf/2403.00543.pdf"
    },
    {
        "title": "Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition",
        "author": "Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng",
        "abstract": "Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an \"adapt then align\" paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel \"Align before Adapt\" (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This paradigm reuses the visual-language alignment of VLP during adaptation and tries to explain an action by the underlying entities. This helps understand actions by bridging the gap with complex activity semantics, particularly when facing unfamiliar or unseen categories. ALT demonstrates competitive performance while maintaining remarkably low computational costs. In fully supervised experiments, it achieves 88.1% top-1 accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the previous state-of-the-art methods in both zero-shot and few-shot experiments, emphasizing its superior generalizability across various learning scenarios.",
        "page": "http://arxiv.org/abs/2311.15619",
        "pdf": "http://arxiv.org/pdf/2311.15619.pdf"
    },
    {
        "title": "FLHetBench: Benchmarking Device and State Heterogeneity in Federated Learning",
        "author": "Junyuan Zhang, Shuang Zeng, Miao Zhang, Runxi Wang, Feifei Wang, Yuyin Zhou, Paul Pu Liang, Liangqiong Qu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Byzantine-robust Decentralized Federated Learning via Dual-domain Clustering and Trust Bootstrapping",
        "author": "Peng Sun, Xinyang Liu, Zhibo Wang, Bo Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding",
        "author": "Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang",
        "abstract": "Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.",
        "page": "http://arxiv.org/abs/2403.03608",
        "pdf": "http://arxiv.org/pdf/2403.03608.pdf"
    },
    {
        "title": "HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting",
        "author": "Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu",
        "abstract": "We have recently seen tremendous progress in photo-real human modeling and rendering. Yet, efficiently rendering realistic human performance and integrating it into the rasterization pipeline remains challenging. In this paper, we present HiFi4G, an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage. Our core intuition is to marry the 3D Gaussian representation with non-rigid tracking, achieving a compact and compression-friendly representation. We first propose a dual-graph mechanism to obtain motion priors, with a coarse deformation graph for effective initialization and a fine-grained Gaussian graph to enforce subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with adaptive spatial-temporal regularizers to effectively balance the non-rigid prior and Gaussian updating. We also present a companion compression scheme with residual compensation for immersive experiences on various platforms. It achieves a substantial compression rate of approximately 25 times, with less than 2MB of storage per frame. Extensive experiments demonstrate the effectiveness of our approach, which significantly outperforms existing approaches in terms of optimization speed, rendering quality, and storage overhead.",
        "page": "http://arxiv.org/abs/2312.03461",
        "pdf": "http://arxiv.org/pdf/2312.03461.pdf"
    },
    {
        "title": "Towards Generalizable Multi-Object Tracking",
        "author": "Zheng Qin, Le Wang, Sanping Zhou, Panpan Fu, Gang Hua, Wei Tang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion",
        "author": "Hsuan-I Ho, Jie Song, Otmar Hilliges",
        "abstract": "A long-standing goal of 3D human reconstruction is to create lifelike and fully detailed 3D humans from single-view images. The main challenge lies in inferring unknown body shapes, appearances, and clothing details in areas not visible in the images. To address this, we propose SiTH, a novel pipeline that uniquely integrates an image-conditioned diffusion model into a 3D mesh reconstruction workflow. At the core of our method lies the decomposition of the challenging single-view reconstruction problem into generative hallucination and reconstruction subproblems. For the former, we employ a powerful generative diffusion model to hallucinate unseen back-view appearance based on the input images. For the latter, we leverage skinned body meshes as guidance to recover full-body texture meshes from the input and back-view images. SiTH requires as few as 500 3D human scans for training while maintaining its generality and robustness to diverse images. Extensive evaluations on two 3D human benchmarks, including our newly created one, highlighted our method's superior accuracy and perceptual quality in 3D textured human reconstruction. Our code and evaluation benchmark are available at https://ait.ethz.ch/sith",
        "page": "http://arxiv.org/abs/2311.15855",
        "pdf": "http://arxiv.org/pdf/2311.15855.pdf"
    },
    {
        "title": "It's All About Your Sketch: Democratising Sketch Control in Diffusion Models",
        "author": "Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
        "abstract": "This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI. We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of \"what you sketch is what you get\". A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning. To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association. Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices! We welcome everyone to examine results presented in the paper and its supplementary. Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments.",
        "page": "http://arxiv.org/abs/2403.07234",
        "pdf": "http://arxiv.org/pdf/2403.07234.pdf"
    },
    {
        "title": "How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?",
        "author": "Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval",
        "author": "Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
        "abstract": "Two primary input modalities prevail in image retrieval: sketch and text. While text is widely used for inter-category retrieval tasks, sketches have been established as the sole preferred modality for fine-grained image retrieval due to their ability to capture intricate visual details. In this paper, we question the reliance on sketches alone for fine-grained image retrieval by simultaneously exploring the fine-grained representation capabilities of both sketch and text, orchestrating a duet between the two. The end result enables precise retrievals previously unattainable, allowing users to pose ever-finer queries and incorporate attributes like colour and contextual cues from text. For this purpose, we introduce a novel compositionality framework, effectively combining sketches and text using pre-trained CLIP models, while eliminating the need for extensive fine-grained textual descriptions. Last but not least, our system extends to novel applications in composed image retrieval, domain attribute transfer, and fine-grained generation, providing solutions for various real-world scenarios.",
        "page": "http://arxiv.org/abs/2403.07222",
        "pdf": "http://arxiv.org/pdf/2403.07222.pdf"
    },
    {
        "title": "Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers",
        "author": "Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
        "abstract": "This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the model's feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements.",
        "page": "http://arxiv.org/abs/2403.07214",
        "pdf": "http://arxiv.org/pdf/2403.07214.pdf"
    },
    {
        "title": "Source-Free Domain Adaptation with Frozen Multimodal Foundation Model",
        "author": "Song Tang, Wenxin Su, Mao Ye, Xiatian Zhu",
        "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a target domain, with only access to unlabeled target training data and the source model pre-trained on a supervised source domain. Relying on pseudo labeling and/or auxiliary supervision, conventional methods are inevitably error-prone. To mitigate this limitation, in this work we for the first time explore the potentials of off-the-shelf vision-language (ViL) multimodal models (e.g.,CLIP) with rich whilst heterogeneous knowledge. We find that directly applying the ViL model to the target domain in a zero-shot fashion is unsatisfactory, as it is not specialized for this particular task but largely generic. To make it task specific, we propose a novel Distilling multimodal Foundation model(DIFO)approach. Specifically, DIFO alternates between two steps during adaptation: (i) Customizing the ViL model by maximizing the mutual information with the target model in a prompt learning manner, (ii) Distilling the knowledge of this customized ViL model to the target model. For more fine-grained and reliable distillation, we further introduce two effective regularization terms, namely most-likely category encouragement and predictive consistency. Extensive experiments show that DIFO significantly outperforms the state-of-the-art alternatives. Code is here",
        "page": "http://arxiv.org/abs/2311.16510",
        "pdf": "http://arxiv.org/pdf/2311.16510.pdf"
    },
    {
        "title": "Learning Degradation-unaware Representation with Prior-based Latent Transformations for Blind Face Restoration",
        "author": "Lianxin Xie, csbingbing zheng, Wen Xue, Le Jiang, Cheng Liu, Si Wu, Hau San Wong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Task-conditioned adaptation of visual features in multi-task policy learning",
        "author": "Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers",
        "author": "Shahaf Arica, Or Rubin, Sapir Gershov, Shlomi Laufer",
        "abstract": "In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models. VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach. Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy. Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models. Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation.",
        "page": "http://arxiv.org/abs/2403.07700",
        "pdf": "http://arxiv.org/pdf/2403.07700.pdf"
    },
    {
        "title": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models",
        "author": "Moreno D&#x27;Inc\u00e0, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, Nicu Sebe",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit Representation for Diverse 3D Shapes",
        "author": "YuJie Lu, Long Wan, Nayu Ding, Yulong Wang, Shuhan Shen, Shen Cai, Lin Gao",
        "abstract": "Neural implicit representation of geometric shapes has witnessed considerable advancements in recent years. However, common distance field based implicit representations, specifically signed distance field (SDF) for watertight shapes or unsigned distance field (UDF) for arbitrary shapes, routinely suffer from degradation of reconstruction accuracy when converting to explicit surface points and meshes. In this paper, we introduce a novel neural implicit representation based on unsigned orthogonal distance fields (UODFs). In UODFs, the minimal unsigned distance from any spatial point to the shape surface is defined solely in one orthogonal direction, contrasting with the multi-directional determination made by SDF and UDF. Consequently, every point in the 3D UODFs can directly access its closest surface points along three orthogonal directions. This distinctive feature leverages the accurate reconstruction of surface points without interpolation errors. We verify the effectiveness of UODFs through a range of reconstruction examples, extending from simple watertight or non-watertight shapes to complex shapes that include hollows, internal or assembling structures.",
        "page": "http://arxiv.org/abs/2403.01414",
        "pdf": "http://arxiv.org/pdf/2403.01414.pdf"
    },
    {
        "title": "SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation",
        "author": "Jiaben Chen, Huaizu Jiang",
        "abstract": "Human-centric video frame interpolation has great potential for improving people's entertainment experiences and finding commercial applications in the sports analysis industry, e.g., synthesizing slow-motion videos. Although there are multiple benchmark datasets available in the community, none of them is dedicated for human-centric scenarios. To bridge this gap, we introduce SportsSloMo, a benchmark consisting of more than 130K video clips and 1M video frames of high-resolution ($\\geq$720p) slow-motion sports videos crawled from YouTube. We re-train several state-of-the-art methods on our benchmark, and the results show a decrease in their accuracy compared to other datasets. It highlights the difficulty of our benchmark and suggests that it poses significant challenges even for the best-performing methods, as human bodies are highly deformable and occlusions are frequent in sports videos. To improve the accuracy, we introduce two loss terms considering the human-aware priors, where we add auxiliary supervision to panoptic segmentation and human keypoints detection, respectively. The loss terms are model agnostic and can be easily plugged into any video frame interpolation approaches. Experimental results validate the effectiveness of our proposed loss terms, leading to consistent performance improvement over 5 existing models, which establish strong baseline models on our benchmark. The dataset and code can be found at: https://neu-vi.github.io/SportsSlomo/.",
        "page": "http://arxiv.org/abs/2308.16876",
        "pdf": "http://arxiv.org/pdf/2308.16876.pdf"
    },
    {
        "title": "Mask4Align: Aligned Entity Prompting with Color Masks for Multi-Entity Localization Problem",
        "author": "Haoquan Zhang, Ronggang Huang, Yi Xie, Huaidong Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting",
        "author": "Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang",
        "abstract": "We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.",
        "page": "http://arxiv.org/abs/2312.09228",
        "pdf": "http://arxiv.org/pdf/2312.09228.pdf"
    },
    {
        "title": "Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?",
        "author": "Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, Jose M. Alvarez",
        "abstract": "End-to-end autonomous driving recently emerged as a promising research direction to target autonomy from a full-stack perspective. Along this line, many of the latest works follow an open-loop evaluation setting on nuScenes to study the planning behavior. In this paper, we delve deeper into the problem by conducting thorough analyses and demystifying more devils in the details. We initially observed that the nuScenes dataset, characterized by relatively simple driving scenarios, leads to an under-utilization of perception information in end-to-end models incorporating ego status, such as the ego vehicle's velocity. These models tend to rely predominantly on the ego vehicle's status for future path planning. Beyond the limitations of the dataset, we also note that current metrics do not comprehensively assess the planning quality, leading to potentially biased conclusions drawn from existing benchmarks. To address this issue, we introduce a new metric to evaluate whether the predicted trajectories adhere to the road. We further propose a simple baseline able to achieve competitive results without relying on perception annotations. Given the current limitations on the benchmark and metrics, we suggest the community reassess relevant prevailing research and be cautious whether the continued pursuit of state-of-the-art would yield convincing and universal conclusions. Code and models are available at \\url{https://github.com/NVlabs/BEV-Planner}",
        "page": "http://arxiv.org/abs/2312.03031",
        "pdf": "http://arxiv.org/pdf/2312.03031.pdf"
    },
    {
        "title": "ADA-Track: End-to-End Multi-Camera 3D Multi-Object Tracking with Alternating Detection and Association",
        "author": "Shuxiao Ding, Lukas Schneider, Marius Cordts, J\u00fcrgen Gall",
        "abstract": "Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the tracking-by-attention paradigm, utilizing track queries for identity-consistent detection and object queries for identity-agnostic track spawning. Tracking-by-attention, however, entangles detection and tracking queries in one embedding for both the detection and tracking task, which is sub-optimal. Other approaches resemble the tracking-by-detection paradigm, detecting objects using decoupled track and detection queries followed by a subsequent association. These methods, however, do not leverage synergies between the detection and association task. Combining the strengths of both paradigms, we introduce ADA-Track, a novel end-to-end framework for 3D MOT from multi-view cameras. We introduce a learnable data association module based on edge-augmented cross-attention, leveraging appearance and geometric features. Furthermore, we integrate this association module into the decoder layer of a DETR-based 3D detector, enabling simultaneous DETR-like query-to-image cross-attention for detection and query-to-query cross-attention for data association. By stacking these decoder layers, queries are refined for the detection and association task alternately, effectively harnessing the task dependencies. We evaluate our method on the nuScenes dataset and demonstrate the advantage of our approach compared to the two previous paradigms. Code is available at https://github.com/dsx0511/ADA-Track.",
        "page": "http://arxiv.org/abs/2405.08909",
        "pdf": "http://arxiv.org/pdf/2405.08909.pdf"
    },
    {
        "title": "Global and Hierarchical Geometry Consistency Priors for Few-shot NeRFs in Indoor Scenes",
        "author": "Xiaotian Sun, Qingshan Xu, Xinjie Yang, Yu Zang, Cheng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FreeU: Free Lunch in Diffusion U-Net",
        "author": "Chenyang Si, Ziqi Huang, Yuming Jiang, Ziwei Liu",
        "abstract": "In this paper, we uncover the untapped potential of diffusion U-Net, which serves as a \"free lunch\" that substantially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to denoising, whereas its skip connections mainly introduce high-frequency features into the decoder module, causing the network to overlook the backbone semantics. Capitalizing on this discovery, we propose a simple yet effective method-termed \"FreeU\" - that enhances generation quality without additional training or finetuning. Our key insight is to strategically re-weight the contributions sourced from the U-Net's skip connections and backbone feature maps, to leverage the strengths of both components of the U-Net architecture. Promising results on image and video generation tasks demonstrate that our FreeU can be readily integrated to existing diffusion models, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference. Project page: https://chenyangsi.top/FreeU/.",
        "page": "http://arxiv.org/abs/2309.11497",
        "pdf": "http://arxiv.org/pdf/2309.11497.pdf"
    },
    {
        "title": "Real-time 3D-aware Portrait Video Relighting",
        "author": "Ziqi Cai, Kaiwen Jiang, Shu-Yu Chen, Yu-Kun Lai, Hongbo Fu, Boxin Shi, Lin Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation",
        "author": "Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, Shengfeng He",
        "abstract": "Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at https://github.com/haofengl/DragNoise.",
        "page": "http://arxiv.org/abs/2404.01050",
        "pdf": "http://arxiv.org/pdf/2404.01050.pdf"
    },
    {
        "title": "Text-to-3D using Gaussian Splatting",
        "author": "Zilong Chen, Feng Wang, Yikai Wang, Huaping Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models",
        "author": "Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, Xue Geng, Wenxiu Sun, Qiong Yan, Weisi Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DSGG: Dense Relation Transformer for an End-to-end Scene Graph Generation",
        "author": "Zeeshan Hayder, Xuming He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data",
        "author": "Yifang Men, Biwen Lei, Yuan Yao, Miaomiao Cui, Zhouhui Lian, Xuansong Xie",
        "abstract": "We present En3D, an enhanced generative scheme for sculpting high-quality 3D human avatars. Unlike previous works that rely on scarce 3D datasets or limited 2D collections with imbalanced viewing angles and imprecise pose priors, our approach aims to develop a zero-shot 3D generative scheme capable of producing visually realistic, geometrically accurate and content-wise diverse 3D humans without relying on pre-existing 3D or 2D assets. To address this challenge, we introduce a meticulously crafted workflow that implements accurate physical modeling to learn the enhanced 3D generative model from synthetic 2D data. During inference, we integrate optimization modules to bridge the gap between realistic appearances and coarse 3D shapes. Specifically, En3D comprises three modules: a 3D generator that accurately models generalizable 3D humans with realistic appearance from synthesized balanced, diverse, and structured human images; a geometry sculptor that enhances shape quality using multi-view normal constraints for intricate human anatomy; and a texturing module that disentangles explicit texture maps with fidelity and editability, leveraging semantical UV partitioning and a differentiable rasterizer. Experimental results show that our approach significantly outperforms prior works in terms of image quality, geometry accuracy and content diversity. We also showcase the applicability of our generated avatars for animation and editing, as well as the scalability of our approach for content-style free adaptation.",
        "page": "http://arxiv.org/abs/2401.01173",
        "pdf": "http://arxiv.org/pdf/2401.01173.pdf"
    },
    {
        "title": "GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians",
        "author": "Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nie\u00dfner",
        "abstract": "We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.",
        "page": "http://arxiv.org/abs/2312.02069",
        "pdf": "http://arxiv.org/pdf/2312.02069.pdf"
    },
    {
        "title": "3DToonify: Creating Your High-Fidelity 3D Stylized Avatar Easily from 2D Portrait Images",
        "author": "Yifang Men, Hanxi Liu, Yuan Yao, Miaomiao Cui, Xuansong Xie, Zhouhui Lian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Molecular Data Programming: Towards Molecule Pseudo-labeling with Systematic Weak Supervision",
        "author": "Xin Juan, Kaixiong Zhou, Ninghao Liu, Tianlong Chen, Xin Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation",
        "author": "Thuan Nguyen, Anh Tran",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DreamVideo: Composing Your Dream Videos with Customized Subject and Motion",
        "author": "Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, Hongming Shan",
        "abstract": "Customized generation using diffusion models has made impressive progress in image generation, but remains unsatisfactory in the challenging video generation task, as it requires the controllability of both subjects and motions. To that end, we present DreamVideo, a novel approach to generating personalized videos from a few static images of the desired subject and a few videos of target motion. DreamVideo decouples this task into two stages, subject learning and motion learning, by leveraging a pre-trained video diffusion model. The subject learning aims to accurately capture the fine appearance of the subject from provided images, which is achieved by combining textual inversion and fine-tuning of our carefully designed identity adapter. In motion learning, we architect a motion adapter and fine-tune it on the given videos to effectively model the target motion pattern. Combining these two lightweight and efficient adapters allows for flexible customization of any subject with any motion. Extensive experimental results demonstrate the superior performance of our DreamVideo over the state-of-the-art methods for customized video generation. Our project page is at https://dreamvideo-t2v.github.io.",
        "page": "http://arxiv.org/abs/2312.04433",
        "pdf": "http://arxiv.org/pdf/2312.04433.pdf"
    },
    {
        "title": "AirPlanes: Accurate Plane Estimation via 3D-Consistent Embeddings",
        "author": "Jamie Watson, Filippo Aleotti, Mohamed Sayed, Zawar Qureshi, Oisin Mac Aodha, Gabriel J. Brostow, Michael Firman, Sara Vicente",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation",
        "author": "Ziyang Chen, Yongsheng Pan, Yiwen Ye, Mengkang Lu, Yong Xia",
        "abstract": "Distribution shift widely exists in medical images acquired from different medical centres and poses a significant obstacle to deploying the pre-trained semantic segmentation model in real-world applications. Test-time adaptation has proven its effectiveness in tackling the cross-domain distribution shift during inference. However, most existing methods achieve adaptation by updating the pre-trained models, rendering them susceptible to error accumulation and catastrophic forgetting when encountering a series of distribution shifts (i.e., under the continual test-time adaptation setup). To overcome these challenges caused by updating the models, in this paper, we freeze the pre-trained model and propose the Visual Prompt-based Test-Time Adaptation (VPTTA) method to train a specific prompt for each test image to align the statistics in the batch normalization layers. Specifically, we present the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration. To enhance prompt initialization, we equip VPTTA with a memory bank to benefit the current prompt from previous ones. Additionally, we design a warm-up mechanism, which mixes source and target statistics to construct warm-up statistics, thereby facilitating the training process. Extensive experiments demonstrate the superiority of our VPTTA over other state-of-the-art methods on two medical image segmentation benchmark tasks. The code and weights of pre-trained source models are available at https://github.com/Chen-Ziyang/VPTTA.",
        "page": "http://arxiv.org/abs/2311.18363",
        "pdf": "http://arxiv.org/pdf/2311.18363.pdf"
    },
    {
        "title": "Prompt-Driven Referring Image Segmentation with Instance Contrasting",
        "author": "Chao Shang, Zichen Song, Heqian Qiu, Lanxiao Wang, Fanman Meng, Hongliang Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Robust Image Denoising through Adversarial Frequency Mixup",
        "author": "Donghun Ryou, Inju Ha, Hyewon Yoo, Dongwan Kim, Bohyung Han",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "vid-TLDR: Training Free Token merging for Light-weight Video Transformer",
        "author": "Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, Hyunwoo J. Kim",
        "abstract": "Video Transformers have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video transformers suffer from heavy computational costs induced by the massive number of tokens across the entire video frames, which has been the major barrier to training the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training free token merging for lightweight video Transformer (vid-TLDR) that aims to enhance the efficiency of video Transformers by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Further, we introduce the saliency-aware token merging strategy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR significantly mitigates the computational complexity of video Transformers while achieving competitive performance compared to the base model without vid-TLDR. Code is available at https://github.com/mlvlab/vid-TLDR.",
        "page": "http://arxiv.org/abs/2403.13347",
        "pdf": "http://arxiv.org/pdf/2403.13347.pdf"
    },
    {
        "title": "Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers",
        "author": "Sanghyeok Lee, Joonmyung Choi, Hyunwoo J. Kim",
        "abstract": "Vision Transformer (ViT) has emerged as a prominent backbone for computer vision. For more efficient ViTs, recent works lessen the quadratic cost of the self-attention layer by pruning or fusing the redundant tokens. However, these works faced the speed-accuracy trade-off caused by the loss of information. Here, we argue that token fusion needs to consider diverse relations between tokens to minimize information loss. In this paper, we propose a Multi-criteria Token Fusion (MCTF), that gradually fuses the tokens based on multi-criteria (e.g., similarity, informativeness, and size of fused tokens). Further, we utilize the one-step-ahead attention, which is the improved approach to capture the informativeness of the tokens. By training the model equipped with MCTF using a token reduction consistency, we achieve the best speed-accuracy trade-off in the image classification (ImageNet1K). Experimental results prove that MCTF consistently surpasses the previous reduction methods with and without training. Specifically, DeiT-T and DeiT-S with MCTF reduce FLOPs by about 44% while improving the performance (+0.5%, and +0.3%) over the base model, respectively. We also demonstrate the applicability of MCTF in various Vision Transformers (e.g., T2T-ViT, LV-ViT), achieving at least 31% speedup without performance degradation. Code is available at https://github.com/mlvlab/MCTF.",
        "page": "http://arxiv.org/abs/2403.10030",
        "pdf": "http://arxiv.org/pdf/2403.10030.pdf"
    },
    {
        "title": "Scalable 3D Registration via Truncated Entry-wise Absolute Residuals",
        "author": "Tianyu Huang, Liangzu Peng, Rene Vidal, Yun-Hui Liu",
        "abstract": "Given an input set of $3$D point pairs, the goal of outlier-robust $3$D registration is to compute some rotation and translation that align as many point pairs as possible. This is an important problem in computer vision, for which many highly accurate approaches have been recently proposed. Despite their impressive performance, these approaches lack scalability, often overflowing the $16$GB of memory of a standard laptop to handle roughly $30,000$ point pairs. In this paper, we propose a $3$D registration approach that can process more than ten million ($10^7$) point pairs with over $99\\%$ random outliers. Moreover, our method is efficient, entails low memory costs, and maintains high accuracy at the same time. We call our method TEAR, as it involves minimizing an outlier-robust loss that computes Truncated Entry-wise Absolute Residuals. To minimize this loss, we decompose the original $6$-dimensional problem into two subproblems of dimensions $3$ and $2$, respectively, solved in succession to global optimality via a customized branch-and-bound method. While branch-and-bound is often slow and unscalable, this does not apply to TEAR as we propose novel bounding functions that are tight and computationally efficient. Experiments on various datasets are conducted to validate the scalability and efficiency of our method.",
        "page": "http://arxiv.org/abs/2404.00915",
        "pdf": "http://arxiv.org/pdf/2404.00915.pdf"
    },
    {
        "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models",
        "author": "Lin Li, Haoyan Guan, Jianing Qiu, Michael Spratling",
        "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
        "page": "http://arxiv.org/abs/2403.01849",
        "pdf": "http://arxiv.org/pdf/2403.01849.pdf"
    },
    {
        "title": "A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition",
        "author": "Yusheng Dai, HangChen, Jun Du, Ruoyu Wang, shihao chen, Haotian Wang, Chin-Hui Lee",
        "abstract": "Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effectiveness of our proposed approach is evaluated and validated through a series of comprehensive experiments using the MISP2021 and MISP2022 datasets. Our code is available at https://github.com/dalision/ModalBiasAVSR",
        "page": "http://arxiv.org/abs/2403.04245",
        "pdf": "http://arxiv.org/pdf/2403.04245.pdf"
    },
    {
        "title": "HIT: Estimating Internal Human Implicit Tissues from the Body Surface",
        "author": "Marilyn Keller, Vaibhav ARORA, Abdelmouttaleb Dakri, Shivam Chandhok, J\u00fcrgen Machann, Andreas Fritsche, Michael J. Black, Sergi Pujades",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VAREN: Very Accurate and Realistic Equine Network",
        "author": "Silvia Zuffi, Ylva Mellbin, Ci Li, Markus H\u00f6schle, Hedvig Kjellstr\u00f6m, Senya Polikovsky, Elin Hernlund, Michael J. Black",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary",
        "author": "Leheng Zhang, Yawei Li, Xingyu Zhou, Xiaorui Zhao, Shuhang Gu",
        "abstract": "Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones. Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention. To address these issues, we introduce a group of auxiliary Adaptive Token Dictionary to SR Transformer and establish an ATD-SR method. The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step. The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories. Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features. The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks.",
        "page": "http://arxiv.org/abs/2401.08209",
        "pdf": "http://arxiv.org/pdf/2401.08209.pdf"
    },
    {
        "title": "A Pedestrian is Worth One Prompt: Towards Language Guidance Person Re-Identification",
        "author": "Zexian Yang, Dayan Wu, Chenming Wu, Zheng Lin, JingziGU, Weiping Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VMINer: Versatile Multi-view Inverse Rendering with Near- and Far-field Light Sources",
        "author": "Fan Fei, Jiajun Tang, Ping Tan, Boxin Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Lane2Seq: Towards Unified Lane Detection via Sequence Generation",
        "author": "Kunyang Zhou",
        "abstract": "In this paper, we present a novel sequence generation-based framework for lane detection, called Lane2Seq. It unifies various lane detection formats by casting lane detection as a sequence generation task. This is different from previous lane detection methods, which depend on well-designed task-specific head networks and corresponding loss functions. Lane2Seq only adopts a plain transformer-based encoder-decoder architecture with a simple cross-entropy loss. Additionally, we propose a new multi-format model tuning based on reinforcement learning to incorporate the task-specific knowledge into Lane2Seq. Experimental results demonstrate that such a simple sequence generation paradigm not only unifies lane detection but also achieves competitive performance on benchmarks. For example, Lane2Seq gets 97.95\\% and 97.42\\% F1 score on Tusimple and LLAMAS datasets, establishing a new state-of-the-art result for two benchmarks.",
        "page": "http://arxiv.org/abs/2402.17172",
        "pdf": "http://arxiv.org/pdf/2402.17172.pdf"
    },
    {
        "title": "Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images",
        "author": "Chaoqin Huang, Aofan Jiang, Jinghao Feng, Ya Zhang, Xinchao Wang, Yanfeng Wang",
        "abstract": "Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot anomaly detection within natural image domains. However, the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical anomaly detection. This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection. Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels. This multi-level adaptation is guided by multi-level, pixel-wise visual-language feature alignment loss functions, which recalibrate the model's focus from object semantics in natural imagery to anomaly identification in medical images. The adapted features exhibit improved generalization across various medical data types, even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training. Our experiments on medical anomaly detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art models, with an average AUC improvement of 6.24% and 7.33% for anomaly classification, 2.03% and 2.37% for anomaly segmentation, under the zero-shot and few-shot settings, respectively. Source code is available at: https://github.com/MediaBrain-SJTU/MVFA-AD",
        "page": "http://arxiv.org/abs/2403.12570",
        "pdf": "http://arxiv.org/pdf/2403.12570.pdf"
    },
    {
        "title": "Modality-agnostic Domain Generalizable Medical Image Segmentation by Multi-Frequency in Multi-Scale Attention",
        "author": "Ju-Hyeon Nam, Nur Suriza Syazwany, Su Jung Kim, Sang-Chul Lee",
        "abstract": "Generalizability in deep neural networks plays a pivotal role in medical image segmentation. However, deep learning-based medical image analyses tend to overlook the importance of frequency variance, which is critical element for achieving a model that is both modality-agnostic and domain-generalizable. Additionally, various models fail to account for the potential information loss that can arise from multi-task learning under deep supervision, a factor that can impair the model representation ability. To address these challenges, we propose a Modality-agnostic Domain Generalizable Network (MADGNet) for medical image segmentation, which comprises two key components: a Multi-Frequency in Multi-Scale Attention (MFMSA) block and Ensemble Sub-Decoding Module (E-SDM). The MFMSA block refines the process of spatial feature extraction, particularly in capturing boundary features, by incorporating multi-frequency and multi-scale features, thereby offering informative cues for tissue outline and anatomical structures. Moreover, we propose E-SDM to mitigate information loss in multi-task learning with deep supervision, especially during substantial upsampling from low resolution. We evaluate the segmentation performance of MADGNet across six modalities and fifteen datasets. Through extensive experiments, we demonstrate that MADGNet consistently outperforms state-of-the-art models across various modalities, showcasing superior segmentation performance. This affirms MADGNet as a robust solution for medical image segmentation that excels in diverse imaging scenarios. Our MADGNet code is available in GitHub Link.",
        "page": "http://arxiv.org/abs/2405.06284",
        "pdf": "http://arxiv.org/pdf/2405.06284.pdf"
    },
    {
        "title": "EditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection",
        "author": "Xuanyu Zhang, Runyi Li, Jiwen Yu, Youmin Xu, Weiqi Li, Jian Zhang",
        "abstract": "In the era where AI-generated content (AIGC) models can produce stunning and lifelike images, the lingering shadow of unauthorized reproductions and malicious tampering poses imminent threats to copyright integrity and information security. Current image watermarking methods, while widely accepted for safeguarding visual content, can only protect copyright and ensure traceability. They fall short in localizing increasingly realistic image tampering, potentially leading to trust crises, privacy violations, and legal disputes. To solve this challenge, we propose an innovative proactive forensics framework EditGuard, to unify copyright protection and tamper-agnostic localization, especially for AIGC-based editing methods. It can offer a meticulous embedding of imperceptible watermarks and precise decoding of tampered areas and copyright information. Leveraging our observed fragility and locality of image-into-image steganography, the realization of EditGuard can be converted into a united image-bit steganography issue, thus completely decoupling the training process from the tampering types. Extensive experiments demonstrate that our EditGuard balances the tamper localization accuracy, copyright recovery precision, and generalizability to various AIGC-based tampering methods, especially for image forgery that is difficult for the naked eye to detect. The project page is available at https://xuanyuzhang21.github.io/project/editguard/.",
        "page": "http://arxiv.org/abs/2312.08883",
        "pdf": "http://arxiv.org/pdf/2312.08883.pdf"
    },
    {
        "title": "360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model",
        "author": "Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, Jian Zhang",
        "abstract": "Panorama video recently attracts more interest in both study and application, courtesy of its immersive experience. Due to the expensive cost of capturing 360-degree panoramic videos, generating desirable panorama videos by prompts is urgently required. Lately, the emerging text-to-video (T2V) diffusion methods demonstrate notable effectiveness in standard video generation. However, due to the significant gap in content and motion patterns between panoramic and standard videos, these methods encounter challenges in yielding satisfactory 360-degree panoramic videos. In this paper, we propose a pipeline named 360-Degree Video Diffusion model (360DVD) for generating 360-degree panoramic videos based on the given prompts and motion conditions. Specifically, we introduce a lightweight 360-Adapter accompanied by 360 Enhancement Techniques to transform pre-trained T2V models for panorama video generation. We further propose a new panorama dataset named WEB360 consisting of panoramic video-text pairs for training 360DVD, addressing the absence of captioned panoramic video datasets. Extensive experiments demonstrate the superiority and effectiveness of 360DVD for panorama video generation. Our project page is at https://akaneqwq.github.io/360DVD/.",
        "page": "http://arxiv.org/abs/2401.06578",
        "pdf": "http://arxiv.org/pdf/2401.06578.pdf"
    },
    {
        "title": "Boosting Spike Camera Image Reconstruction from a Perspective of Dealing with Spike Fluctuations",
        "author": "Rui Zhao, Ruiqin Xiong, Jing Zhao, Jian Zhang, Xiaopeng Fan, Zhaofei Yu, Tiejun Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GRAM: Global Reasoning for Multi-Page VQA",
        "author": "Itshak Blau, Sharon Fogel, Roi Ronen, Alona Golts, Shahar Tsiper, Elad Ben Avraham, Aviad Aberdam, Roy Ganz, Ron Litman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Bridging the Gap Between End-to-End and Two-Step Text Spotting",
        "author": "Mingxin Huang, Hongliang Li, Yuliang Liu, Xiang Bai, Lianwen Jin",
        "abstract": "Modularity plays a crucial role in the development and maintenance of complex systems. While end-to-end text spotting efficiently mitigates the issues of error accumulation and sub-optimal performance seen in traditional two-step methodologies, the two-step methods continue to be favored in many competitions and practical settings due to their superior modularity. In this paper, we introduce Bridging Text Spotting, a novel approach that resolves the error accumulation and suboptimal performance issues in two-step methods while retaining modularity. To achieve this, we adopt a well-trained detector and recognizer that are developed and trained independently and then lock their parameters to preserve their already acquired capabilities. Subsequently, we introduce a Bridge that connects the locked detector and recognizer through a zero-initialized neural network. This zero-initialized neural network, initialized with weights set to zeros, ensures seamless integration of the large receptive field features in detection into the locked recognizer. Furthermore, since the fixed detector and recognizer cannot naturally acquire end-to-end optimization features, we adopt the Adapter to facilitate their efficient learning of these features. We demonstrate the effectiveness of the proposed method through extensive experiments: Connecting the latest detector and recognizer through Bridging Text Spotting, we achieved an accuracy of 83.3% on Total-Text, 69.8% on CTW1500, and 89.5% on ICDAR 2015. The code is available at https://github.com/mxin262/Bridging-Text-Spotting.",
        "page": "http://arxiv.org/abs/2404.04624",
        "pdf": "http://arxiv.org/pdf/2404.04624.pdf"
    },
    {
        "title": "Probing Synergistic High-Order Interaction in Infrared and Visible Image Fusion",
        "author": "Naishan Zheng, Man Zhou, Jie Huang, Junming Hou, Haoying Li, Yuan Xu, Feng Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing",
        "author": "Boqiang Zhang, Hongtao Xie, Zuan Gao, Yuxin Wang",
        "abstract": "Scene text images contain not only style information (font, background) but also content information (character, texture). Different scene text tasks need different information, but previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance. We propose a Disentangled Representation Learning framework (DARLING) aimed at disentangling these two types of features for improved adaptability in better addressing various downstream tasks (choose what you really need). Specifically, we synthesize a dataset of image pairs with identical style but different content. Based on the dataset, we decouple the two types of features by the supervision design. Clearly, we directly split the visual representation into style and content features, the content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs. Then, style features are employed in reconstructing the counterpart image via an image decoder with a prompt that indicates the counterpart's content. Such an operation effectively decouples the features based on their distinctive properties. To the best of our knowledge, this is the first time in the field of scene text that disentangles the inherent properties of the text images. Our method achieves state-of-the-art performance in Scene Text Recognition, Removal, and Editing.",
        "page": "http://arxiv.org/abs/2405.04377",
        "pdf": "http://arxiv.org/pdf/2405.04377.pdf"
    },
    {
        "title": "LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model",
        "author": "Chenjie Cao, Yunuo Cai, Qiaole Dong, Yikai Wang, Yanwei Fu",
        "abstract": "This paper introduces LeftRefill, an innovative approach to efficiently harness large Text-to-Image (T2I) diffusion models for reference-guided image synthesis. As the name implies, LeftRefill horizontally stitches reference and target views together as a whole input. The reference image occupies the left side, while the target canvas is positioned on the right. Then, LeftRefill paints the right-side target canvas based on the left-side reference and specific task instructions. Such a task formulation shares some similarities with contextual inpainting, akin to the actions of a human painter. This novel formulation efficiently learns both structural and textured correspondence between reference and target without other image encoders or adapters. We inject task and view information through cross-attention modules in T2I models, and further exhibit multi-view reference ability via the re-arranged self-attention modules. These enable LeftRefill to perform consistent generation as a generalized model without requiring test-time fine-tuning or model modifications. Thus, LeftRefill can be seen as a simple yet unified framework to address reference-guided synthesis. As an exemplar, we leverage LeftRefill to address two different challenges: reference-guided inpainting and novel view synthesis, based on the pre-trained StableDiffusion. Codes and models are released at https://github.com/ewrfcas/LeftRefill.",
        "page": "http://arxiv.org/abs/2305.11577",
        "pdf": "http://arxiv.org/pdf/2305.11577.pdf"
    },
    {
        "title": "Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships",
        "author": "Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, Timo Ropinski",
        "abstract": "Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an alternative approach to learn 3D scene graph prediction in an open world without requiring labeled scene graph data. We co-embed the features from a 3D scene graph prediction backbone with the feature space of powerful open world 2D vision language foundation models. This enables us to predict 3D scene graphs from 3D point clouds in a zero-shot manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context. Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relationships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene graph. Our experiments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships.",
        "page": "http://arxiv.org/abs/2402.12259",
        "pdf": "http://arxiv.org/pdf/2402.12259.pdf"
    },
    {
        "title": "Evidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception",
        "author": "Lei Fan, Mingfu Liang, Yunxuan Li, Gang Hua, Ying Wu",
        "abstract": "Active recognition enables robots to intelligently explore novel observations, thereby acquiring more information while circumventing undesired viewing conditions. Recent approaches favor learning policies from simulated or collected data, wherein appropriate actions are more frequently selected when the recognition is accurate. However, most recognition modules are developed under the closed-world assumption, which makes them ill-equipped to handle unexpected inputs, such as the absence of the target object in the current observation. To address this issue, we propose treating active recognition as a sequential evidence-gathering process, providing by-step uncertainty quantification and reliable prediction under the evidence combination theory. Additionally, the reward function developed in this paper effectively characterizes the merit of actions when operating in open-world environments. To evaluate the performance, we collect a dataset from an indoor simulator, encompassing various recognition challenges such as distance, occlusion levels, and visibility. Through a series of experiments on recognition and robustness analysis, we demonstrate the necessity of introducing uncertainties to active recognition and the superior performance of the proposed method.",
        "page": "http://arxiv.org/abs/2311.13793",
        "pdf": "http://arxiv.org/pdf/2311.13793.pdf"
    },
    {
        "title": "Physical Property Understanding from Language-Embedded Feature Fields",
        "author": "Albert J. Zhai, Yuan Shen, Emily Y. Chen, Gloria Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, Shenlong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Boosting Flow-based Generative Super-Resolution Models via Learned Prior",
        "author": "Li-Yuan Tsao, Yi-Chen Lo, Chia-Che Chang, Hao-Wei Chen, Roy Tseng, Chien Feng, Chun-Yi Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learned Scanpaths Aid Blind Panoramic Video Quality Assessment",
        "author": "Kanglong FAN, Wen Wen, Mu Li, YIFAN PENG, Kede Ma",
        "abstract": "Panoramic videos have the advantage of providing an immersive and interactive viewing experience. Nevertheless, their spherical nature gives rise to various and uncertain user viewing behaviors, which poses significant challenges for panoramic video quality assessment (PVQA). In this work, we propose an end-to-end optimized, blind PVQA method with explicit modeling of user viewing patterns through visual scanpaths. Our method consists of two modules: a scanpath generator and a quality assessor. The scanpath generator is initially trained to predict future scanpaths by minimizing their expected code length and then jointly optimized with the quality assessor for quality prediction. Our blind PVQA method enables direct quality assessment of panoramic images by treating them as videos composed of identical frames. Experiments on three public panoramic image and video quality datasets, encompassing both synthetic and authentic distortions, validate the superiority of our blind PVQA model over existing methods.",
        "page": "http://arxiv.org/abs/2404.00252",
        "pdf": "http://arxiv.org/pdf/2404.00252.pdf"
    },
    {
        "title": "Cross-Domain Few-Shot Segmentation via Iterative Support-Query Correspondence Mining",
        "author": "Jiahao Nie, Yun Xing, Gongjie Zhang, Pei Yan, Aoran Xiao, Yap-peng Tan, Alex C. Kot, Shijian Lu",
        "abstract": "Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting novel categories from a distinct domain using only limited exemplars. In this paper, we undertake a comprehensive study of CD-FSS and uncover two crucial insights: (i) the necessity of a fine-tuning stage to effectively transfer the learned meta-knowledge across domains, and (ii) the overfitting risk during the na\\\"ive fine-tuning due to the scarcity of novel category examples. With these insights, we propose a novel cross-domain fine-tuning strategy that addresses the challenging CD-FSS tasks. We first design Bi-directional Few-shot Prediction (BFP), which establishes support-query correspondence in a bi-directional manner, crafting augmented supervision to reduce the overfitting risk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which is a recursive framework to capture the support-query correspondence iteratively, targeting maximal exploitation of supervisory signals from the sparse novel category samples. Extensive empirical evaluations show that our method significantly outperforms the state-of-the-arts (+7.8\\%), which verifies that IFA tackles the cross-domain challenges and mitigates the overfitting simultaneously. The code is available at: https://github.com/niejiahao1998/IFA.",
        "page": "http://arxiv.org/abs/2401.08407",
        "pdf": "http://arxiv.org/pdf/2401.08407.pdf"
    },
    {
        "title": "Entangled View-Epipolar Information Aggregation for Generalizable Neural Radiance Fields",
        "author": "Zhiyuan Min, Yawei Luo, Wei Yang, Yuesong Wang, Yi Yang",
        "abstract": "Generalizable NeRF can directly synthesize novel views across new scenes, eliminating the need for scene-specific retraining in vanilla NeRF. A critical enabling factor in these approaches is the extraction of a generalizable 3D representation by aggregating source-view features. In this paper, we propose an Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF. Different from existing methods that consider cross-view and along-epipolar information independently, EVE-NeRF conducts the view-epipolar feature aggregation in an entangled manner by injecting the scene-invariant appearance continuity and geometry consistency priors to the aggregation process. Our approach effectively mitigates the potential lack of inherent geometric and appearance constraint resulting from one-dimensional interactions, thus further boosting the 3D representation generalizablity. EVE-NeRF attains state-of-the-art performance across various evaluation scenarios. Extensive experiments demonstate that, compared to prevailing single-dimensional aggregation, the entangled network excels in the accuracy of 3D scene geometry and appearance reconstruction. Our code is publicly available at https://github.com/tatakai1/EVENeRF.",
        "page": "http://arxiv.org/abs/2311.11845",
        "pdf": "http://arxiv.org/pdf/2311.11845.pdf"
    },
    {
        "title": "H-ViT: A Hierarchical Vision Transformer for Deformable Image Registration",
        "author": "Morteza Ghahremani, Mohammad Khateri, Bailiang Jian, Benedikt Wiestler, Ehsan Adeli, Christian Wachinger",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Local-consistent Transformation Learning for Rotation-invariant Point Cloud Analysis",
        "author": "Yiyang Chen, Lunhao Duan, Shanshan Zhao, Changxing Ding, Dacheng Tao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FCS: Feature Calibration and Separation for Non-Exemplar Class Incremental Learning",
        "author": "Qiwei Li, Yuxin Peng, Jiahuan Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DIEM: Decomposition-Integration Enhancing Multimodal Insights",
        "author": "Xinyi Jiang, Guoming Wang, Junhao Guo, Juncheng Li, Wenqiao Zhang, Rongxing Lu, Siliang Tang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Graph Contrastive Learning via Adaptive Positive Sampling",
        "author": "Jiaming Zhuo, Feiyang Qin, Can Cui, Kun Fu, Bingxin Niu, Mengzhu Wang, Yuanfang Guo, Chuan Wang, Zhen Wang, Xiaochun Cao, Liang Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Watermark-embedded Adversarial Examples for Copyright Protection against Diffusion Models",
        "author": "Peifei Zhu, Tsubasa Takahashi, Hirokatsu Kataoka",
        "abstract": "Diffusion Models (DMs) have shown remarkable capabilities in various image-generation tasks. However, there are growing concerns that DMs could be used to imitate unauthorized creations and thus raise copyright issues. To address this issue, we propose a novel framework that embeds personal watermarks in the generation of adversarial examples. Such examples can force DMs to generate images with visible watermarks and prevent DMs from imitating unauthorized images. We construct a generator based on conditional adversarial networks and design three losses (adversarial loss, GAN loss, and perturbation loss) to generate adversarial examples that have subtle perturbation but can effectively attack DMs to prevent copyright violations. Training a generator for a personal watermark by our method only requires 5-10 samples within 2-3 minutes, and once the generator is trained, it can generate adversarial examples with that watermark significantly fast (0.2s per image). We conduct extensive experiments in various conditional image-generation scenarios. Compared to existing methods that generate images with chaotic textures, our method adds visible watermarks on the generated images, which is a more straightforward way to indicate copyright violations. We also observe that our adversarial examples exhibit good transferability across unknown generative models. Therefore, this work provides a simple yet powerful way to protect copyright from DM-based imitation.",
        "page": "http://arxiv.org/abs/2404.09401",
        "pdf": "http://arxiv.org/pdf/2404.09401.pdf"
    },
    {
        "title": "LAN: Learning to Adapt Noise for Image Denoising",
        "author": "Changjin Kim, Tae Hyun Kim, Sungyong Baik",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Active Open-Vocabulary Recognition: Let Intelligent Moving Mitigate CLIP Limitations",
        "author": "Lei Fan, Jianxiong Zhou, Xiaoying Xing, Ying Wu",
        "abstract": "Active recognition, which allows intelligent agents to explore observations for better recognition performance, serves as a prerequisite for various embodied AI tasks, such as grasping, navigation and room arrangements. Given the evolving environment and the multitude of object classes, it is impractical to include all possible classes during the training stage. In this paper, we aim at advancing active open-vocabulary recognition, empowering embodied agents to actively perceive and classify arbitrary objects. However, directly adopting recent open-vocabulary classification models, like Contrastive Language Image Pretraining (CLIP), poses its unique challenges. Specifically, we observe that CLIP's performance is heavily affected by the viewpoint and occlusions, compromising its reliability in unconstrained embodied perception scenarios. Further, the sequential nature of observations in agent-environment interactions necessitates an effective method for integrating features that maintains discriminative strength for open-vocabulary classification. To address these issues, we introduce a novel agent for active open-vocabulary recognition. The proposed method leverages inter-frame and inter-concept similarities to navigate agent movements and to fuse features, without relying on class-specific knowledge. Compared to baseline CLIP model with 29.6% accuracy on ShapeNet dataset, the proposed agent could achieve 53.3% accuracy for open-vocabulary recognition, without any fine-tuning to the equipped CLIP model. Additional experiments conducted with the Habitat simulator further affirm the efficacy of our method.",
        "page": "http://arxiv.org/abs/2311.17938",
        "pdf": "http://arxiv.org/pdf/2311.17938.pdf"
    },
    {
        "title": "Advancing Saliency Ranking with Human Fixations: Dataset, Models and Benchmarks",
        "author": "Bowen Deng, Siyang Song, Andrew French, Denis Schluppeck, Michael Pound",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AVID: Any-Length Video Inpainting with Diffusion Model",
        "author": "Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris N. Metaxas, Licheng Yu",
        "abstract": "Recent advances in diffusion models have successfully enabled text-guided image inpainting. While it seems straightforward to extend such editing capability into the video domain, there have been fewer works regarding text-guided video inpainting. Given a video, a masked region at its initial frame, and an editing prompt, it requires a model to do infilling at each frame following the editing guidance while keeping the out-of-mask region intact. There are three main challenges in text-guided video inpainting: ($i$) temporal consistency of the edited video, ($ii$) supporting different inpainting types at different structural fidelity levels, and ($iii$) dealing with variable video length. To address these challenges, we introduce Any-Length Video Inpainting with Diffusion Model, dubbed as AVID. At its core, our model is equipped with effective motion modules and adjustable structure guidance, for fixed-length video inpainting. Building on top of that, we propose a novel Temporal MultiDiffusion sampling pipeline with a middle-frame attention guidance mechanism, facilitating the generation of videos with any desired duration. Our comprehensive experiments show our model can robustly deal with various inpainting types at different video duration ranges, with high quality. More visualization results are made publicly available at https://zhang-zx.github.io/AVID/ .",
        "page": "http://arxiv.org/abs/2312.03816",
        "pdf": "http://arxiv.org/pdf/2312.03816.pdf"
    },
    {
        "title": "Efficient Meshflow and Optical Flow Estimation from Event Cameras",
        "author": "Xinglong Luo, Ao Luo, Zhengning Wang, Chunyu Lin, Bing Zeng, Shuaicheng Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Directed Decentralized Collaboration for Personalized Federated Learning",
        "author": "Yingqi Liu, Yifan Shi, Qinglun Li, Baoyuan Wu, Xueqian Wang, Li Shen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adapting to Length Shift: FlexiLength Network for Trajectory Prediction",
        "author": "Yi Xu, Yun Fu",
        "abstract": "Trajectory prediction plays an important role in various applications, including autonomous driving, robotics, and scene understanding. Existing approaches mainly focus on developing compact neural networks to increase prediction precision on public datasets, typically employing a standardized input duration. However, a notable issue arises when these models are evaluated with varying observation lengths, leading to a significant performance drop, a phenomenon we term the Observation Length Shift. To address this issue, we introduce a general and effective framework, the FlexiLength Network (FLN), to enhance the robustness of existing trajectory prediction techniques against varying observation periods. Specifically, FLN integrates trajectory data with diverse observation lengths, incorporates FlexiLength Calibration (FLC) to acquire temporal invariant representations, and employs FlexiLength Adaptation (FLA) to further refine these representations for more accurate future trajectory predictions. Comprehensive experiments on multiple datasets, ie, ETH/UCY, nuScenes, and Argoverse 1, demonstrate the effectiveness and flexibility of our proposed FLN framework.",
        "page": "http://arxiv.org/abs/2404.00742",
        "pdf": "http://arxiv.org/pdf/2404.00742.pdf"
    },
    {
        "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
        "author": "Boyuan Chen, Zhuo Xu, Sean Kirmani, brian ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia",
        "abstract": "Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/",
        "page": "http://arxiv.org/abs/2401.12168",
        "pdf": "http://arxiv.org/pdf/2401.12168.pdf"
    },
    {
        "title": "CycleINR: Cycle Implicit Neural Representation for Arbitrary-Scale Volumetric Super-Resolution of Medical Data",
        "author": "Wei Fang, Yuxing Tang, Heng Guo, Mingze Yuan, Tony C. W. MOK, Ke Yan, Jiawen Yao, Xin Chen, Zaiyi Liu, Le Lu, Ling Zhang, Minfeng Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LEAD: Exploring Logit Space Evolution for Model Selection",
        "author": "Zixuan Hu, Xiaotong Li, SHIXIANG TANG, Jun Liu, Yichun Hu, Ling-Yu Duan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MPOD123: One Image to 3D Content Generation Using Mask-enhanced Progressive Outline-to-Detail Optimization",
        "author": "Jimin Xu, Tianbao Wang, Tao Jin, Shengyu Zhang, Dongjie Fu, Zhe Wang, Jiangjing Lyu, Chengfei Lv, Chaoyue Niu, Zhou Yu, Zhou Zhao, Fei Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PBWR: Parametric Building Wireframe Reconstruction from Aerial LiDAR Point Clouds",
        "author": "Shangfeng Huang, Ruisheng Wang, Bo Guo, Hongxin Yang",
        "abstract": "In this paper, we present an end-to-end 3D building wireframe reconstruction method to regress edges directly from aerial LiDAR point clouds.Our method, named Parametric Building Wireframe Reconstruction (PBWR), takes aerial LiDAR point clouds and initial edge entities as input, and fully uses self-attention mechanism of transformers to regress edge parameters without any intermediate steps such as corner prediction. We propose an edge non-maximum suppression (E-NMS) module based on edge similarityto remove redundant edges. Additionally, a dedicated edge loss function is utilized to guide the PBWR in regressing edges parameters, where simple use of edge distance loss isn't suitable. In our experiments, we demonstrate state-of-the-art results on the Building3D dataset, achieving an improvement of approximately 36% in entry-level dataset edge accuracy and around 42% improvement in the Tallinn dataset.",
        "page": "http://arxiv.org/abs/2311.12062",
        "pdf": "http://arxiv.org/pdf/2311.12062.pdf"
    },
    {
        "title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data",
        "author": "Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, Yueting Zhuang",
        "abstract": "Multi-modal Large Language Models (MLLMs) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. However, the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored. This work aims to investigate various hallucinations (i.e., object, relation, attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. Drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm. We use our framework to identify and eliminate hallucinations in the training data automatically. Interestingly, HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations. Based on that, we execute counterfactual visual instruction expansion to balance data distribution, thereby enhancing MLLMs' resistance to hallucinations. Comprehensive experiments on hallucination evaluation benchmarks show that our method successfully mitigates 44.6% hallucinations relatively and maintains competitive performance compared to LLaVA. The data and code for this paper are publicly available. \\url{https://github.com/Yuqifan1117/HalluciDoctor}.",
        "page": "http://arxiv.org/abs/2311.13614",
        "pdf": "http://arxiv.org/pdf/2311.13614.pdf"
    },
    {
        "title": "UniMix: Towards Domain Adaptive and Generalizable LiDAR Semantic Segmentation in Adverse Weather",
        "author": "Haimei Zhao, Jing Zhang, Zhuo Chen, Shanshan Zhao, Dacheng Tao",
        "abstract": "LiDAR semantic segmentation (LSS) is a critical task in autonomous driving and has achieved promising progress. However, prior LSS methods are conventionally investigated and evaluated on datasets within the same domain in clear weather. The robustness of LSS models in unseen scenes and all weather conditions is crucial for ensuring safety and reliability in real applications. To this end, we propose UniMix, a universal method that enhances the adaptability and generalizability of LSS models. UniMix first leverages physically valid adverse weather simulation to construct a Bridge Domain, which serves to bridge the domain gap between the clear weather scenes and the adverse weather scenes. Then, a Universal Mixing operator is defined regarding spatial, intensity, and semantic distributions to create the intermediate domain with mixed samples from given domains. Integrating the proposed two techniques into a teacher-student framework, UniMix efficiently mitigates the domain gap and enables LSS models to learn weather-robust and domain-invariant representations. We devote UniMix to two main setups: 1) unsupervised domain adaption, adapting the model from the clear weather source domain to the adverse weather target domain; 2) domain generalization, learning a model that generalizes well to unseen scenes in adverse weather. Extensive experiments validate the effectiveness of UniMix across different tasks and datasets, all achieving superior performance over state-of-the-art methods. The code will be released.",
        "page": "http://arxiv.org/abs/2404.05145",
        "pdf": "http://arxiv.org/pdf/2404.05145.pdf"
    },
    {
        "title": "HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations",
        "author": "Peng Dai, Yang Zhang, Tao Liu, ZhenFan, Tianyuan Du, Zhuo Su, Xiaozheng Zheng, Zeming Li",
        "abstract": "It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users' choices for both high tracking accuracy and easy-to-wear. A lightweight temporal-spatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-the-art results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser's on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our code and free-dancing motion dataset are available https://pico-ai-team.github.io/hmd-poser",
        "page": "http://arxiv.org/abs/2403.03561",
        "pdf": "http://arxiv.org/pdf/2403.03561.pdf"
    },
    {
        "title": "Exploring Orthogonality in Open World Object Detection",
        "author": "Zhicheng Sun, Jinghan Li, Yadong Mu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions",
        "author": "Zeyu Han, Fangrui Zhu, Qianru Lao, Huaizu Jiang",
        "abstract": "Zero-shot referring expression comprehension aims at localizing bounding boxes in an image corresponding to provided textual prompts, which requires: (i) a fine-grained disentanglement of complex visual scene and textual context, and (ii) a capacity to understand relationships among disentangled entities. Unfortunately, existing large vision-language alignment (VLA) models, e.g., CLIP, struggle with both aspects so cannot be directly used for this task. To mitigate this gap, we leverage large foundation models to disentangle both images and texts into triplets in the format of (subject, predicate, object). After that, grounding is accomplished by calculating the structural similarity matrix between visual and textual triplets with a VLA model, and subsequently propagate it to an instance-level similarity matrix. Furthermore, to equip VLA models with the ability of relationship understanding, we design a triplet-matching objective to fine-tune the VLA models on a collection of curated dataset containing abundant entity relationships. Experiments demonstrate that our visual grounding performance increase of up to 19.5% over the SOTA zero-shot model on RefCOCO/+/g. On the more challenging Who's Waldo dataset, our zero-shot approach achieves comparable accuracy to the fully supervised model. Code is available at https://github.com/Show-han/Zeroshot_REC.",
        "page": "http://arxiv.org/abs/2311.17048",
        "pdf": "http://arxiv.org/pdf/2311.17048.pdf"
    },
    {
        "title": "Enhancing the Power of OOD Detection via Sample-Aware Model Selection",
        "author": "Feng Xue, Zi He, Yuan Zhang, Chuanlong Xie, Zhenguo Li, Falong Tan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ParamISP: Learned Forward and Inverse ISPs using Camera Parameters",
        "author": "Woohyeok Kim, Geonu Kim, Junyong Lee, Seungyong Lee, Seung-Hwan Baek, Sunghyun Cho",
        "abstract": "RAW images are rarely shared mainly due to its excessive data size compared to their sRGB counterparts obtained by camera ISPs. Learning the forward and inverse processes of camera ISPs has been recently demonstrated, enabling physically-meaningful RAW-level image processing on input sRGB images. However, existing learning-based ISP methods fail to handle the large variations in the ISP processes with respect to camera parameters such as ISO and exposure time, and have limitations when used for various applications. In this paper, we propose ParamISP, a learning-based method for forward and inverse conversion between sRGB and RAW images, that adopts a novel neural-network module to utilize camera parameters, which is dubbed as ParamNet. Given the camera parameters provided in the EXIF data, ParamNet converts them into a feature vector to control the ISP networks. Extensive experiments demonstrate that ParamISP achieve superior RAW and sRGB reconstruction results compared to previous methods and it can be effectively used for a variety of applications such as deblurring dataset synthesis, raw deblurring, HDR reconstruction, and camera-to-camera transfer.",
        "page": "http://arxiv.org/abs/2312.13313",
        "pdf": "http://arxiv.org/pdf/2312.13313.pdf"
    },
    {
        "title": "Relightable Gaussian Codec Avatars",
        "author": "Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam",
        "abstract": "The fidelity of relighting is bounded by both geometry and appearance representations. For geometry, both mesh and volumetric approaches have difficulty modeling intricate structures like 3D hair geometry. For appearance, existing relighting models are limited in fidelity and often too slow to render in real-time with high-resolution continuous environments. In this work, we present Relightable Gaussian Codec Avatars, a method to build high-fidelity relightable head avatars that can be animated to generate novel expressions. Our geometry model based on 3D Gaussians can capture 3D-consistent sub-millimeter details such as hair strands and pores on dynamic face sequences. To support diverse materials of human heads such as the eyes, skin, and hair in a unified manner, we present a novel relightable appearance model based on learnable radiance transfer. Together with global illumination-aware spherical harmonics for the diffuse components, we achieve real-time relighting with all-frequency reflections using spherical Gaussians. This appearance model can be efficiently relit under both point light and continuous illumination. We further improve the fidelity of eye reflections and enable explicit gaze control by introducing relightable explicit eye models. Our method outperforms existing approaches without compromising real-time performance. We also demonstrate real-time relighting of avatars on a tethered consumer VR headset, showcasing the efficiency and fidelity of our avatars.",
        "page": "http://arxiv.org/abs/2312.03704",
        "pdf": "http://arxiv.org/pdf/2312.03704.pdf"
    },
    {
        "title": "Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open Domain Generalization",
        "author": "Mainak Singha, Ankit Jha, Shirsha Bose, Ashwin Nair, Moloud Abdar, Biplab Banerjee",
        "abstract": "We delve into Open Domain Generalization (ODG), marked by domain and category shifts between training's labeled source and testing's unlabeled target domains. Existing solutions to ODG face limitations due to constrained generalizations of traditional CNN backbones and errors in detecting target open samples in the absence of prior knowledge. Addressing these pitfalls, we introduce ODG-CLIP, harnessing the semantic prowess of the vision-language model, CLIP. Our framework brings forth three primary innovations: Firstly, distinct from prevailing paradigms, we conceptualize ODG as a multi-class classification challenge encompassing both known and novel categories. Central to our approach is modeling a unique prompt tailored for detecting unknown class samples, and to train this, we employ a readily accessible stable diffusion model, elegantly generating proxy images for the open class. Secondly, aiming for domain-tailored classification (prompt) weights while ensuring a balance of precision and simplicity, we devise a novel visual stylecentric prompt learning mechanism. Finally, we infuse images with class-discriminative knowledge derived from the prompt space to augment the fidelity of CLIP's visual embeddings. We introduce a novel objective to safeguard the continuity of this infused semantic intel across domains, especially for the shared classes. Through rigorous testing on diverse datasets, covering closed and open-set DG contexts, ODG-CLIP demonstrates clear supremacy, consistently outpacing peers with performance boosts between 8%-16%. Code will be available at https://github.com/mainaksingha01/ODG-CLIP.",
        "page": "http://arxiv.org/abs/2404.00710",
        "pdf": "http://arxiv.org/pdf/2404.00710.pdf"
    },
    {
        "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
        "author": "Xiaoyu Wu, Yang Hua, Chumeng Liang, Jiaru Zhang, Hao Wang, Tao Song, Haibing Guan",
        "abstract": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dreambooth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.",
        "page": "http://arxiv.org/abs/2403.11162",
        "pdf": "http://arxiv.org/pdf/2403.11162.pdf"
    },
    {
        "title": "InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning",
        "author": "Jing Shi, Wei Xiong, Zhe Lin, HyunJoon Jung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models",
        "author": "Pablo Marcos-Manch\u00f3n, Roberto Alcover-Couso, Juan SanMiguel, Jose M. Martinez",
        "abstract": "Diffusion models represent a new paradigm in text-to-image generation. Beyond generating high-quality images from text prompts, models such as Stable Diffusion have been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to prompt words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text prompt. In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for text-to-image diffusion models that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable Diffusion extensions. The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images' pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining.",
        "page": "http://arxiv.org/abs/2403.14291",
        "pdf": "http://arxiv.org/pdf/2403.14291.pdf"
    },
    {
        "title": "Dynamic LiDAR Re-simulation using Compositional Neural Fields",
        "author": "Hanfeng Wu, Xingxing Zuo, Stefan Leutenegger, Or Litany, Konrad Schindler, Shengyu Huang",
        "abstract": "We introduce DyNFL, a novel neural field-based approach for high-fidelity re-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDAR measurements from dynamic environments, accompanied by bounding boxes of moving objects, to construct an editable neural field. This field, comprising separately reconstructed static background and dynamic objects, allows users to modify viewpoints, adjust object positions, and seamlessly add or remove objects in the re-simulated scene. A key innovation of our method is the neural field composition technique, which effectively integrates reconstructed neural assets from various scenes through a ray drop test, accounting for occlusions and transparent surfaces. Our evaluation with both synthetic and real-world environments demonstrates that DyNFL substantially improves dynamic scene LiDAR simulation, offering a combination of physical fidelity and flexible editing capabilities.",
        "page": "http://arxiv.org/abs/2312.05247",
        "pdf": "http://arxiv.org/pdf/2312.05247.pdf"
    },
    {
        "title": "GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering",
        "author": "Abdullah J Hamdi, Luke Melas-Kyriazi, Jinjie Mai, Guocheng Qian, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-Modal Proxy Learning Towards Personalized Visual Multiple Clustering",
        "author": "Jiawei Yao, Qi Qian, Juhua Hu",
        "abstract": "Multiple clustering has gained significant attention in recent years due to its potential to reveal multiple hidden structures of data from different perspectives. The advent of deep multiple clustering techniques has notably advanced the performance by uncovering complex patterns and relationships within large datasets. However, a major challenge arises as users often do not need all the clusterings that algorithms generate, and figuring out the one needed requires a substantial understanding of each clustering result. Traditionally, aligning a user's brief keyword of interest with the corresponding vision components was challenging, but the emergence of multi-modal and large language models (LLMs) has begun to bridge this gap. In response, given unlabeled target visual data, we propose Multi-MaP, a novel method employing a multi-modal proxy learning process. It leverages CLIP encoders to extract coherent text and image embeddings, with GPT-4 integrating users' interests to formulate effective textual contexts. Moreover, reference word constraint and concept-level constraint are designed to learn the optimal text proxy according to the user's interest. Multi-MaP not only adeptly captures a user's interest via a keyword but also facilitates identifying relevant clusterings. Our extensive experiments show that Multi-MaP consistently outperforms state-of-the-art methods in all benchmark multi-clustering vision tasks. Our code is available at https://github.com/Alexander-Yao/Multi-MaP.",
        "page": "http://arxiv.org/abs/2404.15655",
        "pdf": "http://arxiv.org/pdf/2404.15655.pdf"
    },
    {
        "title": "Fairy: Fast Parallellized Instruction-Guided Video-to-Video Synthesis",
        "author": "Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, Licheng Yu, Peter Vajda",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving",
        "author": "Mingfu Liang, Jong-Chyi Su, Samuel Schulter, Sparsh Garg, Shiyu Zhao, Ying Wu, Manmohan Chandraker",
        "abstract": "Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. However, objects encountered on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model. This necessitates an expensive process of continuously curating and annotating data with significant human effort. We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios. This process operates iteratively, allowing for continuous self-improvement of the model. We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method's superior performance at a reduced cost.",
        "page": "http://arxiv.org/abs/2403.17373",
        "pdf": "http://arxiv.org/pdf/2403.17373.pdf"
    },
    {
        "title": "Activity-Biometrics: Person Identification from Daily Activities",
        "author": "Shehreen Azad, Yogesh S. Rawat",
        "abstract": "In this work, we study a novel problem which focuses on person identification while performing daily activities. Learning biometric features from RGB videos is challenging due to spatio-temporal complexity and presence of appearance biases such as clothing color and background. We propose ABNet, a novel framework which leverages disentanglement of biometric and non-biometric features to perform effective person identification from daily activities. ABNet relies on a bias-less teacher to learn biometric features from RGB videos and explicitly disentangle non-biometric features with the help of biometric distortion. In addition, ABNet also exploits activity prior for biometrics which is enabled by joint biometric and activity learning. We perform comprehensive evaluation of the proposed approach across five different datasets which are derived from existing activity recognition benchmarks. Furthermore, we extensively compare ABNet with existing works in person identification and demonstrate its effectiveness for activity-based biometrics across all five datasets. The code and dataset can be accessed at: \\url{https://github.com/sacrcv/Activity-Biometrics/}",
        "page": "http://arxiv.org/abs/2403.17360",
        "pdf": "http://arxiv.org/pdf/2403.17360.pdf"
    },
    {
        "title": "HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting",
        "author": "Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Single Mesh Diffusion Models with Field Latents for Texture Generation",
        "author": "Thomas W. Mitchel, Carlos Esteves, Ameesh Makadia",
        "abstract": "We introduce a framework for intrinsic latent diffusion models operating directly on the surfaces of 3D shapes, with the goal of synthesizing high-quality textures. Our approach is underpinned by two contributions: field latents, a latent representation encoding textures as discrete vector fields on the mesh vertices, and field latent diffusion models, which learn to denoise a diffusion process in the learned latent space on the surface. We consider a single-textured-mesh paradigm, where our models are trained to generate variations of a given texture on a mesh. We show the synthesized textures are of superior fidelity compared those from existing single-textured-mesh generative models. Our models can also be adapted for user-controlled editing tasks such as inpainting and label-guided generation. The efficacy of our approach is due in part to the equivariance of our proposed framework under isometries, allowing our models to seamlessly reproduce details across locally similar regions and opening the door to a notion of generative texture transfer.",
        "page": "http://arxiv.org/abs/2312.09250",
        "pdf": "http://arxiv.org/pdf/2312.09250.pdf"
    },
    {
        "title": "One More Step: A Versatile Plug-and-Play Module for Rectifying Diffusion Schedule Flaws and Enhancing Low-Frequency Controls",
        "author": "Minghui Hu, Jianbin Zheng, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, Tat-Jen Cham",
        "abstract": "It is well known that many open-released foundational diffusion models have difficulty in generating images that substantially depart from average brightness, despite such images being present in the training data. This is due to an inconsistency: while denoising starts from pure Gaussian noise during inference, the training noise schedule retains residual data even in the final timestep distribution, due to difficulties in numerical conditioning in mainstream formulation, leading to unintended bias during inference. To mitigate this issue, certain $\\epsilon$-prediction models are combined with an ad-hoc offset-noise methodology. In parallel, some contemporary models have adopted zero-terminal SNR noise schedules together with $\\mathbf{v}$-prediction, which necessitate major alterations to pre-trained models. However, such changes risk destabilizing a large multitude of community-driven applications anchored on these pre-trained models. In light of this, our investigation revisits the fundamental causes, leading to our proposal of an innovative and principled remedy, called One More Step (OMS). By integrating a compact network and incorporating an additional simple yet effective step during inference, OMS elevates image fidelity and harmonizes the dichotomy between training and inference, while preserving original model parameters. Once trained, various pre-trained diffusion models with the same latent domain can share the same OMS module.",
        "page": "http://arxiv.org/abs/2311.15744",
        "pdf": "http://arxiv.org/pdf/2311.15744.pdf"
    },
    {
        "title": "Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions",
        "author": "Oindrila Saha, Grant Horn, Subhransu Maji",
        "abstract": "The zero-shot performance of existing vision-language models (VLMs) such as CLIP is limited by the availability of large-scale, aligned image and text datasets in specific domains. In this work, we leverage two complementary sources of information -- descriptions of categories generated by large language models (LLMs) and abundant, fine-grained image classification datasets -- to improve the zero-shot classification performance of VLMs across fine-grained domains. On the technical side, we develop methods to train VLMs with this \"bag-level\" image-text supervision. We find that simply using these attributes at test-time does not improve performance, but our training strategy, for example, on the iNaturalist dataset, leads to an average improvement of 4-5% in zero-shot classification accuracy for novel categories of birds and flowers. Similar improvements are observed in domains where a subset of the categories was used to fine-tune the model. By prompting LLMs in various ways, we generate descriptions that capture visual appearance, habitat, and geographic regions and pair them with existing attributes such as the taxonomic structure of the categories. We systematically evaluate their ability to improve zero-shot categorization in natural domains. Our findings suggest that geographic priors can be just as effective and are complementary to visual appearance. Our method also outperforms prior work on prompt-based tuning of VLMs. We release the benchmark, consisting of 14 datasets at https://github.com/cvl-umass/AdaptCLIPZS , which will contribute to future research in zero-shot recognition.",
        "page": "http://arxiv.org/abs/2401.02460",
        "pdf": "http://arxiv.org/pdf/2401.02460.pdf"
    },
    {
        "title": "Distilling Semantic Priors from SAM to Efficient Image Restoration Models",
        "author": "Quan Zhang, Xiaoyu Liu, Wei Li, Hanting Chen, Junchao Liu, Jie Hu, Zhiwei Xiong, Chun Yuan, Yunhe Wang",
        "abstract": "In image restoration (IR), leveraging semantic priors from segmentation models has been a common approach to improve performance. The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks. However, the computational cost of SAM is prohibitive for IR, compared to existing smaller IR models. The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency. To address this issue, we propose a general framework to distill SAM's semantic knowledge to boost exiting IR models without interfering with their inference process. Specifically, our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD) scheme. SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image. SPD leverages a self-distillation manner to distill the fused semantic priors to boost the performance of original IR models. Additionally, we design a semantic-guided relation (SGR) module for SPD, which ensures semantic feature representation space consistency to fully distill the priors. We demonstrate the effectiveness of our framework across multiple IR models and tasks, including deraining, deblurring, and denoising.",
        "page": "http://arxiv.org/abs/2403.16368",
        "pdf": "http://arxiv.org/pdf/2403.16368.pdf"
    },
    {
        "title": "EgoGen: An Egocentric Synthetic Data Generator",
        "author": "Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang",
        "abstract": "Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate EgoGen's efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. EgoGen will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. Refer to our project page: https://ego-gen.github.io/.",
        "page": "http://arxiv.org/abs/2401.08739",
        "pdf": "http://arxiv.org/pdf/2401.08739.pdf"
    },
    {
        "title": "Blind Image Quality Assessment Based on Geometric Order Learning",
        "author": "Nyeong-Ho Shin, Seon-Ho Lee, Chang-Su Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GlitchBench: Can large multimodal models detect video game glitches?",
        "author": "Mohammad Reza Taesiri, Tianjun Feng, Cor-Paul Bezemer, Anh Nguyen",
        "abstract": "Large multimodal models (LMMs) have evolved from large language models (LLMs) to integrate multiple input modalities, such as visual inputs. This integration augments the capacity of LLMs for tasks requiring visual comprehension and reasoning. However, the extent and limitations of their enhanced abilities are not fully understood, especially when it comes to real-world tasks. To address this gap, we introduce GlitchBench, a novel benchmark derived from video game quality assurance tasks, to test and evaluate the reasoning capabilities of LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios from video games and aims to challenge both the visual and linguistic reasoning powers of LMMs in detecting and interpreting out-of-the-ordinary events. We evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents a new challenge for these models. Code and data are available at: https://glitchbench.github.io/",
        "page": "http://arxiv.org/abs/2312.05291",
        "pdf": "http://arxiv.org/pdf/2312.05291.pdf"
    },
    {
        "title": "Layout-Agnostic Scene Text Image Synthesis with Diffusion Models",
        "author": "Qilong Zhangli, Jindong Jiang, Di Liu, Licheng Yu, Xiaoliang Dai, Ankit Ramchandani, Guan Pang, Dimitris N. Metaxas, Praveen Krishnan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Interpretable Measures of Conceptual Similarity by  Complexity-Constrained Descriptive Auto-Encoding",
        "author": "Alessandro Achille, Greg Ver Steeg, Tian Yu Liu, Matthew Trager, Carson Klingenberg, Stefano Soatto",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-Modal Hallucination Control by Visual Information Grounding",
        "author": "Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto",
        "abstract": "Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as \"hallucination\" and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt. M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If training is an option, we show that M3ID can be paired with Direct Preference Optimization (DPO) to improve the model's reliance on the prompt image without requiring any labels. Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24%.",
        "page": "http://arxiv.org/abs/2403.14003",
        "pdf": "http://arxiv.org/pdf/2403.14003.pdf"
    },
    {
        "title": "MonoHair: High-Fidelity Hair Modeling from a Monocular Video",
        "author": "Keyu Wu, LINGCHEN YANG, Zhiyi Kuang, Yao Feng, Xutao Han, Yuefan Shen, Hongbo Fu, Kun Zhou, Youyi Zheng",
        "abstract": "Undoubtedly, high-fidelity 3D hair is crucial for achieving realism, artistic expression, and immersion in computer graphics. While existing 3D hair modeling methods have achieved impressive performance, the challenge of achieving high-quality hair reconstruction persists: they either require strict capture conditions, making practical applications difficult, or heavily rely on learned prior data, obscuring fine-grained details in images. To address these challenges, we propose MonoHair,a generic framework to achieve high-fidelity hair reconstruction from a monocular video, without specific requirements for environments. Our approach bifurcates the hair modeling process into two main stages: precise exterior reconstruction and interior structure inference. The exterior is meticulously crafted using our Patch-based Multi-View Optimization (PMVO). This method strategically collects and integrates hair information from multiple views, independent of prior data, to produce a high-fidelity exterior 3D line map. This map not only captures intricate details but also facilitates the inference of the hair's inner structure. For the interior, we employ a data-driven, multi-view 3D hair reconstruction method. This method utilizes 2D structural renderings derived from the reconstructed exterior, mirroring the synthetic 2D inputs used during training. This alignment effectively bridges the domain gap between our training data and real-world data, thereby enhancing the accuracy and reliability of our interior structure inference. Lastly, we generate a strand model and resolve the directional ambiguity by our hair growth algorithm. Our experiments demonstrate that our method exhibits robustness across diverse hairstyles and achieves state-of-the-art performance. For more results, please refer to our project page https://keyuwu-cs.github.io/MonoHair/.",
        "page": "http://arxiv.org/abs/2403.18356",
        "pdf": "http://arxiv.org/pdf/2403.18356.pdf"
    },
    {
        "title": "Brush2Prompt: Contextual Prompt Generator for Object Inpainting",
        "author": "Mang Tik Chiu, Yuqian Zhou, Lingzhi Zhang, Zhe Lin, Connelly Barnes, Sohrab Amirghodsi, Eli Shechtman, Humphrey Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Backpropagation-free Network for 3D Test-time Adaptation",
        "author": "YANSHUO WANG, Ali Cheraghian, Zeeshan Hayder, JIE HONG, Sameera Ramasinghe, Shafin Rahman, David Ahmedt-Aristizabal, Xuesong Li, Lars Petersson, Mehrtash Harandi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Cross Initialization for Face Personalization of Text-to-Image Models",
        "author": "Lianyu Pang, Jian Yin, Haoran Xie, Qiping Wang, Qing Li, Xudong Mao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges",
        "author": "Tongtong Yuan, Xuange Zhang, Kun Liu, Bo Liu, Chen Chen, Jian Jin, Zhenzhen Jiao",
        "abstract": "Surveillance videos are an essential component of daily life with various critical applications, particularly in public security. However, current surveillance video tasks mainly focus on classifying and localizing anomalous events. Existing methods are limited to detecting and classifying the predefined events with unsatisfactory semantic understanding, although they have obtained considerable performance. To address this issue, we propose a new research direction of surveillance video-and-language understanding, and construct the first multimodal surveillance video dataset. We manually annotate the real-world surveillance dataset UCF-Crime with fine-grained event content and timing. Our newly annotated dataset, UCA (UCF-Crime Annotation), contains 23,542 sentences, with an average length of 20 words, and its annotated videos are as long as 110.7 hours. Furthermore, we benchmark SOTA models for four multimodal tasks on this newly created dataset, which serve as new baselines for surveillance video-and-language understanding. Through our experiments, we find that mainstream models used in previously publicly available datasets perform poorly on surveillance video, which demonstrates the new challenges in surveillance video-and-language understanding. To validate the effectiveness of our UCA, we conducted experiments on multimodal anomaly detection. The results demonstrate that our multimodal surveillance learning can improve the performance of conventional anomaly detection tasks. All the experiments highlight the necessity of constructing this dataset to advance surveillance AI. The link to our dataset is provided at: https://xuange923.github.io/Surveillance-Video-Understanding.",
        "page": "http://arxiv.org/abs/2309.13925",
        "pdf": "http://arxiv.org/pdf/2309.13925.pdf"
    },
    {
        "title": "VRetouchEr: Learning Cross-frame Feature Interdependence  with Imperfection Flow for Face Retouching in Videos",
        "author": "Wen Xue, Le Jiang, Lianxin Xie, Si Wu, Yong Xu, Hau San Wong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging",
        "author": "Haoyang Ge, Qiao Feng, Hailong Jia, Xiongzheng Li, Xiangjun Yin, You Zhou, Jingyu Yang, Kun Li",
        "abstract": "Human pose and shape (HPS) estimation with lensless imaging is not only beneficial to privacy protection but also can be used in covert surveillance scenarios due to the small size and simple structure of this device. However, this task presents significant challenges due to the inherent ambiguity of the captured measurements and lacks effective methods for directly estimating human pose and shape from lensless data. In this paper, we propose the first end-to-end framework to recover 3D human poses and shapes from lensless measurements to our knowledge. We specifically design a multi-scale lensless feature decoder to decode the lensless measurements through the optically encoded mask for efficient feature extraction. We also propose a double-head auxiliary supervision mechanism to improve the estimation accuracy of human limb ends. Besides, we establish a lensless imaging system and verify the effectiveness of our method on various datasets acquired by our lensless imaging system.",
        "page": "http://arxiv.org/abs/2404.01941",
        "pdf": "http://arxiv.org/pdf/2404.01941.pdf"
    },
    {
        "title": "Robust Synthetic-to-Real Transfer for Stereo Matching",
        "author": "Jiawei Zhang, Jiahe Li, Lei Huang, Xiaohan Yu, Lin Gu, Jin Zheng, Xiao Bai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text-conditional Attribute Alignment across Latent Spaces for 3D Controllable Face Image Synthesis",
        "author": "FeiFan Xu, Rui Li, Si Wu, Yong Xu, Hau San Wong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos",
        "author": "Tao Wu, Runyu He, Gangshan Wu, Limin Wang",
        "abstract": "Video-based visual relation detection tasks, such as video scene graph generation, play important roles in fine-grained video understanding. However, current video visual relation detection datasets have two main limitations that hinder the progress of research in this area. First, they do not explore complex human-human interactions in multi-person scenarios. Second, the relation types of existing datasets have relatively low-level semantics and can be often recognized by appearance or simple prior information, without the need for detailed spatio-temporal context reasoning. Nevertheless, comprehending high-level interactions between humans is crucial for understanding complex multi-person videos, such as sports and surveillance videos. To address this issue, we propose a new video visual relation detection task: video human-human interaction detection, and build a dataset named SportsHHI for it. SportsHHI contains 34 high-level interaction classes from basketball and volleyball sports. 118,075 human bounding boxes and 50,649 interaction instances are annotated on 11,398 keyframes. To benchmark this, we propose a two-stage baseline method and conduct extensive experiments to reveal the key factors for a successful human-human interaction detector. We hope that SportsHHI can stimulate research on human interaction understanding in videos and promote the development of spatio-temporal context modeling techniques in video visual relation detection.",
        "page": "http://arxiv.org/abs/2404.04565",
        "pdf": "http://arxiv.org/pdf/2404.04565.pdf"
    },
    {
        "title": "VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence",
        "author": "Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, Kevin Tang",
        "abstract": "Current diffusion-based video editing primarily focuses on structure-preserved editing by utilizing various dense correspondences to ensure temporal consistency and motion alignment. However, these approaches are often ineffective when the target edit involves a shape change. To embark on video editing with shape change, we explore customized video subject swapping in this work, where we aim to replace the main subject in a source video with a target subject having a distinct identity and potentially different shape. In contrast to previous methods that rely on dense correspondences, we introduce the VideoSwap framework that exploits semantic point correspondences, inspired by our observation that only a small number of semantic points are necessary to align the subject's motion trajectory and modify its shape. We also introduce various user-point interactions (\\eg, removing points and dragging points) to address various semantic point correspondence. Extensive experiments demonstrate state-of-the-art video subject swapping results across a variety of real-world videos.",
        "page": "http://arxiv.org/abs/2312.02087",
        "pdf": "http://arxiv.org/pdf/2312.02087.pdf"
    },
    {
        "title": "Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-dataset 3D Object Detection",
        "author": "Zhanwei Zhang, Minghao Chen, Shuai Xiao, Liang Peng, Hengjia Li, Binbin Lin, Ping Li, Wenxiao Wang, Boxi Wu, Deng Cai",
        "abstract": "Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA). These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain. However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background. Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process. To resolve this problem, in this paper, we propose a novel pseudo label refinery framework. Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy. This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box. Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process. We alleviate this issue by generating additional proposals and aligning RoI features across different domains. Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks. Code will be available at https://github.com/Zhanwei-Z/PERE.",
        "page": "http://arxiv.org/abs/2404.19384",
        "pdf": "http://arxiv.org/pdf/2404.19384.pdf"
    },
    {
        "title": "Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation",
        "author": "Shuting He, Henghui Ding",
        "abstract": "Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues. Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues. However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception. In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues. In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension. Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues. Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales. Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects. These contributions yield state-of-the-art performance across five datasets, including a remarkable $\\textbf{9.2%}$ $\\mathcal{J\\&F}$ improvement on the challenging $\\textbf{MeViS}$ dataset. Code is available at https://github.com/heshuting555/DsHmp.",
        "page": "http://arxiv.org/abs/2404.03645",
        "pdf": "http://arxiv.org/pdf/2404.03645.pdf"
    },
    {
        "title": "Unraveling Instance Associations: A Closer Look for Audio-Visual Segmentation",
        "author": "Yuanhong Chen, Yuyuan Liu, Hu Wang, Fengbei Liu, Chong Wang, Helen Frazer, Gustavo Carneiro",
        "abstract": "Audio-visual segmentation (AVS) is a challenging task that involves accurately segmenting sounding objects based on audio-visual cues. The effectiveness of audio-visual learning critically depends on achieving accurate cross-modal alignment between sound and visual objects. Successful audio-visual learning requires two essential components: 1) a challenging dataset with high-quality pixel-level multi-class annotated images associated with audio files, and 2) a model that can establish strong links between audio information and its corresponding visual object. However, these requirements are only partially addressed by current methods, with training sets containing biased audio-visual data, and models that generalise poorly beyond this biased training set. In this work, we propose a new cost-effective strategy to build challenging and relatively unbiased high-quality audio-visual segmentation benchmarks. We also propose a new informative sample mining method for audio-visual supervised contrastive learning to leverage discriminative contrastive samples to enforce cross-modal understanding. We show empirical results that demonstrate the effectiveness of our benchmark. Furthermore, experiments conducted on existing AVS datasets and on our new benchmark show that our method achieves state-of-the-art (SOTA) segmentation accuracy.",
        "page": "http://arxiv.org/abs/2304.02970",
        "pdf": "http://arxiv.org/pdf/2304.02970.pdf"
    },
    {
        "title": "CPP-Net: Embracing Multi-Scale Feature Fusion into Deep Unfolding CP-PPA Network for Compressive Sensing",
        "author": "Zhen Guo, Hongping Gan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Osprey: Pixel Understanding with Visual Instruction Tuning",
        "author": "Yuqian Yuan, Wentong Li, Jian liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, Jianke Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Visual-Augmented Dynamic Semantic Prototype for Generative Zero-Shot Learning",
        "author": "Wenjin Hou, Shiming Chen, Shuhuang Chen, Ziming Hong, Yan Wang, Xuetao Feng, Salman Khan, Fahad Shahbaz Khan, Xinge You",
        "abstract": "Generative Zero-shot learning (ZSL) learns a generator to synthesize visual samples for unseen classes, which is an effective way to advance ZSL. However, existing generative methods rely on the conditions of Gaussian noise and the predefined semantic prototype, which limit the generator only optimized on specific seen classes rather than characterizing each visual instance, resulting in poor generalizations (\\textit{e.g.}, overfitting to seen classes). To address this issue, we propose a novel Visual-Augmented Dynamic Semantic prototype method (termed VADS) to boost the generator to learn accurate semantic-visual mapping by fully exploiting the visual-augmented knowledge into semantic conditions. In detail, VADS consists of two modules: (1) Visual-aware Domain Knowledge Learning module (VDKL) learns the local bias and global prior of the visual features (referred to as domain visual knowledge), which replace pure Gaussian noise to provide richer prior noise information; (2) Vision-Oriented Semantic Updation module (VOSU) updates the semantic prototype according to the visual representations of the samples. Ultimately, we concatenate their output as a dynamic semantic prototype, which serves as the condition of the generator. Extensive experiments demonstrate that our VADS achieves superior CZSL and GZSL performances on three prominent datasets and outperforms other state-of-the-art methods with averaging increases by 6.4\\%, 5.9\\% and 4.2\\% on SUN, CUB and AWA2, respectively.",
        "page": "http://arxiv.org/abs/2404.14808",
        "pdf": "http://arxiv.org/pdf/2404.14808.pdf"
    },
    {
        "title": "DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly",
        "author": "Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue",
        "abstract": "Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph. Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at https://github.com/IIT-PAVIS/DiffAssemble",
        "page": "http://arxiv.org/abs/2402.19302",
        "pdf": "http://arxiv.org/pdf/2402.19302.pdf"
    },
    {
        "title": "CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation",
        "author": "Townim Chowdhury, Kewen Liao, Vu Minh Hieu Phan, Minh-Son To, Yutong Xie, Kevin Hung, David Ross, Anton van den Hengel, Johan Verjans, Zhibin Liao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding",
        "author": "Lorenzo Bianchi, Fabio Carrara, Nicola Messina, Claudio Gennaro, Fabrizio Falchi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GS-IR: 3D Gaussian Splatting for Inverse Rendering",
        "author": "Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, Kui Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UFC-Net: Unrolling Fixed-point Continuous Network for Deep Compressive Sensing",
        "author": "Xiaoyang Wang, Hongping Gan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learn to Rectify the Bias of CLIP for Unsupervised Semantic Segmentation",
        "author": "Jingyun Wang, Guoliang Kang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction",
        "author": "Qihang Ma, Xin Tan, Yanyun Qu, Lizhuang Ma, Zhizhong Zhang, Yuan Xie",
        "abstract": "The autonomous driving community has shown significant interest in 3D occupancy prediction, driven by its exceptional geometric perception and general object recognition capabilities. To achieve this, current works try to construct a Tri-Perspective View (TPV) or Occupancy (OCC) representation extending from the Bird-Eye-View perception. However, compressed views like TPV representation lose 3D geometry information while raw and sparse OCC representation requires heavy but redundant computational costs. To address the above limitations, we propose Compact Occupancy TRansformer (COTR), with a geometry-aware occupancy encoder and a semantic-aware group decoder to reconstruct a compact 3D OCC representation. The occupancy encoder first generates a compact geometrical OCC feature through efficient explicit-implicit view transformation. Then, the occupancy decoder further enhances the semantic discriminability of the compact OCC representation by a coarse-to-fine semantic grouping strategy. Empirical experiments show that there are evident performance gains across multiple baselines, e.g., COTR outperforms baselines with a relative improvement of 8%-15%, demonstrating the superiority of our method.",
        "page": "http://arxiv.org/abs/2312.01919",
        "pdf": "http://arxiv.org/pdf/2312.01919.pdf"
    },
    {
        "title": "ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More",
        "author": "Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, Lin Wang",
        "abstract": "Event cameras have recently been shown beneficial for practical vision tasks, such as action recognition, thanks to their high temporal resolution, power efficiency, and reduced privacy concerns. However, current research is hindered by 1) the difficulty in processing events because of their prolonged duration and dynamic actions with complex and ambiguous semantics and 2) the redundant action depiction of the event frame representation with fixed stacks. We find language naturally conveys abundant semantic information, rendering it stunningly superior in reducing semantic uncertainty. In light of this, we propose ExACT, a novel approach that, for the first time, tackles event-based action recognition from a cross-modal conceptualizing perspective. Our ExACT brings two technical contributions. Firstly, we propose an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones. This subtly enhances the performance of ExACT without extra computational cost. Then, we propose a conceptual reasoning-based uncertainty estimation module, which simulates the recognition process to enrich the semantic representation. In particular, conceptual reasoning builds the temporal relation based on the action semantics, and uncertainty estimation tackles the semantic uncertainty of actions based on the distributional representation. Experiments show that our ExACT achieves superior recognition accuracy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.",
        "page": "http://arxiv.org/abs/2403.12534",
        "pdf": "http://arxiv.org/pdf/2403.12534.pdf"
    },
    {
        "title": "Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models",
        "author": "Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang",
        "abstract": "With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the training set. This novel capability enhances model performance in the few-shot setting and enables model usability in the absence of training data. The two memory networks employ the same flexible memory interactive strategy, which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers. Our approach is tested across 11 datasets under the three task settings. Remarkably, in the zero-shot scenario, it outperforms existing methods by over 3\\% and even shows superior results against methods utilizing external training data. Additionally, our method exhibits robust performance against natural distribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.",
        "page": "http://arxiv.org/abs/2403.17589",
        "pdf": "http://arxiv.org/pdf/2403.17589.pdf"
    },
    {
        "title": "RepViT: Revisiting Mobile CNN From ViT Perspective",
        "author": "Ao Wang, Hui Chen, Zijia Lin, Jungong Han, Guiguang Ding",
        "abstract": "Recently, lightweight Vision Transformers (ViTs) demonstrate superior performance and lower latency, compared with lightweight Convolutional Neural Networks (CNNs), on resource-constrained mobile devices. Researchers have discovered many structural connections between lightweight ViTs and lightweight CNNs. However, the notable architectural disparities in the block structure, macro, and micro designs between them have not been adequately examined. In this study, we revisit the efficient design of lightweight CNNs from ViT perspective and emphasize their promising prospect for mobile devices. Specifically, we incrementally enhance the mobile-friendliness of a standard lightweight CNN, \\ie, MobileNetV3, by integrating the efficient architectural designs of lightweight ViTs. This ends up with a new family of pure lightweight CNNs, namely RepViT. Extensive experiments show that RepViT outperforms existing state-of-the-art lightweight ViTs and exhibits favorable latency in various vision tasks. Notably, on ImageNet, RepViT achieves over 80\\% top-1 accuracy with 1.0 ms latency on an iPhone 12, which is the first time for a lightweight model, to the best of our knowledge. Besides, when RepViT meets SAM, our RepViT-SAM can achieve nearly 10$\\times$ faster inference than the advanced MobileSAM. Codes and models are available at \\url{https://github.com/THU-MIG/RepViT}.",
        "page": "http://arxiv.org/abs/2307.09283",
        "pdf": "http://arxiv.org/pdf/2307.09283.pdf"
    },
    {
        "title": "EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI",
        "author": "Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Xihui Liu, Cewu Lu, Dahua Lin, Jiangmiao Pang",
        "abstract": "In the realm of computer vision and robotics, embodied agents are expected to explore their environment and carry out human instructions. This necessitates the ability to fully understand 3D scenes given their first-person observations and contextualize them into language for interaction. However, traditional research focuses more on scene-level input and output setups from a global view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric 3D perception dataset and benchmark for holistic 3D scene understanding. It encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which partially align with LVIS, and dense semantic occupancy with 80 common categories. Building upon this database, we introduce a baseline framework named Embodied Perceptron. It is capable of processing an arbitrary number of multi-modal inputs and demonstrates remarkable 3D perception capabilities, both within the two series of benchmarks we set up, i.e., fundamental 3D perception tasks and language-grounded tasks, and in the wild. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.",
        "page": "http://arxiv.org/abs/2312.16170",
        "pdf": "http://arxiv.org/pdf/2312.16170.pdf"
    },
    {
        "title": "Fine-grained Bipartite Concept Factorization for Clustering",
        "author": "Chong Peng, Pengfei Zhang, Yongyong Chen, zhao kang, Chenglizhao Chen, Qiang Cheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FlowDiffuser: Advancing Optical Flow Estimation with Diffusion Models",
        "author": "Ao Luo, XIN LI, Fan Yang, Jiangyu Liu, Haoqiang Fan, Shuaicheng Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction",
        "author": "Hao Li, Ying Chen, Yifei Chen, Rongshan Yu, Wenxian Yang, Liansheng Wang, Bowen Ding, Yuchen Han",
        "abstract": "Whole Slide Image (WSI) classification is often formulated as a Multiple Instance Learning (MIL) problem. Recently, Vision-Language Models (VLMs) have demonstrated remarkable performance in WSI classification. However, existing methods leverage coarse-grained pathogenetic descriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks. Additionally, processing high-resolution WSIs can be computationally expensive. In this paper, we propose a novel \"Fine-grained Visual-Semantic Interaction\" (FiVE) framework for WSI classification. It is designed to enhance the model's generalizability by leveraging the interaction between localized visual patterns and fine-grained pathological semantics. Specifically, with meticulously designed queries, we start by utilizing a large language model to extract fine-grained pathological descriptions from various non-standardized raw reports. The output descriptions are then reconstructed into fine-grained labels used for training. By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable prompts to capture crucial visual information in WSIs, which enhances representation learning and augments generalization capabilities significantly. Furthermore, given that pathological visual patterns are redundantly distributed across tissue slices, we sample a subset of visual instances during training. Our method demonstrates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accuracy in few-shot experiments. The code is available at: https://github.com/ls1rius/WSI_FiVE.",
        "page": "http://arxiv.org/abs/2402.19326",
        "pdf": "http://arxiv.org/pdf/2402.19326.pdf"
    },
    {
        "title": "GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction",
        "author": "Xiao Chen, Quanyi Li, Tai Wang, Tianfan Xue, Jiangmiao Pang",
        "abstract": "While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions.",
        "page": "http://arxiv.org/abs/2402.16174",
        "pdf": "http://arxiv.org/pdf/2402.16174.pdf"
    },
    {
        "title": "InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion",
        "author": "Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim",
        "abstract": "We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy.",
        "page": "http://arxiv.org/abs/2403.17422",
        "pdf": "http://arxiv.org/pdf/2403.17422.pdf"
    },
    {
        "title": "Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing",
        "author": "Bingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, Jun Huang",
        "abstract": "Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have recently gained significant popularity for creative Text-to-image generation. Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE) is of greater importance for application developers, which modify objects or object properties in images by manipulating feature components in attention layers during the generation process. However, little is known about what semantic meanings these attention layers have learned and which parts of the attention maps contribute to the success of image editing. In this paper, we conduct an in-depth probing analysis and demonstrate that cross-attention maps in Stable Diffusion often contain object attribution information that can result in editing failures. In contrast, self-attention maps play a crucial role in preserving the geometric and shape details of the source image during the transformation to the target image. Our analysis offers valuable insights into understanding cross and self-attention maps in diffusion models. Moreover, based on our findings, we simplify popular image editing methods and propose a more straightforward yet more stable and efficient tuning-free procedure that only modifies self-attention maps of the specified attention layers during the denoising process. Experimental results show that our simplified method consistently surpasses the performance of popular approaches on multiple datasets.",
        "page": "http://arxiv.org/abs/2403.03431",
        "pdf": "http://arxiv.org/pdf/2403.03431.pdf"
    },
    {
        "title": "MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints",
        "author": "Pengfei Xie, Wenqiang Xu, Tutian Tang, Zhenjun Yu, Cewu Lu",
        "abstract": "This work proposes a novel learning framework for visual hand dynamics analysis that takes into account the physiological aspects of hand motion. The existing models, which are simplified joint-actuated systems, often produce unnatural motions. To address this, we integrate a musculoskeletal system with a learnable parametric hand model, MANO, to create a new model, MS-MANO. This model emulates the dynamics of muscles and tendons to drive the skeletal system, imposing physiologically realistic constraints on the resulting torque trajectories. We further propose a simulation-in-the-loop pose refinement framework, BioPR, that refines the initial estimated pose through a multi-layer perceptron (MLP) network. Our evaluation of the accuracy of MS-MANO and the efficacy of the BioPR is conducted in two separate parts. The accuracy of MS-MANO is compared with MyoSuite, while the efficacy of BioPR is benchmarked against two large-scale public datasets and two recent state-of-the-art methods. The results demonstrate that our approach consistently improves the baseline methods both quantitatively and qualitatively.",
        "page": "http://arxiv.org/abs/2404.10227",
        "pdf": "http://arxiv.org/pdf/2404.10227.pdf"
    },
    {
        "title": "MemoNav: Working Memory Model for Visual Navigation",
        "author": "Hongxin Li, Zeyu Wang, Xu Yang, yuran Yang, Shuqi Mei, Zhaoxiang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Continual Forgetting for Pre-trained Vision Models",
        "author": "Hongbo Zhao, Bolin Ni, Junsong Fan, Yuxi Wang, Yuntao Chen, Gaofeng Meng, Zhaoxiang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model",
        "author": "Hantao Yao, Rui Zhang, Changsheng Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling",
        "author": "Liwen Wu, Sai Bi, Zexiang Xu, Fujun Luan, Kai Zhang, Iliyan Georgiev, Kalyan Sunkavalli, Ravi Ramamoorthi",
        "abstract": "Novel-view synthesis of specular objects like shiny metals or glossy paints remains a significant challenge. Not only the glossy appearance but also global illumination effects, including reflections of other objects in the environment, are critical components to faithfully reproduce a scene. In this paper, we present Neural Directional Encoding (NDE), a view-dependent appearance encoding of neural radiance fields (NeRF) for rendering specular objects. NDE transfers the concept of feature-grid-based spatial encoding to the angular domain, significantly improving the ability to model high-frequency angular signals. In contrast to previous methods that use encoding functions with only angular input, we additionally cone-trace spatial features to obtain a spatially varying directional encoding, which addresses the challenging interreflection effects. Extensive experiments on both synthetic and real datasets show that a NeRF model with NDE (1) outperforms the state of the art on view synthesis of specular objects, and (2) works with small networks to allow fast (real-time) inference. The project webpage and source code are available at: \\url{https://lwwu2.github.io/nde/}.",
        "page": "http://arxiv.org/abs/2405.14847",
        "pdf": "http://arxiv.org/pdf/2405.14847.pdf"
    },
    {
        "title": "In2SET: Intra-Inter Similarity Exploiting Transformer for  Dual-Camera Compressive Hyperspectral Imaging",
        "author": "Xin Wang, Lizhi Wang, Xiangtian Ma, Maoqing Zhang, Lin Zhu, Hua Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Calibrated Multi-label Deep Neural Networks",
        "author": "Jiacheng Cheng, Nuno Vasconcelos",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning to Transform Dynamically for Better Adversarial Transferability",
        "author": "Rongyi Zhu, Zeliang Zhang, Susan Liang, Zhuo Liu, Chenliang Xu",
        "abstract": "Adversarial examples, crafted by adding perturbations imperceptible to humans, can deceive neural networks. Recent studies identify the adversarial transferability across various models, \\textit{i.e.}, the cross-model attack ability of adversarial samples. To enhance such adversarial transferability, existing input transformation-based methods diversify input data with transformation augmentation. However, their effectiveness is limited by the finite number of available transformations. In our study, we introduce a novel approach named Learning to Transform (L2T). L2T increases the diversity of transformed images by selecting the optimal combination of operations from a pool of candidates, consequently improving adversarial transferability. We conceptualize the selection of optimal transformation combinations as a trajectory optimization problem and employ a reinforcement learning strategy to effectively solve the problem. Comprehensive experiments on the ImageNet dataset, as well as practical tests with Google Vision and GPT-4V, reveal that L2T surpasses current methodologies in enhancing adversarial transferability, thereby confirming its effectiveness and practical significance. The code is available at https://github.com/RongyiZhu/L2T.",
        "page": "http://arxiv.org/abs/2405.14077",
        "pdf": "http://arxiv.org/pdf/2405.14077.pdf"
    },
    {
        "title": "Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection",
        "author": "Yicheng Xiao, Zhuoyan Luo, Yong Liu, Yue Ma, Hengwei Bian, Yatai Ji, Yujiu Yang, Xiu Li",
        "abstract": "Video Moment Retrieval (MR) and Highlight Detection (HD) have attracted significant attention due to the growing demand for video analysis. Recent approaches treat MR and HD as similar video grounding problems and address them together with transformer-based architecture. However, we observe that the emphasis of MR and HD differs, with one necessitating the perception of local relationships and the other prioritizing the understanding of global contexts. Consequently, the lack of task-specific design will inevitably lead to limitations in associating the intrinsic specialty of two tasks. To tackle the issue, we propose a Unified Video COMprehension framework (UVCOM) to bridge the gap and jointly solve MR and HD effectively. By performing progressive integration on intra and inter-modality across multi-granularity, UVCOM achieves the comprehensive understanding in processing a video. Moreover, we present multi-aspect contrastive learning to consolidate the local relation modeling and global knowledge accumulation via well aligned multi-modal space. Extensive experiments on QVHighlights, Charades-STA, TACoS , YouTube Highlights and TVSum datasets demonstrate the effectiveness and rationality of UVCOM which outperforms the state-of-the-art methods by a remarkable margin.",
        "page": "http://arxiv.org/abs/2311.16464",
        "pdf": "http://arxiv.org/pdf/2311.16464.pdf"
    },
    {
        "title": "GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting",
        "author": "Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diffusion-driven GAN Inversion for Multi-Modal Face Image Generation",
        "author": "Jihyun Kim, Changjae Oh, Hoseok Do, Soohyun Kim, Kwanghoon Sohn",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Universal Robustness via Median Random Smoothing for Real-World Super-Resolution",
        "author": "Zakariya Chaouai, Mohamed Tamaazousti",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Perceptual Assessment and Optimization of HDR Image Rendering",
        "author": "Peibei Cao, Rafal Mantiuk, Kede Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion",
        "author": "Sofia Casarin, Cynthia Ugwu, Sergio Escalera, Oswald Lanz",
        "abstract": "The landscape of deep learning research is moving towards innovative strategies to harness the true potential of data. Traditionally, emphasis has been on scaling model architectures, resulting in large and complex neural networks, which can be difficult to train with limited computational resources. However, independently of the model size, data quality (i.e. amount and variability) is still a major factor that affects model generalization. In this work, we propose a novel technique to exploit available data through the use of automatic data augmentation for the tasks of image classification and semantic segmentation. We introduce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be processed as videos. Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day. Our intuition is that the increased receptive field in the temporal dimension provided by DAS could lead to benefits also to the spatial receptive field. More specifically, we leverage DAS to guide the reshaping of the spatial receptive field by selecting task-dependant transformations. As a result, compared to standard augmentation alternatives, we improve in terms of accuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when plugging-in our DAS over different light-weight video backbones.",
        "page": "http://arxiv.org/abs/2403.15194",
        "pdf": "http://arxiv.org/pdf/2403.15194.pdf"
    },
    {
        "title": "PNeRV: Enhancing Spatial Consistency via Pyramidal Neural Representation for Videos",
        "author": "Qi Zhao, M. Salman Asif, Zhan Ma",
        "abstract": "The primary focus of Neural Representation for Videos (NeRV) is to effectively model its spatiotemporal consistency. However, current NeRV systems often face a significant issue of spatial inconsistency, leading to decreased perceptual quality. To address this issue, we introduce the Pyramidal Neural Representation for Videos (PNeRV), which is built on a multi-scale information connection and comprises a lightweight rescaling operator, Kronecker Fully-connected layer (KFc), and a Benign Selective Memory (BSM) mechanism. The KFc, inspired by the tensor decomposition of the vanilla Fully-connected layer, facilitates low-cost rescaling and global correlation modeling. BSM merges high-level features with granular ones adaptively. Furthermore, we provide an analysis based on the Universal Approximation Theory of the NeRV system and validate the effectiveness of the proposed PNeRV.We conducted comprehensive experiments to demonstrate that PNeRV surpasses the performance of contemporary NeRV models, achieving the best results in video regression on UVG and DAVIS under various metrics (PSNR, SSIM, LPIPS, and FVD). Compared to vanilla NeRV, PNeRV achieves a +4.49 dB gain in PSNR and a 231% increase in FVD on UVG, along with a +3.28 dB PSNR and 634% FVD increase on DAVIS.",
        "page": "http://arxiv.org/abs/2404.08921",
        "pdf": "http://arxiv.org/pdf/2404.08921.pdf"
    },
    {
        "title": "Kernel Adaptive Convolution for Scene Text Detection via Distance Map Prediction",
        "author": "Jinzhi Zheng, Heng Fan, Libo Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes",
        "author": "Soubhik Sanyal, Partha Ghosh, Jinlong Yang, Michael J. Black, Justus Thies, Timo Bolkart",
        "abstract": "We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns to represent the geometry and appearance distribution of clothed human bodies. Training such a model is challenging, as datasets of textured 3D meshes for humans are limited in size and accessibility. Our key observation is that there exist medium-sized 3D scan datasets like CAPE, as well as large-scale 2D image datasets of clothed humans and multiple appearances can be mapped to a single geometry. To effectively learn from the two data modalities, we propose an unpaired learning procedure for pose-dependent clothed and textured human meshes. Specifically, we learn a pose-dependent geometry space from 3D scan data. We represent this as per vertex displacements w.r.t. the SMPL model. Next, we train a geometry conditioned texture generator in an unsupervised way using the 2D image data. We use intermediate activations of the learned geometry model to condition our texture generator. To alleviate entanglement between pose and clothing type, and pose and clothing appearance, we condition both the texture and geometry generators with attribute labels such as clothing types for the geometry, and clothing colors for the texture generator. We automatically generated these conditioning labels for the 2D images based on the visual question answering model BLIP and CLIP. We validate our method on the SCULPT dataset, and compare to state-of-the-art 3D generative models for clothed human bodies. Our code and data can be found at https://sculpt.is.tue.mpg.de.",
        "page": "http://arxiv.org/abs/2308.10638",
        "pdf": "http://arxiv.org/pdf/2308.10638.pdf"
    },
    {
        "title": "Gradient Reweighting: Towards Imbalanced Class-Incremental Learning",
        "author": "Jiangpeng He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Accurate and Robust Architectures via Neural Architecture Search",
        "author": "Yuwei Ou, Yuqi Feng, Yanan Sun",
        "abstract": "To defend deep neural networks from adversarial attacks, adversarial training has been drawing increasing attention for its effectiveness. However, the accuracy and robustness resulting from the adversarial training are limited by the architecture, because adversarial training improves accuracy and robustness by adjusting the weight connection affiliated to the architecture. In this work, we propose ARNAS to search for accurate and robust architectures for adversarial training. First we design an accurate and robust search space, in which the placement of the cells and the proportional relationship of the filter numbers are carefully determined. With the design, the architectures can obtain both accuracy and robustness by deploying accurate and robust structures to their sensitive positions, respectively. Then we propose a differentiable multi-objective search strategy, performing gradient descent towards directions that are beneficial for both natural loss and adversarial loss, thus the accuracy and robustness can be guaranteed at the same time. We conduct comprehensive experiments in terms of white-box attacks, black-box attacks, and transferability. Experimental results show that the searched architecture has the strongest robustness with the competitive accuracy, and breaks the traditional idea that NAS-based architectures cannot transfer well to complex tasks in robustness scenarios. By analyzing outstanding architectures searched, we also conclude that accurate and robust neural architectures tend to deploy different structures near the input and output, which has great practical significance on both hand-crafting and automatically designing of accurate and robust architectures.",
        "page": "http://arxiv.org/abs/2405.05502",
        "pdf": "http://arxiv.org/pdf/2405.05502.pdf"
    },
    {
        "title": "BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection",
        "author": "Zhenxin Li, Shiyi Lan, Jose M. Alvarez, Zuxuan Wu",
        "abstract": "Recently, the rise of query-based Transformer decoders is reshaping camera-based 3D object detection. These query-based decoders are surpassing the traditional dense BEV (Bird's Eye View)-based methods. However, we argue that dense BEV frameworks remain important due to their outstanding abilities in depth estimation and object localization, depicting 3D scenes accurately and comprehensively. This paper aims to address the drawbacks of the existing dense BEV-based 3D object detectors by introducing our proposed enhanced components, including a CRF-modulated depth estimation module enforcing object-level consistencies, a long-term temporal aggregation module with extended receptive fields, and a two-stage object decoder combining perspective techniques with CRF-modulated depth embedding. These enhancements lead to a \"modernized\" dense BEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms both BEV-based and query-based frameworks under various settings, achieving a state-of-the-art result of 64.2 NDS on the nuScenes test set. Code will be available at \\url{https://github.com/woxihuanjiangguo/BEVNeXt}.",
        "page": "http://arxiv.org/abs/2312.01696",
        "pdf": "http://arxiv.org/pdf/2312.01696.pdf"
    },
    {
        "title": "D3still: Decoupled Differential Distillation for Asymmetric Image Retrieval",
        "author": "Yi Xie, Yihong Lin, Wenjie Cai, Xuemiao Xu, Huaidong Zhang, Yong Du, Shengfeng He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Empowering Resampling Operation for Ultra-High-Definition Image Enhancement with Model-Aware Guidance",
        "author": "Yu, Jie Huang, Li, Kaiwen Zheng, Qi Zhu, Man Zhou, Feng Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "3D Neural Edge Reconstruction",
        "author": "Lei Li, Songyou Peng, Zehao Yu, Shaohui Liu, R\u00e9mi Pautrat, Xiaochuan Yin, Marc Pollefeys",
        "abstract": "Real-world objects and environments are predominantly composed of edge features, including straight lines and curves. Such edges are crucial elements for various applications, such as CAD modeling, surface meshing, lane mapping, etc. However, existing traditional methods only prioritize lines over curves for simplicity in geometric modeling. To this end, we introduce EMAP, a new method for learning 3D edge representations with a focus on both lines and curves. Our method implicitly encodes 3D edge distance and direction in Unsigned Distance Functions (UDF) from multi-view edge maps. On top of this neural representation, we propose an edge extraction algorithm that robustly abstracts parametric 3D edges from the inferred edge points and their directions. Comprehensive evaluations demonstrate that our method achieves better 3D edge reconstruction on multiple challenging datasets. We further show that our learned UDF field enhances neural surface reconstruction by capturing more details.",
        "page": "http://arxiv.org/abs/2405.19295",
        "pdf": "http://arxiv.org/pdf/2405.19295.pdf"
    },
    {
        "title": "Human Gaussian Splatting : Real-time Rendering of Animatable Avatars",
        "author": "Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, Eduardo P\u00e9rez-Pellitero",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Detailed and Robust 3D Clothed Human Reconstruction with High-Frequency and Low-Frequency Information of Parametric Body Models",
        "author": "Yifan Yang, Dong Liu, Shuhai Zhang, Zeshuai Deng, Zixiong Huang, Mingkui Tan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Stable Neighbor Denoising for Source-free Domain Adaptive Segmentation.",
        "author": "Dong Zhao, Shuang Wang, Qi Zang, Licheng Jiao, Nicu Sebe, Zhun Zhong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction",
        "author": "Devikalyan Das, Christopher Wewer, Raza Yunus, Eddy Ilg, Jan Lenssen",
        "abstract": "Reconstructing dynamic objects from monocular videos is a severely underconstrained and challenging problem, and recent work has approached it in various directions. However, owing to the ill-posed nature of this problem, there has been no solution that can provide consistent, high-quality novel views from camera positions that are significantly different from the training views. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on this challenge by imposing a two-stage approach: first, we fit a low-rank neural deformation model, which then is used as regularization for non-rigid reconstruction in the second stage. The first stage learns the object's deformations such that it preserves consistency in novel views. The second stage obtains high reconstruction quality by optimizing 3D Gaussians that are driven by the coarse model. To this end, we introduce a local 3D Gaussian representation, where temporally shared Gaussians are anchored in and deformed by local oriented volumes. The resulting combined model can be rendered as radiance fields, resulting in high-quality photo-realistic reconstructions of the non-rigidly deforming objects. We demonstrate that NPGs achieve superior results compared to previous works, especially in challenging scenarios with few multi-view cues.",
        "page": "http://arxiv.org/abs/2312.01196",
        "pdf": "http://arxiv.org/pdf/2312.01196.pdf"
    },
    {
        "title": "Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation",
        "author": "Jin Wang, Bingfeng Zhang, Jian Pang, Honglong Chen, Weifeng Liu",
        "abstract": "Few-shot segmentation remains challenging due to the limitations of its labeling information for unseen classes. Most previous approaches rely on extracting high-level feature maps from the frozen visual encoder to compute the pixel-wise similarity as a key prior guidance for the decoder. However, such a prior representation suffers from coarse granularity and poor generalization to new classes since these high-level feature maps have obvious category bias. In this work, we propose to replace the visual prior representation with the visual-text alignment capacity to capture more reliable guidance and enhance the model generalization. Specifically, we design two kinds of training-free prior information generation strategy that attempts to utilize the semantic alignment capability of the Contrastive Language-Image Pre-training model (CLIP) to locate the target class. Besides, to acquire more accurate prior guidance, we build a high-order relationship of attention maps and utilize it to refine the initial prior information. Experiments on both the PASCAL-5{i} and COCO-20{i} datasets show that our method obtains a clearly substantial improvement and reaches the new state-of-the-art performance.",
        "page": "http://arxiv.org/abs/2405.08458",
        "pdf": "http://arxiv.org/pdf/2405.08458.pdf"
    },
    {
        "title": "CapHuman: Capture Your Moments in Parallel Universes",
        "author": "Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "In-distribution Public Data Synthesis with Diffusion Models for Differentially Private Image Classification",
        "author": "Jinseong Park, Yujin Choi, Jaewook Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Self-Adaptive Reality-Guided Diffusion for Artifact-Free Super-Resolution",
        "author": "Qingping Zheng, Ling Zheng, Yuanfan Guo, Ying Li, Songcen Xu, Jiankang Deng, Hang Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unmixing before Fusion: A Generalized Paradigm for Multi-Source-based Hyperspectral Image Synthesis",
        "author": "Yang Yu, Erting Pan, Xinya Wang, Yuheng Wu, Xiaoguang Mei, Jiayi Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model",
        "author": "Lirui Zhao, Yue Yang, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, Rongrong Ji",
        "abstract": "Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent's capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. Our evaluations reveal that DiffAgent not only excels in identifying the appropriate T2I API but also underscores the effectiveness of the SFTA training framework. Codes are available at https://github.com/OpenGVLab/DiffAgent.",
        "page": "http://arxiv.org/abs/2404.01342",
        "pdf": "http://arxiv.org/pdf/2404.01342.pdf"
    },
    {
        "title": "Riemannian Multinomial Logistics Regression for SPD Neural Networks",
        "author": "Ziheng Chen, Yue Song, Gaowen Liu, Ramana Kompella, Xiaojun Wu, Nicu Sebe",
        "abstract": "Deep neural networks for learning Symmetric Positive Definite (SPD) matrices are gaining increasing attention in machine learning. Despite the significant progress, most existing SPD networks use traditional Euclidean classifiers on an approximated space rather than intrinsic classifiers that accurately capture the geometry of SPD manifolds. Inspired by Hyperbolic Neural Networks (HNNs), we propose Riemannian Multinomial Logistics Regression (RMLR) for the classification layers in SPD networks. We introduce a unified framework for building Riemannian classifiers under the metrics pulled back from the Euclidean space, and showcase our framework under the parameterized Log-Euclidean Metric (LEM) and Log-Cholesky Metric (LCM). Besides, our framework offers a novel intrinsic explanation for the most popular LogEig classifier in existing SPD networks. The effectiveness of our method is demonstrated in three applications: radar recognition, human action recognition, and electroencephalography (EEG) classification. The code is available at https://github.com/GitZH-Chen/SPDMLR.git.",
        "page": "http://arxiv.org/abs/2305.11288",
        "pdf": "http://arxiv.org/pdf/2305.11288.pdf"
    },
    {
        "title": "Programmable Motion Generation for Open-set Motion Control Tasks",
        "author": "Hanchao Liu, Xiaohang Zhan, Shaoli Huang, Tai-Jiang Mu, Ying Shan",
        "abstract": "Character animation in real-world scenarios necessitates a variety of constraints, such as trajectories, key-frames, interactions, etc. Existing methodologies typically treat single or a finite set of these constraint(s) as separate control tasks. They are often specialized, and the tasks they address are rarely extendable or customizable. We categorize these as solutions to the close-set motion control problem. In response to the complexity of practical motion control, we propose and attempt to solve the open-set motion control problem. This problem is characterized by an open and fully customizable set of motion control tasks. To address this, we introduce a new paradigm, programmable motion generation. In this paradigm, any given motion control task is broken down into a combination of atomic constraints. These constraints are then programmed into an error function that quantifies the degree to which a motion sequence adheres to them. We utilize a pre-trained motion generation model and optimize its latent code to minimize the error function of the generated motion. Consequently, the generated motion not only inherits the prior of the generative model but also satisfies the required constraints. Experiments show that we can generate high-quality motions when addressing a wide range of unseen tasks. These tasks encompass motion control by motion dynamics, geometric constraints, physical laws, interactions with scenes, objects or the character own body parts, etc. All of these are achieved in a unified approach, without the need for ad-hoc paired training data collection or specialized network designs. During the programming of novel tasks, we observed the emergence of new skills beyond those of the prior model. With the assistance of large language models, we also achieved automatic programming. We hope that this work will pave the way for the motion control of general AI agents.",
        "page": "http://arxiv.org/abs/2405.19283",
        "pdf": "http://arxiv.org/pdf/2405.19283.pdf"
    },
    {
        "title": "How to Configure Good In-Context Sequence for Visual Question Answering",
        "author": "Li Li, Jiawei Peng, huiyi chen, Chongyang Gao, Xu Yang",
        "abstract": "Inspired by the success of Large Language Models in dealing with new tasks via In-Context Learning (ICL) in NLP, researchers have also developed Large Vision-Language Models (LVLMs) with ICL capabilities. However, when implementing ICL using these LVLMs, researchers usually resort to the simplest way like random sampling to configure the in-context sequence, thus leading to sub-optimal results. To enhance the ICL performance, in this study, we use Visual Question Answering (VQA) as case study to explore diverse in-context configurations to find the powerful ones. Additionally, through observing the changes of the LVLM outputs by altering the in-context sequence, we gain insights into the inner properties of LVLMs, improving our understanding of them. Specifically, to explore in-context configurations, we design diverse retrieval methods and employ different strategies to manipulate the retrieved demonstrations. Through exhaustive experiments on three VQA datasets: VQAv2, VizWiz, and OK-VQA, we uncover three important inner properties of the applied LVLM and demonstrate which strategies can consistently improve the ICL VQA performance. Our code is provided in: https://github.com/GaryJiajia/OFv2_ICL_VQA.",
        "page": "http://arxiv.org/abs/2312.01571",
        "pdf": "http://arxiv.org/pdf/2312.01571.pdf"
    },
    {
        "title": "Open-Vocabulary Semantic Segmentation with Image Embedding Balancing",
        "author": "Xiangheng Shan, Dongyue Wu, Guilin Zhu, Yuanjie Shao, Nong Sang, Changxin Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking Few-shot 3D Point Cloud Semantic Segmentation",
        "author": "Zhaochong An, Guolei Sun, Yun Liu, Fayao Liu, Zongwei Wu, Dan Wang, Luc Van Gool, Serge Belongie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-device Queries",
        "author": "Huajian Huang, Changkun Liu, Yipeng Zhu, Hui Cheng, Tristan Braud, Sai-Kit Yeung",
        "abstract": "Portable 360$^\\circ$ cameras are becoming a cheap and efficient tool to establish large visual databases. By capturing omnidirectional views of a scene, these cameras could expedite building environment models that are essential for visual localization. However, such an advantage is often overlooked due to the lack of valuable datasets. This paper introduces a new benchmark dataset, 360Loc, composed of 360$^\\circ$ images with ground truth poses for visual localization. We present a practical implementation of 360$^\\circ$ mapping combining 360$^\\circ$ images with lidar data to generate the ground truth 6DoF poses. 360Loc is the first dataset and benchmark that explores the challenge of cross-device visual positioning, involving 360$^\\circ$ reference frames, and query frames from pinhole, ultra-wide FoV fisheye, and 360$^\\circ$ cameras. We propose a virtual camera approach to generate lower-FoV query frames from 360$^\\circ$ images, which ensures a fair comparison of performance among different query types in visual localization tasks. We also extend this virtual camera approach to feature matching-based and pose regression-based methods to alleviate the performance loss caused by the cross-device domain gap, and evaluate its effectiveness against state-of-the-art baselines. We demonstrate that omnidirectional visual localization is more robust in challenging large-scale scenes with symmetries and repetitive structures. These results provide new insights into 360-camera mapping and omnidirectional visual localization with cross-device queries.",
        "page": "http://arxiv.org/abs/2311.17389",
        "pdf": "http://arxiv.org/pdf/2311.17389.pdf"
    },
    {
        "title": "MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant",
        "author": "Chenlu Zhan, Gaoang Wang, Yu LIN, Hongwei Wang, Jian Wu",
        "abstract": "Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model. Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation. By conditioning the adaptive cross-guided parameters into the multi-flow diffusion framework, our model promotes flexible interactions among medical multi-modal for generation. MedM2G is the first medical generative model that unifies medical generation tasks of text-to-image, image-to-text, and unified generation of medical modalities (CT, MRI, X-ray). It performs 5 medical generation tasks across 10 datasets, consistently outperforming various state-of-the-art works.",
        "page": "http://arxiv.org/abs/2403.04290",
        "pdf": "http://arxiv.org/pdf/2403.04290.pdf"
    },
    {
        "title": "Intraoperative 2D/3D Image Registration via Differentiable X-ray Rendering",
        "author": "Vivek Gopalakrishnan, Neel Dey, Polina Golland",
        "abstract": "Surgical decisions are informed by aligning rapid portable 2D intraoperative images (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g., CT). 2D/3D image registration often fails in practice: conventional optimization methods are prohibitively slow and susceptible to local minima, while neural networks trained on small datasets fail on new patients or require impractical landmark supervision. We present DiffPose, a self-supervised approach that leverages patient-specific simulation and differentiable physics-based rendering to achieve accurate 2D/3D registration without relying on manually labeled data. Preoperatively, a CNN is trained to regress the pose of a randomly oriented synthetic X-ray rendered from the preoperative CT. The CNN then initializes rapid intraoperative test-time optimization that uses the differentiable X-ray renderer to refine the solution. Our work further proposes several geometrically principled methods for sampling camera poses from $\\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving registration in the tangent space $\\mathfrak{se}(3)$ with geodesic and multiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy across surgical datasets at intraoperative speeds, improving upon existing unsupervised methods by an order of magnitude and even outperforming supervised baselines. Our code is available at https://github.com/eigenvivek/DiffPose.",
        "page": "http://arxiv.org/abs/2312.06358",
        "pdf": "http://arxiv.org/pdf/2312.06358.pdf"
    },
    {
        "title": "Multi-Attribute Interactions Matter for 3D Visual Grounding",
        "author": "Can Xu, Yuehui Han, Rui Xu, Le Hui, Jin Xie, Jian Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "From Activation to Initialization: Scaling Insights for Optimizing Neural Fields",
        "author": "Hemanth Saratchandran, Sameera Ramasinghe, Simon Lucey",
        "abstract": "In the realm of computer vision, Neural Fields have gained prominence as a contemporary tool harnessing neural networks for signal representation. Despite the remarkable progress in adapting these networks to solve a variety of problems, the field still lacks a comprehensive theoretical framework. This article aims to address this gap by delving into the intricate interplay between initialization and activation, providing a foundational basis for the robust optimization of Neural Fields. Our theoretical insights reveal a deep-seated connection among network initialization, architectural choices, and the optimization process, emphasizing the need for a holistic approach when designing cutting-edge Neural Fields.",
        "page": "http://arxiv.org/abs/2403.19205",
        "pdf": "http://arxiv.org/pdf/2403.19205.pdf"
    },
    {
        "title": "MatFuse: Controllable Material Generation with Diffusion Models",
        "author": "Giuseppe Vecchio, Renato Sortino, Simone Palazzo, Concetto Spampinato",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching",
        "author": "Matteo Bastico, Etienne Decenci\u00e8re, Laurent Cort\u00e9, Yannick TILLIER, David Ryckelynck",
        "abstract": "Point cloud matching, a crucial technique in computer vision, medical and robotics fields, is primarily concerned with finding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights on the paired geometries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account fine local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian, that allows to easily generate aligned eigenspaces for multiple rigidly-registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We initially evaluate the performance of the proposed technique in a point-wise manner, specifically focusing on the task of object anomaly localization using the MVTec 3D-AD dataset. Additionally, we define a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks. The code to reproduce our experiments is publicly available at https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary Code.",
        "page": "http://arxiv.org/abs/2402.17372",
        "pdf": "http://arxiv.org/pdf/2402.17372.pdf"
    },
    {
        "title": "Logit Standardization in Knowledge Distillation",
        "author": "Shangquan Sun, Wenqi Ren, Jingzhi Li, Rui Wang, Xiaochun Cao",
        "abstract": "Knowledge distillation involves transferring soft labels from a teacher to a student using a shared temperature-based softmax function. However, the assumption of a shared temperature between teacher and student implies a mandatory exact match between their logits in terms of logit range and variance. This side-effect limits the performance of student, considering the capacity discrepancy between them and the finding that the innate logit relations of teacher are sufficient for student to learn. To address this issue, we propose setting the temperature as the weighted standard deviation of logit and performing a plug-and-play Z-score pre-process of logit standardization before applying softmax and Kullback-Leibler divergence. Our pre-process enables student to focus on essential logit relations from teacher rather than requiring a magnitude match, and can improve the performance of existing logit-based distillation methods. We also show a typical case where the conventional setting of sharing temperature between teacher and student cannot reliably yield the authentic distillation evaluation; nonetheless, this challenge is successfully alleviated by our Z-score. We extensively evaluate our method for various student and teacher models on CIFAR-100 and ImageNet, showing its significant superiority. The vanilla knowledge distillation powered by our pre-process can achieve favorable performance against state-of-the-art methods, and other distillation variants can obtain considerable gain with the assistance of our pre-process.",
        "page": "http://arxiv.org/abs/2403.01427",
        "pdf": "http://arxiv.org/pdf/2403.01427.pdf"
    },
    {
        "title": "PeVL: Pose-Enhanced Vision-Language Model for Fine-Grained Human Action Recognition",
        "author": "Haosong Zhang, Mei Leong, Liyuan Li, Weisi Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PAD: Patch-Agnostic Defense against Adversarial Patch Attacks",
        "author": "Lihua Jing, Rui Wang, Wenqi Ren, Xin Dong, Cong Zou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images",
        "author": "Yushuang Wu, Luyue Shi, Junhao Cai, Weihao Yuan, Lingteng Qiu, Zilong Dong, Liefeng Bo, Shuguang Cui, Xiaoguang Han",
        "abstract": "Generalizable 3D object reconstruction from single-view RGB-D images remains a challenging task, particularly with real-world data. Current state-of-the-art methods develop Transformer-based implicit field learning, necessitating an intensive learning paradigm that requires dense query-supervision uniformly sampled throughout the entire space. We propose a novel approach, IPoD, which harmonizes implicit field learning with point diffusion. This approach treats the query points for implicit field learning as a noisy point cloud for iterative denoising, allowing for their dynamic adaptation to the target object shape. Such adaptive query points harness diffusion learning's capability for coarse shape recovery and also enhances the implicit representation's ability to delineate finer details. Besides, an additional self-conditioning mechanism is designed to use implicit predictions as the guidance of diffusion learning, leading to a cooperative system. Experiments conducted on the CO3D-v2 dataset affirm the superiority of IPoD, achieving 7.8% improvement in F-score and 28.6% in Chamfer distance over existing methods. The generalizability of IPoD is also demonstrated on the MVImgNet dataset. Our project page is at https://yushuang-wu.github.io/IPoD.",
        "page": "http://arxiv.org/abs/2404.00269",
        "pdf": "http://arxiv.org/pdf/2404.00269.pdf"
    },
    {
        "title": "Virtual Immunohistochemistry Staining for Histological Images Assisted by Weakly-supervised Learning",
        "author": "Jiahan Li, Jiuyang Dong, Shenjin Huang, Xi Li, Junjun Jiang, Xiaopeng Fan, Yongbing Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TexOct: Generating Textures of 3D Models with Octree-based Diffusion",
        "author": "Jialun Liu, Chenming Wu, Xinqi Liu, Xing Liu, Jinbo Wu, Haotian Peng, Chen Zhao, Haocheng Feng, Jingtuo Liu, Errui Ding",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "$\\textbf{LaRE}^2$: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection",
        "author": "Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning",
        "author": "Shiming Chen, Wenjin Hou, Salman Khan, Fahad Shahbaz Khan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "3D-Aware Face Editing via Warping-Guided Latent Direction Learning",
        "author": "Yuhao Cheng, Zhuo Chen, Xingyu Ren, Wenhan Zhu, Zhengqin Xu, Di Xu, Yang Changpeng, Yichao Yan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation",
        "author": "Lingjun Zhao, Jingyu Song, Katherine Skinner",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance",
        "author": "Phuc Nguyen, Tuan Duc Ngo, Evangelos Kalogerakis, Chuang Gan, Anh Tran, Cuong Pham, Khoi Nguyen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition",
        "author": "Qixuan Zheng, Ming Zhang, Hong Yan",
        "abstract": "To achieve greater accuracy, hypergraph matching algorithms require exponential increases in computational resources. Recent kd-tree-based approximate nearest neighbor (ANN) methods, despite the sparsity of their compatibility tensor, still require exhaustive calculations for large-scale graph matching. This work utilizes CUR tensor decomposition and introduces a novel cascaded second and third-order hypergraph matching framework (CURSOR) for efficient hypergraph matching. A CUR-based second-order graph matching algorithm is used to provide a rough match, and then the core of CURSOR, a fiber-CUR-based tensor generation method, directly calculates entries of the compatibility tensor by leveraging the initial second-order match result. This significantly decreases the time complexity and tensor density. A probability relaxation labeling (PRL)-based matching algorithm, especially suitable for sparse tensors, is developed. Experiment results on large-scale synthetic datasets and widely-adopted benchmark sets demonstrate the superiority of CURSOR over existing methods. The tensor generation method in CURSOR can be integrated seamlessly into existing hypergraph matching methods to improve their performance and lower their computational costs.",
        "page": "http://arxiv.org/abs/2402.16594",
        "pdf": "http://arxiv.org/pdf/2402.16594.pdf"
    },
    {
        "title": "UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity",
        "author": "Jialong Zuo, Hanyu Zhou, Ying Nie, Feng Zhang, Tianyu Guo, Nong Sang, Yunhe Wang, Changxin Gao",
        "abstract": "Existing text-based person retrieval datasets often have relatively coarse-grained text annotations. This hinders the model to comprehend the fine-grained semantics of query texts in real scenarios. To address this problem, we contribute a new benchmark named \\textbf{UFineBench} for text-based person retrieval with ultra-fine granularity. Firstly, we construct a new \\textbf{dataset} named UFine6926. We collect a large number of person images and manually annotate each image with two detailed textual descriptions, averaging 80.8 words each. The average word count is three to four times that of the previous datasets. In addition of standard in-domain evaluation, we also propose a special \\textbf{evaluation paradigm} more representative of real scenarios. It contains a new evaluation set with cross domains, cross textual granularity and cross textual styles, named UFine3C, and a new evaluation metric for accurately measuring retrieval ability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a more efficient \\textbf{algorithm} especially designed for text-based person retrieval with ultra fine-grained texts. It achieves fine granularity mining by adopting a shared cross-modal granularity decoder and hard negative match mechanism. With standard in-domain evaluation, CFAM establishes competitive performance across various datasets, especially on our ultra fine-grained UFine6926. Furthermore, by evaluating on UFine3C, we demonstrate that training on our UFine6926 significantly improves generalization to real scenarios compared with other coarse-grained datasets. The dataset and code will be made publicly available at \\url{https://github.com/Zplusdragon/UFineBench}.",
        "page": "http://arxiv.org/abs/2312.03441",
        "pdf": "http://arxiv.org/pdf/2312.03441.pdf"
    },
    {
        "title": "Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters",
        "author": "Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, You He",
        "abstract": "Continual learning can empower vision-language models to continuously acquire new knowledge, without the need for access to the entire historical dataset. However, mitigating the performance degradation in large-scale models is non-trivial due to (i) parameter shifts throughout lifelong learning and (ii) significant computational burdens associated with full-model tuning. In this work, we present a parameter-efficient continual learning framework to alleviate long-term forgetting in incremental learning with vision-language models. Our approach involves the dynamic expansion of a pre-trained CLIP model, through the integration of Mixture-of-Experts (MoE) adapters in response to new tasks. To preserve the zero-shot recognition capability of vision-language models, we further introduce a Distribution Discriminative Auto-Selector (DDAS) that automatically routes in-distribution and out-of-distribution inputs to the MoE Adapter and the original CLIP, respectively. Through extensive experiments across various settings, our proposed method consistently outperforms previous state-of-the-art approaches while concurrently reducing parameter training burdens by 60%. Our code locates at https://github.com/JiazuoYu/MoE-Adapters4CL",
        "page": "http://arxiv.org/abs/2403.11549",
        "pdf": "http://arxiv.org/pdf/2403.11549.pdf"
    },
    {
        "title": "A Conditional Denoising Diffusion Probabilistic Model for Point Cloud Upsampling",
        "author": "Wentao Qu, Yuantian Shao, Lingwu Meng, Xiaoshui Huang, Liang Xiao",
        "abstract": "Point cloud upsampling (PCU) enriches the representation of raw point clouds, significantly improving the performance in downstream tasks such as classification and reconstruction. Most of the existing point cloud upsampling methods focus on sparse point cloud feature extraction and upsampling module design. In a different way, we dive deeper into directly modelling the gradient of data distribution from dense point clouds. In this paper, we proposed a conditional denoising diffusion probability model (DDPM) for point cloud upsampling, called PUDM. Specifically, PUDM treats the sparse point cloud as a condition, and iteratively learns the transformation relationship between the dense point cloud and the noise. Simultaneously, PUDM aligns with a dual mapping paradigm to further improve the discernment of point features. In this context, PUDM enables learning complex geometry details in the ground truth through the dominant features, while avoiding an additional upsampling module design. Furthermore, to generate high-quality arbitrary-scale point clouds during inference, PUDM exploits the prior knowledge of the scale between sparse point clouds and dense point clouds during training by parameterizing a rate factor. Moreover, PUDM exhibits strong noise robustness in experimental results. In the quantitative and qualitative evaluations on PU1K and PUGAN, PUDM significantly outperformed existing methods in terms of Chamfer Distance (CD) and Hausdorff Distance (HD), achieving state of the art (SOTA) performance.",
        "page": "http://arxiv.org/abs/2312.02719",
        "pdf": "http://arxiv.org/pdf/2312.02719.pdf"
    },
    {
        "title": "InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization",
        "author": "Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, Di Huang",
        "abstract": "Recent strides in the development of diffusion models, exemplified by advancements such as Stable Diffusion, have underscored their remarkable prowess in generating visually compelling images. However, the imperative of achieving a seamless alignment between the generated image and the provided prompt persists as a formidable challenge. This paper traces the root of these difficulties to invalid initial noise, and proposes a solution in the form of Initial Noise Optimization (InitNO), a paradigm that refines this noise. Considering text prompts, not all random noises are effective in synthesizing semantically-faithful images. We design the cross-attention response score and the self-attention conflict score to evaluate the initial noise, bifurcating the initial latent space into valid and invalid sectors. A strategically crafted noise optimization pipeline is developed to guide the initial noise towards valid regions. Our method, validated through rigorous experimentation, shows a commendable proficiency in generating images in strict accordance with text prompts. Our code is available at https://github.com/xiefan-guo/initno.",
        "page": "http://arxiv.org/abs/2404.04650",
        "pdf": "http://arxiv.org/pdf/2404.04650.pdf"
    },
    {
        "title": "Depth  Information Assisted Collaborative Mutual Promotion Network for Single Image Dehazing",
        "author": "Yafei Zhang, Shen Zhou, Huafeng Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields",
        "author": "Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik Lensch",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Guided Slot Attention for Unsupervised Video Object Segmentation",
        "author": "Minhyeok Lee, Suhwan Cho, Dogyoon Lee, Chaewon Park, Jungho Lee, Sangyoun Lee",
        "abstract": "Unsupervised video object segmentation aims to segment the most prominent object in a video sequence. However, the existence of complex backgrounds and multiple foreground objects make this task challenging. To address this issue, we propose a guided slot attention network to reinforce spatial structural information and obtain better foreground--background separation. The foreground and background slots, which are initialized with query guidance, are iteratively refined based on interactions with template information. Furthermore, to improve slot--template interaction and effectively fuse global and local features in the target and reference frames, K-nearest neighbors filtering and a feature aggregation transformer are introduced. The proposed model achieves state-of-the-art performance on two popular datasets. Additionally, we demonstrate the robustness of the proposed model in challenging scenes through various comparative experiments.",
        "page": "http://arxiv.org/abs/2303.08314",
        "pdf": "http://arxiv.org/pdf/2303.08314.pdf"
    },
    {
        "title": "Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models",
        "author": "Xinpeng Ding, Jianhua Han, Hang Xu, Xiaodan Liang, Wei Zhang, Xiaomeng Li",
        "abstract": "The rise of multimodal large language models (MLLMs) has spurred interest in language-based driving tasks. However, existing research typically focuses on limited tasks and often omits key multi-view and temporal information which is crucial for robust autonomous driving. To bridge these gaps, we introduce NuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17 subtasks, where each task demands holistic information (e.g., temporal, multi-view, and spatial), significantly elevating the challenge level. To obtain NuInstruct, we propose a novel SQL-based method to generate instruction-response pairs automatically, which is inspired by the driving logical progression of humans. We further present BEV-InMLLM, an end-to-end method for efficiently deriving instruction-aware Bird's-Eye-View (BEV) features, language-aligned for large language models. BEV-InMLLM integrates multi-view, spatial awareness, and temporal semantics to enhance MLLMs' capabilities on NuInstruct tasks. Moreover, our proposed BEV injection module is a plug-and-play method for existing MLLMs. Our experiments on NuInstruct demonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g. around 9% improvement on various tasks. We plan to release our NuInstruct for future research development.",
        "page": "http://arxiv.org/abs/2401.00988",
        "pdf": "http://arxiv.org/pdf/2401.00988.pdf"
    },
    {
        "title": "DiffLoc: Diffusion Model for Outdoor LiDAR Localization",
        "author": "Wen Li, Yuyang Yang, Shangshu Yu, Guosheng Hu, Chenglu Wen, Ming Cheng, Cheng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Video-P2P: Video Editing with Cross-attention Control",
        "author": "Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, Jiaya Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification",
        "author": "Pingping Zhang, Yuhao Wang, Yang Liu, Zhengzheng Tu, Huchuan Lu",
        "abstract": "Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios. In contrast, multi-modal object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications. However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps. To address above issues, we propose a novel learning framework named \\textbf{EDITOR} to select diverse tokens from vision Transformers for multi-modal object ReID. We begin with a shared vision Transformer to extract tokenized features from different input modalities. Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information. Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities. Finally, to further reduce the effect of backgrounds, we propose a Background Consistency Constraint (BCC) and an Object-Centric Feature Refinement (OCFR). They are formulated as two new loss functions, which improve the feature discrimination with background suppression. As a result, our framework can generate more discriminative features for multi-modal object ReID. Extensive experiments on three multi-modal ReID benchmarks verify the effectiveness of our methods. The code is available at https://github.com/924973292/EDITOR.",
        "page": "http://arxiv.org/abs/2403.10254",
        "pdf": "http://arxiv.org/pdf/2403.10254.pdf"
    },
    {
        "title": "Single Domain Generalization for Crowd Counting",
        "author": "Zhuoxuan Peng, S.-H. Gary Chan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields",
        "author": "TIANQI LIU, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation",
        "author": "Xi Liu, Ying Guo, Cheng Zhen, Tong Li, Yingying Ao, Pengfei Yan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HDRFlow: Real-Time HDR Video Reconstruction with Large Motions",
        "author": "Gangwei Xu, Yujin Wang, Jinwei Gu, Tianfan Xue, Xin Yang",
        "abstract": "Reconstructing High Dynamic Range (HDR) video from image sequences captured with alternating exposures is challenging, especially in the presence of large camera or object motion. Existing methods typically align low dynamic range sequences using optical flow or attention mechanism for deghosting. However, they often struggle to handle large complex motions and are computationally expensive. To address these challenges, we propose a robust and efficient flow estimator tailored for real-time HDR video reconstruction, named HDRFlow. HDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an efficient flow network with a multi-size large kernel (MLK), and a new HDR flow training scheme. The HALoss supervises our flow network to learn an HDR-oriented flow for accurate alignment in saturated and dark regions. The MLK can effectively model large motions at a negligible cost. In addition, we incorporate synthetic data, Sintel, into our training dataset, utilizing both its provided forward flow and backward flow generated by us to supervise our flow network, enhancing our performance in large motion regions. Extensive experiments demonstrate that our HDRFlow outperforms previous methods on standard benchmarks. To the best of our knowledge, HDRFlow is the first real-time HDR video reconstruction method for video sequences captured with alternating exposures, capable of processing 720p resolution inputs at 25ms.",
        "page": "http://arxiv.org/abs/2403.03447",
        "pdf": "http://arxiv.org/pdf/2403.03447.pdf"
    },
    {
        "title": "Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis",
        "author": "Yuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, Mike Zheng Shou",
        "abstract": "Vector-Quantized (VQ-based) generative models usually consist of two basic components, i.e., VQ tokenizers and generative transformers. Prior research focuses on improving the reconstruction fidelity of VQ tokenizers but rarely examines how the improvement in reconstruction affects the generation ability of generative transformers. In this paper, we surprisingly find that improving the reconstruction fidelity of VQ tokenizers does not necessarily improve the generation. Instead, learning to compress semantic features within VQ tokenizers significantly improves generative transformers' ability to capture textures and structures. We thus highlight two competing objectives of VQ tokenizers for image synthesis: semantic compression and details preservation. Different from previous work that only pursues better details preservation, we propose Semantic-Quantized GAN (SeQ-GAN) with two learning phases to balance the two objectives. In the first phase, we propose a semantic-enhanced perceptual loss for better semantic compression. In the second phase, we fix the encoder and codebook, but enhance and finetune the decoder to achieve better details preservation. The proposed SeQ-GAN greatly improves VQ-based generative models and surpasses the GAN and Diffusion Models on both unconditional and conditional image generation. Our SeQ-GAN (364M) achieves Frechet Inception Distance (FID) of 6.25 and Inception Score (IS) of 140.9 on 256x256 ImageNet generation, a remarkable improvement over VIT-VQGAN (714M), which obtains 11.2 FID and 97.2 IS.",
        "page": "http://arxiv.org/abs/2212.03185",
        "pdf": "http://arxiv.org/pdf/2212.03185.pdf"
    },
    {
        "title": "Dual Prototype Attention for Unsupervised Video Object Segmentation",
        "author": "Suhwan Cho, Minhyeok Lee, Seunghoon Lee, Dogyoon Lee, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee",
        "abstract": "Unsupervised video object segmentation (VOS) aims to detect and segment the most salient object in videos. The primary techniques used in unsupervised VOS are 1) the collaboration of appearance and motion information; and 2) temporal fusion between different frames. This paper proposes two novel prototype-based attention mechanisms, inter-modality attention (IMA) and inter-frame attention (IFA), to incorporate these techniques via dense propagation across different modalities and frames. IMA densely integrates context information from different modalities based on a mutual refinement. IFA injects global context of a video to the query frame, enabling a full utilization of useful properties from multiple frames. Experimental results on public benchmark datasets demonstrate that our proposed approach outperforms all existing methods by a substantial margin. The proposed two components are also thoroughly validated via ablative study.",
        "page": "http://arxiv.org/abs/2211.12036",
        "pdf": "http://arxiv.org/pdf/2211.12036.pdf"
    },
    {
        "title": "MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers",
        "author": "Haoyu Ma, Shahin Mahdizadehaghdam, Bichen Wu, Zhipeng Fan, Yuchao Gu, Wenliang Zhao, Lior Shapira, Xiaohui Xie",
        "abstract": "Recent advances in generative AI have significantly enhanced image and video editing, particularly in the context of text prompt control. State-of-the-art approaches predominantly rely on diffusion models to accomplish these tasks. However, the computational demands of diffusion-based methods are substantial, often necessitating large-scale paired datasets for training, and therefore challenging the deployment in real applications. To address these issues, this paper breaks down the text-based video editing task into two stages. First, we leverage an pre-trained text-to-image diffusion model to simultaneously edit few keyframes in an zero-shot way. Second, we introduce an efficient model called MaskINT, which is built on non-autoregressive masked generative transformers and specializes in frame interpolation between the edited keyframes, using the structural guidance from intermediate frames. Experimental results suggest that our MaskINT achieves comparable performance with diffusion-based methodologies, while significantly improve the inference time. This research offers a practical solution for text-based video editing and showcases the potential of non-autoregressive masked generative transformers in this domain.",
        "page": "http://arxiv.org/abs/2312.12468",
        "pdf": "http://arxiv.org/pdf/2312.12468.pdf"
    },
    {
        "title": "DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing",
        "author": "Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou",
        "abstract": "Despite recent progress in diffusion-based video editing, existing methods are limited to short-length videos due to the contradiction between long-range consistency and frame-wise editing. Prior attempts to address this challenge by introducing video-2D representations encounter significant difficulties with large-scale motion- and view-change videos, especially in human-centric scenarios. To overcome this, we propose to introduce the dynamic Neural Radiance Fields (NeRF) as the innovative video representation, where the editing can be performed in the 3D spaces and propagated to the entire video via the deformation field. To provide consistent and controllable editing, we propose the image-based video-NeRF editing pipeline with a set of innovative designs, including multi-view multi-pose Score Distillation Sampling (SDS) from both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction losses, text-guided local parts super-resolution, and style transfer. Extensive experiments demonstrate that our method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two challenging datasets by a large margin of 50% ~ 95% for human preference. Code will be released at https://showlab.github.io/DynVideo-E/.",
        "page": "http://arxiv.org/abs/2310.10624",
        "pdf": "http://arxiv.org/pdf/2310.10624.pdf"
    },
    {
        "title": "MemSAM: Taming Segment Anything Model for Echocardiography Video Segmentation",
        "author": "Xiaolong Deng, Huisi Wu, Runhao Zeng, Jing Qin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis",
        "author": "Tianci Bi, Xiaoyi Zhang, Zhizheng Zhang, Wenxuan Xie, Cuiling Lan, Yan Lu, Nanning Zheng",
        "abstract": "Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace. Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one. All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets. In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features. To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment. Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training. In the case of full parameter fine-tuning, we can further improve layout analysis performance.",
        "page": "http://arxiv.org/abs/2405.07481",
        "pdf": "http://arxiv.org/pdf/2405.07481.pdf"
    },
    {
        "title": "LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example",
        "author": "Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, Junyong Noh",
        "abstract": "Recent advances in 3D face stylization have made significant strides in few to zero-shot settings. However, the degree of stylization achieved by existing methods is often not sufficient for practical applications because they are mostly based on statistical 3D Morphable Models (3DMM) with limited variations. To this end, we propose a method that can produce a highly stylized 3D face model with desired topology. Our methods train a surface deformation network with 3DMM and translate its domain to the target style using a paired exemplar. The network achieves stylization of the 3D face mesh by mimicking the style of the target using a differentiable renderer and directional CLIP losses. Additionally, during the inference process, we utilize a Mesh Agnostic Encoder (MAGE) that takes deformation target, a mesh of diverse topologies as input to the stylization process and encodes its shape into our latent space. The resulting stylized face model can be animated by commonly used 3DMM blend shapes. A set of quantitative and qualitative evaluations demonstrate that our method can produce highly stylized face meshes according to a given style and output them in a desired topology. We also demonstrate example applications of our method including image-based stylized avatar generation, linear interpolation of geometric styles, and facial animation of stylized avatars.",
        "page": "http://arxiv.org/abs/2403.15227",
        "pdf": "http://arxiv.org/pdf/2403.15227.pdf"
    },
    {
        "title": "Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image",
        "author": "Yiqun Mei, Yu Zeng, He Zhang, Zhixin Shu, Xuaner Zhang, Sai Bi, Jianming Zhang, HyunJoon Jung, Vishal M. Patel",
        "abstract": "At the core of portrait photography is the search for ideal lighting and viewpoint. The process often requires advanced knowledge in photography and an elaborate studio setup. In this work, we propose Holo-Relighting, a volumetric relighting method that is capable of synthesizing novel viewpoints, and novel lighting from a single image. Holo-Relighting leverages the pretrained 3D GAN (EG3D) to reconstruct geometry and appearance from an input portrait as a set of 3D-aware features. We design a relighting module conditioned on a given lighting to process these features, and predict a relit 3D representation in the form of a tri-plane, which can render to an arbitrary viewpoint through volume rendering. Besides viewpoint and lighting control, Holo-Relighting also takes the head pose as a condition to enable head-pose-dependent lighting effects. With these novel designs, Holo-Relighting can generate complex non-Lambertian lighting effects (e.g., specular highlights and cast shadows) without using any explicit physical lighting priors. We train Holo-Relighting with data captured with a light stage, and propose two data-rendering techniques to improve the data quality for training the volumetric relighting system. Through quantitative and qualitative experiments, we demonstrate Holo-Relighting can achieve state-of-the-arts relighting quality with better photorealism, 3D consistency and controllability.",
        "page": "http://arxiv.org/abs/2403.09632",
        "pdf": "http://arxiv.org/pdf/2403.09632.pdf"
    },
    {
        "title": "MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes",
        "author": "Bor Shiun Wang, Chien-Yi Wang, Wei-Chen Chiu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Driving-Video Dehazing with Non-Aligned Regularization for Safety Assistance",
        "author": "Junkai Fan, Jiangwei Weng, Kun Wang, Yijun Yang, Jianjun Qian, Jun Li, Jian Yang",
        "abstract": "Real driving-video dehazing poses a significant challenge due to the inherent difficulty in acquiring precisely aligned hazy/clear video pairs for effective model training, especially in dynamic driving scenarios with unpredictable weather conditions. In this paper, we propose a pioneering approach that addresses this challenge through a nonaligned regularization strategy. Our core concept involves identifying clear frames that closely match hazy frames, serving as references to supervise a video dehazing network. Our approach comprises two key components: reference matching and video dehazing. Firstly, we introduce a non-aligned reference frame matching module, leveraging an adaptive sliding window to match high-quality reference frames from clear videos. Video dehazing incorporates flow-guided cosine attention sampler and deformable cosine attention fusion modules to enhance spatial multiframe alignment and fuse their improved information. To validate our approach, we collect a GoProHazy dataset captured effortlessly with GoPro cameras in diverse rural and urban road environments. Extensive experiments demonstrate the superiority of the proposed method over current state-of-the-art methods in the challenging task of real driving-video dehazing. Project page.",
        "page": "http://arxiv.org/abs/2405.09996",
        "pdf": "http://arxiv.org/pdf/2405.09996.pdf"
    },
    {
        "title": "Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models",
        "author": "Weiwei Cao, Jianpeng Zhang, Yingda Xia, Tony C. W. MOK, Zi Li, Xianghua Ye, Le Lu, Jian Zheng, Yuxing Tang, Ling Zhang",
        "abstract": "Radiologists highly desire fully automated versatile AI for medical imaging interpretation. However, the lack of extensively annotated large-scale multi-disease datasets has hindered the achievement of this goal. In this paper, we explore the feasibility of leveraging language as a naturally high-quality supervision for chest CT imaging. In light of the limited availability of image-report pairs, we bootstrap the understanding of 3D chest CT images by distilling chest-related diagnostic knowledge from an extensively pre-trained 2D X-ray expert model. Specifically, we propose a language-guided retrieval method to match each 3D CT image with its semantically closest 2D X-ray image, and perform pair-wise and semantic relation knowledge distillation. Subsequently, we use contrastive learning to align images and reports within the same patient while distinguishing them from the other patients. However, the challenge arises when patients have similar semantic diagnoses, such as healthy patients, potentially confusing if treated as negatives. We introduce a robust contrastive learning that identifies and corrects these false negatives. We train our model with over 12,000 pairs of chest CT images and radiology reports. Extensive experiments across multiple scenarios, including zero-shot learning, report generation, and fine-tuning processes, demonstrate the model's feasibility in interpreting chest CT images.",
        "page": "http://arxiv.org/abs/2404.04936",
        "pdf": "http://arxiv.org/pdf/2404.04936.pdf"
    },
    {
        "title": "Semantic Human Mesh Reconstruction with Textures",
        "author": "xiaoyu zhan, Jianxin Yang, Yuanqi Li, Jie Guo, Yanwen Guo, Wenping Wang",
        "abstract": "The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (e.g. mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2403.02561",
        "pdf": "http://arxiv.org/pdf/2403.02561.pdf"
    },
    {
        "title": "Efficient Dataset Distillation via Minimax Diffusion",
        "author": "Jianyang Gu, Saeed Vahidian, Vyacheslav Kungurtsev, Haonan Wang, Wei Jiang, Yang You, Yiran Chen",
        "abstract": "Dataset distillation reduces the storage and computational consumption of training a network by generating a small surrogate dataset that encapsulates rich information of the original large-scale one. However, previous distillation methods heavily rely on the sample-wise iterative optimization scheme. As the images-per-class (IPC) setting or image resolution grows larger, the necessary computation will demand overwhelming time and resources. In this work, we intend to incorporate generative diffusion techniques for computing the surrogate dataset. Observing that key factors for constructing an effective surrogate dataset are representativeness and diversity, we design additional minimax criteria in the generative training to enhance these facets for the generated images of diffusion models. We present a theoretical model of the process as hierarchical diffusion control demonstrating the flexibility of the diffusion process to target these criteria without jeopardizing the faithfulness of the sample to the desired distribution. The proposed method achieves state-of-the-art validation performance while demanding much less computational resources. Under the 100-IPC setting on ImageWoof, our method requires less than one-twentieth the distillation time of previous methods, yet yields even better performance. Source code and generated data are available in https://github.com/vimar-gu/MinimaxDiffusion.",
        "page": "http://arxiv.org/abs/2311.15529",
        "pdf": "http://arxiv.org/pdf/2311.15529.pdf"
    },
    {
        "title": "Generate Like Experts: Multi-Stage Font Generation by Incorporating Font Transfer Process into Diffusion Models",
        "author": "Bin Fu, Fanghua Yu, Anran Liu, Zixuan Wang, Jie Wen, Junjun He, Yu Qiao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Scene Adaptive Sparse Transformer for Event-based Object Detection",
        "author": "Yansong Peng, Li Hebei, Yueyi Zhang, Xiaoyan Sun, Feng Wu",
        "abstract": "While recent Transformer-based approaches have shown impressive performances on event-based object detection tasks, their high computational costs still diminish the low power consumption advantage of event cameras. Image-based works attempt to reduce these costs by introducing sparse Transformers. However, they display inadequate sparsity and adaptability when applied to event-based object detection, since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based Transformers, leading to reduced performance and efficiency. Furthermore, they lack scene-specific sparsity optimization, resulting in information loss and a lower recall rate. To overcome these limitations, we propose the Scene Adaptive Sparse Transformer (SAST). SAST enables window-token co-sparsification, significantly enhancing fault tolerance and reducing computational overhead. Leveraging the innovative scoring and selection modules, along with the Masked Sparse Window Self-Attention, SAST showcases remarkable scene-aware adaptability: It focuses only on important objects and dynamically optimizes sparsity level according to scene complexity, maintaining a remarkable balance between performance and computational cost. The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based object detection datasets (1Mpx and Gen1). Code: https://github.com/Peterande/SAST",
        "page": "http://arxiv.org/abs/2404.01882",
        "pdf": "http://arxiv.org/pdf/2404.01882.pdf"
    },
    {
        "title": "GEARS: Local Geometry-aware Hand-object Interaction Synthesis",
        "author": "Keyang Zhou, Bharat Lal Bhatnagar, Jan Lenssen, Gerard Pons-Moll",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Mind Artist: Creating Artistic Snapshots with Human Thought",
        "author": "Jiaxuan Chen, Yu Qi, Yueming Wang, Gang Pan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dynamic Cues-Assisted Transformer for Robust Point Cloud Registration",
        "author": "Hong Chen, Pei Yan, sihe xiang, Yihua Tan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Do Vision and Language Encoders Represent the World Similarly?",
        "author": "Mayug Maniparambil, Raiymbek Akshulakov, YASSER ABDELAZIZ DAHOU DJILALI, Mohamed El Amine Seddik, Sanath Narayan, Karttikeya Mangalam, Noel O&#x27;Connor",
        "abstract": "Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demonstrate the effectiveness of this on several downstream tasks including cross-lingual, cross-domain caption matching and image classification. Code available at github.com/mayug/0-shot-llm-vision.",
        "page": "http://arxiv.org/abs/2401.05224",
        "pdf": "http://arxiv.org/pdf/2401.05224.pdf"
    },
    {
        "title": "Named Entity Driven Zero-Shot Image Manipulation",
        "author": "Zhida Feng, Li Chen, Jing Tian, Jiaxiang Liu, Shikun Feng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PSDPM: Prototype-based Secondary Discriminative Pixels Mining for Weakly Supervised Semantic Segmentation",
        "author": "Xinqiao Zhao, Ziqian Yang, Tianhong Dai, Bingfeng Zhang, Jimin Xiao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds",
        "author": "Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang",
        "abstract": "3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to annotating new domains. Self-training is a competitive approach for this task, but its performance is limited by different sensor sampling patterns (i.e., variations in point density) and incomplete training strategies. In this work, we propose a density-guided translator (DGT), which translates point density between domains, and integrates it into a two-stage self-training pipeline named DGT-ST. First, in contrast to existing works that simultaneously conduct data generation and feature/output alignment within unstable adversarial training, we employ the non-learnable DGT to bridge the domain gap at the input level. Second, to provide a well-initialized model for self-training, we propose a category-level adversarial network in stage one that utilizes the prototype to prevent negative transfer. Finally, by leveraging the designs above, a domain-mixed self-training method with source-aware consistency loss is proposed in stage two to narrow the domain gap further. Experiments on two synthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and SynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms state-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements, respectively. Code is available at \\url{https://github.com/yuan-zm/DGT-ST}.",
        "page": "http://arxiv.org/abs/2403.18469",
        "pdf": "http://arxiv.org/pdf/2403.18469.pdf"
    },
    {
        "title": "OmniLocalRF: Omnidirectional Local Radiance Fields from Dynamic Videos",
        "author": "Dongyoung Choi, Hyeonjoong Jang, Min H. Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding",
        "author": "Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PairAug: What Can Augmented Image-Text Pairs Do for Radiology?",
        "author": "Yutong Xie, Qi Chen, Sinuo Wang, Minh-Son To, Iris Lee, Ee Win Khoo, Kerolos Hendy, Daniel Koh, Yong Xia, Qi Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos",
        "author": "Yufei Zhang, Jeffrey Kephart, Zijun Cui, Qiang Ji",
        "abstract": "While current methods have shown promising progress on estimating 3D human motion from monocular videos, their motion estimates are often physically unrealistic because they mainly consider kinematics. In this paper, we introduce Physics-aware Pretrained Transformer (PhysPT), which improves kinematics-based motion estimates and infers motion forces. PhysPT exploits a Transformer encoder-decoder backbone to effectively learn human dynamics in a self-supervised manner. Moreover, it incorporates physics principles governing human motion. Specifically, we build a physics-based body representation and contact force model. We leverage them to impose novel physics-inspired training losses (i.e., force loss, contact loss, and Euler-Lagrange loss), enabling PhysPT to capture physical properties of the human body and the forces it experiences. Experiments demonstrate that, once trained, PhysPT can be directly applied to kinematics-based estimates to significantly enhance their physical plausibility and generate favourable motion forces. Furthermore, we show that these physically meaningful quantities translate into improved accuracy of an important downstream task: human action recognition.",
        "page": "http://arxiv.org/abs/2404.04430",
        "pdf": "http://arxiv.org/pdf/2404.04430.pdf"
    },
    {
        "title": "ZeroRF: Fast Sparse View 360\u00b0 Reconstruction with Zero Pretraining",
        "author": "Ruoxi Shi, Xinyue Wei, Cheng Wang, Hao Su",
        "abstract": "We present ZeroRF, a novel per-scene optimization method addressing the challenge of sparse view 360{\\deg} reconstruction in neural field representations. Current breakthroughs like Neural Radiance Fields (NeRF) have demonstrated high-fidelity image synthesis but struggle with sparse input views. Existing methods, such as Generalizable NeRFs and per-scene optimization approaches, face limitations in data dependency, computational cost, and generalization across diverse scenarios. To overcome these challenges, we propose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into a factorized NeRF representation. Unlike traditional methods, ZeroRF parametrizes feature grids with a neural network generator, enabling efficient sparse view 360{\\deg} reconstruction without any pretraining or additional regularization. Extensive experiments showcase ZeroRF's versatility and superiority in terms of both quality and speed, achieving state-of-the-art results on benchmark datasets. ZeroRF's significance extends to applications in 3D content generation and editing. Project page: https://sarahweiii.github.io/zerorf/",
        "page": "http://arxiv.org/abs/2312.09249",
        "pdf": "http://arxiv.org/pdf/2312.09249.pdf"
    },
    {
        "title": "One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion",
        "author": "Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, Hao Su",
        "abstract": "Recent advancements in open-world 3D object generation have been remarkable, with image-to-3D methods offering superior fine-grained control over their text-to-3D counterparts. However, most existing models fall short in simultaneously providing rapid generation speeds and high fidelity to input images - two features essential for practical applications. In this paper, we present One-2-3-45++, an innovative method that transforms a single image into a detailed 3D textured mesh in approximately one minute. Our approach aims to fully harness the extensive knowledge embedded in 2D diffusion models and priors from valuable yet limited 3D data. This is achieved by initially finetuning a 2D diffusion model for consistent multi-view image generation, followed by elevating these images to 3D with the aid of multi-view conditioned 3D native diffusion models. Extensive experimental evaluations demonstrate that our method can produce high-quality, diverse 3D assets that closely mirror the original input image. Our project webpage: https://sudo-ai-3d.github.io/One2345plus_page.",
        "page": "http://arxiv.org/abs/2311.07885",
        "pdf": "http://arxiv.org/pdf/2311.07885.pdf"
    },
    {
        "title": "Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use",
        "author": "Imad Eddine Toubal, Aditya Avinash, Neil Alldrin, Jan Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou, Ranjay Krishna, Ariel Fuxman, Tom Duerig",
        "abstract": "From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective visual concepts is growing. Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training. Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier. Drawing on Fiske's Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions. Our framework leverages recent advances in foundation models, both large language models and vision-language models, to carve out the concept space through conversation and by automatically labeling training data points. Most importantly, our framework eliminates the need for crowd-sourced annotations. Moreover, our framework ultimately produces lightweight classification models that are deployable in cost-sensitive scenarios. Across 15 subjective concepts and across 2 public image classification datasets, our trained models outperform traditional Agile Modeling as well as state-of-the-art zero-shot classification models like ALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.",
        "page": "http://arxiv.org/abs/2403.02626",
        "pdf": "http://arxiv.org/pdf/2403.02626.pdf"
    },
    {
        "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
        "author": "Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman",
        "abstract": "Solving complex visual tasks such as \"Who invented the musical instrument on the right?\" involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incorrect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Extensive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data.",
        "page": "http://arxiv.org/abs/2312.03052",
        "pdf": "http://arxiv.org/pdf/2312.03052.pdf"
    },
    {
        "title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
        "author": "Jihan Yang, Runyu Ding, Weipeng DENG, Zhe Wang, Xiaojuan Qi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Joint-Task Regularization for Partially Labeled Multi-Task Learning",
        "author": "Kento Nishi, Junsik Kim, Wanhua Li, Hanspeter Pfister",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models",
        "author": "Changhoon Kim, Kyle Min, Maitreya Patel, Sheng Cheng, 'YZ' Yezhou Yang",
        "abstract": "The rapid advancement of generative models, facilitating the creation of hyper-realistic images from textual descriptions, has concurrently escalated critical societal concerns such as misinformation. Although providing some mitigation, traditional fingerprinting mechanisms fall short in attributing responsibility for the malicious use of synthetic images. This paper introduces a novel approach to model fingerprinting that assigns responsibility for the generated images, thereby serving as a potential countermeasure to model misuse. Our method modifies generative models based on each user's unique digital fingerprint, imprinting a unique identifier onto the resultant content that can be traced back to the user. This approach, incorporating fine-tuning into Text-to-Image (T2I) tasks using the Stable Diffusion Model, demonstrates near-perfect attribution accuracy with a minimal impact on output quality. Through extensive evaluation, we show that our method outperforms baseline methods with an average improvement of 11\\% in handling image post-processes. Our method presents a promising and novel avenue for accountable model distribution and responsible use. Our code is available in \\url{https://github.com/kylemin/WOUAF}.",
        "page": "http://arxiv.org/abs/2306.04744",
        "pdf": "http://arxiv.org/pdf/2306.04744.pdf"
    },
    {
        "title": "MicroDiffusion: Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Microscopy Projections",
        "author": "mude hui, Zihao Wei, Hongru Zhu, Fei Xia, Yuyin Zhou",
        "abstract": "Volumetric optical microscopy using non-diffracting beams enables rapid imaging of 3D volumes by projecting them axially to 2D images but lacks crucial depth information. Addressing this, we introduce MicroDiffusion, a pioneering tool facilitating high-quality, depth-resolved 3D volume reconstruction from limited 2D projections. While existing Implicit Neural Representation (INR) models often yield incomplete outputs and Denoising Diffusion Probabilistic Models (DDPM) excel at capturing details, our method integrates INR's structural coherence with DDPM's fine-detail enhancement capabilities. We pretrain an INR model to transform 2D axially-projected images into a preliminary 3D volume. This pretrained INR acts as a global prior guiding DDPM's generative process through a linear interpolation between INR outputs and noise inputs. This strategy enriches the diffusion process with structured 3D information, enhancing detail and reducing noise in localized 2D images. By conditioning the diffusion model on the closest 2D projection, MicroDiffusion substantially enhances fidelity in resulting 3D reconstructions, surpassing INR and standard DDPM outputs with unparalleled image quality and structural fidelity. Our code and dataset are available at https://github.com/UCSC-VLAA/MicroDiffusion.",
        "page": "http://arxiv.org/abs/2403.10815",
        "pdf": "http://arxiv.org/pdf/2403.10815.pdf"
    },
    {
        "title": "Gaussian Shadow Casting for Neural Characters",
        "author": "Luis Bolanos, Shih-Yang Su, Helge Rhodin",
        "abstract": "Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.",
        "page": "http://arxiv.org/abs/2401.06116",
        "pdf": "http://arxiv.org/pdf/2401.06116.pdf"
    },
    {
        "title": "PAIR Diffusion: A Comprehensive Multimodal Object-Level Image Editor",
        "author": "Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, Humphrey Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing",
        "author": "Ajian Liu, Shuai Xue, Gan Jianwen, Jun Wan, Yanyan Liang, Jiankang Deng, Sergio Escalera, Zhen Lei",
        "abstract": "Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model's performance on unseen domains. Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization. In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier's weights for exploring generalizable visual features. Specifically, we propose a novel Class Free Prompt Learning (CFPL) paradigm for DG FAS, which utilizes two lightweight transformers, namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic prompts conditioned on content and style features by using a set of learnable query vectors, respectively. Thus, the generalizable prompt can be learned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description. (2) A Diversified Style Prompt (DSP) technology is proposed to diversify the learning of style prompts by mixing feature statistics between instance-specific styles. Finally, the learned text features modulate visual features to generalization through the designed Prompt Modulation (PM). Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets.",
        "page": "http://arxiv.org/abs/2403.14333",
        "pdf": "http://arxiv.org/pdf/2403.14333.pdf"
    },
    {
        "title": "Monocular Identity-Conditioned Facial Reflectance Reconstruction",
        "author": "Xingyu Ren, Jiankang Deng, Yuhao Cheng, Jia Guo, Chao Ma, Yichao Yan, Wenhan Zhu, Xiaokang Yang",
        "abstract": "Recent 3D face reconstruction methods have made remarkable advancements, yet there remain huge challenges in monocular high-quality facial reflectance reconstruction. Existing methods rely on a large amount of light-stage captured data to learn facial reflectance models. However, the lack of subject diversity poses challenges in achieving good generalization and widespread applicability. In this paper, we learn the reflectance prior in image space rather than UV space and present a framework named ID2Reflectance. Our framework can directly estimate the reflectance maps of a single image while using limited reflectance data for training. Our key insight is that reflectance data shares facial structures with RGB faces, which enables obtaining expressive facial prior from inexpensive RGB data thus reducing the dependency on reflectance data. We first learn a high-quality prior for facial reflectance. Specifically, we pretrain multi-domain facial feature codebooks and design a codebook fusion method to align the reflectance and RGB domains. Then, we propose an identity-conditioned swapping module that injects facial identity from the target image into the pre-trained autoencoder to modify the identity of the source reflectance image. Finally, we stitch multi-view swapped reflectance images to obtain renderable assets. Extensive experiments demonstrate that our method exhibits excellent generalization capability and achieves state-of-the-art facial reflectance reconstruction results for in-the-wild faces. Our project page is https://xingyuren.github.io/id2reflectance/.",
        "page": "http://arxiv.org/abs/2404.00301",
        "pdf": "http://arxiv.org/pdf/2404.00301.pdf"
    },
    {
        "title": "WaveFace: Authentic Face Restoration with Efficient Frequency Recovery",
        "author": "Yunqi Miao, Jiankang Deng, Jungong Han",
        "abstract": "Although diffusion models are rising as a powerful solution for blind face restoration, they are criticized for two problems: 1) slow training and inference speed, and 2) failure in preserving identity and recovering fine-grained facial details. In this work, we propose WaveFace to solve the problems in the frequency domain, where low- and high-frequency components decomposed by wavelet transformation are considered individually to maximize authenticity as well as efficiency. The diffusion model is applied to recover the low-frequency component only, which presents general information of the original image but 1/16 in size. To preserve the original identity, the generation is conditioned on the low-frequency component of low-quality images at each denoising step. Meanwhile, high-frequency components at multiple decomposition levels are handled by a unified network, which recovers complex facial details in a single step. Evaluations on four benchmark datasets show that: 1) WaveFace outperforms state-of-the-art methods in authenticity, especially in terms of identity preservation, and 2) authentic images are restored with the efficiency 10x faster than existing diffusion model-based BFR methods.",
        "page": "http://arxiv.org/abs/2403.12760",
        "pdf": "http://arxiv.org/pdf/2403.12760.pdf"
    },
    {
        "title": "Building Optimal Neural Architectures using Interpretable Knowledge",
        "author": "Keith Mills, Fred Han, Mohammad Salameh, Shengyao Lu, CHUNHUA ZHOU, Jiao He, Fengyu Sun, Di Niu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation",
        "author": "Kangfu Mei, Mauricio Delbracio, Hossein Talebi, Zhengzhong Tu, Vishal M. Patel, Peyman Milanfar",
        "abstract": "Large generative diffusion models have revolutionized text-to-image generation and offer immense potential for conditional generation tasks such as image enhancement, restoration, editing, and compositing. However, their widespread adoption is hindered by the high computational cost, which limits their real-time application. To address this challenge, we introduce a novel method dubbed CoDi, that adapts a pre-trained latent diffusion model to accept additional image conditioning inputs while significantly reducing the sampling steps required to achieve high-quality results. Our method can leverage architectures such as ControlNet to incorporate conditioning inputs without compromising the model's prior knowledge gained during large scale pre-training. Additionally, a conditional consistency loss enforces consistent predictions across diffusion steps, effectively compelling the model to generate high-quality images with conditions in a few steps. Our conditional-task learning and distillation approach outperforms previous distillation methods, achieving a new state-of-the-art in producing high-quality images with very few steps (e.g., 1-4) across multiple tasks, including super-resolution, text-guided image editing, and depth-to-image generation.",
        "page": "http://arxiv.org/abs/2310.01407",
        "pdf": "http://arxiv.org/pdf/2310.01407.pdf"
    },
    {
        "title": "Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation",
        "author": "Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers",
        "abstract": "Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about geometry even in occluded regions. With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches. e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work, we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction. To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully self-supervised only from image data. Using knowledge distillation, we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions.",
        "page": "http://arxiv.org/abs/2404.07933",
        "pdf": "http://arxiv.org/pdf/2404.07933.pdf"
    },
    {
        "title": "Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration",
        "author": "Tony C. W. MOK, Zi Li, Yunhao Bai, Jianpeng Zhang, Wei Liu, Yan-Jie Zhou, Ke Yan, Dakai Jin, Yu Shi, Xiaoli Yin, Le Lu, Ling Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "All Rivers Run to the Sea: Private Learning with Asymmetric Flows",
        "author": "Yue Niu, Ramy E. Ali, Saurav Prakash, Salman Avestimehr",
        "abstract": "Data privacy is of great concern in cloud machine-learning service platforms, when sensitive data are exposed to service providers. While private computing environments (e.g., secure enclaves), and cryptographic approaches (e.g., homomorphic encryption) provide strong privacy protection, their computing performance still falls short compared to cloud GPUs. To achieve privacy protection with high computing performance, we propose Delta, a new private training and inference framework, with comparable model performance as non-private centralized training. Delta features two asymmetric data flows: the main information-sensitive flow and the residual flow. The main part flows into a small model while the residuals are offloaded to a large model. Specifically, Delta embeds the information-sensitive representations into a low-dimensional space while pushing the information-insensitive part into high-dimension residuals. To ensure privacy protection, the low-dimensional information-sensitive part is secured and fed to a small model in a private environment. On the other hand, the residual part is sent to fast cloud GPUs, and processed by a large model. To further enhance privacy and reduce the communication cost, Delta applies a random binary quantization technique along with a DP-based technique to the residuals before sharing them with the public platform. We theoretically show that Delta guarantees differential privacy in the public environment and greatly reduces the complexity in the private environment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet datasets and ResNet-18 and ResNet-34, showing that Delta achieves strong privacy protection, fast training, and inference without significantly compromising the model utility.",
        "page": "http://arxiv.org/abs/2312.05264",
        "pdf": "http://arxiv.org/pdf/2312.05264.pdf"
    },
    {
        "title": "Synergistic Global-space Camera and Human Reconstruction from Videos",
        "author": "Yizhou Zhao, Tuanfeng Y. Wang, Bhiksha Raj, Min Xu, Jimei Yang, Chun-Hao P. Huang",
        "abstract": "Remarkable strides have been made in reconstructing static scenes or human bodies from monocular videos. Yet, the two problems have largely been approached independently, without much synergy. Most visual SLAM methods can only reconstruct camera trajectories and scene structures up to scale, while most HMR methods reconstruct human meshes in metric scale but fall short in reasoning with cameras and scenes. This work introduces Synergistic Camera and Human Reconstruction (SynCHMR) to marry the best of both worlds. Specifically, we design Human-aware Metric SLAM to reconstruct metric-scale camera poses and scene point clouds using camera-frame HMR as a strong prior, addressing depth, scale, and dynamic ambiguities. Conditioning on the dense scene recovered, we further learn a Scene-aware SMPL Denoiser to enhance world-frame HMR by incorporating spatio-temporal coherency and dynamic scene constraints. Together, they lead to consistent reconstructions of camera trajectories, human meshes, and dense scene point clouds in a common world frame. Project page: https://paulchhuang.github.io/synchmr",
        "page": "http://arxiv.org/abs/2405.14855",
        "pdf": "http://arxiv.org/pdf/2405.14855.pdf"
    },
    {
        "title": "Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding",
        "author": "Peng Jin, Ryuichi Takanobu, Cai Zhang, Xiaochun Cao, Li Yuan",
        "abstract": "Large language models have demonstrated impressive universal capabilities across a wide range of open-ended tasks and have extended their utility to encompass multimodal conversations. However, existing methods encounter challenges in effectively handling both image and video understanding, particularly with limited visual tokens. In this work, we introduce Chat-UniVi, a Unified Vision-language model capable of comprehending and engaging in conversations involving images and videos through a unified visual representation. Specifically, we employ a set of dynamic visual tokens to uniformly represent images and videos. This representation framework empowers the model to efficiently utilize a limited number of visual tokens to simultaneously capture the spatial details necessary for images and the comprehensive temporal relationship required for videos. Moreover, we leverage a multi-scale representation, enabling the model to perceive both high-level semantic concepts and low-level visual details. Notably, Chat-UniVi is trained on a mixed dataset containing both images and videos, allowing direct application to tasks involving both mediums without requiring any modifications. Extensive experimental results demonstrate that Chat-UniVi consistently outperforms even existing methods exclusively designed for either images or videos. Code is available at https://github.com/PKU-YuanGroup/Chat-UniVi.",
        "page": "http://arxiv.org/abs/2311.08046",
        "pdf": "http://arxiv.org/pdf/2311.08046.pdf"
    },
    {
        "title": "FastMAC: Stochastic Spectral Sampling of Correspondence Graph",
        "author": "Yifei Zhang, Hao Zhao, Hongyang Li, Siheng Chen",
        "abstract": "3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision. A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph. This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC). However, its properties have not been well understood. So we present the first study that introduces graph signal processing into the domain of correspondence graph. We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal. To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy. As such, the core of our method is the stochastic spectral sampling of correspondence graph. As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop. Through extensive experiments, we validate that FastMAC works for both indoor and outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI. Codes are publicly available at https://github.com/Forrest-110/FastMAC.",
        "page": "http://arxiv.org/abs/2403.08770",
        "pdf": "http://arxiv.org/pdf/2403.08770.pdf"
    },
    {
        "title": "UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes",
        "author": "David Rozenberszki, Or Litany, Angela Dai",
        "abstract": "3D instance segmentation is fundamental to geometric understanding of the world around us. Existing methods for instance segmentation of 3D scenes rely on supervision from expensive, manual 3D annotations. We propose UnScene3D, the first fully unsupervised 3D learning approach for class-agnostic 3D instance segmentation of indoor scans. UnScene3D first generates pseudo masks by leveraging self-supervised color and geometry features to find potential object regions. We operate on a basis of geometric oversegmentation, enabling efficient representation and learning on high-resolution 3D data. The coarse proposals are then refined through self-training our model on its predictions. Our approach improves over state-of-the-art unsupervised 3D instance segmentation methods by more than 300% Average Precision score, demonstrating effective instance segmentation even in challenging, cluttered 3D scenes.",
        "page": "http://arxiv.org/abs/2303.14541",
        "pdf": "http://arxiv.org/pdf/2303.14541.pdf"
    },
    {
        "title": "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image",
        "author": "Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui",
        "abstract": "Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/",
        "page": "http://arxiv.org/abs/2404.02152",
        "pdf": "http://arxiv.org/pdf/2404.02152.pdf"
    },
    {
        "title": "ADFactory: An Effective Framework for Generalizing Optical Flow with NeRF",
        "author": "Han Ling, Quansen Sun, Yinghui Sun, Xian Xu, Xingfeng Li",
        "abstract": "A significant challenge facing current optical flow methods is the difficulty in generalizing them well to the real world. This is mainly due to the high cost of hand-crafted datasets, and existing self-supervised methods are limited by indirect loss and occlusions, resulting in fuzzy outcomes. To address this challenge, we introduce a novel optical flow training framework: automatic data factory (ADF). ADF only requires RGB images as input to effectively train the optical flow network on the target data domain. Specifically, we use advanced Nerf technology to reconstruct scenes from photo groups collected by a monocular camera, and then calculate optical flow labels between camera pose pairs based on the rendering results. To eliminate erroneous labels caused by defects in the scene reconstructed by Nerf, we screened the generated labels from multiple aspects, such as optical flow matching accuracy, radiation field confidence, and depth consistency. The filtered labels can be directly used for network supervision. Experimentally, the generalization ability of ADF on KITTI surpasses existing self-supervised optical flow and monocular scene flow algorithms. In addition, ADF achieves impressive results in real-world zero-point generalization evaluations and surpasses most supervised methods.",
        "page": "http://arxiv.org/abs/2311.04246",
        "pdf": "http://arxiv.org/pdf/2311.04246.pdf"
    },
    {
        "title": "Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models",
        "author": "Pengze Zhang, Hubery Yin, Chen Li, Xiaohua Xie",
        "abstract": "Most diffusion models assume that the reverse process adheres to a Gaussian distribution. However, this approximation has not been rigorously validated, especially at singularities, where t=0 and t=1. Improperly dealing with such singularities leads to an average brightness issue in applications, and limits the generation of images with extreme brightness or darkness. We primarily focus on tackling singularities from both theoretical and practical perspectives. Initially, we establish the error bounds for the reverse process approximation, and showcase its Gaussian characteristics at singularity time steps. Based on this theoretical insight, we confirm the singularity at t=1 is conditionally removable while it at t=0 is an inherent property. Upon these significant conclusions, we propose a novel plug-and-play method SingDiffusion to address the initial singular time step sampling, which not only effectively resolves the average brightness issue for a wide range of diffusion models without extra training efforts, but also enhances their generation capability in achieving notable lower FID scores.",
        "page": "http://arxiv.org/abs/2403.08381",
        "pdf": "http://arxiv.org/pdf/2403.08381.pdf"
    },
    {
        "title": "Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices",
        "author": "Huancheng Chen, Haris Vikalo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis",
        "author": "Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang",
        "abstract": "Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes are available at https://github.com/ispc-lab/LiDAR4D.",
        "page": "http://arxiv.org/abs/2404.02742",
        "pdf": "http://arxiv.org/pdf/2404.02742.pdf"
    },
    {
        "title": "Steganographic Passport: An Owner and User Verifiable Credential for Deep Model IP Protection Without Retraining",
        "author": "Qi Cui, Ruohan Meng, Chaohui Xu, Chip Hong Chang",
        "abstract": "Ensuring the legal usage of deep models is crucial to promoting trustable, accountable, and responsible artificial intelligence innovation. Current passport-based methods that obfuscate model functionality for license-to-use and ownership verifications suffer from capacity and quality constraints, as they require retraining the owner model for new users. They are also vulnerable to advanced Expanded Residual Block ambiguity attacks. We propose Steganographic Passport, which uses an invertible steganographic network to decouple license-to-use from ownership verification by hiding the user's identity images into the owner-side passport and recovering them from their respective user-side passports. An irreversible and collision-resistant hash function is used to avoid exposing the owner-side passport from the derived user-side passports and increase the uniqueness of the model signature. To safeguard both the passport and model's weights against advanced ambiguity attacks, an activation-level obfuscation is proposed for the verification branch of the owner's model. By jointly training the verification and deployment branches, their weights become tightly coupled. The proposed method supports agile licensing of deep models by providing a strong ownership proof and license accountability without requiring a separate model retraining for the admission of every new user. Experiment results show that our Steganographic Passport outperforms other passport-based deep model protection methods in robustness against various known attacks.",
        "page": "http://arxiv.org/abs/2404.02889",
        "pdf": "http://arxiv.org/pdf/2404.02889.pdf"
    },
    {
        "title": "LTA-PCS: Learnable Task-Agnostic Point Cloud Sampling",
        "author": "Jiaheng Liu, Jianhao Li, Kaisiyuan Wang, Hongcheng Guo, Jian Yang, Junran Peng, Ke Xu, Xianglong Liu, Jinyang Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Boosting Adversarial Training via Fisher-Rao Norm-based Regularization",
        "author": "Xiangyu Yin, Wenjie Ruan",
        "abstract": "Adversarial training is extensively utilized to improve the adversarial robustness of deep neural networks. Yet, mitigating the degradation of standard generalization performance in adversarial-trained models remains an open problem. This paper attempts to resolve this issue through the lens of model complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron. Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in adversarial training. Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between adversarial-trained and standard-trained models, especially during the initial and final phases of the training process. Building upon this observation, we propose a novel regularization framework, called Logit-Oriented Adversarial Training (LOAT), which can mitigate the trade-off between robustness and accuracy while imposing only a negligible increase in computational overhead. Our extensive experiments demonstrate that the proposed regularization strategy can boost the performance of the prevalent adversarial training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT, across various network architectures. Our code will be available at https://github.com/TrustAI/LOAT.",
        "page": "http://arxiv.org/abs/2403.17520",
        "pdf": "http://arxiv.org/pdf/2403.17520.pdf"
    },
    {
        "title": "Towards Learning a Generalist Model for Embodied Navigation",
        "author": "Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang",
        "abstract": "Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries. Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios. Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation. Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based instruction. The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks. This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide range of capabilities required by embodied navigation. We conduct extensive experiments to evaluate the performance and generalizability of our model. The experimental results demonstrate that our unified model achieves state-of-the-art performance on CVDN, SOON, and ScanQA. Specifically, it surpasses the previous stats-of-the-art method by a significant margin of 29% in goal progress on CVDN. Moreover, our model also demonstrates strong generalizability and presents impressive results on unseen tasks, e.g., embodied question answering and 3D captioning.",
        "page": "http://arxiv.org/abs/2312.02010",
        "pdf": "http://arxiv.org/pdf/2312.02010.pdf"
    },
    {
        "title": "Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation",
        "author": "Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard",
        "abstract": "Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both pose-dominant and shape-dominant deformations. Using different challenging datasets, we demonstrate that our method outperforms previous state-of-the-art methods for both shape matching and interpolation, even compared to supervised approaches.",
        "page": "http://arxiv.org/abs/2402.18920",
        "pdf": "http://arxiv.org/pdf/2402.18920.pdf"
    },
    {
        "title": "MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection",
        "author": "Jakub Micorek, Horst Possegger, Dominik Narnhofer, Horst Bischof, Mateusz Kozinski",
        "abstract": "We propose a novel approach to video anomaly detection: we treat feature vectors extracted from videos as realizations of a random variable with a fixed distribution and model this distribution with a neural network. This lets us estimate the likelihood of test videos and detect video anomalies by thresholding the likelihood estimates. We train our video anomaly detector using a modification of denoising score matching, a method that injects training data with noise to facilitate modeling its distribution. To eliminate hyperparameter selection, we model the distribution of noisy video features across a range of noise levels and introduce a regularizer that tends to align the models for different levels of noise. At test time, we combine anomaly indications at multiple noise scales with a Gaussian mixture model. Running our video anomaly detector induces minimal delays as inference requires merely extracting the features and forward-propagating them through a shallow neural network and a Gaussian mixture model. Our experiments on five popular video anomaly detection benchmarks demonstrate state-of-the-art performance, both in the object-centric and in the frame-centric setup.",
        "page": "http://arxiv.org/abs/2403.14497",
        "pdf": "http://arxiv.org/pdf/2403.14497.pdf"
    },
    {
        "title": "Zero-Painter: Training-Free Layout Control for Text-to-Image Synthesis",
        "author": "Marianna Ohanyan, Hayk Manukyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "View From Above: Orthogonal viewpoint aware Cross-view Localization",
        "author": "Shan Wang, Chuong Nguyen, Jiawei Liu, Yanhao Zhang, Sundaram Muthu, Fahira Afzal Maken, Kaihao Zhang, Hongdong Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Context-Aware Integration of Language and Visual References for Natural Language Tracking",
        "author": "Yanyan Shao, Shuting He, Qi Ye, Yuchao Feng, Wenhan Luo, Jiming Chen",
        "abstract": "Tracking by natural language specification (TNL) aims to consistently localize a target in a video sequence given a linguistic description in the initial frame. Existing methodologies perform language-based and template-based matching for target reasoning separately and merge the matching results from two sources, which suffer from tracking drift when language and visual templates miss-align with the dynamic target state and ambiguity in the later merging stage. To tackle the issues, we propose a joint multi-modal tracking framework with 1) a prompt modulation module to leverage the complementarity between temporal visual templates and language expressions, enabling precise and context-aware appearance and linguistic cues, and 2) a unified target decoding module to integrate the multi-modal reference cues and executes the integrated queries on the search image to predict the target location in an end-to-end manner directly. This design ensures spatio-temporal consistency by leveraging historical visual information and introduces an integrated solution, generating predictions in a single step. Extensive experiments conducted on TNL2K, OTB-Lang, LaSOT, and RefCOCOg validate the efficacy of our proposed approach. The results demonstrate competitive performance against state-of-the-art methods for both tracking and grounding.",
        "page": "http://arxiv.org/abs/2403.19975",
        "pdf": "http://arxiv.org/pdf/2403.19975.pdf"
    },
    {
        "title": "DIMAT: Decentralized Iterative Merging-And-Training for Deep Learning Models",
        "author": "Nastaran Saadati, Minh Pham, Nasla Saleem, Joshua R. Waite, Aditya Balu, Zhanhong Jiang, Chinmay Hegde, Soumik Sarkar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes",
        "author": "Alexandros Delitzas, Ay\u00e7a Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, Francis Engelmann",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation",
        "author": "Xiao Ma, Sumit Patidar, Iain Haughton, Stephen James",
        "abstract": "This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics. Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world.",
        "page": "http://arxiv.org/abs/2403.03890",
        "pdf": "http://arxiv.org/pdf/2403.03890.pdf"
    },
    {
        "title": "Revisiting Spatial-Frequency Information Integration from a Hierarchical Perspective for Panchromatic and Multi-Spectral Image Fusion",
        "author": "Jiangtong Tan, Jie Huang, Naishan Zheng, Man Zhou, Keyu Yan, Danfeng Hong, Feng Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Robust Learning to Optimize with Theoretical Guarantees",
        "author": "Qingyu Song, Wei Lin, Juncheng Wang, Hong Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Boosting Neural Representations for Videos with a Conditional Decoder",
        "author": "XINJIE ZHANG, Ren Yang, Dailan He, Xingtong Ge, Tongda Xu, Yan Wang, Hongwei Qin, Jun Zhang",
        "abstract": "Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts multiple baseline INRs in the reconstruction quality and convergence speed for video regression, and exhibits superior inpainting and interpolation results. Further, we integrate a consistent entropy minimization technique and develop video codecs based on these boosted INRs. Experiments on the UVG dataset confirm that our enhanced codecs significantly outperform baseline INRs and offer competitive rate-distortion performance compared to traditional and learning-based codecs. Code is available at https://github.com/Xinjie-Q/Boosting-NeRV.",
        "page": "http://arxiv.org/abs/2402.18152",
        "pdf": "http://arxiv.org/pdf/2402.18152.pdf"
    },
    {
        "title": "Generating Non-Stationary Textures using Self-Rectification",
        "author": "Yang Zhou, Rongjun Xiao, Dani Lischinski, Daniel Cohen-Or, Hui Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking the Evaluation Protocol of Domain Generalization",
        "author": "Han Yu, Xingxuan Zhang, Renzhe Xu, Jiashuo Liu, Yue He, Peng Cui",
        "abstract": "Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is required that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the risks of test data information leakage from two aspects of the current evaluation protocol: supervised pretraining on ImageNet and oracle model selection. We propose modifications to the current protocol that we should employ self-supervised pretraining or train from scratch instead of employing the current supervised pretraining, and we should use multiple test domains. These would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the modified protocol and introduce new leaderboards to encourage future research in domain generalization with a fairer comparison.",
        "page": "http://arxiv.org/abs/2305.15253",
        "pdf": "http://arxiv.org/pdf/2305.15253.pdf"
    },
    {
        "title": "A Generative Approach for Wikipedia-Scale Visual Entity Recognition",
        "author": "Mathilde Caron, Ahmet Iscen, Alireza Fathi, Cordelia Schmid",
        "abstract": "In this paper, we address web-scale visual entity recognition, specifically the task of mapping a given query image to one of the 6 million existing entities in Wikipedia. One way of approaching a problem of such scale is using dual-encoder models (eg CLIP), where all the entity names and query images are embedded into a unified space, paving the way for an approximate k-NN search. Alternatively, it is also possible to re-purpose a captioning model to directly generate the entity names for a given image. In contrast, we introduce a novel Generative Entity Recognition (GER) framework, which given an input image learns to auto-regressively decode a semantic and discriminative ``code'' identifying the target entity. Our experiments demonstrate the efficacy of this GER paradigm, showcasing state-of-the-art performance on the challenging OVEN benchmark. GER surpasses strong captioning, dual-encoder, visual matching and hierarchical classification baselines, affirming its advantage in tackling the complexities of web-scale recognition.",
        "page": "http://arxiv.org/abs/2403.02041",
        "pdf": "http://arxiv.org/pdf/2403.02041.pdf"
    },
    {
        "title": "Enhance Image Classification Via Inter-Class Image Mixup With Diffusion Model",
        "author": "Zhicai Wang, Longhui Wei, Tan Wang, Heyu Chen, Yanbin Hao, Xiang Wang, Xiangnan He, Qi Tian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models",
        "author": "David Stotko, Nils Wandel, Reinhard Klein",
        "abstract": "3D reconstruction of dynamic scenes is a long-standing problem in computer graphics and increasingly difficult the less information is available. Shape-from-Template (SfT) methods aim to reconstruct a template-based geometry from RGB images or video sequences, often leveraging just a single monocular camera without depth information, such as regular smartphone recordings. Unfortunately, existing reconstruction methods are either unphysical and noisy or slow in optimization. To solve this problem, we propose a novel SfT reconstruction algorithm for cloth using a pre-trained neural surrogate model that is fast to evaluate, stable, and produces smooth reconstructions due to a regularizing physics simulation. Differentiable rendering of the simulated mesh enables pixel-wise comparisons between the reconstruction and a target video sequence that can be used for a gradient-based optimization procedure to extract not only shape information but also physical parameters such as stretching, shearing, or bending stiffness of the cloth. This allows to retain a precise, stable, and smooth reconstructed geometry while reducing the runtime by a factor of 400-500 compared to $\\phi$-SfT, a state-of-the-art physics-based SfT approach.",
        "page": "http://arxiv.org/abs/2311.12796",
        "pdf": "http://arxiv.org/pdf/2311.12796.pdf"
    },
    {
        "title": "Time-Efficient Light-Field Acquisition Using Coded Aperture and Events",
        "author": "Shuji Habuchi, Keita Takahashi, Chihiro Tsutake, Toshiaki Fujii, Hajime Nagahara",
        "abstract": "We propose a computational imaging method for time-efficient light-field acquisition that combines a coded aperture with an event-based camera. Different from the conventional coded-aperture imaging method, our method applies a sequence of coding patterns during a single exposure for an image frame. The parallax information, which is related to the differences in coding patterns, is recorded as events. The image frame and events, all of which are measured in a single exposure, are jointly used to computationally reconstruct a light field. We also designed an algorithm pipeline for our method that is end-to-end trainable on the basis of deep optics and compatible with real camera hardware. We experimentally showed that our method can achieve more accurate reconstruction than several other imaging methods with a single exposure. We also developed a hardware prototype with the potential to complete the measurement on the camera within 22 msec and demonstrated that light fields from real 3-D scenes can be obtained with convincing visual quality. Our software and supplementary video are available from our project website.",
        "page": "http://arxiv.org/abs/2403.07244",
        "pdf": "http://arxiv.org/pdf/2403.07244.pdf"
    },
    {
        "title": "Hierarchical Histogram Threshold Segmentation \u2013 Auto-terminating High-detail Oversegmentation",
        "author": "Thomas Chang, Simon Seibt, Bartosz von Rymon Lipinski",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction",
        "author": "Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu",
        "abstract": "Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction. To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement. However, they either incur a large amount of computation or bring limited improvement, if not both. In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation. Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a quality score to measure the prediction quality and remaining refinement potential of each scenario. SmartRefine is designed as a generic and flexible approach that can be seamlessly integrated into most state-of-the-art motion prediction models. Experiments on Argoverse (1 & 2) show that our method consistently improves the prediction accuracy of multiple state-of-the-art prediction models. Specifically, by adding SmartRefine to QCNet, we outperform all published ensemble-free works on the Argoverse 2 leaderboard (single agent track) at submission. Comprehensive studies are also conducted to ablate design choices and explore the mechanism behind multi-iteration refinement. Codes are available at https://github.com/opendilab/SmartRefine/",
        "page": "http://arxiv.org/abs/2403.11492",
        "pdf": "http://arxiv.org/pdf/2403.11492.pdf"
    },
    {
        "title": "Hierarchical Intra-modal Correlation Learning for Label-free 3D Semantic Segmentation",
        "author": "Xin Kang, Lei Chu, Jiahao Li, Xuejin Chen, Yan Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Transferable Structural Sparse Adversarial Attack Via Exact Group Sparsity Training",
        "author": "Di Ming, Peng Ren, Yunlong Wang, Xin Feng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsegment Anything by Simulating Deformation",
        "author": "Jiahao Lu, Xingyi Yang, Xinchao Wang",
        "abstract": "Foundation segmentation models, while powerful, pose a significant risk: they enable users to effortlessly extract any objects from any digital content with a single click, potentially leading to copyright infringement or malicious misuse. To mitigate this risk, we introduce a new task \"Anything Unsegmentable\" to grant any image \"the right to be unsegmented\". The ambitious pursuit of the task is to achieve highly transferable adversarial attacks against all prompt-based segmentation models, regardless of model parameterizations and prompts. We highlight the non-transferable and heterogeneous nature of prompt-specific adversarial noises. Our approach focuses on disrupting image encoder features to achieve prompt-agnostic attacks. Intriguingly, targeted feature attacks exhibit better transferability compared to untargeted ones, suggesting the optimal update direction aligns with the image manifold. Based on the observations, we design a novel attack named Unsegment Anything by Simulating Deformation (UAD). Our attack optimizes a differentiable deformation function to create a target deformed image, which alters structural information while preserving achievable feature distance by adversarial example. Extensive experiments verify the effectiveness of our approach, compromising a variety of promptable segmentation models with different architectures and prompt interfaces. We release the code at https://github.com/jiahaolu97/anything-unsegmentable.",
        "page": "http://arxiv.org/abs/2404.02585",
        "pdf": "http://arxiv.org/pdf/2404.02585.pdf"
    },
    {
        "title": "Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance",
        "author": "Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, Yu Liu",
        "abstract": "Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space. However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality. To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models. Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step. In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions. Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level. Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost. our codes are available at https://github.com/SmilesDZgk/S-CFG.",
        "page": "http://arxiv.org/abs/2404.05384",
        "pdf": "http://arxiv.org/pdf/2404.05384.pdf"
    },
    {
        "title": "Learn from View Correlation: An Anchor Enhancement Strategy for Multi-view Clustering",
        "author": "Suyuan Liu, KE LIANG, Zhibin Dong, Siwei Wang, Xihong Yang, sihang zhou, En Zhu, Xinwang Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Residual Denoising Diffusion Models",
        "author": "Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, Liangqiong Qu",
        "abstract": "We propose residual denoising diffusion models (RDDM), a novel dual diffusion process that decouples the traditional single denoising diffusion process into residual diffusion and noise diffusion. This dual diffusion framework expands the denoising-based diffusion models, initially uninterpretable for image restoration, into a unified and interpretable model for both image generation and restoration by introducing residuals. Specifically, our residual diffusion represents directional diffusion from the target image to the degraded input image and explicitly guides the reverse generation process for image restoration, while noise diffusion represents random perturbations in the diffusion process. The residual prioritizes certainty, while the noise emphasizes diversity, enabling RDDM to effectively unify tasks with varying certainty or diversity requirements, such as image generation and restoration. We demonstrate that our sampling process is consistent with that of DDPM and DDIM through coefficient transformation, and propose a partially path-independent generation process to better understand the reverse process. Notably, our RDDM enables a generic UNet, trained with only an L1 loss and a batch size of 1, to compete with state-of-the-art image restoration methods. We provide code and pre-trained models to encourage further exploration, application, and development of our innovative framework (https://github.com/nachifur/RDDM).",
        "page": "http://arxiv.org/abs/2308.13712",
        "pdf": "http://arxiv.org/pdf/2308.13712.pdf"
    },
    {
        "title": "3D Multi-frame Fusion for Video Stabilization",
        "author": "Zhan Peng, Xinyi Ye, Weiyue Zhao, TIANQI LIU, Huiqiang Sun, Baopu Li, Zhiguo Cao",
        "abstract": "In this paper, we present RStab, a novel framework for video stabilization that integrates 3D multi-frame fusion through volume rendering. Departing from conventional methods, we introduce a 3D multi-frame perspective to generate stabilized images, addressing the challenge of full-frame generation while preserving structure. The core of our approach lies in Stabilized Rendering (SR), a volume rendering module, which extends beyond the image fusion by incorporating feature fusion. The core of our RStab framework lies in Stabilized Rendering (SR), a volume rendering module, fusing multi-frame information in 3D space. Specifically, SR involves warping features and colors from multiple frames by projection, fusing them into descriptors to render the stabilized image. However, the precision of warped information depends on the projection accuracy, a factor significantly influenced by dynamic regions. In response, we introduce the Adaptive Ray Range (ARR) module to integrate depth priors, adaptively defining the sampling range for the projection process. Additionally, we propose Color Correction (CC) assisting geometric constraints with optical flow for accurate color aggregation. Thanks to the three modules, our RStab demonstrates superior performance compared with previous stabilizers in the field of view (FOV), image quality, and video stability across various datasets.",
        "page": "http://arxiv.org/abs/2404.12887",
        "pdf": "http://arxiv.org/pdf/2404.12887.pdf"
    },
    {
        "title": "Unifying Automatic and Interactive Matting with Pretrained ViTs",
        "author": "Zixuan Ye, Wenze Liu, He Guo, Yujia Liang, Chaoyi Hong, Hao Lu, Zhiguo Cao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HINTED: Hard Instance Enhanced Detector with Mixed-Density Feature Fusion for Sparsely-Supervised 3D Object Detection",
        "author": "Qiming Xia, Wei Ye, Hai Wu, Shijia Zhao, Leyuan Xing, Xun Huang, Jinhao Deng, Xin Li, Chenglu Wen, Cheng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians",
        "author": "Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao ZHOU, Boning Liu, Shengping Zhang, Liqiang Nie",
        "abstract": "We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.",
        "page": "http://arxiv.org/abs/2312.02134",
        "pdf": "http://arxiv.org/pdf/2312.02134.pdf"
    },
    {
        "title": "Towards HDR and HFR Video from Rolling-Mixed-Bit Spikings",
        "author": "Yakun Chang, Yeliduosi Xiaokaiti, Yujia Liu, Bin Fan, Zhaojun Huang, Tiejun Huang, Boxin Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Federated Generalized Category Discovery",
        "author": "Nan Pu, Wenjing Li, Xinyuan Ji, Yalan Qin, Nicu Sebe, Zhun Zhong",
        "abstract": "Generalized category discovery (GCD) aims at grouping unlabeled samples from known and unknown classes, given labeled data of known classes. To meet the recent decentralization trend in the community, we introduce a practical yet challenging task, namely Federated GCD (Fed-GCD), where the training data are distributively stored in local clients and cannot be shared among clients. The goal of Fed-GCD is to train a generic GCD model by client collaboration under the privacy-protected constraint. The Fed-GCD leads to two challenges: 1) representation degradation caused by training each client model with fewer data than centralized GCD learning, and 2) highly heterogeneous label spaces across different clients. To this end, we propose a novel Associated Gaussian Contrastive Learning (AGCL) framework based on learnable GMMs, which consists of a Client Semantics Association (CSA) and a global-local GMM Contrastive Learning (GCL). On the server, CSA aggregates the heterogeneous categories of local-client GMMs to generate a global GMM containing more comprehensive category knowledge. On each client, GCL builds class-level contrastive learning with both local and global GMMs. The local GCL learns robust representation with limited local data. The global GCL encourages the model to produce more discriminative representation with the comprehensive category relationships that may not exist in local data. We build a benchmark based on six visual datasets to facilitate the study of Fed-GCD. Extensive experiments show that our AGCL outperforms the FedAvg-based baseline on all datasets.",
        "page": "http://arxiv.org/abs/2305.14107",
        "pdf": "http://arxiv.org/pdf/2305.14107.pdf"
    },
    {
        "title": "Characteristics Matching Based Hash Codes Generation for Efficient Fine-grained Image Retrieval",
        "author": "Zhen-Duo Chen, Li-Jun Zhao, Zi-Chao Zhang, Xin Luo, Xin-Shun Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision",
        "author": "Chenyangguang Zhang, Guanlong Jiao, Yan Di, Gu Wang, Ziqin Huang, Ruida Zhang, Fabian Manhardt, Bowen Fu, Federico Tombari, Xiangyang Ji",
        "abstract": "Previous works concerning single-view hand-held object reconstruction typically rely on supervision from 3D ground-truth models, which are hard to collect in real world. In contrast, readily accessible hand-object videos offer a promising training data source, but they only give heavily occluded object observations. In this paper, we present a novel synthetic-to-real framework to exploit Multi-view Occlusion-aware supervision from hand-object videos for Hand-held Object reconstruction (MOHO) from a single image, tackling two predominant challenges in such setting: hand-induced occlusion and object's self-occlusion. First, in the synthetic pre-training stage, we render a large-scaled synthetic dataset SOMVideo with hand-object images and multi-view occlusion-free supervisions, adopted to address hand-induced occlusion in both 2D and 3D spaces. Second, in the real-world finetuning stage, MOHO leverages the amodal-mask-weighted geometric supervision to mitigate the unfaithful guidance caused by the hand-occluded supervising views in real world. Moreover, domain-consistent occlusion-aware features are amalgamated in MOHO to resist object's self-occlusion for inferring the complete object shape. Extensive experiments on HO3D and DexYCB datasets demonstrate 2D-supervised MOHO gains superior results against 3D-supervised methods by a large margin.",
        "page": "http://arxiv.org/abs/2310.11696",
        "pdf": "http://arxiv.org/pdf/2310.11696.pdf"
    },
    {
        "title": "ShapeMatcher: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation",
        "author": "Yan Di, Chenyangguang Zhang, Chaowei Wang, Ruida Zhang, Guangyao Zhai, Yanyan Li, Bowen Fu, Xiangyang Ji, Shan Gao",
        "abstract": "In this paper, we present ShapeMatcher, a unified self-supervised learning framework for joint shape canonicalization, segmentation, retrieval and deformation. Given a partially-observed object in an arbitrary pose, we first canonicalize the object by extracting point-wise affine-invariant features, disentangling inherent structure of the object with its pose and size. These learned features are then leveraged to predict semantically consistent part segmentation and corresponding part centers. Next, our lightweight retrieval module aggregates the features within each part as its retrieval token and compare all the tokens with source shapes from a pre-established database to identify the most geometrically similar shape. Finally, we deform the retrieved shape in the deformation module to tightly fit the input object by harnessing part center guided neural cage deformation. The key insight of ShapeMaker is the simultaneous training of the four highly-associated processes: canonicalization, segmentation, retrieval, and deformation, leveraging cross-task consistency losses for mutual supervision. Extensive experiments on synthetic datasets PartNet, ComplementMe, and real-world dataset Scan2CAD demonstrate that ShapeMaker surpasses competitors by a large margin.",
        "page": "http://arxiv.org/abs/2311.11106",
        "pdf": "http://arxiv.org/pdf/2311.11106.pdf"
    },
    {
        "title": "KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation",
        "author": "Ruida Zhang, Chenyangguang Zhang, Yan Di, Fabian Manhardt, Xingyu Liu, Federico Tombari, Xiangyang Ji",
        "abstract": "In this paper, we present KP-RED, a unified KeyPoint-driven REtrieval and Deformation framework that takes object scans as input and jointly retrieves and deforms the most geometrically similar CAD models from a pre-processed database to tightly match the target. Unlike existing dense matching based methods that typically struggle with noisy partial scans, we propose to leverage category-consistent sparse keypoints to naturally handle both full and partial object scans. Specifically, we first employ a lightweight retrieval module to establish a keypoint-based embedding space, measuring the similarity among objects by dynamically aggregating deformation-aware local-global features around extracted keypoints. Objects that are close in the embedding space are considered similar in geometry. Then we introduce the neural cage-based deformation module that estimates the influence vector of each keypoint upon cage vertices inside its local support region to control the deformation of the retrieved shape. Extensive experiments on the synthetic dataset PartNet and the real-world dataset Scan2CAD demonstrate that KP-RED surpasses existing state-of-the-art approaches by a large margin. Codes and trained models will be released in https://github.com/lolrudy/KP-RED.",
        "page": "http://arxiv.org/abs/2403.10099",
        "pdf": "http://arxiv.org/pdf/2403.10099.pdf"
    },
    {
        "title": "Vlogger: Make Your Dream A Vlog",
        "author": "Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, Yali Wang",
        "abstract": "In this work, we present Vlogger, a generic AI system for generating a minute-level video blog (i.e., vlog) of user descriptions. Different from short videos with a few seconds, vlog often contains a complex storyline with diversified scenes, which is challenging for most existing video generation approaches. To break through this bottleneck, our Vlogger smartly leverages Large Language Model (LLM) as Director and decomposes a long video generation task of vlog into four key stages, where we invoke various foundation models to play the critical roles of vlog professionals, including (1) Script, (2) Actor, (3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings, our Vlogger can generate vlogs through explainable cooperation of top-down planning and bottom-up shooting. Moreover, we introduce a novel video diffusion model, ShowMaker, which serves as a videographer in our Vlogger for generating the video snippet of each shooting scene. By incorporating Script and Actor attentively as textual and visual prompts, it can effectively enhance spatial-temporal coherence in the snippet. Besides, we design a concise mixed training paradigm for ShowMaker, boosting its capacity for both T2V generation and prediction. Finally, the extensive experiments show that our method achieves state-of-the-art performance on zero-shot T2V generation and prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs from open-world descriptions, without loss of video coherence on script and actor. The code and model is all available at https://github.com/zhuangshaobin/Vlogger.",
        "page": "http://arxiv.org/abs/2401.09414",
        "pdf": "http://arxiv.org/pdf/2401.09414.pdf"
    },
    {
        "title": "Selective-Stereo: Adaptive Frequency Information Selection for Stereo Matching",
        "author": "Xianqi Wang, Gangwei Xu, Hao Jia, Xin Yang",
        "abstract": "Stereo matching methods based on iterative optimization, like RAFT-Stereo and IGEV-Stereo, have evolved into a cornerstone in the field of stereo matching. However, these methods struggle to simultaneously capture high-frequency information in edges and low-frequency information in smooth regions due to the fixed receptive field. As a result, they tend to lose details, blur edges, and produce false matches in textureless areas. In this paper, we propose Selective Recurrent Unit (SRU), a novel iterative update operator for stereo matching. The SRU module can adaptively fuse hidden disparity information at multiple frequencies for edge and smooth regions. To perform adaptive fusion, we introduce a new Contextual Spatial Attention (CSA) module to generate attention maps as fusion weights. The SRU empowers the network to aggregate hidden disparity information across multiple frequencies, mitigating the risk of vital hidden disparity information loss during iterative processes. To verify SRU's universality, we apply it to representative iterative stereo matching methods, collectively referred to as Selective-Stereo. Our Selective-Stereo ranks $1^{st}$ on KITTI 2012, KITTI 2015, ETH3D, and Middlebury leaderboards among all published methods. Code is available at https://github.com/Windsrain/Selective-Stereo.",
        "page": "http://arxiv.org/abs/2403.00486",
        "pdf": "http://arxiv.org/pdf/2403.00486.pdf"
    },
    {
        "title": "AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search",
        "author": "Junghyup Lee, Bumsub Ham",
        "abstract": "Training-free network architecture search (NAS) aims to discover high-performing networks with zero-cost proxies, capturing network characteristics related to the final performance. However, network rankings estimated by previous training-free NAS methods have shown weak correlations with the performance. To address this issue, we propose AZ-NAS, a novel approach that leverages the ensemble of various zero-cost proxies to enhance the correlation between a predicted ranking of networks and the ground truth substantially in terms of the performance. To achieve this, we introduce four novel zero-cost proxies that are complementary to each other, analyzing distinct traits of architectures in the views of expressivity, progressivity, trainability, and complexity. The proxy scores can be obtained simultaneously within a single forward and backward pass, making an overall NAS process highly efficient. In order to integrate the rankings predicted by our proxies effectively, we introduce a non-linear ranking aggregation method that highlights the networks highly-ranked consistently across all the proxies. Experimental results conclusively demonstrate the efficacy and efficiency of AZ-NAS, outperforming state-of-the-art methods on standard benchmarks, all while maintaining a reasonable runtime cost.",
        "page": "http://arxiv.org/abs/2403.19232",
        "pdf": "http://arxiv.org/pdf/2403.19232.pdf"
    },
    {
        "title": "Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion",
        "author": "Su Sun, Cheng Zhao, Yuliang Guo, Ruoyu Wang, Xinyu Huang, Yingjie Victor Chen, Liu Ren",
        "abstract": "In this paper, we present a novel indoor 3D reconstruction method with occluded surface completion, given a sequence of depth readings. Prior state-of-the-art (SOTA) methods only focus on the reconstruction of the visible areas in a scene, neglecting the invisible areas due to the occlusions, e.g., the contact surface between furniture, occluded wall and floor. Our method tackles the task of completing the occluded scene surfaces, resulting in a complete 3D scene mesh. The core idea of our method is learning 3D geometry prior from various complete scenes to infer the occluded geometry of an unseen scene from solely depth measurements. We design a coarse-fine hierarchical octree representation coupled with a dual-decoder architecture, i.e., Geo-decoder and 3D Inpainter, which jointly reconstructs the complete 3D scene geometry. The Geo-decoder with detailed representation at fine levels is optimized online for each scene to reconstruct visible surfaces. The 3D Inpainter with abstract representation at coarse levels is trained offline using various scenes to complete occluded surfaces. As a result, while the Geo-decoder is specialized for an individual scene, the 3D Inpainter can be generally applied across different scenes. We evaluate the proposed method on the 3D Completed Room Scene (3D-CRS) and iTHOR datasets, significantly outperforming the SOTA methods by a gain of 16.8% and 24.2% in terms of the completeness of 3D reconstruction. 3D-CRS dataset including a complete 3D mesh of each scene is provided at project webpage.",
        "page": "http://arxiv.org/abs/2404.03070",
        "pdf": "http://arxiv.org/pdf/2404.03070.pdf"
    },
    {
        "title": "StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On",
        "author": "Jeongho Kim, Gyojung Gu, Minho Park, Sunghyun Park, Jaegul Choo",
        "abstract": "Given a clothing image and a person image, an image-based virtual try-on aims to generate a customized image that appears natural and accurately reflects the characteristics of the clothing image. In this work, we aim to expand the applicability of the pre-trained diffusion model so that it can be utilized independently for the virtual try-on task.The main challenge is to preserve the clothing details while effectively utilizing the robust generative capability of the pre-trained model. In order to tackle these issues, we propose StableVITON, learning the semantic correspondence between the clothing and the human body within the latent space of the pre-trained diffusion model in an end-to-end manner. Our proposed zero cross-attention blocks not only preserve the clothing details by learning the semantic correspondence but also generate high-fidelity images by utilizing the inherent knowledge of the pre-trained model in the warping process. Through our proposed novel attention total variation loss and applying augmentation, we achieve the sharp attention map, resulting in a more precise representation of clothing details. StableVITON outperforms the baselines in qualitative and quantitative evaluation, showing promising quality in arbitrary person images. Our code is available at https://github.com/rlawjdghek/StableVITON.",
        "page": "http://arxiv.org/abs/2312.01725",
        "pdf": "http://arxiv.org/pdf/2312.01725.pdf"
    },
    {
        "title": "MaxQ: Multi-Axis Query for N:M Sparsity Network",
        "author": "Jingyang Xiang, Siqi Li, Junhao Chen, Zhuangzhi Chen, Tianxin Huang, Linpeng Peng, Yong Liu",
        "abstract": "N:M sparsity has received increasing attention due to its remarkable performance and latency trade-off compared with structured and unstructured sparsity. However, existing N:M sparsity methods do not differentiate the relative importance of weights among blocks and leave important weights underappreciated. Besides, they directly apply N:M sparsity to the whole network, which will cause severe information loss. Thus, they are still sub-optimal. In this paper, we propose an efficient and effective Multi-Axis Query methodology, dubbed as MaxQ, to rectify these problems. During the training, MaxQ employs a dynamic approach to generate soft N:M masks, considering the weight importance across multiple axes. This method enhances the weights with more importance and ensures more effective updates. Meanwhile, a sparsity strategy that gradually increases the percentage of N:M weight blocks is applied, which allows the network to heal from the pruning-induced damage progressively. During the runtime, the N:M soft masks can be precomputed as constants and folded into weights without causing any distortion to the sparse pattern and incurring additional computational overhead. Comprehensive experiments demonstrate that MaxQ achieves consistent improvements across diverse CNN architectures in various computer vision tasks, including image classification, object detection and instance segmentation. For ResNet50 with 1:16 sparse pattern, MaxQ can achieve 74.6\\% top-1 accuracy on ImageNet and improve by over 2.8\\% over the state-of-the-art. Codes and checkpoints are available at \\url{https://github.com/JingyangXiang/MaxQ}.",
        "page": "http://arxiv.org/abs/2312.07061",
        "pdf": "http://arxiv.org/pdf/2312.07061.pdf"
    },
    {
        "title": "T4P: Test-Time Training of Trajectory Prediction via Masked Autoencoder and Actor-specific Token Memory",
        "author": "Daehee Park, Jaeseok Jeong, Sung-Hoon Yoon, Jaewoo Jeong, Kuk-Jin Yoon",
        "abstract": "Trajectory prediction is a challenging problem that requires considering interactions among multiple actors and the surrounding environment. While data-driven approaches have been used to address this complex problem, they suffer from unreliable predictions under distribution shifts during test time. Accordingly, several online learning methods have been proposed using regression loss from the ground truth of observed data leveraging the auto-labeling nature of trajectory prediction task. We mainly tackle the following two issues. First, previous works underfit and overfit as they only optimize the last layer of the motion decoder. To this end, we employ the masked autoencoder (MAE) for representation learning to encourage complex interaction modeling in shifted test distribution for updating deeper layers. Second, utilizing the sequential nature of driving data, we propose an actor-specific token memory that enables the test-time learning of actor-wise motion characteristics. Our proposed method has been validated across various challenging cross-dataset distribution shift scenarios including nuScenes, Lyft, Waymo, and Interaction. Our method surpasses the performance of existing state-of-the-art online learning methods in terms of both prediction accuracy and computational efficiency. The code is available at https://github.com/daeheepark/T4P.",
        "page": "http://arxiv.org/abs/2403.10052",
        "pdf": "http://arxiv.org/pdf/2403.10052.pdf"
    },
    {
        "title": "ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired Image Dehazing",
        "author": "Zhongze Wang, Haitao Zhao, Jingchao Peng, Lujian Yao, Kaijie Zhao",
        "abstract": "Unpaired image dehazing (UID) holds significant research importance due to the challenges in acquiring haze/clear image pairs with identical backgrounds. This paper proposes a novel method for UID named Orthogonal Decoupling Contrastive Regularization (ODCR). Our method is grounded in the assumption that an image consists of both haze-related features, which influence the degree of haze, and haze-unrelated features, such as texture and semantic information. ODCR aims to ensure that the haze-related features of the dehazing result closely resemble those of the clear image, while the haze-unrelated features align with the input hazy image. To accomplish the motivation, Orthogonal MLPs optimized geometrically on the Stiefel manifold are proposed, which can project image features into an orthogonal space, thereby reducing the relevance between different features. Furthermore, a task-driven Depth-wise Feature Classifier (DWFC) is proposed, which assigns weights to the orthogonal features based on the contribution of each channel's feature in predicting whether the feature source is hazy or clear in a self-supervised fashion. Finally, a Weighted PatchNCE (WPNCE) loss is introduced to achieve the pulling of haze-related features in the output image toward those of clear images, while bringing haze-unrelated features close to those of the hazy input. Extensive experiments demonstrate the superior performance of our ODCR method on UID.",
        "page": "http://arxiv.org/abs/2404.17825",
        "pdf": "http://arxiv.org/pdf/2404.17825.pdf"
    },
    {
        "title": "HashPoint: Accelerated Point Searching and Sampling for Neural Rendering",
        "author": "Jiahao Ma, Miaomiao Liu, David Ahmedt-Aristizabal, Chuong Nguyen",
        "abstract": "In this paper, we address the problem of efficient point searching and sampling for volume neural rendering. Within this realm, two typical approaches are employed: rasterization and ray tracing. The rasterization-based methods enable real-time rendering at the cost of increased memory and lower fidelity. In contrast, the ray-tracing-based methods yield superior quality but demand longer rendering time. We solve this problem by our HashPoint method combining these two strategies, leveraging rasterization for efficient point searching and sampling, and ray marching for rendering. Our method optimizes point searching by rasterizing points within the camera's view, organizing them in a hash table, and facilitating rapid searches. Notably, we accelerate the rendering process by adaptive sampling on the primary surface encountered by the ray. Our approach yields substantial speed-up for a range of state-of-the-art ray-tracing-based methods, maintaining equivalent or superior accuracy across synthetic and real test datasets. The code will be available at https://jiahao-ma.github.io/hashpoint/.",
        "page": "http://arxiv.org/abs/2404.14044",
        "pdf": "http://arxiv.org/pdf/2404.14044.pdf"
    },
    {
        "title": "OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation",
        "author": "Ganlong Zhao, Guanbin Li, Weikai Chen, Yizhou Yu",
        "abstract": "Recent advances in Iterative Vision-and-Language Navigation (IVLN) introduce a more meaningful and practical paradigm of VLN by maintaining the agent's memory across tours of scenes. Although the long-term memory aligns better with the persistent nature of the VLN task, it poses more challenges on how to utilize the highly unstructured navigation memory with extremely sparse supervision. Towards this end, we propose OVER-NAV, which aims to go over and beyond the current arts of IVLN techniques. In particular, we propose to incorporate LLMs and open-vocabulary detectors to distill key information and establish correspondence between multi-modal signals. Such a mechanism introduces reliable cross-modal supervision and enables on-the-fly generalization to unseen scenes without the need of extra annotation and re-training. To fully exploit the interpreted navigation data, we further introduce a structured representation, coded Omnigraph, to effectively integrate multi-modal information along the tour. Accompanied with a novel omnigraph fusion mechanism, OVER-NAV is able to extract the most relevant knowledge from omnigraph for a more accurate navigating action. In addition, OVER-NAV seamlessly supports both discrete and continuous environments under a unified framework. We demonstrate the superiority of OVER-NAV in extensive experiments.",
        "page": "http://arxiv.org/abs/2403.17334",
        "pdf": "http://arxiv.org/pdf/2403.17334.pdf"
    },
    {
        "title": "Text2Loc: 3D Point Cloud Localization from Natural Language",
        "author": "Yan Xia, Letian Shi, Zifeng Ding, Jo\u00e3o F. Henriques, Daniel Cremers",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
        "author": "Lei Zhu, Fangyun Wei, Yanye Lu",
        "abstract": "In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multi-modal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a ``foreign language'' with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rigorous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2L-Tokenizer.",
        "page": "http://arxiv.org/abs/2403.07874",
        "pdf": "http://arxiv.org/pdf/2403.07874.pdf"
    },
    {
        "title": "Building Bridges across Spatial and Temporal Resolutions: Reference-Based Super-Resolution via Change Priors and Conditional Diffusion Model",
        "author": "Runmin Dong, Shuai Yuan, Bin Luo, Mengxuan Chen, Jinxiao Zhang, Lixian Zhang, Weijia Li, Juepeng Zheng, Haohuan Fu",
        "abstract": "Reference-based super-resolution (RefSR) has the potential to build bridges across spatial and temporal resolutions of remote sensing images. However, existing RefSR methods are limited by the faithfulness of content reconstruction and the effectiveness of texture transfer in large scaling factors. Conditional diffusion models have opened up new opportunities for generating realistic high-resolution images, but effectively utilizing reference images within these models remains an area for further exploration. Furthermore, content fidelity is difficult to guarantee in areas without relevant reference information. To solve these issues, we propose a change-aware diffusion model named Ref-Diff for RefSR, using the land cover change priors to guide the denoising process explicitly. Specifically, we inject the priors into the denoising model to improve the utilization of reference information in unchanged areas and regulate the reconstruction of semantically relevant content in changed areas. With this powerful guidance, we decouple the semantics-guided denoising and reference texture-guided denoising processes to improve the model performance. Extensive experiments demonstrate the superior effectiveness and robustness of the proposed method compared with state-of-the-art RefSR methods in both quantitative and qualitative evaluations. The code and data are available at https://github.com/dongrunmin/RefDiff.",
        "page": "http://arxiv.org/abs/2403.17460",
        "pdf": "http://arxiv.org/pdf/2403.17460.pdf"
    },
    {
        "title": "Gradient Alignment for Cross-domain Face Anti-Spoofing",
        "author": "MINH BINH LE, Simon Woo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Exploring Pose-Aware Human-Object Interaction via Hybrid Learning",
        "author": "EASTMAN Z Y WU, Yali Li, Yuan Wang, Shengjin Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Friendly Sharpness-Aware Minimization",
        "author": "Tao Li, Pan Zhou, Zhengbao He, Xinwen Cheng, Xiaolin Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MonoCD: Monocular 3D Object Detection with Complementary Depths",
        "author": "Longfei Yan, Pei Yan, Shengzhou Xiong, Xuanyu Xiang, Yihua Tan",
        "abstract": "Monocular 3D object detection has attracted widespread attention due to its potential to accurately obtain object 3D localization from a single image at a low cost. Depth estimation is an essential but challenging subtask of monocular 3D object detection due to the ill-posedness of 2D to 3D mapping. Many methods explore multiple local depth clues such as object heights and keypoints and then formulate the object depth estimation as an ensemble of multiple depth predictions to mitigate the insufficiency of single-depth information. However, the errors of existing multiple depths tend to have the same sign, which hinders them from neutralizing each other and limits the overall accuracy of combined depth. To alleviate this problem, we propose to increase the complementarity of depths with two novel designs. First, we add a new depth prediction branch named complementary depth that utilizes global and efficient depth clues from the entire image rather than the local clues to reduce the correlation of depth predictions. Second, we propose to fully exploit the geometric relations between multiple depth clues to achieve complementarity in form. Benefiting from these designs, our method achieves higher complementarity. Experiments on the KITTI benchmark demonstrate that our method achieves state-of-the-art performance without introducing extra data. In addition, complementary depth can also be a lightweight and plug-and-play module to boost multiple existing monocular 3d object detectors. Code is available at https://github.com/elvintanhust/MonoCD.",
        "page": "http://arxiv.org/abs/2404.03181",
        "pdf": "http://arxiv.org/pdf/2404.03181.pdf"
    },
    {
        "title": "Unifying Top-down and Bottom-up Scanpath Prediction using Transformers",
        "author": "Zhibo Yang, Sounak Mondal, Seoyoung Ahn, Ruoyu Xue, Gregory Zelinsky, Minh Hoai, Dimitris Samaras",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MuGE: Multiple Granularity Edge Detection",
        "author": "Caixia Zhou, Yaping Huang, Mengyang Pu, Qingji Guan, Ruoxi Deng, Haibin Ling",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FedUV: Uniformity and Variance for Heterogeneous Federated Learning",
        "author": "Ha Min Son, Moon-Hyun Kim, Tai-Myoung Chung, Chao Huang, Xin Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture",
        "author": "Juanwu Lu, Can Cui, Yunsheng Ma, Aniket Bera, Ziran Wang",
        "abstract": "Safety and robustness are crucial factors in developing trustworthy autonomous vehicles. One essential aspect of addressing these factors is to equip vehicles with the capability to predict future trajectories for all moving objects in the surroundings and quantify prediction uncertainties. In this paper, we propose the Sequential Neural Variational Agent (SeNeVA), a generative model that describes the distribution of future trajectories for a single moving object. Our approach can distinguish Out-of-Distribution data while quantifying uncertainty and achieving competitive performance compared to state-of-the-art methods on the Argoverse 2 and INTERACTION datasets. Specifically, a 0.446 meters minimum Final Displacement Error, a 0.203 meters minimum Average Displacement Error, and a 5.35% Miss Rate are achieved on the INTERACTION test set. Extensive qualitative and quantitative analysis is also provided to evaluate the proposed model. Our open-source code is available at https://github.com/PurdueDigitalTwin/seneva.",
        "page": "http://arxiv.org/abs/2404.03789",
        "pdf": "http://arxiv.org/pdf/2404.03789.pdf"
    },
    {
        "title": "LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs",
        "author": "Yunsheng Ma, Can Cui, Xu Cao, Wenqian Ye, Peiran Liu, Juanwu Lu, Amr Abdelraouf, Rohit Gupta, Kyungtae Han, Aniket Bera, James Rehg, Ziran Wang",
        "abstract": "Autonomous driving (AD) has made significant strides in recent years. However, existing frameworks struggle to interpret and execute spontaneous user instructions, such as \"overtake the car ahead.\" Large Language Models (LLMs) have demonstrated impressive reasoning capabilities showing potential to bridge this gap. In this paper, we present LaMPilot, a novel framework that integrates LLMs into AD systems, enabling them to follow user instructions by generating code that leverages established functional primitives. We also introduce LaMPilot-Bench, the first benchmark dataset specifically designed to quantitatively evaluate the efficacy of language model programs in AD. Adopting the LaMPilot framework, we conduct extensive experiments to assess the performance of off-the-shelf LLMs on LaMPilot-Bench. Our results demonstrate the potential of LLMs in handling diverse driving scenarios and following user instructions in driving. To facilitate further research in this area, we release our code and data at https://github.com/PurdueDigitalTwin/LaMPilot.",
        "page": "http://arxiv.org/abs/2312.04372",
        "pdf": "http://arxiv.org/pdf/2312.04372.pdf"
    },
    {
        "title": "Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation",
        "author": "Razvan Pasca, Alexey Gavryushin, Muhammad Hamza, Yen-Ling Kuo, Kaichun Mo, Luc Van Gool, Otmar Hilliges, Xi Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification",
        "author": "Yiyu Chen, Zheyi Fan, Zhaoru Chen, Yixuan Zhu",
        "abstract": "Person re-identification (re-ID) is a challenging task that aims to learn discriminative features for person retrieval. In person re-ID, Jaccard distance is a widely used distance metric, especially in re-ranking and clustering scenarios. However, we discover that camera variation has a significant negative impact on the reliability of Jaccard distance. In particular, Jaccard distance calculates the distance based on the overlap of relevant neighbors. Due to camera variation, intra-camera samples dominate the relevant neighbors, which reduces the reliability of the neighbors by introducing intra-camera negative samples and excluding inter-camera positive samples. To overcome this problem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that leverages camera information to enhance the reliability of Jaccard distance. Specifically, we design camera-aware k-reciprocal nearest neighbors (CKRNNs) to find k-reciprocal nearest neighbors on the intra-camera and inter-camera ranking lists, which improves the reliability of relevant neighbors and guarantees the contribution of inter-camera samples in the overlap. Moreover, we propose a camera-aware local query expansion (CLQE) to mine reliable samples in relevant neighbors by exploiting camera variation as a strong constraint and assign these samples higher weights in overlap, further improving the reliability. Our CA-Jaccard distance is simple yet effective and can serve as a general distance metric for person re-ID methods with high reliability and low computational cost. Extensive experiments demonstrate the effectiveness of our method.",
        "page": "http://arxiv.org/abs/2311.10605",
        "pdf": "http://arxiv.org/pdf/2311.10605.pdf"
    },
    {
        "title": "Face2Diffusion for Fast and Editable Face Personalization",
        "author": "Kaede Shiohara, Toshihiko Yamasaki",
        "abstract": "Face personalization aims to insert specific faces, taken from images, into pretrained text-to-image diffusion models. However, it is still challenging for previous methods to preserve both the identity similarity and editability due to overfitting to training samples. In this paper, we propose Face2Diffusion (F2D) for high-editability face personalization. The core idea behind F2D is that removing identity-irrelevant information from the training pipeline prevents the overfitting problem and improves editability of encoded faces. F2D consists of the following three novel components: 1) Multi-scale identity encoder provides well-disentangled identity features while keeping the benefits of multi-scale information, which improves the diversity of camera poses. 2) Expression guidance disentangles face expressions from identities and improves the controllability of face expressions. 3) Class-guided denoising regularization encourages models to learn how faces should be denoised, which boosts the text-alignment of backgrounds. Extensive experiments on the FaceForensics++ dataset and diverse prompts demonstrate our method greatly improves the trade-off between the identity- and text-fidelity compared to previous state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2403.05094",
        "pdf": "http://arxiv.org/pdf/2403.05094.pdf"
    },
    {
        "title": "UniPTS: A Unified Framework for Proficient Post-Training Sparsity",
        "author": "JingJing Xie, Yuxin Zhang, Mingbao Lin, ZhiHang Lin, Liujuan Cao, Rongrong Ji",
        "abstract": "Post-training Sparsity (PTS) is a recently emerged avenue that chases efficient network sparsity with limited data in need. Existing PTS methods, however, undergo significant performance degradation compared with traditional methods that retrain the sparse networks via the whole dataset, especially at high sparsity ratios. In this paper, we attempt to reconcile this disparity by transposing three cardinal factors that profoundly alter the performance of conventional sparsity into the context of PTS. Our endeavors particularly comprise (1) A base-decayed sparsity objective that promotes efficient knowledge transferring from dense network to the sparse counterpart. (2) A reducing-regrowing search algorithm designed to ascertain the optimal sparsity distribution while circumventing overfitting to the small calibration set in PTS. (3) The employment of dynamic sparse training predicated on the preceding aspects, aimed at comprehensively optimizing the sparsity structure while ensuring training stability. Our proposed framework, termed UniPTS, is validated to be much superior to existing PTS methods across extensive benchmarks. As an illustration, it amplifies the performance of POT, a recently proposed recipe, from 3.9% to 68.6% when pruning ResNet-50 at 90% sparsity ratio on ImageNet. We release the code of our paper at https://github.com/xjjxmu/UniPTS.",
        "page": "http://arxiv.org/abs/2405.18810",
        "pdf": "http://arxiv.org/pdf/2405.18810.pdf"
    },
    {
        "title": "Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network",
        "author": "Sizhe Zheng, Pan Gao, Peng Zhou, Jie Qin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EvDiG: Event-guided Direct and Global Components Separation",
        "author": "xinyu zhou, Peiqi Duan, Boyu Li, Chu Zhou, Chao Xu, Boxin Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing",
        "author": "Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang",
        "abstract": "Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality. Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting). Our source code is released at https://github.com/MC-E/DragonDiffusion.",
        "page": "http://arxiv.org/abs/2402.02583",
        "pdf": "http://arxiv.org/pdf/2402.02583.pdf"
    },
    {
        "title": "Enhancing Video Super-Resolution via Implicit Resampling-based Alignment",
        "author": "Kai Xu, Ziwei Yu, Xin Wang, Michael Bi Mi, Angela Yao",
        "abstract": "In video super-resolution, it is common to use a frame-wise alignment to support the propagation of information over time. The role of alignment is well-studied for low-level enhancement in video, but existing works overlook a critical step -- resampling. We show through extensive experiments that for alignment to be effective, the resampling should preserve the reference frequency spectrum while minimizing spatial distortions. However, most existing works simply use a default choice of bilinear interpolation for resampling even though bilinear interpolation has a smoothing effect and hinders super-resolution. From these observations, we propose an implicit resampling-based alignment. The sampling positions are encoded by a sinusoidal positional encoding, while the value is estimated with a coordinate network and a window-based cross-attention. We show that bilinear interpolation inherently attenuates high-frequency information while an MLP-based coordinate network can approximate more frequencies. Experiments on synthetic and real-world datasets show that alignment with our proposed implicit resampling enhances the performance of state-of-the-art frameworks with minimal impact on both compute and parameters.",
        "page": "http://arxiv.org/abs/2305.00163",
        "pdf": "http://arxiv.org/pdf/2305.00163.pdf"
    },
    {
        "title": "Template Free Reconstruction of Human-object Interaction with Procedural Interaction Generation",
        "author": "Xianghui Xie, Bharat Lal Bhatnagar, Jan Lenssen, Gerard Pons-Moll",
        "abstract": "Reconstructing human-object interaction in 3D from a single RGB image is a challenging task and existing data driven methods do not generalize beyond the objects present in the carefully curated 3D interaction datasets. Capturing large-scale real data to learn strong interaction and 3D shape priors is very expensive due to the combinatorial nature of human-object interactions. In this paper, we propose ProciGen (Procedural interaction Generation), a method to procedurally generate datasets with both, plausible interaction and diverse object variation. We generate 1M+ human-object interaction pairs in 3D and leverage this large-scale data to train our HDM (Hierarchical Diffusion Model), a novel method to reconstruct interacting human and unseen objects, without any templates. Our HDM is an image-conditioned diffusion model that learns both realistic interaction and highly accurate human and object shapes. Experiments show that our HDM trained with ProciGen significantly outperforms prior methods that requires template meshes and that our dataset allows training methods with strong generalization ability to unseen object instances. Our code and data are released.",
        "page": "http://arxiv.org/abs/2312.07063",
        "pdf": "http://arxiv.org/pdf/2312.07063.pdf"
    },
    {
        "title": "PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation",
        "author": "Zhenyu Li, Shariq Bhat, Peter Wonka",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Linguistic-Aware Patch Slimming Framework for Fine-grained Cross-Modal Alignment",
        "author": "Zheren Fu, Lei Zhang, Hou Xia, Zhendong Mao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PELA: Learning Parameter-Efficient Models with Low-Rank Approximation",
        "author": "Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Targeted Representation Alignment for Open-World Semi-Supervised Learning",
        "author": "Ruixuan Xiao, Lei Feng, Kai Tang, Junbo Zhao, Yixuan Li, Gang Chen, Haobo Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NeRFiller: Completing Scenes via Generative 3D Inpainting",
        "author": "Ethan Weber, Aleksander Holynski, Varun Jampani, Saurabh Saxena, Noah Snavely, Abhishek Kar, Angjoo Kanazawa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CausalPC: Improving the Robustness of Point Cloud Classification by Causal Effect Identification",
        "author": "Yuanmin Huang, Mi Zhang, Daizong Ding, Erling Jiang, Zhaoxiang Wang, Min Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization",
        "author": "Hongwei Ren, Jiadong Zhu, Yue Zhou, Haotian FU, Yulong Huang, Bojun Cheng",
        "abstract": "Event cameras exhibit remarkable attributes such as high dynamic range, asynchronicity, and low latency, making them highly suitable for vision tasks that involve high-speed motion in challenging lighting conditions. These cameras implicitly capture movement and depth information in events, making them appealing sensors for Camera Pose Relocalization (CPR) tasks. Nevertheless, existing CPR networks based on events neglect the pivotal fine-grained temporal information in events, resulting in unsatisfactory performance. Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices. In this paper, we introduce PEPNet, a simple and effective point-based network designed to regress six degrees of freedom (6-DOFs) event camera poses. We rethink the relationship between the event camera and CPR tasks, leveraging the raw Point Cloud directly as network input to harness the high-temporal resolution and inherent sparsity of events. PEPNet is adept at abstracting the spatial and implicit temporal features through hierarchical structure and explicit temporal features by Attentive Bi-directional Long Short-Term Memory (A-Bi-LSTM). By employing a carefully crafted lightweight design, PEPNet delivers state-of-the-art (SOTA) performance on both indoor and outdoor datasets with meager computational resources. Specifically, PEPNet attains a significant 38% and 33% performance improvement on the random split IJRR and M3ED datasets, respectively. Moreover, the lightweight design version PEPNet$_{tiny}$ accomplishes results comparable to the SOTA while employing a mere 0.5% of the parameters.",
        "page": "http://arxiv.org/abs/2403.19412",
        "pdf": "http://arxiv.org/pdf/2403.19412.pdf"
    },
    {
        "title": "3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation",
        "author": "Songchun Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, Changqing Zou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Understanding and Improving Source-free Domain Adaptation from a Theoretical Perspective",
        "author": "Yu Mitsuzumi, Akisato Kimura, Hisashi Kashima",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EASE-DETR: Easing the Competition among Object Queries",
        "author": "Yulu Gao, Yifan Sun, Xudong Ding, Chuyang Zhao, Si Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder",
        "author": "Dihan Zheng, Yihang Zou, Xiaowen Zhang, Chenglong Bao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Continuous Optical Zooming: A Benchmark for Arbitrary-Scale Image Super-Resolution in Real World",
        "author": "Huiyuan Fu, Fei Peng, Xianwei Li, Yejun Li, Xin Wang, Huadong Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints",
        "author": "Muxin Zhang, Qiao Feng, Zhuo Su, Chao Wen, Zhou Xue, Kun Li",
        "abstract": "3D human generation is increasingly significant in various applications. However, the direct use of 2D generative methods in 3D generation often results in losing local details, while methods that reconstruct geometry from generated images struggle with global view consistency. In this work, we introduce Joint2Human, a novel method that leverages 2D diffusion models to generate detailed 3D human geometry directly, ensuring both global structure and local details. To achieve this, we employ the Fourier occupancy field (FOF) representation, enabling the direct generation of 3D shapes as preliminary results with 2D generative models. With the proposed high-frequency enhancer and the multi-view recarving strategy, our method can seamlessly integrate the details from different views into a uniform global shape. To better utilize the 3D human prior and enhance control over the generated geometry, we introduce a compact spherical embedding of 3D joints. This allows for an effective guidance of pose during the generation process. Additionally, our method can generate 3D humans guided by textual inputs. Our experimental results demonstrate the capability of our method to ensure global structure, local details, high resolution, and low computational cost simultaneously. More results and the code can be found on our project page at http://cic.tju.edu.cn/faculty/likun/projects/Joint2Human.",
        "page": "http://arxiv.org/abs/2312.08591",
        "pdf": "http://arxiv.org/pdf/2312.08591.pdf"
    },
    {
        "title": "Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation",
        "author": "Sihan liu, Yiwei Ma, Xiaoqing Zhang, Haowei Wang, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji",
        "abstract": "Referring Remote Sensing Image Segmentation (RRSIS) is a new challenge that combines computer vision and natural language processing, delineating specific regions in aerial images as described by textual queries. Traditional Referring Image Segmentation (RIS) approaches have been impeded by the complex spatial scales and orientations found in aerial imagery, leading to suboptimal segmentation results. To address these challenges, we introduce the Rotated Multi-Scale Interaction Network (RMSIN), an innovative approach designed for the unique demands of RRSIS. RMSIN incorporates an Intra-scale Interaction Module (IIM) to effectively address the fine-grained detail required at multiple scales and a Cross-scale Interaction Module (CIM) for integrating these details coherently across the network. Furthermore, RMSIN employs an Adaptive Rotated Convolution (ARC) to account for the diverse orientations of objects, a novel contribution that significantly enhances segmentation accuracy. To assess the efficacy of RMSIN, we have curated an expansive dataset comprising 17,402 image-caption-mask triplets, which is unparalleled in terms of scale and variety. This dataset not only presents the model with a wide range of spatial and rotational scenarios but also establishes a stringent benchmark for the RRSIS task, ensuring a rigorous evaluation of performance. Our experimental evaluations demonstrate the exceptional performance of RMSIN, surpassing existing state-of-the-art models by a significant margin. All datasets and code are made available at https://github.com/Lsan2401/RMSIN.",
        "page": "http://arxiv.org/abs/2312.12470",
        "pdf": "http://arxiv.org/pdf/2312.12470.pdf"
    },
    {
        "title": "DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses",
        "author": "Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann",
        "abstract": "Determining the relative pose of an object between two images is pivotal to the success of generalizable object pose estimation. Existing approaches typically approximate the continuous pose representation with a large number of discrete pose hypotheses, which incurs a computationally expensive process of scoring each hypothesis at test time. By contrast, we present a Deep Voxel Matching Network (DVMNet) that eliminates the need for pose hypotheses and computes the relative object pose in a single pass. To this end, we map the two input RGB images, reference and query, to their respective voxelized 3D representations. We then pass the resulting voxels through a pose estimation module, where the voxels are aligned and the pose is computed in an end-to-end fashion by solving a least-squares problem. To enhance robustness, we introduce a weighted closest voxel algorithm capable of mitigating the impact of noisy voxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse datasets, demonstrating that our method delivers more accurate relative pose estimates for novel objects at a lower computational cost compared to state-of-the-art methods. Our code is released at: https://github.com/sailor-z/DVMNet/.",
        "page": "http://arxiv.org/abs/2403.13683",
        "pdf": "http://arxiv.org/pdf/2403.13683.pdf"
    },
    {
        "title": "Decoupled Pseudo-labeling in Semi-Supervised Monocular 3D Object Detection",
        "author": "Jiacheng Zhang, Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Scale Decoupled Distillation",
        "author": "Shicai Wei, Chunbo Luo, Yang Luo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Blur-aware Spatio-temporal Sparse Transformer for Video Deblurring",
        "author": "Huicong Zhang, Haozhe Xie, Hongxun Yao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Low-power, Continuous Remote Behavioral Localization with Event Cameras",
        "author": "Friedhelm Hamann, Suman Ghosh, Ignacio Juarez Martinez, Tom Hart, Alex Kacelnik, Guillermo Gallego",
        "abstract": "Researchers in natural science need reliable methods for quantifying animal behavior. Recently, numerous computer vision methods emerged to automate the process. However, observing wild species at remote locations remains a challenging task due to difficult lighting conditions and constraints on power supply and data storage. Event cameras offer unique advantages for battery-dependent remote monitoring due to their low power consumption and high dynamic range capabilities. We use this novel sensor to quantify a behavior in Chinstrap penguins called ecstatic display. We formulate the problem as a temporal action detection task, determining the start and end times of the behavior. For this purpose, we recorded a colony of breeding penguins in Antarctica for several weeks and labeled event data on 16 nests. The developed method consists of a generator of candidate time intervals (proposals) and a classifier of the actions within them. The experiments show that the event cameras' natural response to motion is effective for continuous behavior monitoring and detection, reaching a mean average precision (mAP) of 58% (which increases to 63% in good weather conditions). The results also demonstrate the robustness against various lighting conditions contained in the challenging dataset. The low-power capabilities of the event camera allow it to record significantly longer than with a conventional camera. This work pioneers the use of event cameras for remote wildlife observation, opening new interdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/",
        "page": "http://arxiv.org/abs/2312.03799",
        "pdf": "http://arxiv.org/pdf/2312.03799.pdf"
    },
    {
        "title": "PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation",
        "author": "Yuqi Wang, Yuntao Chen, Xingyu Liao, Lue Fan, Zhaoxiang Zhang",
        "abstract": "Comprehensive modeling of the surrounding 3D world is key to the success of autonomous driving. However, existing perception tasks like object detection, road structure segmentation, depth & elevation estimation, and open-set object localization each only focus on a small facet of the holistic 3D scene understanding task. This divide-and-conquer strategy simplifies the algorithm development procedure at the cost of losing an end-to-end unified solution to the problem. In this work, we address this limitation by studying camera-based 3D panoptic segmentation, aiming to achieve a unified occupancy representation for camera-only 3D scene understanding. To achieve this, we introduce a novel method called PanoOcc, which utilizes voxel queries to aggregate spatiotemporal information from multi-frame and multi-view images in a coarse-to-fine scheme, integrating feature learning and scene representation into a unified occupancy representation. We have conducted extensive ablation studies to verify the effectiveness and efficiency of the proposed method. Our approach achieves new state-of-the-art results for camera-based semantic segmentation and panoptic segmentation on the nuScenes dataset. Furthermore, our method can be easily extended to dense occupancy prediction and has shown promising performance on the Occ3D benchmark. The code will be released at https://github.com/Robertwyq/PanoOcc.",
        "page": "http://arxiv.org/abs/2306.10013",
        "pdf": "http://arxiv.org/pdf/2306.10013.pdf"
    },
    {
        "title": "Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes",
        "author": "Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa",
        "abstract": "Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics. However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present. In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds. Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies. This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation. To assess our methodology, we created an urban scene dataset masked with moving objects. Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively.",
        "page": "http://arxiv.org/abs/2403.16141",
        "pdf": "http://arxiv.org/pdf/2403.16141.pdf"
    },
    {
        "title": "OmniVid: A Generative Framework for Universal Video Understanding",
        "author": "Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang",
        "abstract": "The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution. Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats. In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as GPT-3, with extensive training corpora. Inspired by this, we seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens. In this way, a variety of video tasks could be formulated as video-grounded token generation. This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video question answering, and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework. Through comprehensive experiments, we demonstrate such a simple and straightforward idea is quite effective and can achieve state-of-the-art or competitive results on seven video benchmarks, providing a novel perspective for more universal video understanding. Code is available at https://github.com/wangjk666/OmniVid.",
        "page": "http://arxiv.org/abs/2403.17935",
        "pdf": "http://arxiv.org/pdf/2403.17935.pdf"
    },
    {
        "title": "Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency",
        "author": "Xu Yingjie, Bangzhen Liu, Hao Tang, Bailin Deng, Shengfeng He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "RTracker: Recoverable Tracking via PN Tree Structured Memory",
        "author": "Yuqing Huang, Xin Li, Zikun Zhou, Yaowei Wang, Zhenyu He, Ming-Hsuan Yang",
        "abstract": "Existing tracking methods mainly focus on learning better target representation or developing more robust prediction models to improve tracking performance. While tracking performance has significantly improved, the target loss issue occurs frequently due to tracking failures, complete occlusion, or out-of-view situations. However, considerably less attention is paid to the self-recovery issue of tracking methods, which is crucial for practical applications. To this end, we propose a recoverable tracking framework, RTracker, that uses a tree-structured memory to dynamically associate a tracker and a detector to enable self-recovery ability. Specifically, we propose a Positive-Negative Tree-structured memory to chronologically store and maintain positive and negative target samples. Upon the PN tree memory, we develop corresponding walking rules for determining the state of the target and define a set of control flows to unite the tracker and the detector in different tracking scenarios. Our core idea is to use the support samples of positive and negative target categories to establish a relative distance-based criterion for a reliable assessment of target loss. The favorable performance in comparison against the state-of-the-art methods on numerous challenging benchmarks demonstrates the effectiveness of the proposed algorithm.",
        "page": "http://arxiv.org/abs/2403.19242",
        "pdf": "http://arxiv.org/pdf/2403.19242.pdf"
    },
    {
        "title": "Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation",
        "author": "Xinyao Li, Yuke Li, Zhekai Du, Fengling Li, Ke Lu, Jingjing Li",
        "abstract": "Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation. Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across domains using a modality discriminator. Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS",
        "page": "http://arxiv.org/abs/2403.06946",
        "pdf": "http://arxiv.org/pdf/2403.06946.pdf"
    },
    {
        "title": "Capturing Closely Interacted Two-Person Motions with Reaction Priors",
        "author": "Qi Fang, Yinghui Fan, Yanjun Li, Junting Dong, Dingwei Wu, Weidong Zhang, Kang Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling",
        "author": "Jiawei Shi, Hui Deng, Yuchao Dai",
        "abstract": "Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively studied and great progress has been made, there are still key challenges that hinder their broad real-world applications: 1) the inherent motion/rotation ambiguity requires either explicit camera motion recovery with extra constraint or complex Procrustean Alignment; 2) existing low-rank modeling of the global shape can over-penalize drastic deformations in the 3D shape sequence. This paper proposes to resolve the above issues from a spatial-temporal modeling perspective. First, we propose a novel Temporally-smooth Procrustean Alignment module that estimates 3D deforming shapes and adjusts the camera motion by aligning the 3D shape sequence consecutively. Our new alignment module remedies the requirement of complex reference 3D shape during alignment, which is more conductive to non-isotropic deformation modeling. Second, we propose a spatial-weighted approach to enforce the low-rank constraint adaptively at different locations to accommodate drastic spatially-variant deformation reconstruction better. Our modeling outperform existing low-rank based methods, and extensive experiments across different datasets validate the effectiveness of our method.",
        "page": "http://arxiv.org/abs/2405.04309",
        "pdf": "http://arxiv.org/pdf/2405.04309.pdf"
    },
    {
        "title": "MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model",
        "author": "Kaiyu Song, Hanjiang Lai, Yan Pan, Jian Yin",
        "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial perturbation, where an imperceptible perturbation is added to the image that can fool the DNNs. Diffusion-based adversarial purification focuses on using the diffusion model to generate a clean image against such adversarial attacks. Unfortunately, the generative process of the diffusion model is also inevitably affected by adversarial perturbation since the diffusion model is also a deep network where its input has adversarial perturbation. In this work, we propose MimicDiffusion, a new diffusion-based adversarial purification technique, that directly approximates the generative process of the diffusion model with the clean image as input. Concretely, we analyze the differences between the guided terms using the clean image and the adversarial sample. After that, we first implement MimicDiffusion based on Manhattan distance. Then, we propose two guidance to purify the adversarial perturbation and approximate the clean diffusion model. Extensive experiments on three image datasets including CIFAR-10, CIFAR-100, and ImageNet with three classifier backbones including WideResNet-70-16, WideResNet-28-10, and ResNet50 demonstrate that MimicDiffusion significantly performs better than the state-of-the-art baselines. On CIFAR-10, CIFAR-100, and ImageNet, it achieves 92.67\\%, 61.35\\%, and 61.53\\% average robust accuracy, which are 18.49\\%, 13.23\\%, and 17.64\\% higher, respectively. The code is available in the supplementary material.",
        "page": "http://arxiv.org/abs/2312.04802",
        "pdf": "http://arxiv.org/pdf/2312.04802.pdf"
    },
    {
        "title": "Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias",
        "author": "Wenyu Zhang, Qingmu Liu, Felix Ong, Mohamed Ragab, Chuan-Sheng Foo",
        "abstract": "Domain adaptation is a critical task in machine learning that aims to improve model performance on a target domain by leveraging knowledge from a related source domain. In this work, we introduce Universal Semi-Supervised Domain Adaptation (UniSSDA), a practical yet challenging setting where the target domain is partially labeled, and the source and target label space may not strictly match. UniSSDA is at the intersection of Universal Domain Adaptation (UniDA) and Semi-Supervised Domain Adaptation (SSDA): the UniDA setting does not allow for fine-grained categorization of target private classes not represented in the source domain, while SSDA focuses on the restricted closed-set setting where source and target label spaces match exactly. Existing UniDA and SSDA methods are susceptible to common-class bias in UniSSDA settings, where models overfit to data distributions of classes common to both domains at the expense of private classes. We propose a new prior-guided pseudo-label refinement strategy to reduce the reinforcement of common-class bias due to pseudo-labeling, a common label propagation strategy in domain adaptation. We demonstrate the effectiveness of the proposed strategy on benchmark datasets Office-Home, DomainNet, and VisDA. The proposed strategy attains the best performance across UniSSDA adaptation settings and establishes a new baseline for UniSSDA.",
        "page": "http://arxiv.org/abs/2403.11234",
        "pdf": "http://arxiv.org/pdf/2403.11234.pdf"
    },
    {
        "title": "Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch",
        "author": "Xidong Wu, Shangqian Gao, Zeyu Zhang, Zhenzhen Li, Runxue Bao, Yanfu Zhang, Xiaoqian Wang, Heng Huang",
        "abstract": "Current techniques for deep neural network (DNN) pruning often involve intricate multi-step processes that require domain-specific expertise, making their widespread adoption challenging. To address the limitation, the Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for additional fine-tuning steps by directly training and compressing a general DNN from scratch. Nevertheless, the static design of optimizers (in OTO) can lead to convergence issues of local optima. In this paper, we proposed the Auto-Train-Once (ATO), an innovative network pruning algorithm designed to automatically reduce the computational and storage costs of DNNs. During the model training phase, our approach not only trains the target model but also leverages a controller network as an architecture generator to guide the learning of target model weights. Furthermore, we developed a novel stochastic gradient algorithm that enhances the coordination between model training and controller network training, thereby improving pruning performance. We provide a comprehensive convergence analysis as well as extensive experiments, and the results show that our approach achieves state-of-the-art performance across various model architectures (including ResNet18, ResNet34, ResNet50, ResNet56, and MobileNetv2) on standard benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet).",
        "page": "http://arxiv.org/abs/2403.14729",
        "pdf": "http://arxiv.org/pdf/2403.14729.pdf"
    },
    {
        "title": "SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis",
        "author": "Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan",
        "abstract": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. Traditional Generative Adversarial Networks (GAN) struggle to maintain consistent facial identity, while Neural Radiance Fields (NeRF) methods, although they can address this issue, often produce mismatched lip movements, inadequate facial expressions, and unstable head poses. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic and artificial outcomes. To address the critical issue of synchronization, identified as the \"devil\" in creating realistic talking heads, we introduce SyncTalk. This NeRF-based method effectively maintains subject identity, enhancing synchronization and realism in talking head synthesis. SyncTalk employs a Face-Sync Controller to align lip movements with speech and innovatively uses a 3D facial blendshape model to capture accurate facial expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more natural head movements. The Portrait-Sync Generator restores hair details and blends the generated head with the torso for a seamless visual experience. Extensive experiments and user studies demonstrate that SyncTalk outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk",
        "page": "http://arxiv.org/abs/2311.17590",
        "pdf": "http://arxiv.org/pdf/2311.17590.pdf"
    },
    {
        "title": "CSTA: CNN-based Spatiotemporal Attention for Video Summarization",
        "author": "Jaewon Son, Jaehun Park, Kwangsu Kim",
        "abstract": "Video summarization aims to generate a concise representation of a video, capturing its essential content and key moments while reducing its overall length. Although several methods employ attention mechanisms to handle long-term dependencies, they often fail to capture the visual significance inherent in frames. To address this limitation, we propose a CNN-based SpatioTemporal Attention (CSTA) method that stacks each feature of frames from a single video to form image-like frame representations and applies 2D CNN to these frame features. Our methodology relies on CNN to comprehend the inter and intra-frame relations and to find crucial attributes in videos by exploiting its ability to learn absolute positions within images. In contrast to previous work compromising efficiency by designing additional modules to focus on spatial importance, CSTA requires minimal computational overhead as it uses CNN as a sliding window. Extensive experiments on two benchmark datasets (SumMe and TVSum) demonstrate that our proposed approach achieves state-of-the-art performance with fewer MACs compared to previous methods. Codes are available at https://github.com/thswodnjs3/CSTA.",
        "page": "http://arxiv.org/abs/2405.11905",
        "pdf": "http://arxiv.org/pdf/2405.11905.pdf"
    },
    {
        "title": "Domain-Specific Block Selection and Paired-View Pseudo-Labeling for Online Test-Time Adaptation",
        "author": "Yeonguk Yu, Sungho Shin, Seunghyeok Back, Minhwan Ko, Sangjun Noh, Kyoobin Lee",
        "abstract": "Test-time adaptation (TTA) aims to adapt a pre-trained model to a new test domain without access to source data after deployment. Existing approaches typically rely on self-training with pseudo-labels since ground-truth cannot be obtained from test data. Although the quality of pseudo labels is important for stable and accurate long-term adaptation, it has not been previously addressed. In this work, we propose DPLOT, a simple yet effective TTA framework that consists of two components: (1) domain-specific block selection and (2) pseudo-label generation using paired-view images. Specifically, we select blocks that involve domain-specific feature extraction and train these blocks by entropy minimization. After blocks are adjusted for current test domain, we generate pseudo-labels by averaging given test images and corresponding flipped counterparts. By simply using flip augmentation, we prevent a decrease in the quality of the pseudo-labels, which can be caused by the domain gap resulting from strong augmentation. Our experimental results demonstrate that DPLOT outperforms previous TTA methods in CIFAR10-C, CIFAR100-C, and ImageNet-C benchmarks, reducing error by up to 5.4%, 9.1%, and 2.9%, respectively. Also, we provide an extensive analysis to demonstrate effectiveness of our framework. Code is available at https://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA.",
        "page": "http://arxiv.org/abs/2404.10966",
        "pdf": "http://arxiv.org/pdf/2404.10966.pdf"
    },
    {
        "title": "Learning Adaptive Spatial Coherent Correlations for Speech-Preserving Facial Expression Manipulation",
        "author": "Tianshui Chen, Jianman Lin, Zhijing Yang, Chunmei Qing, Liang Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Point Cloud Pre-training with Diffusion Models",
        "author": "xiao zheng, Xiaoshui Huang, Guofeng Mei, Zhaoyang Lyu, Yuenan Hou, Wanli Ouyang, Bo Dai, Yongshun Gong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation",
        "author": "Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, Peng Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CoG-DQA: Chain-of-Guiding Learning with Large Language Models for Diagram Question Answering",
        "author": "Shaowei Wang, Lingling Zhang, Longji Zhu, Tao Qin, Kim-Hui Yap, Xinyu Zhang, Jun Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Autoregressive Queries for Adaptive Tracking with Spatio-Temporal Transformers",
        "author": "Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang, Liangtao Shi, Shuxiang Song, Rongrong Ji",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation",
        "author": "Bin Xie, Jiale Cao, Jin Xie, Fahad Shahbaz Khan, Yanwei Pang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis",
        "author": "Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, Diana Marculescu",
        "abstract": "Diffusion models have transformed the image-to-image (I2I) synthesis and are now permeating into videos. However, the advancement of video-to-video (V2V) synthesis has been hampered by the challenge of maintaining temporal consistency across video frames. This paper proposes a consistent V2V synthesis framework by jointly leveraging spatial conditions and temporal optical flow clues within the source video. Contrary to prior methods that strictly adhere to optical flow, our approach harnesses its benefits while handling the imperfection in flow estimation. We encode the optical flow via warping from the first frame and serve it as a supplementary reference in the diffusion model. This enables our model for video synthesis by editing the first frame with any prevalent I2I models and then propagating edits to successive frames. Our V2V model, FlowVid, demonstrates remarkable properties: (1) Flexibility: FlowVid works seamlessly with existing I2I models, facilitating various modifications, including stylization, object swaps, and local edits. (2) Efficiency: Generation of a 4-second video with 30 FPS and 512x512 resolution takes only 1.5 minutes, which is 3.1x, 7.2x, and 10.5x faster than CoDeF, Rerender, and TokenFlow, respectively. (3) High-quality: In user studies, our FlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender (10.2%), and TokenFlow (40.4%).",
        "page": "http://arxiv.org/abs/2312.17681",
        "pdf": "http://arxiv.org/pdf/2312.17681.pdf"
    },
    {
        "title": "OneFormer3D: One Transformer for Unified Point Cloud Segmentation",
        "author": "Maksim Kolodiazhnyi, Anna Vorontsova, Anton Konushin, Danila Rukhovich",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Privacy-preserving Optics for Enhancing Protection in Face De-identification",
        "author": "Jhon Lopez, Carlos Hinojosa, Henry Arguello, Bernard Ghanem",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ESCAPE: Encoding Super-keypoints for Category-Agnostic Pose Estimation",
        "author": "Khoi D Nguyen, Chen Li, Gim Hee Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis",
        "author": "Shunyuan Zheng, Boyao ZHOU, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu",
        "abstract": "We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.",
        "page": "http://arxiv.org/abs/2312.02155",
        "pdf": "http://arxiv.org/pdf/2312.02155.pdf"
    },
    {
        "title": "SpiderMatch: 3D Shape Matching with Global Optimality and Geometric Consistency",
        "author": "Paul Roetzer, Florian Bernard",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Wired Perspectives: Multi-View Wire Art Embraces Generative AI",
        "author": "Zhiyu Qu, LAN YANG, Honggang Zhang, Tao Xiang, Kaiyue Pang, Yi-Zhe Song",
        "abstract": "Creating multi-view wire art (MVWA), a static 3D sculpture with diverse interpretations from different viewpoints, is a complex task even for skilled artists. In response, we present DreamWire, an AI system enabling everyone to craft MVWA easily. Users express their vision through text prompts or scribbles, freeing them from intricate 3D wire organisation. Our approach synergises 3D B\\'ezier curves, Prim's algorithm, and knowledge distillation from diffusion models or their variants (e.g., ControlNet). This blend enables the system to represent 3D wire art, ensuring spatial continuity and overcoming data scarcity. Extensive evaluation and analysis are conducted to shed insight on the inner workings of the proposed system, including the trade-off between connectivity and visual aesthetics.",
        "page": "http://arxiv.org/abs/2311.15421",
        "pdf": "http://arxiv.org/pdf/2311.15421.pdf"
    },
    {
        "title": "Video Frame Interpolation via Direct Synthesis with the Event-based Reference",
        "author": "Yuhan Liu, Yongjian Deng, Hao Chen, Zhen Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "C$^2$KD: Bridging the Modality Gap for Cross-Modal Knowledge Distillation",
        "author": "Fushuo Huo, Wenchao Xu, Jingcai Guo, Haozhao Wang, Song Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment",
        "author": "Sajid Javed, Arif Mahmood, IYYAKUTTI IYAPPAN GANAPATHI, Fayaz Ali, Naoufel Werghi, Mohammed Bennamoun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior",
        "author": "Tianyu Huang, Yihan Zeng, Zhilu Zhang, Wan Xu, Hang Xu, Songcen Xu, Rynson W.H. Lau, Wangmeng Zuo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffusionTrack:  Point Set Diffusion Model for Visual Object Tracking",
        "author": "Fei Xie, Zhongdao Wang, Chao Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model",
        "author": "Xu He, Qiaochu Huang, Zhensong Zhang, Zhiwei Lin, Zhiyong Wu, Sicheng Yang, Minglei Li, Zhiyi Chen, Songcen Xu, Xiaofei Wu",
        "abstract": "Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion.",
        "page": "http://arxiv.org/abs/2404.01862",
        "pdf": "http://arxiv.org/pdf/2404.01862.pdf"
    },
    {
        "title": "DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation",
        "author": "Junming Chen, Yunfei Liu, Jianan Wang, Ailing Zeng, Yu Li, Qifeng Chen",
        "abstract": "We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms the superiority of DiffSHEG over prior approaches. By enabling the real-time generation of expressive and synchronized motions, DiffSHEG showcases its potential for various applications in the development of digital humans and embodied agents.",
        "page": "http://arxiv.org/abs/2401.04747",
        "pdf": "http://arxiv.org/pdf/2401.04747.pdf"
    },
    {
        "title": "PostureHMR: Posture Transformation for 3D Human Mesh Recovery",
        "author": "Yu-Pei Song, Xiao WU, Zhaoquan Yuan, Jian-Jun Qiao, Qiang Peng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline",
        "author": "Xiao Wang, Shiao Wang, Chuanming Tang, Lin Zhu, Bo Jiang, Yonghong Tian, Jin Tang",
        "abstract": "Tracking using bio-inspired event cameras has drawn more and more attention in recent years. Existing works either utilize aligned RGB and event data for accurate tracking or directly learn an event-based tracker. The first category needs more cost for inference and the second one may be easily influenced by noisy events or sparse spatial resolution. In this paper, we propose a novel hierarchical knowledge distillation framework that can fully utilize multi-modal / multi-view information during training to facilitate knowledge transfer, enabling us to achieve high-speed and low-latency visual tracking during testing by using only event signals. Specifically, a teacher Transformer-based multi-modal tracking framework is first trained by feeding the RGB frame and event stream simultaneously. Then, we design a new hierarchical knowledge distillation strategy which includes pairwise similarity, feature representation, and response maps-based knowledge distillation to guide the learning of the student Transformer network. Moreover, since existing event-based tracking datasets are all low-resolution ($346 \\times 260$), we propose the first large-scale high-resolution ($1280 \\times 720$) dataset named EventVOT. It contains 1141 videos and covers a wide range of categories such as pedestrians, vehicles, UAVs, ping pongs, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, COESOT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. The dataset, evaluation toolkit, and source code are available on \\url{https://github.com/Event-AHU/EventVOT_Benchmark}",
        "page": "http://arxiv.org/abs/2309.14611",
        "pdf": "http://arxiv.org/pdf/2309.14611.pdf"
    },
    {
        "title": "ZERO-IG: Zero-Shot Illumination-Guided Joint Denoising and Adaptive Enhancement for Low-Light Images",
        "author": "Yiqi Shi, Duo Liu, Liguo Zhang, Ye Tian, Xuezhi Xia, fuxiaojing",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation",
        "author": "Shanshan Zhong, Zhongzhan Huang, Shanghua Gao, Wushao Wen, Liang Lin, Marinka Zitnik, Pan Zhou",
        "abstract": "Chain-of-Thought (CoT) guides large language models (LLMs) to reason step-by-step, and can motivate their logical reasoning ability. While effective for logical tasks, CoT is not conducive to creative problem-solving which often requires out-of-box thoughts and is crucial for innovation advancements. In this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a non-sequential, creative paradigm involving strong associations and knowledge leaps. To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study. Then to investigate LLMs' LoT ability in the Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset which contains over 130,000 samples from the Oogiri game, and observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game. Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve LLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into LoT-oriented instruction tuning data to train pretrained LLM for achieving certain LoT humor generation and discrimination abilities. Then CLoT designs an explorative self-refinement that encourages the LLM to generate more creative LoT data via exploring parallels between seemingly unrelated concepts and selects high-quality data to train itself for self-refinement. CLoT not only excels in humor generation in the Oogiri game but also boosts creative abilities in various tasks like cloud guessing game and divergent association task. These findings advance our understanding and offer a pathway to improve LLMs' creative capacities for innovative applications across domains. The dataset, code, and models will be released online. https://zhongshsh.github.io/CLoT/.",
        "page": "http://arxiv.org/abs/2312.02439",
        "pdf": "http://arxiv.org/pdf/2312.02439.pdf"
    },
    {
        "title": "Purified and Unified Steganographic Network",
        "author": "GuoBiao Li, Sheng Li, Zicong Luo, Zhenxing Qian, Xinpeng Zhang",
        "abstract": "Steganography is the art of hiding secret data into the cover media for covert communication. In recent years, more and more deep neural network (DNN)-based steganographic schemes are proposed to train steganographic networks for secret embedding and recovery, which are shown to be promising. Compared with the handcrafted steganographic tools, steganographic networks tend to be large in size. It raises concerns on how to imperceptibly and effectively transmit these networks to the sender and receiver to facilitate the covert communication. To address this issue, we propose in this paper a Purified and Unified Steganographic Network (PUSNet). It performs an ordinary machine learning task in a purified network, which could be triggered into steganographic networks for secret embedding or recovery using different keys. We formulate the construction of the PUSNet into a sparse weight filling problem to flexibly switch between the purified and steganographic networks. We further instantiate our PUSNet as an image denoising network with two steganographic networks concealed for secret image embedding and recovery. Comprehensive experiments demonstrate that our PUSNet achieves good performance on secret image embedding, secret image recovery, and image denoising in a single architecture. It is also shown to be capable of imperceptibly carrying the steganographic networks in a purified network. Code is available at \\url{https://github.com/albblgb/PUSNet}",
        "page": "http://arxiv.org/abs/2402.17210",
        "pdf": "http://arxiv.org/pdf/2402.17210.pdf"
    },
    {
        "title": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
        "author": "Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation",
        "author": "Ji-Jia Wu, Andy Chia-Hao Chang, Chieh-Yu Chuang, Chun-Pei Chen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Yung-Yu Chuang, Yen-Yu Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction",
        "author": "Junwen Xiong, Peng Zhang, Tao You, Chuanyue Li, Wei Huang, Yufei Zha",
        "abstract": "Audio-visual saliency prediction can draw support from diverse modality complements, but further performance enhancement is still challenged by customized architectures as well as task-specific loss functions. In recent studies, denoising diffusion models have shown more promising in unifying task frameworks owing to their inherent ability of generalization. Following this motivation, a novel Diffusion architecture for generalized audio-visual Saliency prediction (DiffSal) is proposed in this work, which formulates the prediction problem as a conditional generative task of the saliency map by utilizing input audio and video as the conditions. Based on the spatio-temporal audio-visual features, an extra network Saliency-UNet is designed to perform multi-modal attention modulation for progressive refinement of the ground-truth saliency map from the noisy map. Extensive experiments demonstrate that the proposed DiffSal can achieve excellent performance across six challenging audio-visual benchmarks, with an average relative improvement of 6.3\\% over the previous state-of-the-art results by six metrics.",
        "page": "http://arxiv.org/abs/2403.01226",
        "pdf": "http://arxiv.org/pdf/2403.01226.pdf"
    },
    {
        "title": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models",
        "author": "Zheng Li, Xiang Li, xinyi fu, Xin Zhang, Weiqiang Wang, Shuo Chen, Jian Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting",
        "author": "Zhijing Shao, Wang Zhaolong, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LLM-AR: When Large Language Model Meets Skeleton-Based Action Recognition",
        "author": "Haoxuan Qu, Yujun Cai, Jun Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Authentic Hand Avatar from a Phone Scan via Universal Hand Model",
        "author": "Gyeongsik Moon, Weipeng Xu, Rohan Joshi, Chenglei Wu, Takaaki Shiratori",
        "abstract": "The authentic 3D hand avatar with every identifiable information, such as hand shapes and textures, is necessary for immersive experiences in AR/VR. In this paper, we present a universal hand model (UHM), which 1) can universally represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can be adapted to each person with a short phone scan for the authentic hand avatar. For effective universal hand modeling, we perform tracking and modeling at the same time, while previous 3D hand models perform them separately. The conventional separate pipeline suffers from the accumulated errors from the tracking stage, which cannot be recovered in the modeling stage. On the other hand, ours does not suffer from the accumulated errors while having a much more concise overall pipeline. We additionally introduce a novel image matching loss function to address a skin sliding during the tracking and modeling, while existing works have not focused on it much. Finally, using learned priors from our UHM, we effectively adapt our UHM to each person's short phone scan for the authentic hand avatar.",
        "page": "http://arxiv.org/abs/2405.07933",
        "pdf": "http://arxiv.org/pdf/2405.07933.pdf"
    },
    {
        "title": "6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation",
        "author": "Li Xu, Haoxuan Qu, Yujun Cai, Jun Liu",
        "abstract": "Estimating the 6D object pose from a single RGB image often involves noise and indeterminacy due to challenges such as occlusions and cluttered backgrounds. Meanwhile, diffusion models have shown appealing performance in generating high-quality images from random noise with high indeterminacy through step-by-step denoising. Inspired by their denoising capability, we propose a novel diffusion-based framework (6D-Diff) to handle the noise and indeterminacy in object pose estimation for better performance. In our framework, to establish accurate 2D-3D correspondence, we formulate 2D keypoints detection as a reverse diffusion (denoising) process. To facilitate such a denoising process, we design a Mixture-of-Cauchy-based forward diffusion process and condition the reverse process on the object features. Extensive experiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our framework.",
        "page": "http://arxiv.org/abs/2401.00029",
        "pdf": "http://arxiv.org/pdf/2401.00029.pdf"
    },
    {
        "title": "PIGEON: Predicting Image Geolocations",
        "author": "Lukas Haas, Skreta, Silas Alberti, Chelsea Finn",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FairRAG: Fair Human Generation via Fair Retrieval Augmentation",
        "author": "Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi Deng",
        "abstract": "Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrate that FairRAG outperforms existing methods in terms of demographic diversity, image-text alignment, and image fidelity while incurring minimal computational overhead during inference.",
        "page": "http://arxiv.org/abs/2403.19964",
        "pdf": "http://arxiv.org/pdf/2403.19964.pdf"
    },
    {
        "title": "Improved Visual Grounding through Self-Consistent Explanations",
        "author": "Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alex Berg, Vicente Ordonez",
        "abstract": "Vision-and-language models trained to match images with text can be combined with visual explanation methods to point to the locations of specific objects in an image. Our work shows that the localization --\"grounding\"-- abilities of these models can be further improved by finetuning for self-consistent visual explanations. We propose a strategy for augmenting existing text-image datasets with paraphrases using a large language model, and SelfEQ, a weakly-supervised strategy on visual explanation maps for paraphrases that encourages self-consistency. Specifically, for an input textual phrase, we attempt to generate a paraphrase and finetune the model so that the phrase and paraphrase map to the same region in the image. We posit that this both expands the vocabulary that the model is able to handle, and improves the quality of the object locations highlighted by gradient-based visual explanation methods (e.g. GradCAM). We demonstrate that SelfEQ improves performance on Flickr30k, ReferIt, and RefCOCO+ over a strong baseline method and several prior works. Particularly, comparing to other methods that do not use any type of box annotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%), 67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on RefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on average).",
        "page": "http://arxiv.org/abs/2312.04554",
        "pdf": "http://arxiv.org/pdf/2312.04554.pdf"
    },
    {
        "title": "Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion",
        "author": "Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, Cyrill Stachniss",
        "abstract": "Computer vision techniques play a central role in the perception stack of autonomous vehicles. Such methods are employed to perceive the vehicle surroundings given sensor data. 3D LiDAR sensors are commonly used to collect sparse 3D point clouds from the scene. However, compared to human perception, such systems struggle to deduce the unseen parts of the scene given those sparse point clouds. In this matter, the scene completion task aims at predicting the gaps in the LiDAR measurements to achieve a more complete scene representation. Given the promising results of recent diffusion models as generative models for images, we propose extending them to achieve scene completion from a single 3D LiDAR scan. Previous works used diffusion models over range images extracted from LiDAR data, directly applying image-based diffusion methods. Distinctly, we propose to directly operate on the points, reformulating the noising and denoising diffusion process such that it can efficiently work at scene scale. Together with our approach, we propose a regularization loss to stabilize the noise predicted during the denoising process. Our experimental evaluation shows that our method can complete the scene given a single LiDAR scan as input, producing a scene with more details compared to state-of-the-art scene completion methods. We believe that our proposed diffusion process formulation can support further research in diffusion models applied to scene-scale point cloud data.",
        "page": "http://arxiv.org/abs/2403.13470",
        "pdf": "http://arxiv.org/pdf/2403.13470.pdf"
    },
    {
        "title": "HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding",
        "author": "Trong-Thuan Nguyen, Pha Nguyen, Khoa Luu",
        "abstract": "Visual interactivity understanding within visual scenes presents a significant challenge in computer vision. Existing methods focus on complex interactivities while leveraging a simple relationship model. These methods, however, struggle with a diversity of appearance, situation, position, interaction, and relation in videos. This limitation hinders the ability to fully comprehend the interplay within the complex visual dynamics of subjects. In this paper, we delve into interactivities understanding within visual content by deriving scene graph representations from dense interactivities among humans and objects. To achieve this goal, we first present a new dataset containing Appearance-Situation-Position-Interaction-Relation predicates, named ASPIRe, offering an extensive collection of videos marked by a wide range of interactivities. Then, we propose a new approach named Hierarchical Interlacement Graph (HIG), which leverages a unified layer and graph within a hierarchical structure to provide deep insights into scene changes across five distinct tasks. Our approach demonstrates superior performance to other methods through extensive experiments conducted in various scenarios.",
        "page": "http://arxiv.org/abs/2312.03050",
        "pdf": "http://arxiv.org/pdf/2312.03050.pdf"
    },
    {
        "title": "TiNO-Edit: Timestep and Noise Optimization for Robust Diffusion-Based Image Editing",
        "author": "Sherry X. Chen, Yaron Vaxman, Elad Ben Baruch, David Asulin, Aviad Moreshet, Kuo-Chin Lien, Misha Sra, Pradeep Sen",
        "abstract": "Despite many attempts to leverage pre-trained text-to-image models (T2I) like Stable Diffusion (SD) for controllable image editing, producing good predictable results remains a challenge. Previous approaches have focused on either fine-tuning pre-trained T2I models on specific datasets to generate certain kinds of images (e.g., with a specific object or person), or on optimizing the weights, text prompts, and/or learning features for each input image in an attempt to coax the image generator to produce the desired result. However, these approaches all have shortcomings and fail to produce good results in a predictable and controllable manner. To address this problem, we present TiNO-Edit, an SD-based method that focuses on optimizing the noise patterns and diffusion timesteps during editing, something previously unexplored in the literature. With this simple change, we are able to generate results that both better align with the original images and reflect the desired result. Furthermore, we propose a set of new loss functions that operate in the latent domain of SD, greatly speeding up the optimization when compared to prior approaches, which operate in the pixel domain. Our method can be easily applied to variations of SD including Textual Inversion and DreamBooth that encode new concepts and incorporate them into the edited results. We present a host of image-editing capabilities enabled by our approach. Our code is publicly available at https://github.com/SherryXTChen/TiNO-Edit.",
        "page": "http://arxiv.org/abs/2404.11120",
        "pdf": "http://arxiv.org/pdf/2404.11120.pdf"
    },
    {
        "title": "Design2Cloth: 3D Cloth Generation from 2D Masks",
        "author": "Jiali Zheng, Rolandos Alexandros Potamias, Stefanos Zafeiriou",
        "abstract": "In recent years, there has been a significant shift in the field of digital avatar research, towards modeling, animating and reconstructing clothed human representations, as a key step towards creating realistic avatars. However, current 3D cloth generation methods are garment specific or trained completely on synthetic data, hence lacking fine details and realism. In this work, we make a step towards automatic realistic garment design and propose Design2Cloth, a high fidelity 3D generative model trained on a real world dataset from more than 2000 subject scans. To provide vital contribution to the fashion industry, we developed a user-friendly adversarial model capable of generating diverse and detailed clothes simply by drawing a 2D cloth mask. Under a series of both qualitative and quantitative experiments, we showcase that Design2Cloth outperforms current state-of-the-art cloth generative models by a large margin. In addition to the generative properties of our network, we showcase that the proposed method can be used to achieve high quality reconstructions from single in-the-wild images and 3D scans. Dataset, code and pre-trained model will become publicly available.",
        "page": "http://arxiv.org/abs/2404.02686",
        "pdf": "http://arxiv.org/pdf/2404.02686.pdf"
    },
    {
        "title": "ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering",
        "author": "Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann",
        "abstract": "Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods.",
        "page": "http://arxiv.org/abs/2312.05941",
        "pdf": "http://arxiv.org/pdf/2312.05941.pdf"
    },
    {
        "title": "TetraSphere: A Neural Descriptor for O(3)-Invariant Point Cloud Analysis",
        "author": "Pavlo Melnyk, Andreas Robinson, Michael Felsberg, M\u00e5rten Wadenb\u00e4ck",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance",
        "author": "Zixuan Wang, Jia Jia, Shikun Sun, Haozhe Wu, Rong Han, Zhenyu Li, Di Tang, Jiaqing Zhou, Jiebo Luo",
        "abstract": "Choreographers determine what the dances look like, while cameramen determine the final presentation of dances. Recently, various methods and datasets have showcased the feasibility of dance synthesis. However, camera movement synthesis with music and dance remains an unsolved challenging problem due to the scarcity of paired data. Thus, we present DCM, a new multi-modal 3D dataset, which for the first time combines camera movement with dance motion and music audio. This dataset encompasses 108 dance sequences (3.2 hours) of paired dance-camera-music data from the anime community, covering 4 music genres. With this dataset, we uncover that dance camera movement is multifaceted and human-centric, and possesses multiple influencing factors, making dance camera synthesis a more challenging task compared to camera or dance synthesis alone. To overcome these difficulties, we propose DanceCamera3D, a transformer-based diffusion model that incorporates a novel body attention loss and a condition separation strategy. For evaluation, we devise new metrics measuring camera movement quality, diversity, and dancer fidelity. Utilizing these metrics, we conduct extensive experiments on our DCM dataset, providing both quantitative and qualitative evidence showcasing the effectiveness of our DanceCamera3D model. Code and video demos are available at https://github.com/Carmenw1203/DanceCamera3D-Official.",
        "page": "http://arxiv.org/abs/2403.13667",
        "pdf": "http://arxiv.org/pdf/2403.13667.pdf"
    },
    {
        "title": "Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer",
        "author": "Zhen Zhao, Jingqun Tang, Chunhui Lin, Binghong Wu, Can Huang, Hao Liu, Xin Tan, Zhizhong Zhang, Yuan Xie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network",
        "author": "Ruichen Ma, Guanchao Qiao, Yian Liu, Liwei Meng, Ning Ning, Yang Liu, Shaogang Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Semantic Line Combination Detector",
        "author": "JINWON KO, Dongkwon Jin, Chang-Su Kim",
        "abstract": "A novel algorithm, called semantic line combination detector (SLCD), to find an optimal combination of semantic lines is proposed in this paper. It processes all lines in each line combination at once to assess the overall harmony of the lines. First, we generate various line combinations from reliable lines. Second, we estimate the score of each line combination and determine the best one. Experimental results demonstrate that the proposed SLCD outperforms existing semantic line detectors on various datasets. Moreover, it is shown that SLCD can be applied effectively to three vision tasks of vanishing point detection, symmetry axis detection, and composition-based image retrieval. Our codes are available at https://github.com/Jinwon-Ko/SLCD.",
        "page": "http://arxiv.org/abs/2404.18399",
        "pdf": "http://arxiv.org/pdf/2404.18399.pdf"
    },
    {
        "title": "DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization",
        "author": "Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu",
        "abstract": "Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$ faster rendering speed.",
        "page": "http://arxiv.org/abs/2403.06912",
        "pdf": "http://arxiv.org/pdf/2403.06912.pdf"
    },
    {
        "title": "Frequency-Adaptive Dilated Convolution for Semantic Segmentation",
        "author": "Linwei Chen, Lin Gu, Dezhi Zheng, Ying Fu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos",
        "author": "Tomas Soucek, Dima Damen, Michael Wray, Ivan Laptev, Josef Sivic",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation guided by the Characteristic Dance Primitives",
        "author": "Ronghui Li, Yuxiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li",
        "abstract": "We propose Lodge, a network capable of generating extremely long dance sequences conditioned on given music. We design Lodge as a two-stage coarse to fine diffusion architecture, and propose the characteristic dance primitives that possess significant expressiveness as intermediate representations between two diffusion models. The first stage is global diffusion, which focuses on comprehending the coarse-level music-dance correlation and production characteristic dance primitives. In contrast, the second-stage is the local diffusion, which parallelly generates detailed motion sequences under the guidance of the dance primitives and choreographic rules. In addition, we propose a Foot Refine Block to optimize the contact between the feet and the ground, enhancing the physical realism of the motion. Our approach can parallelly generate dance sequences of extremely long length, striking a balance between global choreographic patterns and local motion quality and expressiveness. Extensive experiments validate the efficacy of our method.",
        "page": "http://arxiv.org/abs/2403.10518",
        "pdf": "http://arxiv.org/pdf/2403.10518.pdf"
    },
    {
        "title": "RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method",
        "author": "Ming Yan, Yan Zhang, Shuqiang Cai, Shuqi Fan, Xincheng Lin, Yudi Dai, Siqi Shen, Chenglu Wen, Lan Xu, Yuexin Ma, Cheng Wang",
        "abstract": "Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes. Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements. For holistic human motion understanding, we present RELI11D, a high-quality multimodal human motion dataset involves LiDAR, IMU system, RGB camera, and Event camera. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams. Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location. To address the challenge of integrating different modalities, we propose LEIR, a multimodal baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy. We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance. Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field.",
        "page": "http://arxiv.org/abs/2403.19501",
        "pdf": "http://arxiv.org/pdf/2403.19501.pdf"
    },
    {
        "title": "Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks",
        "author": "Shin&#x27;ya Yamaguchi, Sekitoshi Kanai, Kazuki Adachi, Daiki Chijiwa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data",
        "author": "Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu, Huabin Zhu, Binhui Yao, Tao Wang, Xiaolin Zheng, Yanchao Tan",
        "abstract": "Federated learning achieves effective performance in modeling decentralized data. In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data. However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models. The former indicates that representation collapse in local model will subsequently impact the global model and other local models. The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data. Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA). FUR in each client avoids representation collapse via dispersing samples uniformly, and EUA in server promotes unified representation by constraining consistent client model updating. To extensively validate the performance of FedU2, we conduct both cross-device and cross-silo evaluation experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.",
        "page": "http://arxiv.org/abs/2403.16398",
        "pdf": "http://arxiv.org/pdf/2403.16398.pdf"
    },
    {
        "title": "SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation",
        "author": "Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, Zhongliang Jing",
        "abstract": "Recent advancements in subject-driven image generation have led to zero-shot generation, yet precise selection and focus on crucial subject representations remain challenging. Addressing this, we introduce the SSR-Encoder, a novel architecture designed for selectively capturing any subject from single or multiple reference images. It responds to various query modalities including text and masks, without necessitating test-time fine-tuning. The SSR-Encoder combines a Token-to-Patch Aligner that aligns query inputs with image patches and a Detail-Preserving Subject Encoder for extracting and preserving fine features of the subjects, thereby generating subject embeddings. These embeddings, used in conjunction with original text embeddings, condition the generation process. Characterized by its model generalizability and efficiency, the SSR-Encoder adapts to a range of custom models and control modules. Enhanced by the Embedding Consistency Regularization Loss for improved training, our extensive experiments demonstrate its effectiveness in versatile and high-quality image generation, indicating its broad applicability. Project page: https://ssr-encoder.github.io",
        "page": "http://arxiv.org/abs/2312.16272",
        "pdf": "http://arxiv.org/pdf/2312.16272.pdf"
    },
    {
        "title": "Any-Shift Prompting for Generalization over Distributions",
        "author": "Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, Cees G. M. Snoek",
        "abstract": "Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.",
        "page": "http://arxiv.org/abs/2402.10099",
        "pdf": "http://arxiv.org/pdf/2402.10099.pdf"
    },
    {
        "title": "Language-Driven Anchors for Zero-Shot Adversarial Robustness",
        "author": "Xiao Li, Wei Zhang, Yining Liu, Zhanhao Hu, Bo Zhang, Xiaolin Hu",
        "abstract": "Deep Neural Networks (DNNs) are known to be susceptible to adversarial attacks. Previous researches mainly focus on improving adversarial robustness in the fully supervised setting, leaving the challenging domain of zero-shot adversarial robustness an open question. In this work, we investigate this domain by leveraging the recent advances in large vision-language models, such as CLIP, to introduce zero-shot adversarial robustness to DNNs. We propose LAAT, a Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes the features of a text encoder for each category as fixed anchors (normalized feature embeddings) for each category, which are then employed for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT aims to enhance the adversarial robustness of the image model on novel categories. However, naively using text encoders leads to poor results. Through analysis, we identified the issue to be the high cosine similarity between text encoders. We then design an expansion algorithm and an alignment cross-entropy loss to alleviate the problem. Our experimental results demonstrated that LAAT significantly improves zero-shot adversarial robustness over state-of-the-art methods. LAAT has the potential to enhance adversarial robustness by large-scale multimodal models, especially when labeled data is unavailable during training.",
        "page": "http://arxiv.org/abs/2301.13096",
        "pdf": "http://arxiv.org/pdf/2301.13096.pdf"
    },
    {
        "title": "HRVDA: High-Resolution Visual Document Assistant",
        "author": "Chaohu Liu, Kun Yin, Haoyu Cao, Xinghua Jiang, Xin Li, Yinsong Liu, Deqiang Jiang, Xing Sun, Linli Xu",
        "abstract": "Leveraging vast training data, multimodal large language models (MLLMs) have demonstrated formidable general visual comprehension capabilities and achieved remarkable performance across various tasks. However, their performance in visual document understanding still leaves much room for improvement. This discrepancy is primarily attributed to the fact that visual document understanding is a fine-grained prediction task. In natural scenes, MLLMs typically use low-resolution images, leading to a substantial loss of visual information. Furthermore, general-purpose MLLMs do not excel in handling document-oriented instructions. In this paper, we propose a High-Resolution Visual Document Assistant (HRVDA), which bridges the gap between MLLMs and visual document understanding. This model employs a content filtering mechanism and an instruction filtering module to separately filter out the content-agnostic visual tokens and instruction-agnostic visual tokens, thereby achieving efficient model training and inference for high-resolution images. In addition, we construct a document-oriented visual instruction tuning dataset and apply a multi-stage training strategy to enhance the model's document modeling capabilities. Extensive experiments demonstrate that our model achieves state-of-the-art performance across multiple document understanding datasets, while maintaining training efficiency and inference speed comparable to low-resolution models.",
        "page": "http://arxiv.org/abs/2404.06918",
        "pdf": "http://arxiv.org/pdf/2404.06918.pdf"
    },
    {
        "title": "Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training",
        "author": "Runze He, Shaofei Huang, Xuecheng Nie, Tianrui Hui, Luoqi Liu, Jiao Dai, Jizhong Han, Guanbin Li, Si Liu",
        "abstract": "In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings.",
        "page": "http://arxiv.org/abs/2312.01663",
        "pdf": "http://arxiv.org/pdf/2312.01663.pdf"
    },
    {
        "title": "A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning",
        "author": "Yuelin Zhang, Pengyu Zheng, Wanquan Yan, Chengyu Fang, Shing Shin Cheng",
        "abstract": "Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem, a unified framework including the multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and data deficiency. The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context. The EFCR addresses the data deficiency problem by exploring latent deblur signals from different frequency bands. It also enables deblur knowledge transfer to learn cross-domain information from extra data, improving deblur performance for labeled and unlabeled data. Extensive experiments and downstream task validation show the framework achieves state-of-the-art performance across multiple datasets. Project page: https://github.com/PieceZhang/MPT-CataBlur.",
        "page": "http://arxiv.org/abs/2403.02611",
        "pdf": "http://arxiv.org/pdf/2403.02611.pdf"
    },
    {
        "title": "Neural Modes: Self-supervised Learning of Nonlinear Modal Subspaces",
        "author": "Jiahong Wang, Yinwei DU, Stelian Coros, Bernhard Thomaszewski",
        "abstract": "We propose a self-supervised approach for learning physics-based subspaces for real-time simulation. Existing learning-based methods construct subspaces by approximating pre-defined simulation data in a purely geometric way. However, this approach tends to produce high-energy configurations, leads to entangled latent space dimensions, and generalizes poorly beyond the training set. To overcome these limitations, we propose a self-supervised approach that directly minimizes the system's mechanical energy during training. We show that our method leads to learned subspaces that reflect physical equilibrium constraints, resolve overfitting issues of previous methods, and offer interpretable latent space parameters.",
        "page": "http://arxiv.org/abs/2404.17620",
        "pdf": "http://arxiv.org/pdf/2404.17620.pdf"
    },
    {
        "title": "Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting",
        "author": "Haipeng Liu, Yang Wang, Biao Qian, Meng Wang, Yong Rui",
        "abstract": "Denoising diffusion probabilistic models for image inpainting aim to add the noise to the texture of image during the forward process and recover masked regions with unmasked ones of the texture via the reverse denoising process. Despite the meaningful semantics generation, the existing arts suffer from the semantic discrepancy between masked and unmasked regions, since the semantically dense unmasked texture fails to be completely degraded while the masked regions turn to the pure noise in diffusion process, leading to the large discrepancy between them. In this paper, we aim to answer how unmasked semantics guide texture denoising process;together with how to tackle the semantic discrepancy, to facilitate the consistent and meaningful semantics generation. To this end, we propose a novel structure-guided diffusion model named StrDiffusion, to reformulate the conventional texture denoising process under structure guidance to derive a simplified denoising objective for image inpainting, while revealing: 1) the semantically sparse structure is beneficial to tackle semantic discrepancy in early stage, while dense texture generates reasonable semantics in late stage; 2) the semantics from unmasked regions essentially offer the time-dependent structure guidance for the texture denoising process, benefiting from the time-dependent sparsity of the structure semantics. For the denoising process, a structure-guided neural network is trained to estimate the simplified denoising objective by exploiting the consistency of the denoised structure between masked and unmasked regions. Besides, we devise an adaptive resampling strategy as a formal criterion as whether structure is competent to guide the texture denoising process, while regulate their semantic correlations. Extensive experiments validate the merits of StrDiffusion over the state-of-the-arts. Our code is available at https://github.com/htyjers/StrDiffusion.",
        "page": "http://arxiv.org/abs/2403.19898",
        "pdf": "http://arxiv.org/pdf/2403.19898.pdf"
    },
    {
        "title": "SketchINR: A First Look into Sketches as Implicit Neural Representations",
        "author": "Hmrishav Bandyopadhyay, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Tao Xiang, Timothy Hospedales, Yi-Zhe Song",
        "abstract": "We propose SketchINR, to advance the representation of vector sketches with implicit neural models. A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes. The learned function predicts the $xy$ point coordinates in a sketch at each time and stroke. Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives $60\\times$ and $10\\times$ data compression over raster and vector sketches, respectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO. (iii) SketchINR supports parallelisation that can decode/render $\\sim$$100\\times$ faster than other learned vector representations such as SketchRNN. (iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complexity of strokes. As a first look at implicit sketches, SketchINR's compact high-fidelity representation will support future work in modelling long and complex sketches.",
        "page": "http://arxiv.org/abs/2403.09344",
        "pdf": "http://arxiv.org/pdf/2403.09344.pdf"
    },
    {
        "title": "What Sketch Explainability Really Means for Downstream Tasks ?",
        "author": "Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Tao Xiang, Yi-Zhe Song",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Doodle Your 3D: From Abstract Freehand Sketches to Precise 3D Shapes",
        "author": "Hmrishav Bandyopadhyay, Subhadeep Koley, Ayan Das, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
        "abstract": "In this paper, we democratise 3D content creation, enabling precise generation of 3D shapes from abstract sketches while overcoming limitations tied to drawing skills. We introduce a novel part-level modelling and alignment framework that facilitates abstraction modelling and cross-modal correspondence. Leveraging the same part-level decoder, our approach seamlessly extends to sketch modelling by establishing correspondence between CLIPasso edgemaps and projected 3D part regions, eliminating the need for a dataset pairing human sketches and 3D shapes. Additionally, our method introduces a seamless in-position editing process as a byproduct of cross-modal part-aligned modelling. Operating in a low-dimensional implicit space, our approach significantly reduces computational demands and processing time.",
        "page": "http://arxiv.org/abs/2312.04043",
        "pdf": "http://arxiv.org/pdf/2312.04043.pdf"
    },
    {
        "title": "Portrait4D: Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data",
        "author": "Yu Deng, Duomin Wang, Xiaohang Ren, Xingyu Chen, Baoyuan Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners",
        "author": "Chun Feng, Joy Hsu, Weiyu Liu, Jiajun Wu",
        "abstract": "3D visual grounding is a challenging task that often requires direct and dense supervision, notably the semantic label for each object in the scene. In this paper, we instead study the naturally supervised setting that learns from only 3D scene and QA pairs, where prior works underperform. We propose the Language-Regularized Concept Learner (LARC), which uses constraints from language as regularization to significantly improve the accuracy of neuro-symbolic concept learners in the naturally supervised setting. Our approach is based on two core insights: the first is that language constraints (e.g., a word's relation to another) can serve as effective regularization for structured representations in neuro-symbolic models; the second is that we can query large language models to distill such constraints from language properties. We show that LARC improves performance of prior works in naturally supervised 3D visual grounding, and demonstrates a wide range of 3D visual reasoning capabilities-from zero-shot composition, to data efficiency and transferability. Our method represents a promising step towards regularizing structured visual reasoning frameworks with language-based priors, for learning in settings without dense supervision.",
        "page": "http://arxiv.org/abs/2404.19696",
        "pdf": "http://arxiv.org/pdf/2404.19696.pdf"
    },
    {
        "title": "Tuning Stable Rank Shrinkage: Aiming at the Overlooked Structural Risk in Fine-tuning",
        "author": "Sicong Shen, Yang Zhou, Bingzheng Wei, Eric Chang, Yan Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adaptive Multi-Modal Cross-Entropy Loss for Stereo Matching",
        "author": "Peng Xu, Zhiyu Xiang, Chengyu Qiao, Jingyun Fu, Tianyu Pu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching",
        "author": "Xinghui Li, Jingyi Lu, Kai Han, Victor Adrian Prisacariu",
        "abstract": "In this paper, we address the challenge of matching semantically similar keypoints across image pairs. Existing research indicates that the intermediate output of the UNet within the Stable Diffusion (SD) can serve as robust image feature maps for such a matching task. We demonstrate that by employing a basic prompt tuning technique, the inherent potential of Stable Diffusion can be harnessed, resulting in a significant enhancement in accuracy over previous approaches. We further introduce a novel conditional prompting module that conditions the prompt on the local details of the input image pairs, leading to a further improvement in performance. We designate our approach as SD4Match, short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets new benchmarks in accuracy across all these datasets. Particularly, SD4Match outperforms the previous state-of-the-art by a margin of 12 percentage points on the challenging SPair-71k dataset.",
        "page": "http://arxiv.org/abs/2310.17569",
        "pdf": "http://arxiv.org/pdf/2310.17569.pdf"
    },
    {
        "title": "Towards Real-World HDR Video Reconstruction: A Large-Scale Benchmark Dataset and A Two-Stage Alignment Network",
        "author": "Yong Shu, Liquan Shen, Xiangyu Hu, Mengyao Li, Zihao Zhou",
        "abstract": "As an important and practical way to obtain high dynamic range (HDR) video, HDR video reconstruction from sequences with alternating exposures is still less explored, mainly due to the lack of large-scale real-world datasets. Existing methods are mostly trained on synthetic datasets, which perform poorly in real scenes. In this work, to facilitate the development of real-world HDR video reconstruction, we present Real-HDRV, a large-scale real-world benchmark dataset for HDR video reconstruction, featuring various scenes, diverse motion patterns, and high-quality labels. Specifically, our dataset contains 500 LDRs-HDRs video pairs, comprising about 28,000 LDR frames and 4,000 HDR labels, covering daytime, nighttime, indoor, and outdoor scenes. To our best knowledge, our dataset is the largest real-world HDR video reconstruction dataset. Correspondingly, we propose an end-to-end network for HDR video reconstruction, where a novel two-stage strategy is designed to perform alignment sequentially. Specifically, the first stage performs global alignment with the adaptively estimated global offsets, reducing the difficulty of subsequent alignment. The second stage implicitly performs local alignment in a coarse-to-fine manner at the feature level using the adaptive separable convolution. Extensive experiments demonstrate that: (1) models trained on our dataset can achieve better performance on real scenes than those trained on synthetic datasets; (2) our method outperforms previous state-of-the-art methods. Our dataset is available at https://github.com/yungsyu99/Real-HDRV.",
        "page": "http://arxiv.org/abs/2405.00244",
        "pdf": "http://arxiv.org/pdf/2405.00244.pdf"
    },
    {
        "title": "Close Imitation of Expert Retouching for Black-and-White Photography",
        "author": "Seunghyun Shin, Jisu Shin, Jihwan Bae, Inwook Shim, Hae-Gon Jeon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Cross-dimension Affinity Distillation for 3D EM Neuron Segmentation",
        "author": "Xiaoyu Liu, Miaomiao Cai, Yinda Chen, Yueyi Zhang, Te Shi, Ruobing Zhang, Xuejin Chen, Zhiwei Xiong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees",
        "author": "Hakyeong Kim, Andreas Meuleman, Hyeonjoong Jang, James Tompkin, Min H. Kim",
        "abstract": "We present a method to reconstruct indoor and outdoor static scene geometry and appearance from an omnidirectional video moving in a small circular sweep. This setting is challenging because of the small baseline and large depth ranges, making it difficult to find ray crossings. To better constrain the optimization, we estimate geometry as a signed distance field within a spherical binoctree data structure and use a complementary efficient tree traversal strategy based on a breadth-first search for sampling. Unlike regular grids or trees, the shape of this structure well-matches the camera setting, creating a better memory-quality trade-off. From an initial depth estimate, the binoctree is adaptively subdivided throughout the optimization; previous methods use a fixed depth that leaves the scene undersampled. In comparison with three neural optimization methods and two non-neural methods, ours shows decreased geometry error on average, especially in a detailed scene, while significantly reducing the required number of voxels to represent such details.",
        "page": "http://arxiv.org/abs/2404.00678",
        "pdf": "http://arxiv.org/pdf/2404.00678.pdf"
    },
    {
        "title": "Relation Rectification in Diffusion Model",
        "author": "Yinwei Wu, Xingyi Yang, Xinchao Wang",
        "abstract": "Despite their exceptional generative abilities, large text-to-image diffusion models, much like skilled but careless artists, often struggle with accurately depicting visual relationships between objects. This issue, as we uncover through careful analysis, arises from a misaligned text encoder that struggles to interpret specific relationships and differentiate the logical order of associated objects. To resolve this, we introduce a novel task termed Relation Rectification, aiming to refine the model to accurately represent a given relationship it initially fails to generate. To address this, we propose an innovative solution utilizing a Heterogeneous Graph Convolutional Network (HGCN). It models the directional relationships between relation terms and corresponding objects within the input prompts. Specifically, we optimize the HGCN on a pair of prompts with identical relational words but reversed object orders, supplemented by a few reference images. The lightweight HGCN adjusts the text embeddings generated by the text encoder, ensuring the accurate reflection of the textual relation in the embedding space. Crucially, our method retains the parameters of the text encoder and diffusion model, preserving the model's robust performance on unrelated descriptions. We validated our approach on a newly curated dataset of diverse relational data, demonstrating both quantitative and qualitative enhancements in generating images with precise visual relations. Project page: https://wuyinwei-hah.github.io/rrnet.github.io/.",
        "page": "http://arxiv.org/abs/2403.20249",
        "pdf": "http://arxiv.org/pdf/2403.20249.pdf"
    },
    {
        "title": "AdaRevD: Adaptive Patch Exiting Reversible Decoder Pushes the Limit of Image Deblurring",
        "author": "Xintian Mao, Xiwen Gao, Yan Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization",
        "author": "Shuai Tan, Bin Ji, Ye Pan",
        "abstract": "Generating emotional talking faces is a practical yet challenging endeavor. To create a lifelike avatar, we draw upon two critical insights from a human perspective: 1) The connection between audio and the non-deterministic facial dynamics, encompassing expressions, blinks, poses, should exhibit synchronous and one-to-many mapping. 2) Vibrant expressions are often accompanied by emotion-aware high-definition (HD) textures and finely detailed teeth. However, both aspects are frequently overlooked by existing methods. To this end, this paper proposes using normalizing Flow and Vector-Quantization modeling to produce emotional talking faces that satisfy both insights concurrently (FlowVQTalker). Specifically, we develop a flow-based coefficient generator that encodes the dynamics of facial emotion into a multi-emotion-class latent space represented as a mixture distribution. The generation process commences with random sampling from the modeled distribution, guided by the accompanying audio, enabling both lip-synchronization and the uncertain nonverbal facial cues generation. Furthermore, our designed vector-quantization image generator treats the creation of expressive facial images as a code query task, utilizing a learned codebook to provide rich, high-quality textures that enhance the emotional perception of the results. Extensive experiments are conducted to showcase the effectiveness of our approach.",
        "page": "http://arxiv.org/abs/2403.06375",
        "pdf": "http://arxiv.org/pdf/2403.06375.pdf"
    },
    {
        "title": "Infer from What You Have Seen Before: Temporally-dependent Classifier for Semi-supervised Video Semantic Segmentation",
        "author": "Jiafan Zhuang, Zilei Wang, Yixin Zhang, Zhun Fan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models",
        "author": "LIn Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, Yu Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DeiT-LT: Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets",
        "author": "Harsh Rangwani, Pradipto Mondal, Mayank Mishra, Ashish Asokan, R. Venkatesh Babu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth",
        "author": "Zhaoyang Sun, Shengwu Xiong, Yaxiong Chen, Yi Rong",
        "abstract": "The absence of real targets to guide the model training is one of the main problems with the makeup transfer task. Most existing methods tackle this problem by synthesizing pseudo ground truths (PGTs). However, the generated PGTs are often sub-optimal and their imprecision will eventually lead to performance degradation. To alleviate this issue, in this paper, we propose a novel Content-Style Decoupled Makeup Transfer (CSD-MT) method, which works in a purely unsupervised manner and thus eliminates the negative effects of generating PGTs. Specifically, based on the frequency characteristics analysis, we assume that the low-frequency (LF) component of a face image is more associated with its makeup style information, while the high-frequency (HF) component is more related to its content details. This assumption allows CSD-MT to decouple the content and makeup style information in each face image through the frequency decomposition. After that, CSD-MT realizes makeup transfer by maximizing the consistency of these two types of information between the transferred result and input images, respectively. Two newly designed loss functions are also introduced to further improve the transfer performance. Extensive quantitative and qualitative analyses show the effectiveness of our CSD-MT method. Our code is available at https://github.com/Snowfallingplum/CSD-MT.",
        "page": "http://arxiv.org/abs/2405.17240",
        "pdf": "http://arxiv.org/pdf/2405.17240.pdf"
    },
    {
        "title": "DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data",
        "author": "Chengxiang Fan, Muzhi Zhu, Hao Chen, Yang Liu, Weijia Wu, Huaqi Zhang, Chunhua Shen",
        "abstract": "Instance segmentation is data-hungry, and as model capacity increases, data scale becomes crucial for improving the accuracy. Most instance segmentation datasets today require costly manual annotation, limiting their data scale. Models trained on such data are prone to overfitting on the training set, especially for those rare categories. While recent works have delved into exploiting generative models to create synthetic datasets for data augmentation, these approaches do not efficiently harness the full potential of generative models. To address these issues, we introduce a more efficient strategy to construct generative datasets for data augmentation, termed DiverGen. Firstly, we provide an explanation of the role of generative data from the perspective of distribution discrepancy. We investigate the impact of different data on the distribution learned by the model. We argue that generative data can expand the data distribution that the model can learn, thus mitigating overfitting. Additionally, we find that the diversity of generative data is crucial for improving model performance and enhance it through various strategies, including category diversity, prompt diversity, and generative model diversity. With these strategies, we can scale the data to millions while maintaining the trend of model performance improvement. On the LVIS dataset, DiverGen significantly outperforms the strong model X-Paste, achieving +1.1 box AP and +1.1 mask AP across all categories, and +1.9 box AP and +2.5 mask AP for rare categories.",
        "page": "http://arxiv.org/abs/2405.10185",
        "pdf": "http://arxiv.org/pdf/2405.10185.pdf"
    },
    {
        "title": "ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning",
        "author": "Beomyoung Kim, Joonsang Yu, Sung Ju Hwang",
        "abstract": "Panoptic segmentation, combining semantic and instance segmentation, stands as a cutting-edge computer vision task. Despite recent progress with deep learning models, the dynamic nature of real-world applications necessitates continual learning, where models adapt to new classes (plasticity) over time without forgetting old ones (catastrophic forgetting). Current continual segmentation methods often rely on distillation strategies like knowledge distillation and pseudo-labeling, which are effective but result in increased training complexity and computational overhead. In this paper, we introduce a novel and efficient method for continual panoptic segmentation based on Visual Prompt Tuning, dubbed ECLIPSE. Our approach involves freezing the base model parameters and fine-tuning only a small set of prompt embeddings, addressing both catastrophic forgetting and plasticity and significantly reducing the trainable parameters. To mitigate inherent challenges such as error propagation and semantic drift in continual segmentation, we propose logit manipulation to effectively leverage common knowledge across the classes. Experiments on ADE20K continual panoptic segmentation benchmark demonstrate the superiority of ECLIPSE, notably its robustness against catastrophic forgetting and its reasonable plasticity, achieving a new state-of-the-art. The code is available at https://github.com/clovaai/ECLIPSE.",
        "page": "http://arxiv.org/abs/2403.20126",
        "pdf": "http://arxiv.org/pdf/2403.20126.pdf"
    },
    {
        "title": "Person in Place: Generating Associative Skeleton-Guidance Maps for Human-Object Interaction Image Editing",
        "author": "ChangHee Yang, ChanHee Kang, Kyeongbo Kong, Hanni Oh, Suk-Ju Kang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TE-TAD: Towards Fully End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression",
        "author": "Ho-Joong Kim, Jung-Ho Hong, Heejo Kong, Seong-Whan Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diversified and Personalized Multi-rater Medical Image Segmentation",
        "author": "Yicheng Wu, Xiangde Luo, Zhe Xu, Xiaoqing Guo, Lie Ju, Zongyuan Ge, Wenjun Liao, Jianfei Cai",
        "abstract": "Annotation ambiguity due to inherent data uncertainties such as blurred boundaries in medical scans and different observer expertise and preferences has become a major obstacle for training deep-learning based medical image segmentation models. To address it, the common practice is to gather multiple annotations from different experts, leading to the setting of multi-rater medical image segmentation. Existing works aim to either merge different annotations into the \"groundtruth\" that is often unattainable in numerous medical contexts, or generate diverse results, or produce personalized results corresponding to individual expert raters. Here, we bring up a more ambitious goal for multi-rater medical image segmentation, i.e., obtaining both diversified and personalized results. Specifically, we propose a two-stage framework named D-Persona (first Diversification and then Personalization). In Stage I, we exploit multiple given annotations to train a Probabilistic U-Net model, with a bound-constrained loss to improve the prediction diversity. In this way, a common latent space is constructed in Stage I, where different latent codes denote diversified expert opinions. Then, in Stage II, we design multiple attention-based projection heads to adaptively query the corresponding expert prompts from the shared latent space, and then perform the personalized medical image segmentation. We evaluated the proposed model on our in-house Nasopharyngeal Carcinoma dataset and the public lung nodule dataset (i.e., LIDC-IDRI). Extensive experiments demonstrated our D-Persona can provide diversified and personalized results at the same time, achieving new SOTA performance for multi-rater medical image segmentation. Our code will be released at https://github.com/ycwu1997/D-Persona.",
        "page": "http://arxiv.org/abs/2403.13417",
        "pdf": "http://arxiv.org/pdf/2403.13417.pdf"
    },
    {
        "title": "Taming Stable Diffusion for Text to 360$^{\\circ}$ Panorama Image Generation",
        "author": "Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments",
        "author": "Duy Tho Le, Chenhui Gou, Stavya Datta, Hengcan Shi, Ian Reid, Jianfei Cai, Hamid Rezatofighi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "How Far Can We Compress Instant NGP-Based NeRF?",
        "author": "Yihang Chen, Qianyi Wu, Mehrtash Harandi, Jianfei Cai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ReconFusion: 3D Reconstruction with Diffusion Priors",
        "author": "Rundi Wu, Ben Mildenhall, Philipp Henzler, Ruiqi Gao, Keunhong Park, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, Aleksander Holynski",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generative Powers of Ten",
        "author": "Xiaojuan Wang, Janne Kontkanen, Brian Curless, Steve Seitz, Ira Kemelmacher-Shlizerman, Ben Mildenhall, Pratul P. Srinivasan, Dor Verbin, Aleksander Holynski",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CoDe: An Explicit Content Decoupling Framework for Image Restoration",
        "author": "Enxuan Gu, Hongwei Ge, Yong Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Unified Framework for Human-centric Point Cloud Video Understanding",
        "author": "Yiteng Xu, Kecheng Ye, xiao han, yiming ren, Xinge Zhu, Yuexin Ma",
        "abstract": "Human-centric Point Cloud Video Understanding (PVU) is an emerging field focused on extracting and interpreting human-related features from sequences of human point clouds, further advancing downstream human-centric tasks and applications. Previous works usually focus on tackling one specific task and rely on huge labeled data, which has poor generalization capability. Considering that human has specific characteristics, including the structural semantics of human body and the dynamics of human motions, we propose a unified framework to make full use of the prior knowledge and explore the inherent features in the data itself for generalized human-centric point cloud video understanding. Extensive experiments demonstrate that our method achieves state-of-the-art performance on various human-related tasks, including action recognition and 3D pose estimation. All datasets and code will be released soon.",
        "page": "http://arxiv.org/abs/2403.20031",
        "pdf": "http://arxiv.org/pdf/2403.20031.pdf"
    },
    {
        "title": "Delving into the Trajectory Long-tail Distribution for Muti-object Tracking",
        "author": "Sijia Chen, En Yu, Jinyang Li, Wenbing Tao",
        "abstract": "Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations. Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques. Yet, there has been a lack of thorough examination concerning the nature of tracking data it self. In this study, we pioneer an exploration into the distribution patterns of tracking data and identify a pronounced long-tail distribution issue within existing MOT datasets. We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as ``pedestrians trajectory long-tail distribution''. Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution. Specifically, we propose two data augmentation strategies, including Stationary Camera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID. SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use diffusion model to change the background of the scene. GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually. Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance. The code is available at https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.",
        "page": "http://arxiv.org/abs/2403.04700",
        "pdf": "http://arxiv.org/pdf/2403.04700.pdf"
    },
    {
        "title": "Flexible Biometrics Recognition: Bridging the Multimodality Gap through Attention, Alignment and Prompt Tuning",
        "author": "Leslie Ching Ow Tiong, Dick Sigmund, Chen-Hui Chan, Andrew Beng Jin Teoh",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Language-Driven Video Inpainting via Multimodal Large Language Models",
        "author": "Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy",
        "abstract": "We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.",
        "page": "http://arxiv.org/abs/2401.10226",
        "pdf": "http://arxiv.org/pdf/2401.10226.pdf"
    },
    {
        "title": "MotionEditor: Editing Video Motion via Content-Aware Diffusion",
        "author": "Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, Yu-Gang Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks",
        "author": "Marina Neseem, Conor McCullough, Randy Hsin, Chas Leichner, Shan Li, In Suk Chong, Andrew Howard, Lukasz Lew, Sherief Reda, Ville-Mikko Rautio, Daniele Moro",
        "abstract": "Low-precision quantization is recognized for its efficacy in neural network optimization. Our analysis reveals that non-quantized elementwise operations which are prevalent in layers such as parameterized activation functions, batch normalization, and quantization scaling dominate the inference cost of low-precision models. These non-quantized elementwise operations are commonly overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort (ACE). In this paper, we propose ACEv2 - an extended version of ACE which offers a better alignment with the inference cost of quantized models and their energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that addresses these efficiency issues by applying quantization to both elementwise operations and multiply-accumulate operations. In particular, we present a novel quantization technique for batch normalization layers named QuantNorm which allows for quantizing the batch normalization parameters without compromising the model performance. Additionally, we propose applying Double Quantization where the quantization scaling parameters are quantized. Furthermore, we recognize and resolve the issue of distribution mismatch in Separable Convolution layers by introducing Distribution-Heterogeneous Quantization which enables quantizing them to low-precision. PikeLPN achieves Pareto-optimality in efficiency-accuracy trade-off with up to 3X efficiency improvement compared to SOTA low-precision models.",
        "page": "http://arxiv.org/abs/2404.00103",
        "pdf": "http://arxiv.org/pdf/2404.00103.pdf"
    },
    {
        "title": "Video Harmonization with Triplet Spatio-Temporal Variation Patterns",
        "author": "Zonghui Guo, XinYu Han, Jie Zhang, Shiguang Shan, Haiyong Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Structure-Aware Sparse-View X-ray 3D Reconstruction",
        "author": "Yuanhao Cai, Jiahao Wang, Alan L. Yuille, Zongwei Zhou, Angtian Wang",
        "abstract": "X-ray, known for its ability to reveal internal structures of objects, is expected to provide richer information for 3D reconstruction than visible light. Yet, existing neural radiance fields (NeRF) algorithms overlook this important nature of X-ray, leading to their limitations in capturing structural contents of imaged objects. In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. Code, models, and data are released at https://github.com/caiyuanhao1998/SAX-NeRF",
        "page": "http://arxiv.org/abs/2311.10959",
        "pdf": "http://arxiv.org/pdf/2311.10959.pdf"
    },
    {
        "title": "Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance",
        "author": "Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang",
        "abstract": "Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.",
        "page": "http://arxiv.org/abs/2403.18036",
        "pdf": "http://arxiv.org/pdf/2403.18036.pdf"
    },
    {
        "title": "Scaling Up Dynamic 3D Human-Scene Interaction Modelling",
        "author": "Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CityDreamer: Compositional Generative Model of Unbounded 3D Cities",
        "author": "Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu",
        "abstract": "3D city generation is a desirable yet challenging task, since humans are more sensitive to structural distortions in urban environments. Additionally, generating 3D cities is more complex than 3D natural scenes since buildings, as objects of the same class, exhibit a wider range of appearances compared to the relatively consistent appearance of objects like trees in natural scenes. To address these challenges, we propose \\textbf{CityDreamer}, a compositional generative model designed specifically for unbounded 3D cities. Our key insight is that 3D city generation should be a composition of different types of neural fields: 1) various building instances, and 2) background stuff, such as roads and green lands. Specifically, we adopt the bird's eye view scene representation and employ a volumetric render for both instance-oriented and stuff-oriented neural fields. The generative hash grid and periodic positional embedding are tailored as scene parameterization to suit the distinct characteristics of building instances and background stuff. Furthermore, we contribute a suite of CityGen Datasets, including OSM and GoogleEarth, which comprises a vast amount of real-world city imagery to enhance the realism of the generated 3D cities both in their layouts and appearances. CityDreamer achieves state-of-the-art performance not only in generating realistic 3D cities but also in localized editing within the generated cities.",
        "page": "http://arxiv.org/abs/2309.00610",
        "pdf": "http://arxiv.org/pdf/2309.00610.pdf"
    },
    {
        "title": "Bayesian Differentiable Physics for Cloth Digitalization",
        "author": "Deshan Gong, Ningtao Mao, He Wang",
        "abstract": "We propose a new method for cloth digitalization. Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths. However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements. Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process. To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths. It can provide highly accurate digitalization from very limited data samples. Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations. Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization",
        "page": "http://arxiv.org/abs/2402.17664",
        "pdf": "http://arxiv.org/pdf/2402.17664.pdf"
    },
    {
        "title": "Multiscale Vision Transformers meet Bipartite Matching for efficient single-stage Action Localization",
        "author": "Ioanna Ntinou, Enrique Sanchez, Georgios Tzimiropoulos",
        "abstract": "Action Localization is a challenging problem that combines detection and recognition tasks, which are often addressed separately. State-of-the-art methods rely on off-the-shelf bounding box detections pre-computed at high resolution, and propose transformer models that focus on the classification task alone. Such two-stage solutions are prohibitive for real-time deployment. On the other hand, single-stage methods target both tasks by devoting part of the network (generally the backbone) to sharing the majority of the workload, compromising performance for speed. These methods build on adding a DETR head with learnable queries that after cross- and self-attention can be sent to corresponding MLPs for detecting a person's bounding box and action. However, DETR-like architectures are challenging to train and can incur in big complexity. In this paper, we observe that \\textbf{a straight bipartite matching loss can be applied to the output tokens of a vision transformer}. This results in a backbone + MLP architecture that can do both tasks without the need of an extra encoder-decoder head and learnable queries. We show that a single MViTv2-S architecture trained with bipartite matching to perform both tasks surpasses the same MViTv2-S when trained with RoI align on pre-computed bounding boxes. With a careful design of token pooling and the proposed training pipeline, our Bipartite-Matching Vision Transformer model, \\textbf{BMViT}, achieves +3 mAP on AVA2.2. w.r.t. the two-stage MViTv2-S counterpart. Code is available at \\href{https://github.com/IoannaNti/BMViT}{https://github.com/IoannaNti/BMViT}",
        "page": "http://arxiv.org/abs/2312.17686",
        "pdf": "http://arxiv.org/pdf/2312.17686.pdf"
    },
    {
        "title": "VidLA: Video-Language Alignment at Scale",
        "author": "Mamshad Nayeem Rizve, Fan Fei, Jayakrishnan Unnikrishnan, Son Dinh Tran, Benjamin Yao, Belinda Zeng, Mubarak Shah, Trishul Chilimbi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ProMark: Proactive Diffusion Watermarking for Causal Attribution",
        "author": "Vishal Asnani, John Collomosse, Tu Bui, Xiaoming Liu, Shruti Agarwal",
        "abstract": "Generative AI (GenAI) is transforming creative workflows through the capability to synthesize and manipulate images via high-level prompts. Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a synthetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input training images using imperceptible watermarks, and the diffusion models (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as $2^{16}$ unique watermarks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attribution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images.",
        "page": "http://arxiv.org/abs/2403.09914",
        "pdf": "http://arxiv.org/pdf/2403.09914.pdf"
    },
    {
        "title": "MemFlow: Optical Flow Estimation and Prediction with Memory",
        "author": "Qiaole Dong, Yanwei Fu",
        "abstract": "Optical flow is a classical task that is important to the vision community. Classical optical flow estimation uses two frames as input, whilst some recent methods consider multiple frames to explicitly model long-range information. The former ones limit their ability to fully leverage temporal coherence along the video sequence; and the latter ones incur heavy computational overhead, typically not possible for real-time flow estimation. Some multi-frame-based approaches even necessitate unseen future frames for current estimation, compromising real-time applicability in safety-critical scenarios. To this end, we present MemFlow, a real-time method for optical flow estimation and prediction with memory. Our method enables memory read-out and update modules for aggregating historical motion information in real-time. Furthermore, we integrate resolution-adaptive re-scaling to accommodate diverse video resolutions. Besides, our approach seamlessly extends to the future prediction of optical flow based on past observations. Leveraging effective historical motion aggregation, our method outperforms VideoFlow with fewer parameters and faster inference speed on Sintel and KITTI-15 datasets in terms of generalization performance. At the time of submission, MemFlow also leads in performance on the 1080p Spring dataset. Codes and models will be available at: https://dqiaole.github.io/MemFlow/.",
        "page": "http://arxiv.org/abs/2404.04808",
        "pdf": "http://arxiv.org/pdf/2404.04808.pdf"
    },
    {
        "title": "Improving Depth Completion via Depth Feature Upsampling",
        "author": "Yufei Wang, Ge Zhang, Shaoqian Wang, Bo Li, Qi Liu, Le Hui, Yuchao Dai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Bootstrapping Autonomous Driving Radars with Self-Supervised Learning",
        "author": "Yiduo Hao, Sohrab Madani, Junfeng Guan, Mo Alloulah, Saurabh Gupta, Haitham Al Hassanieh",
        "abstract": "The perception of autonomous vehicles using radars has attracted increased research interest due its ability to operate in fog and bad weather. However, training radar models is hindered by the cost and difficulty of annotating large-scale radar data. To overcome this bottleneck, we propose a self-supervised learning framework to leverage the large amount of unlabeled radar data to pre-train radar-only embeddings for self-driving perception tasks. The proposed method combines radar-to-radar and radar-to-vision contrastive losses to learn a general representation from unlabeled radar heatmaps paired with their corresponding camera images. When used for downstream object detection, we demonstrate that the proposed self-supervision framework can improve the accuracy of state-of-the-art supervised baselines by $5.8\\%$ in mAP. Code is available at \\url{https://github.com/yiduohao/Radical}.",
        "page": "http://arxiv.org/abs/2312.04519",
        "pdf": "http://arxiv.org/pdf/2312.04519.pdf"
    },
    {
        "title": "Misalignment-Robust Frequency Distribution Loss for Image Transformation",
        "author": "Zhangkai Ni, Juncheng Wu, Zian Wang, Wenhan Yang, Hanli Wang, Lin Ma",
        "abstract": "This paper aims to address a common challenge in deep learning-based image transformation methods, such as image enhancement and super-resolution, which heavily rely on precisely aligned paired datasets with pixel-level alignments. However, creating precisely aligned paired images presents significant challenges and hinders the advancement of methods trained on such data. To overcome this challenge, this paper introduces a novel and simple Frequency Distribution Loss (FDL) for computing distribution distance within the frequency domain. Specifically, we transform image features into the frequency domain using Discrete Fourier Transformation (DFT). Subsequently, frequency components (amplitude and phase) are processed separately to form the FDL loss function. Our method is empirically proven effective as a training constraint due to the thoughtful utilization of global information in the frequency domain. Extensive experimental evaluations, focusing on image enhancement and super-resolution tasks, demonstrate that FDL outperforms existing misalignment-robust loss functions. Furthermore, we explore the potential of our FDL for image style transfer that relies solely on completely misaligned data. Our code is available at: https://github.com/eezkni/FDL",
        "page": "http://arxiv.org/abs/2402.18192",
        "pdf": "http://arxiv.org/pdf/2402.18192.pdf"
    },
    {
        "title": "ViewDiff: 3D-Consistent Image Generation with Text-To-Image Models",
        "author": "Lukas H\u00f6llein, Alja\u017e Bo\u017ei\u010d, Norman M\u00fcller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhoefer, Matthias Nie\u00dfner",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Breathing Life Into Sketches Using Text-to-Video Priors",
        "author": "Rinon Gal, Yael Vinker, Yuval Alaluf, Amit H. Bermano, Daniel Cohen-Or, Ariel Shamir, Gal Chechik",
        "abstract": "A sketch is one of the most intuitive and versatile tools humans use to convey their ideas visually. An animated sketch opens another dimension to the expression of ideas and is widely used by designers for a variety of purposes. Animating sketches is a laborious process, requiring extensive experience and professional design skills. In this work, we present a method that automatically adds motion to a single-subject sketch (hence, \"breathing life into it\"), merely by providing a text prompt indicating the desired motion. The output is a short animation provided in vector representation, which can be easily edited. Our method does not require extensive training, but instead leverages the motion prior of a large pretrained text-to-video diffusion model using a score-distillation loss to guide the placement of strokes. To promote natural and smooth motion and to better preserve the sketch's appearance, we model the learned motion through two components. The first governs small local deformations and the second controls global affine transformations. Surprisingly, we find that even models that struggle to generate sketch videos on their own can still serve as a useful backbone for animating abstract representations.",
        "page": "http://arxiv.org/abs/2311.13608",
        "pdf": "http://arxiv.org/pdf/2311.13608.pdf"
    },
    {
        "title": "Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping",
        "author": "Alex Costanzino, Pierluigi Zama Ramirez, Giuseppe Lisanti, Luigi Di Stefano",
        "abstract": "The paper explores the industrial multimodal Anomaly Detection (AD) task, which exploits point clouds and RGB images to localize anomalies. We introduce a novel light and fast framework that learns to map features from one modality to the other on nominal samples. At test time, anomalies are detected by pinpointing inconsistencies between observed and mapped features. Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. Moreover, we propose a layer-pruning technique to improve memory and time efficiency with a marginal sacrifice in performance.",
        "page": "http://arxiv.org/abs/2312.04521",
        "pdf": "http://arxiv.org/pdf/2312.04521.pdf"
    },
    {
        "title": "RMT: Retentive Networks Meet Vision Transformers",
        "author": "Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, Ran He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts",
        "author": "Cansu Korkmaz, Ahmet Murat Tekalp, Zafer Dogan",
        "abstract": "Super-resolution (SR) is an ill-posed inverse problem, where the size of the set of feasible solutions that are consistent with a given low-resolution image is very large. Many algorithms have been proposed to find a \"good\" solution among the feasible solutions that strike a balance between fidelity and perceptual quality. Unfortunately, all known methods generate artifacts and hallucinations while trying to reconstruct high-frequency (HF) image details. A fundamental question is: Can a model learn to distinguish genuine image details from artifacts? Although some recent works focused on the differentiation of details and artifacts, this is a very challenging problem and a satisfactory solution is yet to be found. This paper shows that the characterization of genuine HF details versus artifacts can be better learned by training GAN-based SR models using wavelet-domain loss functions compared to RGB-domain or Fourier-space losses. Although wavelet-domain losses have been used in the literature before, they have not been used in the context of the SR task. More specifically, we train the discriminator only on the HF wavelet sub-bands instead of on RGB images and the generator is trained by a fidelity loss over wavelet subbands to make it sensitive to the scale and orientation of structures. Extensive experimental results demonstrate that our model achieves better perception-distortion trade-off according to multiple objective measures and visual evaluations.",
        "page": "http://arxiv.org/abs/2402.19215",
        "pdf": "http://arxiv.org/pdf/2402.19215.pdf"
    },
    {
        "title": "Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint Moment Retrieval and Highlight Detection",
        "author": "Jin Yang, Ping Wei, Huan Li, Ziyang Ren",
        "abstract": "Video moment retrieval and highlight detection are two highly valuable tasks in video understanding, but until recently they have been jointly studied. Although existing studies have made impressive advancement recently, they predominantly follow the data-driven bottom-up paradigm. Such paradigm overlooks task-specific and inter-task effects, resulting in poor model performance. In this paper, we propose a novel task-driven top-down framework TaskWeave for joint moment retrieval and highlight detection. The framework introduces a task-decoupled unit to capture task-specific and common representations. To investigate the interplay between the two tasks, we propose an inter-task feedback mechanism, which transforms the results of one task as guiding masks to assist the other task. Different from existing methods, we present a task-dependent joint loss function to optimize the model. Comprehensive experiments and in-depth ablation studies on QVHighlights, TVSum, and Charades-STA datasets corroborate the effectiveness and flexibility of the proposed framework. Codes are available at https://github.com/EdenGabriel/TaskWeave.",
        "page": "http://arxiv.org/abs/2404.09263",
        "pdf": "http://arxiv.org/pdf/2404.09263.pdf"
    },
    {
        "title": "HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative",
        "author": "CONG MA, Qiao Lei, Chengkai Zhu, Kai Liu, Zelong Kong, Liqing, Xueqi Zhou, Yuheng KAN, Wei Wu",
        "abstract": "Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.",
        "page": "http://arxiv.org/abs/2403.02640",
        "pdf": "http://arxiv.org/pdf/2403.02640.pdf"
    },
    {
        "title": "Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment",
        "author": "Angchi Xu, Wei-Shi Zheng",
        "abstract": "Weakly-supervised action segmentation is a task of learning to partition a long video into several action segments, where training videos are only accompanied by transcripts (ordered list of actions). Most of existing methods need to infer pseudo segmentation for training by serial alignment between all frames and the transcript, which is time-consuming and hard to be parallelized while training. In this work, we aim to escape from this inefficient alignment with massive but redundant frames, and instead to directly localize a few action transitions for pseudo segmentation generation, where a transition refers to the change from an action segment to its next adjacent one in the transcript. As the true transitions are submerged in noisy boundaries due to intra-segment visual variation, we propose a novel Action-Transition-Aware Boundary Alignment (ATBA) framework to efficiently and effectively filter out noisy boundaries and detect transitions. In addition, to boost the semantic learning in the case that noise is inevitably present in the pseudo segmentation, we also introduce video-level losses to utilize the trusted video-level supervision. Extensive experiments show the effectiveness of our approach on both performance and training speed.",
        "page": "http://arxiv.org/abs/2403.19225",
        "pdf": "http://arxiv.org/pdf/2403.19225.pdf"
    },
    {
        "title": "Animating General Image with Large Visual Motion Model",
        "author": "Dengsheng Chen, Xiaoming Wei, Xiaolin Wei",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning",
        "author": "Yuxiang Lu, Suizhi Huang, Yuwen Yang, Shalayiding Sirejiding, Yue Ding, Hongtao Lu",
        "abstract": "Federated Learning (FL) enables joint training across distributed clients using their local data privately. Federated Multi-Task Learning (FMTL) builds on FL to handle multiple tasks, assuming model congruity that identical model architecture is deployed in each client. To relax this assumption and thus extend real-world applicability, we introduce a novel problem setting, Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse task setups. The main challenge of HC-FMTL is the model incongruity issue that invalidates conventional aggregation methods. It also escalates the difficulties in accurate model aggregation to deal with data and task heterogeneity inherent in FMTL. To address these challenges, we propose the FedHCA$^2$ framework, which allows for federated training of personalized models by modeling relationships among heterogeneous clients. Drawing on our theoretical insights into the difference between multi-task and federated optimization, we propose the Hyper Conflict-Averse Aggregation scheme to mitigate conflicts during encoder updates. Additionally, inspired by task interaction in MTL, the Hyper Cross Attention Aggregation scheme uses layer-wise cross attention to enhance decoder interactions while alleviating model incongruity. Moreover, we employ learnable Hyper Aggregation Weights for each client to customize personalized parameter updates. Extensive experiments demonstrate the superior performance of FedHCA$^2$ in various HC-FMTL scenarios compared to representative methods. Our code will be made publicly available.",
        "page": "http://arxiv.org/abs/2311.13250",
        "pdf": "http://arxiv.org/pdf/2311.13250.pdf"
    },
    {
        "title": "Generalizable Novel-View Synthesis using a Stereo Camera",
        "author": "Haechan Lee, Wonjoon Jin, Seung-Hwan Baek, Sunghyun Cho",
        "abstract": "In this paper, we propose the first generalizable view synthesis approach that specifically targets multi-view stereo-camera images. Since recent stereo matching has demonstrated accurate geometry prediction, we introduce stereo matching into novel-view synthesis for high-quality geometry reconstruction. To this end, this paper proposes a novel framework, dubbed StereoNeRF, which integrates stereo matching into a NeRF-based generalizable view synthesis approach. StereoNeRF is equipped with three key components to effectively exploit stereo matching in novel-view synthesis: a stereo feature extractor, a depth-guided plane-sweeping, and a stereo depth loss. Moreover, we propose the StereoNVS dataset, the first multi-view dataset of stereo-camera images, encompassing a wide variety of both real and synthetic scenes. Our experimental results demonstrate that StereoNeRF surpasses previous approaches in generalizable view synthesis.",
        "page": "http://arxiv.org/abs/2404.13541",
        "pdf": "http://arxiv.org/pdf/2404.13541.pdf"
    },
    {
        "title": "Random Entangled Tokens for Adversarially Robust Vision Transformer",
        "author": "Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Identifying Important Group of Pixels using Interactions",
        "author": "Kosuke Sumiyasu, Kazuhiko Kawamoto, Hiroshi Kera",
        "abstract": "To better understand the behavior of image classifiers, it is useful to visualize the contribution of individual pixels to the model prediction. In this study, we propose a method, MoXI ($\\textbf{Mo}$del e$\\textbf{X}$planation by $\\textbf{I}$nteractions), that efficiently and accurately identifies a group of pixels with high prediction confidence. The proposed method employs game-theoretic concepts, Shapley values and interactions, taking into account the effects of individual pixels and the cooperative influence of pixels on model confidence. Theoretical analysis and experiments demonstrate that our method better identifies the pixels that are highly contributing to the model outputs than widely-used visualization by Grad-CAM, Attention rollout, and Shapley value. While prior studies have suffered from the exponential computational cost in the computation of Shapley value and interactions, we show that this can be reduced to quadratic cost for our task. The code is available at https://github.com/KosukeSumiyasu/MoXI.",
        "page": "http://arxiv.org/abs/2401.03785",
        "pdf": "http://arxiv.org/pdf/2401.03785.pdf"
    },
    {
        "title": "Rethinking Multi-domain Generalization with A General Learning Objective",
        "author": "Zhaorui Tan, Xi Yang, Kaizhu Huang",
        "abstract": "Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \\textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \\textbf{optimize partially the objective} and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.",
        "page": "http://arxiv.org/abs/2402.18853",
        "pdf": "http://arxiv.org/pdf/2402.18853.pdf"
    },
    {
        "title": "S2MAE: A Spatial-Spectral Pretraining Foundation Model for Spectral Remote Sensing Data",
        "author": "Xuyang Li, Danfeng Hong, Jocelyn Chanussot",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Instruct-Imagen: Image Generation with Multi-modal Instruction",
        "author": "Hexiang Hu, Kelvin C.K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, William Cohen, Ming-Wei Chang, Xuhui Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PTQ4SAM: Post-Training Quantization for Segment Anything",
        "author": "Chengtao Lv, Hong Chen, Jinyang Guo, Yifu Ding, Xianglong Liu",
        "abstract": "Segment Anything Model (SAM) has achieved impressive performance in many computer vision tasks. However, as a large-scale model, the immense memory and computation costs hinder its practical deployment. In this paper, we propose a post-training quantization (PTQ) framework for Segment Anything Model, namely PTQ4SAM. First, we investigate the inherent bottleneck of SAM quantization attributed to the bimodal distribution in post-Key-Linear activations. We analyze its characteristics from both per-tensor and per-channel perspectives, and propose a Bimodal Integration strategy, which utilizes a mathematically equivalent sign operation to transform the bimodal distribution into a relatively easy-quantized normal distribution offline. Second, SAM encompasses diverse attention mechanisms (i.e., self-attention and two-way cross-attention), resulting in substantial variations in the post-Softmax distributions. Therefore, we introduce an Adaptive Granularity Quantization for Softmax through searching the optimal power-of-two base, which is hardware-friendly. Extensive experimental results across various vision tasks (instance segmentation, semantic segmentation and object detection), datasets and model variants show the superiority of PTQ4SAM. For example, when quantizing SAM-L to 6-bit, we achieve lossless accuracy for instance segmentation, about 0.5\\% drop with theoretical 3.9$\\times$ acceleration. The code is available at \\url{https://github.com/chengtao-lv/PTQ4SAM}.",
        "page": "http://arxiv.org/abs/2405.03144",
        "pdf": "http://arxiv.org/pdf/2405.03144.pdf"
    },
    {
        "title": "Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID",
        "author": "Wentao Tan, Changxing Ding, Jiayu Jiang, Fei Wang, Yibing Zhan, Dapeng Tao",
        "abstract": "Text-to-image person re-identification (ReID) retrieves pedestrian images according to textual descriptions. Manually annotating textual descriptions is time-consuming, restricting the scale of existing datasets and therefore the generalization ability of ReID models. As a result, we study the transferable text-to-image ReID problem, where we train a model on our proposed large-scale database and directly deploy it to various datasets for evaluation. We obtain substantial training data via Multi-modal Large Language Models (MLLMs). Moreover, we identify and address two key challenges in utilizing the obtained textual descriptions. First, an MLLM tends to generate descriptions with similar structures, causing the model to overfit specific sentence patterns. Thus, we propose a novel method that uses MLLMs to caption images according to various templates. These templates are obtained using a multi-turn dialogue with a Large Language Model (LLM). Therefore, we can build a large-scale dataset with diverse textual descriptions. Second, an MLLM may produce incorrect descriptions. Hence, we introduce a novel method that automatically identifies words in a description that do not correspond with the image. This method is based on the similarity between one text and all patch token embeddings in the image. Then, we mask these words with a larger probability in the subsequent training epoch, alleviating the impact of noisy textual descriptions. The experimental results demonstrate that our methods significantly boost the direct transfer text-to-image ReID performance. Benefiting from the pre-trained model weights, we also achieve state-of-the-art performance in the traditional evaluation settings.",
        "page": "http://arxiv.org/abs/2405.04940",
        "pdf": "http://arxiv.org/pdf/2405.04940.pdf"
    },
    {
        "title": "SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field",
        "author": "Lizhe Liu, Bohua Wang, Hongwei Xie, Daqi Liu, Li Liu, Kuiyuan Yang, Bing Wang, Zhiqiang Tian",
        "abstract": "Vision-centric 3D environment understanding is both vital and challenging for autonomous driving systems. Recently, object-free methods have attracted considerable attention. Such methods perceive the world by predicting the semantics of discrete voxel grids but fail to construct continuous and accurate obstacle surfaces. To this end, in this paper, we propose SurroundSDF to implicitly predict the signed distance field (SDF) and semantic field for the continuous perception from surround images. Specifically, we introduce a query-based approach and utilize SDF constrained by the Eikonal formulation to accurately describe the surfaces of obstacles. Furthermore, considering the absence of precise SDF ground truth, we propose a novel weakly supervised paradigm for SDF, referred to as the Sandwich Eikonal formulation, which emphasizes applying correct and dense constraints on both sides of the surface, thereby enhancing the perceptual accuracy of the surface. Experiments suggest that our method achieves SOTA for both occupancy prediction and 3D scene reconstruction tasks on the nuScenes dataset.",
        "page": "http://arxiv.org/abs/2403.14366",
        "pdf": "http://arxiv.org/pdf/2403.14366.pdf"
    },
    {
        "title": "PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization",
        "author": "Zining Chen, Weiqiu Wang, Zhicheng Zhao, Fei Su, Aidong Men, Hongying Meng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SocialCircle: Learning the Angle-based Social Interaction Representation for Pedestrian Trajectory Prediction",
        "author": "Conghao Wong, Beihao Xia, Ziqian Zou, Yulong Wang, Xinge You",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Equi-angular Representations for Online Continual Learning",
        "author": "Minhyuk Seo, Hyunseo Koh, Wonje Jeung, Minjae Lee, San Kim, Hankook Lee, Sungjun Cho, Sungik Choi, Hyunwoo Kim, Jonghyun Choi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PrPSeg: Universal Proposition Learning for Panoramic Renal Pathology Segmentation",
        "author": "Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Jialin Yue, Juming Xiong, Lining yu, Yifei Wu, Mengmeng Yin, Yu Wang, Shilin Zhao, Yucheng Tang, Haichun Yang, Yuankai Huo",
        "abstract": "Understanding the anatomy of renal pathology is crucial for advancing disease diagnostics, treatment evaluation, and clinical research. The complex kidney system comprises various components across multiple levels, including regions (cortex, medulla), functional units (glomeruli, tubules), and cells (podocytes, mesangial cells in glomerulus). Prior studies have predominantly overlooked the intricate spatial interrelations among objects from clinical knowledge. In this research, we introduce a novel universal proposition learning approach, called panoramic renal pathology segmentation (PrPSeg), designed to segment comprehensively panoramic structures within kidney by integrating extensive knowledge of kidney anatomy. In this paper, we propose (1) the design of a comprehensive universal proposition matrix for renal pathology, facilitating the incorporation of classification and spatial relationships into the segmentation process; (2) a token-based dynamic head single network architecture, with the improvement of the partial label image segmentation and capability for future data enlargement; and (3) an anatomy loss function, quantifying the inter-object relationships across the kidney.",
        "page": "http://arxiv.org/abs/2402.19286",
        "pdf": "http://arxiv.org/pdf/2402.19286.pdf"
    },
    {
        "title": "Discriminative Sample-Guided and Parameter-Efficient Feature Space Adaptation for Cross-Domain Few-Shot Learning",
        "author": "Rashindrie Perera, Saman Halgamuge",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CLOAF: CoLlisiOn-Aware Human Flow",
        "author": "Andrey Davydov, Martin Engilberge, Mathieu Salzmann, Pascal Fua",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding",
        "author": "Hoang-Quan Nguyen, Thanh-Dat Truong, Xuan-Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu",
        "abstract": "In precision agriculture, the detection and recognition of insects play an essential role in the ability of crops to grow healthy and produce a high-quality yield. The current machine vision model requires a large volume of data to achieve high performance. However, there are approximately 5.5 million different insect species in the world. None of the existing insect datasets can cover even a fraction of them due to varying geographic locations and acquisition costs. In this paper, we introduce a novel \"Insect-1M\" dataset, a game-changing resource poised to revolutionize insect-related foundation model training. Covering a vast spectrum of insect species, our dataset, including 1 million images with dense identification labels of taxonomy hierarchy and insect descriptions, offers a panoramic view of entomology, enabling foundation models to comprehend visual and semantic information about insects like never before. Then, to efficiently establish an Insect Foundation Model, we develop a micro-feature self-supervised learning method with a Patch-wise Relevant Attention mechanism capable of discerning the subtle differences among insect images. In addition, we introduce Description Consistency loss to improve micro-feature modeling via insect descriptions. Through our experiments, we illustrate the effectiveness of our proposed approach in insect modeling and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks. Our Insect Foundation Model and Dataset promise to empower the next generation of insect-related vision models, bringing them closer to the ultimate goal of precision agriculture.",
        "page": "http://arxiv.org/abs/2311.15206",
        "pdf": "http://arxiv.org/pdf/2311.15206.pdf"
    },
    {
        "title": "Making Vision Transformers Truly Shift-Equivariant",
        "author": "Renan A. Rojas-Gomez, Teck-Yian Lim, Minh Do, Raymond A. Yeh",
        "abstract": "For computer vision, Vision Transformers (ViTs) have become one of the go-to deep net architectures. Despite being inspired by Convolutional Neural Networks (CNNs), ViTs' output remains sensitive to small spatial shifts in the input, i.e., not shift invariant. To address this shortcoming, we introduce novel data-adaptive designs for each of the modules in ViTs, such as tokenization, self-attention, patch merging, and positional encoding. With our proposed modules, we achieve true shift-equivariance on four well-established ViTs, namely, Swin, SwinV2, CvT, and MViTv2. Empirically, we evaluate the proposed adaptive models on image classification and semantic segmentation tasks. These models achieve competitive performance across three different datasets while maintaining 100% shift consistency.",
        "page": "http://arxiv.org/abs/2305.16316",
        "pdf": "http://arxiv.org/pdf/2305.16316.pdf"
    },
    {
        "title": "BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP",
        "author": "Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, Wei Liu",
        "abstract": "Contrastive Vision-Language Pre-training, known as CLIP, has shown promising effectiveness in addressing downstream image recognition tasks. However, recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks, one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor, existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model, which makes them inapplicable to data-limited scenarios. In this work, motivated by the recent success of learnable prompts, we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator, such that the trigger can change text features via trigger-aware prompts, resulting in a powerful and generalizable attack. Extensive experiments conducted on 11 datasets verify that the clean accuracy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to unseen classes, and shows a strong generalization capability under cross-dataset and cross-domain settings.",
        "page": "http://arxiv.org/abs/2311.16194",
        "pdf": "http://arxiv.org/pdf/2311.16194.pdf"
    },
    {
        "title": "LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching",
        "author": "Yixun Liang, Xin Yang, Jiantao Lin, Haodong LI, Xiaogang Xu, Ying-Cong Chen",
        "abstract": "The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency.",
        "page": "http://arxiv.org/abs/2311.11284",
        "pdf": "http://arxiv.org/pdf/2311.11284.pdf"
    },
    {
        "title": "Facial Identity Anonymization via Intrinsic and Extrinsic Attention Distraction",
        "author": "Zhenzhong Kuang, Xiaochen Yang, Yingjie Shen, Chao Hu, Jun Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generating Human Motion in 3D Scenes from Text Descriptions",
        "author": "Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, Xiaowei Zhou",
        "abstract": "Generating human motions from textual descriptions has gained growing research interest due to its wide range of applications. However, only a few works consider human-scene interactions together with text conditions, which is crucial for visual and physical realism. This paper focuses on the task of generating human motions in 3D indoor scenes given text descriptions of the human-scene interactions. This task presents challenges due to the multi-modality nature of text, scene, and motion, as well as the need for spatial reasoning. To address these challenges, we propose a new approach that decomposes the complex problem into two more manageable sub-problems: (1) language grounding of the target object and (2) object-centric motion generation. For language grounding of the target object, we leverage the power of large language models. For motion generation, we design an object-centric scene representation for the generative model to focus on the target object, thereby reducing the scene complexity and facilitating the modeling of the relationship between human motions and the object. Experiments demonstrate the better motion quality of our approach compared to baselines and validate our design choices.",
        "page": "http://arxiv.org/abs/2405.07784",
        "pdf": "http://arxiv.org/pdf/2405.07784.pdf"
    },
    {
        "title": "Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining",
        "author": "Xiang Chen, Jinshan Pan, Jiangxin Dong",
        "abstract": "How to effectively explore multi-scale representations of rain streaks is important for image deraining. In contrast to existing Transformer-based methods that depend mostly on single-scale rain appearance, we develop an end-to-end multi-scale Transformer that leverages the potentially useful features in various scales to facilitate high-quality image reconstruction. To better explore the common degradation representations from spatially-varying rain streaks, we incorporate intra-scale implicit neural representations based on pixel coordinates with the degraded inputs in a closed-loop design, enabling the learned features to facilitate rain removal and improve the robustness of the model in complex scenarios. To ensure richer collaborative representation from different scales, we embed a simple yet effective inter-scale bidirectional feedback operation into our multi-scale Transformer by performing coarse-to-fine and fine-to-coarse information communication. Extensive experiments demonstrate that our approach, named as NeRD-Rain, performs favorably against the state-of-the-art ones on both synthetic and real-world benchmark datasets. The source code and trained models are available at https://github.com/cschenxiang/NeRD-Rain.",
        "page": "http://arxiv.org/abs/2404.01547",
        "pdf": "http://arxiv.org/pdf/2404.01547.pdf"
    },
    {
        "title": "Panacea: Panoramic and Controllable Video Generation for Autonomous Driving",
        "author": "Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, Xiangyu Zhang",
        "abstract": "The field of autonomous driving increasingly demands high-quality annotated training data. In this paper, we propose Panacea, an innovative approach to generate panoramic and controllable videos in driving scenarios, capable of yielding an unlimited numbers of diverse, annotated samples pivotal for autonomous driving advancements. Panacea addresses two critical challenges: 'Consistency' and 'Controllability.' Consistency ensures temporal and cross-view coherence, while Controllability ensures the alignment of generated content with corresponding annotations. Our approach integrates a novel 4D attention and a two-stage generation pipeline to maintain coherence, supplemented by the ControlNet framework for meticulous control by the Bird's-Eye-View (BEV) layouts. Extensive qualitative and quantitative evaluations of Panacea on the nuScenes dataset prove its effectiveness in generating high-quality multi-view driving-scene videos. This work notably propels the field of autonomous driving by effectively augmenting the training dataset used for advanced BEV perception techniques.",
        "page": "http://arxiv.org/abs/2311.16813",
        "pdf": "http://arxiv.org/pdf/2311.16813.pdf"
    },
    {
        "title": "SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction",
        "author": "Zechuan Zhang, Zongxin Yang, Yi Yang",
        "abstract": "Creating high-quality 3D models of clothed humans from single images for real-world applications is crucial. Despite recent advancements, accurately reconstructing humans in complex poses or with loose clothing from in-the-wild images, along with predicting textures for unseen areas, remains a significant challenge. A key limitation of previous methods is their insufficient prior guidance in transitioning from 2D to 3D and in texture prediction. In response, we introduce SIFU (Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction), a novel approach combining a Side-view Decoupling Transformer with a 3D Consistent Texture Refinement pipeline.SIFU employs a cross-attention mechanism within the transformer, using SMPL-X normals as queries to effectively decouple side-view features in the process of mapping 2D features to 3D. This method not only improves the precision of the 3D models but also their robustness, especially when SMPL-X estimates are not perfect. Our texture refinement process leverages text-to-image diffusion-based prior to generate realistic and consistent textures for invisible views. Through extensive experiments, SIFU surpasses SOTA methods in both geometry and texture reconstruction, showcasing enhanced robustness in complex scenarios and achieving an unprecedented Chamfer and P2S measurement. Our approach extends to practical applications such as 3D printing and scene building, demonstrating its broad utility in real-world scenarios. Project page https://river-zhang.github.io/SIFU-projectpage/ .",
        "page": "http://arxiv.org/abs/2312.06704",
        "pdf": "http://arxiv.org/pdf/2312.06704.pdf"
    },
    {
        "title": "One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications",
        "author": "Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He, Hui Xue, Jungong Han, Guiguang Ding",
        "abstract": "The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alternation towards erosion: Parameter drift during target elimination causes alternations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer inability & deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimensional adapters to erase multiple concepts from most DMs at once across versatile erasing applications. The concept-SemiPermeable structure is injected as a Membrane (SPM) into any DM to learn targeted erasing, and meantime the alteration and erosion phenomenon is effectively mitigated via a novel Latent Anchoring fine-tuning strategy. Once obtained, SPMs can be flexibly combined and plug-and-play for other DMs without specific re-tuning, enabling timely and efficient adaptation to diverse scenarios. During generation, our Facilitated Transport mechanism dynamically regulates the permeability of each SPM to respond to different input prompts, further minimizing the impact on other concepts. Quantitative and qualitative results across ~40 concepts, 7 DMs and 4 erasing applications have demonstrated the superior erasing of SPM. Our code and pre-tuned SPMs are available on the project page https://lyumengyao.github.io/projects/spm.",
        "page": "http://arxiv.org/abs/2312.16145",
        "pdf": "http://arxiv.org/pdf/2312.16145.pdf"
    },
    {
        "title": "Dynamic Inertial Poser (DynaIP): Part-Based Motion Dynamics Learning for Enhanced Human Pose Estimation with Sparse Inertial Sensors",
        "author": "Yu Zhang, Songpengcheng Xia, Lei Chu, Jiarui Yang, Qi Wu, Ling Pei",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes",
        "author": "Diandian Guo, Deng-Ping Fan, Tongyu Lu, Christos Sakaridis, Luc Van Gool",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "USE: Universal Segment Embeddings for Open-Vocabulary Image Segmentation",
        "author": "Xiaoqi Wang, Wenbin He, Xiwei Xuan, Clint Sebastian, Jorge Piazentin Ono, Xin Li, Sima Behpour, Thang Doan, Liang Gou, Shen, Liu Ren",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SeaBird: Segmentation in Bird\u2019s View with Dice Loss Improves Monocular 3D Detection of Large Objects",
        "author": "Abhinav Kumar, Yuliang Guo, Xinyu Huang, Liu Ren, Xiaoming Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Overload: Latency Attacks on Object Detection for Edge Devices",
        "author": "Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-Rung Lee",
        "abstract": "Nowadays, the deployment of deep learning-based applications is an essential task owing to the increasing demands on intelligent services. In this paper, we investigate latency attacks on deep learning applications. Unlike common adversarial attacks for misclassification, the goal of latency attacks is to increase the inference time, which may stop applications from responding to the requests within a reasonable time. This kind of attack is ubiquitous for various applications, and we use object detection to demonstrate how such kind of attacks work. We also design a framework named Overload to generate latency attacks at scale. Our method is based on a newly formulated optimization problem and a novel technique, called spatial attention. This attack serves to escalate the required computing costs during the inference time, consequently leading to an extended inference time for object detection. It presents a significant threat, especially to systems with limited computing resources. We conducted experiments using YOLOv5 models on Nvidia NX. Compared to existing methods, our method is simpler and more effective. The experimental results show that with latency attacks, the inference time of a single image can be increased ten times longer in reference to the normal setting. Moreover, our findings pose a potential new threat to all object detection tasks requiring non-maximum suppression (NMS), as our attack is NMS-agnostic.",
        "page": "http://arxiv.org/abs/2304.05370",
        "pdf": "http://arxiv.org/pdf/2304.05370.pdf"
    },
    {
        "title": "NetTrack: Tracking Highly Dynamic Objects with a Net",
        "author": "Guangze Zheng, Shijie Lin, Haobo Zuo, Changhong Fu, Jia Pan",
        "abstract": "The complex dynamicity of open-world objects presents non-negligible challenges for multi-object tracking (MOT), often manifested as severe deformations, fast motion, and occlusions. Most methods that solely depend on coarse-grained object cues, such as boxes and the overall appearance of the object, are susceptible to degradation due to distorted internal relationships of dynamic objects. To address this problem, this work proposes NetTrack, an efficient, generic, and affordable tracking framework to introduce fine-grained learning that is robust to dynamicity. Specifically, NetTrack constructs a dynamicity-aware association with a fine-grained Net, leveraging point-level visual cues. Correspondingly, a fine-grained sampler and matching method have been incorporated. Furthermore, NetTrack learns object-text correspondence for fine-grained localization. To evaluate MOT in extremely dynamic open-world scenarios, a bird flock tracking (BFT) dataset is constructed, which exhibits high dynamicity with diverse species and open-world scenarios. Comprehensive evaluation on BFT validates the effectiveness of fine-grained learning on object dynamicity, and thorough transfer experiments on challenging open-world benchmarks, i.e., TAO, TAO-OW, AnimalTrack, and GMOT-40, validate the strong generalization ability of NetTrack even without finetuning. Project page: https://george-zhuang.github.io/nettrack/.",
        "page": "http://arxiv.org/abs/2403.11186",
        "pdf": "http://arxiv.org/pdf/2403.11186.pdf"
    },
    {
        "title": "GOAT-Bench: A Benchmark for Multi-modal Lifelong Navigation",
        "author": "Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theo Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi",
        "abstract": "The Embodied AI community has made significant strides in visual navigation tasks, exploring targets from 3D coordinates, objects, language descriptions, and images. However, these navigation models often handle only a single input modality as the target. With the progress achieved so far, it is time to move towards universal navigation models capable of handling various goal types, enabling more effective user interaction with robots. To facilitate this goal, we propose GOAT-Bench, a benchmark for the universal navigation task referred to as GO to AnyThing (GOAT). In this task, the agent is directed to navigate to a sequence of targets specified by the category name, language description, or image in an open-vocabulary fashion. We benchmark monolithic RL and modular methods on the GOAT task, analyzing their performance across modalities, the role of explicit and implicit scene memories, their robustness to noise in goal specifications, and the impact of memory in lifelong scenarios.",
        "page": "http://arxiv.org/abs/2404.06609",
        "pdf": "http://arxiv.org/pdf/2404.06609.pdf"
    },
    {
        "title": "Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion",
        "author": "Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, Mar Gonzalez-Franco",
        "abstract": "Producing quality segmentation masks for images is a fundamental problem in computer vision. Recent research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU. The project page is at \\url{https://sites.google.com/view/diffseg/home}.",
        "page": "http://arxiv.org/abs/2308.12469",
        "pdf": "http://arxiv.org/pdf/2308.12469.pdf"
    },
    {
        "title": "Deformable One-shot Face Stylization via DINO Semantic Guidance",
        "author": "Yang Zhou, Zichong Chen, Hui Huang",
        "abstract": "This paper addresses the complex issue of one-shot face stylization, focusing on the simultaneous consideration of appearance and structure, where previous methods have fallen short. We explore deformation-aware face stylization that diverges from traditional single-image style reference, opting for a real-style image pair instead. The cornerstone of our method is the utilization of a self-supervised vision transformer, specifically DINO-ViT, to establish a robust and consistent facial structure representation across both real and style domains. Our stylization process begins by adapting the StyleGAN generator to be deformation-aware through the integration of spatial transformers (STN). We then introduce two innovative constraints for generator fine-tuning under the guidance of DINO semantics: i) a directional deformation loss that regulates directional vectors in DINO space, and ii) a relative structural consistency constraint based on DINO token self-similarities, ensuring diverse generation. Additionally, style-mixing is employed to align the color generation with the reference, minimizing inconsistent correspondences. This framework delivers enhanced deformability for general one-shot face stylization, achieving notable efficiency with a fine-tuning duration of approximately 10 minutes. Extensive qualitative and quantitative comparisons demonstrate our superiority over state-of-the-art one-shot face stylization methods. Code is available at https://github.com/zichongc/DoesFS",
        "page": "http://arxiv.org/abs/2403.00459",
        "pdf": "http://arxiv.org/pdf/2403.00459.pdf"
    },
    {
        "title": "LoCoNet: Long-Short Context Network for Active Speaker Detection",
        "author": "Xizi Wang, Feng Cheng, Gedas Bertasius",
        "abstract": "Active Speaker Detection (ASD) aims to identify who is speaking in each frame of a video. ASD reasons from audio and visual information from two contexts: long-term intra-speaker context and short-term inter-speaker context. Long-term intra-speaker context models the temporal dependencies of the same speaker, while short-term inter-speaker context models the interactions of speakers in the same scene. These two contexts are complementary to each other and can help infer the active speaker. Motivated by these observations, we propose LoCoNet, a simple yet effective Long-Short Context Network that models the long-term intra-speaker context and short-term inter-speaker context. We use self-attention to model long-term intra-speaker context due to its effectiveness in modeling long-range dependencies, and convolutional blocks that capture local patterns to model short-term inter-speaker context. Extensive experiments show that LoCoNet achieves state-of-the-art performance on multiple datasets, achieving an mAP of 95.2%(+1.1%) on AVA-ActiveSpeaker, 68.1%(+22%) on Columbia dataset, 97.2%(+2.8%) on Talkies dataset and 59.7%(+8.0%) on Ego4D dataset. Moreover, in challenging cases where multiple speakers are present, or face of active speaker is much smaller than other faces in the same scene, LoCoNet outperforms previous state-of-the-art methods by 3.4% on the AVA-ActiveSpeaker dataset. The code will be released at https://github.com/SJTUwxz/LoCoNet_ASD.",
        "page": "http://arxiv.org/abs/2301.08237",
        "pdf": "http://arxiv.org/pdf/2301.08237.pdf"
    },
    {
        "title": "MaGGIe: Masked Guided Gradual Human Instance Matting",
        "author": "Chuong Huynh, Seoung Wug Oh, Abhinav Shrivastava, Joon-Young Lee",
        "abstract": "Human matting is a foundation task in image and video processing, where human foreground pixels are extracted from the input. Prior works either improve the accuracy by additional guidance or improve the temporal consistency of a single instance across frames. We propose a new framework MaGGIe, Masked Guided Gradual Human Instance Matting, which predicts alpha mattes progressively for each human instances while maintaining the computational cost, precision, and consistency. Our method leverages modern architectures, including transformer attention and sparse convolution, to output all instance mattes simultaneously without exploding memory and latency. Although keeping constant inference costs in the multiple-instance scenario, our framework achieves robust and versatile performance on our proposed synthesized benchmarks. With the higher quality image and video matting benchmarks, the novel multi-instance synthesis approach from publicly available sources is introduced to increase the generalization of models in real-world scenarios.",
        "page": "http://arxiv.org/abs/2404.16035",
        "pdf": "http://arxiv.org/pdf/2404.16035.pdf"
    },
    {
        "title": "EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams",
        "author": "Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik",
        "abstract": "Monocular egocentric 3D human motion capture is a challenging and actively researched problem. Existing methods use synchronously operating visual sensors (e.g. RGB cameras) and often fail under low lighting and fast motions, which can be restricting in many applications involving head-mounted devices. In response to the existing limitations, this paper 1) introduces a new problem, i.e., 3D human motion capture from an egocentric monocular event camera with a fisheye lens, and 2) proposes the first approach to it called EventEgo3D (EE3D). Event streams have high temporal resolution and provide reliable cues for 3D human motion capture under high-speed human motions and rapidly changing illumination. The proposed EE3D framework is specifically tailored for learning with event streams in the LNES representation, enabling high 3D reconstruction accuracy. We also design a prototype of a mobile head-mounted device with an event camera and record a real dataset with event observations and the ground-truth 3D human poses (in addition to the synthetic dataset). Our EE3D demonstrates robustness and superior 3D accuracy compared to existing solutions across various challenging experiments while supporting real-time 3D pose update rates of 140Hz.",
        "page": "http://arxiv.org/abs/2404.08640",
        "pdf": "http://arxiv.org/pdf/2404.08640.pdf"
    },
    {
        "title": "SonicVisionLM: Playing Sound with Vision Language Models",
        "author": "Zhifeng Xie, Shengye Yu, Qile He, Mengtian Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Beyond Textual Constraints: Learning Novel Diffusion Conditions with Fewer Examples",
        "author": "Yuyang Yu, Bangzhen Liu, Chenxi Zheng, Xuemiao Xu, Huaidong Zhang, Shengfeng He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Exploring Regional Clues in CLIP for Zero-Shot Semantic Segmentation",
        "author": "Yi Zhang, Meng-Hao Guo, Miao Wang, Shi-Min Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EasyDrag: Efficient Point-based Manipulation on Diffusion Models",
        "author": "Xingzhong Hou, Boxiao Liu, Yi Zhang, Jihao Liu, Yu Liu, Haihang You",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MCNet: Rethinking the Core Ingredients for Accurate and Efficient Homography Estimation",
        "author": "Haokai Zhu, Si-Yuan Cao, Jianxin Hu, Sitong Zuo, Beinan Yu, Jiacheng Ying, Junwei Li, Hui-Liang Shen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LEAD: Learning Decomposition for Source-free Universal Domain Adaptation",
        "author": "Sanqing Qu, Tianpei Zou, Lianghua He, Florian R\u00f6hrbein, Alois Knoll, Guang Chen, Changjun Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection",
        "author": "Chengjie Wang, wenbing zhu, Bin-Bin Gao, Zhenye Gan, Jiangning Zhang, Zhihao Gu, Bruce Qian, Mingang Chen, Lizhuang Ma",
        "abstract": "Industrial anomaly detection (IAD) has garnered significant attention and experienced rapid development. However, the recent development of IAD approach has encountered certain difficulties due to dataset limitations. On the one hand, most of the state-of-the-art methods have achieved saturation (over 99% in AUROC) on mainstream datasets such as MVTec, and the differences of methods cannot be well distinguished, leading to a significant gap between public datasets and actual application scenarios. On the other hand, the research on various new practical anomaly detection settings is limited by the scale of the dataset, posing a risk of overfitting in evaluation results. Therefore, we propose a large-scale, Real-world, and multi-view Industrial Anomaly Detection dataset, named Real-IAD, which contains 150K high-resolution images of 30 different objects, an order of magnitude larger than existing datasets. It has a larger range of defect area and ratio proportions, making it more challenging than previous datasets. To make the dataset closer to real application scenarios, we adopted a multi-view shooting method and proposed sample-level evaluation metrics. In addition, beyond the general unsupervised anomaly detection setting, we propose a new setting for Fully Unsupervised Industrial Anomaly Detection (FUIAD) based on the observation that the yield rate in industrial production is usually greater than 60%, which has more practical application value. Finally, we report the results of popular IAD methods on the Real-IAD dataset, providing a highly challenging benchmark to promote the development of the IAD field.",
        "page": "http://arxiv.org/abs/2403.12580",
        "pdf": "http://arxiv.org/pdf/2403.12580.pdf"
    },
    {
        "title": "Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing",
        "author": "Dongyoung Kim, Jinwoo Kim, Junsang Yu, Seon Joo Kim",
        "abstract": "White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene. Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene's actual lighting conditions, including the number and color of light sources. This often results in unnatural outcomes lacking in overall consistency. To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants. This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map. Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively. Our method achieves the state-of-the-art performance on both single- and multi-illuminant WB benchmarks, and also offers additional information such as the number of illuminants in the scene and their chromaticity. This capability allows for illumination editing, an application not feasible with prior methods.",
        "page": "http://arxiv.org/abs/2402.18277",
        "pdf": "http://arxiv.org/pdf/2402.18277.pdf"
    },
    {
        "title": "Fully Geometric Panoramic Localization",
        "author": "Junho Kim, Jiwon Jeong, Young Min Kim",
        "abstract": "We introduce a lightweight and accurate localization method that only utilizes the geometry of 2D-3D lines. Given a pre-captured 3D map, our approach localizes a panorama image, taking advantage of the holistic 360 view. The system mitigates potential privacy breaches or domain discrepancies by avoiding trained or hand-crafted visual descriptors. However, as lines alone can be ambiguous, we express distinctive yet compact spatial contexts from relationships between lines, namely the dominant directions of parallel lines and the intersection between non-parallel lines. The resulting representations are efficient in processing time and memory compared to conventional visual descriptor-based methods. Given the groups of dominant line directions and their intersections, we accelerate the search process to test thousands of pose candidates in less than a millisecond without sacrificing accuracy. We empirically show that the proposed 2D-3D matching can localize panoramas for challenging scenes with similar structures, dramatic domain shifts or illumination changes. Our fully geometric approach does not involve extensive parameter tuning or neural network training, making it a practical algorithm that can be readily deployed in the real world. Project page including the code is available through this link: https://82magnolia.github.io/fgpl/.",
        "page": "http://arxiv.org/abs/2403.19904",
        "pdf": "http://arxiv.org/pdf/2403.19904.pdf"
    },
    {
        "title": "Adversarial Distillation Based on Slack Matching and Attribution Region Alignment",
        "author": "Shenglin Yin, Zhen Xiao, Mingxuan Song, Jieyi Long",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Referring Expression Counting",
        "author": "Siyang Dai, Jun Liu, Ngai-Man Cheung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces",
        "author": "Haithem Turki, Vasu Agrawal, Samuel Rota Bul\u00f2, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollhoefer, Christian Richardt",
        "abstract": "Neural radiance fields provide state-of-the-art view synthesis quality but tend to be slow to render. One reason is that they make use of volume rendering, thus requiring many samples (and model queries) per ray at render time. Although this representation is flexible and easy to optimize, most real-world objects can be modeled more efficiently with surfaces instead of volumes, requiring far fewer samples per ray. This observation has spurred considerable progress in surface representations such as signed distance functions, but these may struggle to model semi-opaque and thin structures. We propose a method, HybridNeRF, that leverages the strengths of both representations by rendering most objects as surfaces while modeling the (typically) small fraction of challenging regions volumetrically. We evaluate HybridNeRF against the challenging Eyeful Tower dataset along with other commonly used view synthesis datasets. When comparing to state-of-the-art baselines, including recent rasterization-based approaches, we improve error rates by 15-30% while achieving real-time framerates (at least 36 FPS) for virtual-reality resolutions (2Kx2K).",
        "page": "http://arxiv.org/abs/2312.03160",
        "pdf": "http://arxiv.org/pdf/2312.03160.pdf"
    },
    {
        "title": "Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation",
        "author": "Wenxiao Deng, Wenbin Li, Tianyu Ding, Lei Wang, Hongguang Zhang, Kuihua Huang, Jing Huo, Yang Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection",
        "author": "Mikhail Kennerley, Jian-Gang Wang, Bharadwaj Veeravalli, Robby T. Tan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dual-View Visual Contextualization for Web Navigation",
        "author": "Jihyung Kil, Chan Hee Song, Boyuan Zheng, Xiang Deng, Yu Su, Wei-Lun Chao",
        "abstract": "Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their \"dual views\" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We validate our method on the recently released Mind2Web dataset, which features diverse navigation domains and tasks on real-world websites. Our method consistently outperforms the baseline in all the scenarios, including cross-task, cross-website, and cross-domain ones.",
        "page": "http://arxiv.org/abs/2402.04476",
        "pdf": "http://arxiv.org/pdf/2402.04476.pdf"
    },
    {
        "title": "Precise Image Editing via Recognition and Generation Tasks",
        "author": "Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, Yaniv Taigman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives",
        "author": "Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zachary Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian David Forigua Diaz, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Fu Xinzhu, Ryosuke Furuta, Cristina Gonz\u00e1lez, Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo ARBELAEZ, Gedas Bertasius, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C.V. Jawahar, Richard Newcombe, Hyun Soo Park, James Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Video Prediction by Modeling Videos as Continuous Multi-Dimensional Processes",
        "author": "Gaurav Shrivastava, Abhinav Shrivastava",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On",
        "author": "Xu Yang, Changxing Ding, Zhibin Hong, Junhao Huang, Jin Tao, Xiangmin Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs",
        "author": "Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, Bernhard Sch\u00f6lkopf",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Open Vocabulary Semantic Scene Sketch Understanding",
        "author": "Ahmed Bourouis, Judith Fan, Yulia Gryaditskaya",
        "abstract": "We study the underexplored but fundamental vision problem of machine understanding of abstract freehand scene sketches. We introduce a sketch encoder that results in semantically-aware feature space, which we evaluate by testing its performance on a semantic sketch segmentation task. To train our model we rely only on the availability of bitmap sketches with their brief captions and do not require any pixel-level annotations. To obtain generalization to a large set of sketches and categories, we build on a vision transformer encoder pretrained with the CLIP model. We freeze the text encoder and perform visual-prompt tuning of the visual encoder branch while introducing a set of critical modifications. Firstly, we augment the classical key-query (k-q) self-attention blocks with value-value (v-v) self-attention blocks. Central to our model is a two-level hierarchical network design that enables efficient semantic disentanglement: The first level ensures holistic scene sketch encoding, and the second level focuses on individual categories. We, then, in the second level of the hierarchy, introduce a cross-attention between textual and visual branches. Our method outperforms zero-shot CLIP pixel accuracy of segmentation results by 37 points, reaching an accuracy of $85.5\\%$ on the FS-COCO sketch dataset. Finally, we conduct a user study that allows us to identify further improvements needed over our method to reconcile machine and human understanding of scene sketches.",
        "page": "http://arxiv.org/abs/2312.12463",
        "pdf": "http://arxiv.org/pdf/2312.12463.pdf"
    },
    {
        "title": "DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields",
        "author": "Cheng-You Lu, Peisen Zhou, Angela Xing, Chandradeep Pokhariya, Arnab Dey, Ishaan Shah, Rugved Mavidipalli, Dylan Hu, Andrew Comport, Kefan Chen, Srinath Sridhar",
        "abstract": "Advances in neural fields are enabling high-fidelity capture of the shape and appearance of dynamic 3D scenes. However, their capabilities lag behind those offered by conventional representations such as 2D videos because of algorithmic challenges and the lack of large-scale multi-view real-world datasets. We address the dataset limitation with DiVa-360, a real-world 360 dynamic visual dataset that contains synchronized high-resolution and long-duration multi-view video sequences of table-scale scenes captured using a customized low-cost system with 53 cameras. It contains 21 object-centric sequences categorized by different motion types, 25 intricate hand-object interaction sequences, and 8 long-duration sequences for a total of 17.4 M image frames. In addition, we provide foreground-background segmentation masks, synchronized audio, and text descriptions. We benchmark the state-of-the-art dynamic neural field methods on DiVa-360 and provide insights about existing methods and future challenges on long-duration neural field capture.",
        "page": "http://arxiv.org/abs/2307.16897",
        "pdf": "http://arxiv.org/pdf/2307.16897.pdf"
    },
    {
        "title": "MANUS: Markerless Grasp Capture using Articulated 3D Gaussians",
        "author": "Chandradeep Pokhariya, Ishaan Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar",
        "abstract": "Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality. However, this challenging problem requires accurate modeling of the contact between hands and objects. To capture grasps, existing methods use skeletons, meshes, or parametric models that does not represent hand shape accurately resulting in inaccurate contacts. We present MANUS, a method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians. We build a novel articulated 3D Gaussians representation that extends 3D Gaussian splatting for high-fidelity representation of articulating hands. Since our representation uses Gaussian primitives, it enables us to efficiently and accurately estimate contacts between the hand and the object. For the most accurate results, our method requires tens of camera views that current datasets do not provide. We therefore build MANUS-Grasps, a new dataset that contains hand-object grasps viewed from 50+ cameras across 30+ scenes, 3 subjects, and comprising over 7M frames. In addition to extensive qualitative results, we also show that our method outperforms others on a quantitative contact evaluation method that uses paint transfer from the object to the hand.",
        "page": "http://arxiv.org/abs/2312.02137",
        "pdf": "http://arxiv.org/pdf/2312.02137.pdf"
    },
    {
        "title": "TEA: Test-time Energy Adaptation",
        "author": "Yige Yuan, Bingbing Xu, Liang Hou, Fei Sun, Huawei Shen, Xueqi Cheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Instance-based Max-margin for Practical Few-shot Recognition",
        "author": "Minghao Fu, Ke Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge",
        "author": "Andong Wang, Bo Wu, Sunli Chen, Zhenfang Chen, Haotian Guan, Wei-Ning Lee, Li Erran Li, Chuang Gan",
        "abstract": "Learning commonsense reasoning from visual contexts and scenes in real-world is a crucial step toward advanced artificial intelligence. However, existing video reasoning benchmarks are still inadequate since they were mainly designed for factual or situated reasoning and rarely involve broader knowledge in the real world. Our work aims to delve deeper into reasoning evaluations, specifically within dynamic, open-world, and structured context knowledge. We propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K situations with instance-level annotations depicted in the videos. The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving. To create such a dataset, we propose an automatic and scalable generation method to generate question-answer pairs, knowledge graphs, and rationales by instructing the combinations of LLMs and MLLMs. Concretely, we first extract observable situated entities, relations, and processes from videos for situated knowledge and then extend to open-world knowledge beyond the visible content. The task generation is facilitated through multiple dialogues as iterations and subsequently corrected and refined by our designed self-promptings and demonstrations. With a corpus of both explicit situated facts and implicit commonsense, we generate associated question-answer pairs and reasoning processes, finally followed by manual reviews for quality assurance. We evaluated recent mainstream large vision-language models on the benchmark and found several insightful conclusions. For more information, please refer to our benchmark at www.bobbywu.com/SOKBench.",
        "page": "http://arxiv.org/abs/2405.09713",
        "pdf": "http://arxiv.org/pdf/2405.09713.pdf"
    },
    {
        "title": "Towards Realistic Scene Generation with LiDAR Diffusion Models",
        "author": "Haoxi Ran, Vitor Guizilini, Yue Wang",
        "abstract": "Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, geometry realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR generation in 64-beam scenario and state of the art on conditional LiDAR generation, while maintaining high efficiency compared to point-based DMs (up to 107$\\times$ faster). Furthermore, by compressing LiDAR scenes into a latent space, we enable the controllability of DMs with various conditions such as semantic maps, camera views, and text prompts.",
        "page": "http://arxiv.org/abs/2404.00815",
        "pdf": "http://arxiv.org/pdf/2404.00815.pdf"
    },
    {
        "title": "Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement",
        "author": "Xiuquan Hou, Meiqin Liu, Senlin Zhang, Ping Wei, Badong Chen",
        "abstract": "DETR-like methods have significantly increased detection performance in an end-to-end manner. The mainstream two-stage frameworks of them perform dense self-attention and select a fraction of queries for sparse cross-attention, which is proven effective for improving performance but also introduces a heavy computational burden and high dependence on stable query selection. This paper demonstrates that suboptimal two-stage selection strategies result in scale bias and redundancy due to the mismatch between selected queries and objects in two-stage initialization. To address these issues, we propose hierarchical salience filtering refinement, which performs transformer encoding only on filtered discriminative queries, for a better trade-off between computational efficiency and precision. The filtering process overcomes scale bias through a novel scale-independent salience supervision. To compensate for the semantic misalignment among queries, we introduce elaborate query refinement modules for stable two-stage initialization. Based on above improvements, the proposed Salience DETR achieves significant improvements of +4.0% AP, +0.2% AP, +4.4% AP on three challenging task-specific detection datasets, as well as 49.2% AP on COCO 2017 with less FLOPs. The code is available at https://github.com/xiuqhou/Salience-DETR.",
        "page": "http://arxiv.org/abs/2403.16131",
        "pdf": "http://arxiv.org/pdf/2403.16131.pdf"
    },
    {
        "title": "UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs",
        "author": "Yanwu Xu, Yang Zhao, Zhisheng Xiao, Tingbo Hou",
        "abstract": "Text-to-image diffusion models have demonstrated remarkable capabilities in transforming textual prompts into coherent images, yet the computational cost of their inference remains a persistent challenge. To address this issue, we present UFOGen, a novel generative model designed for ultra-fast, one-step text-to-image synthesis. In contrast to conventional approaches that focus on improving samplers or employing distillation techniques for diffusion models, UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN objective. Leveraging a newly introduced diffusion-GAN objective and initialization with pre-trained diffusion models, UFOGen excels in efficiently generating high-quality images conditioned on textual descriptions in a single step. Beyond traditional text-to-image generation, UFOGen showcases versatility in applications. Notably, UFOGen stands among the pioneering models enabling one-step text-to-image generation and diverse downstream tasks, presenting a significant advancement in the landscape of efficient generative models.",
        "page": "http://arxiv.org/abs/2311.09257",
        "pdf": "http://arxiv.org/pdf/2311.09257.pdf"
    },
    {
        "title": "SynSP: Synergy of Smoothness and Precision in Pose Sequences Refinement",
        "author": "Tao Wang, Lei Jin, Zheng Wang, Jianshu Li, Liang Li, Fang Zhao, Yu Cheng, Li Yuan, Li ZHOU, Junliang Xing, Jian Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation",
        "author": "Daichi Horita, Naoto Inoue, Kotaro Kikuchi, Kota Yamaguchi, Kiyoharu Aizawa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CAT-DM: Controllable Accelerated Virtual Try-on with Diffusion Model",
        "author": "Jianhao Zeng, Dan Song, Weizhi Nie, Hongshuo Tian, Tongtong Wang, An-An Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HOIAnimator: Text-Prompt Human-Object Animations Generation with Perceptive Diffusion Models",
        "author": "Wenfeng Song, Xinyu Zhang, Shuai Li, Yang Gao, Aimin Hao, Xia HOU, Chenglizhao Chen, Ning Li, Hong Qin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Real-World Efficient Blind Motion Deblurring via Blur Pixel Discretization",
        "author": "Insoo Kim, Jae Seok Choi, Geonseok Seo, Kinam Kwon, Jinwoo Shin, Hyong-Euk Lee",
        "abstract": "As recent advances in mobile camera technology have enabled the capability to capture high-resolution images, such as 4K images, the demand for an efficient deblurring model handling large motion has increased. In this paper, we discover that the image residual errors, i.e., blur-sharp pixel differences, can be grouped into some categories according to their motion blur type and how complex their neighboring pixels are. Inspired by this, we decompose the deblurring (regression) task into blur pixel discretization (pixel-level blur classification) and discrete-to-continuous conversion (regression with blur class map) tasks. Specifically, we generate the discretized image residual errors by identifying the blur pixels and then transform them to a continuous form, which is computationally more efficient than naively solving the original regression problem with continuous values. Here, we found that the discretization result, i.e., blur segmentation map, remarkably exhibits visual similarity with the image residual errors. As a result, our efficient model shows comparable performance to state-of-the-art methods in realistic benchmarks, while our method is up to 10 times computationally more efficient.",
        "page": "http://arxiv.org/abs/2404.12168",
        "pdf": "http://arxiv.org/pdf/2404.12168.pdf"
    },
    {
        "title": "MuseChat: A Conversational Music Recommendation System for Videos",
        "author": "Zhikang Dong, Bin Chen, Xiulong Liu, Pawel Polak, Peng Zhang",
        "abstract": "Music recommendation for videos attracts growing interest in multi-modal research. However, existing systems focus primarily on content compatibility, often ignoring the users' preferences. Their inability to interact with users for further refinements or to provide explanations leads to a less satisfying experience. We address these issues with MuseChat, a first-of-its-kind dialogue-based recommendation system that personalizes music suggestions for videos. Our system consists of two key functionalities with associated modules: recommendation and reasoning. The recommendation module takes a video along with optional information including previous suggested music and user's preference as inputs and retrieves an appropriate music matching the context. The reasoning module, equipped with the power of Large Language Model (Vicuna-7B) and extended to multi-modal inputs, is able to provide reasonable explanation for the recommended music. To evaluate the effectiveness of MuseChat, we build a large-scale dataset, conversational music recommendation for videos, that simulates a two-turn interaction between a user and a recommender based on accurate music track information. Experiment results show that MuseChat achieves significant improvements over existing video-based music retrieval methods as well as offers strong interpretability and interactability.",
        "page": "http://arxiv.org/abs/2310.06282",
        "pdf": "http://arxiv.org/pdf/2310.06282.pdf"
    },
    {
        "title": "FREE: Faster and Better Data-Free Meta-Learning",
        "author": "Yongxian Wei, Zixuan Hu, Zhenyi Wang, Li Shen, Chun Yuan, Dacheng Tao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology",
        "author": "Wenhao Tang, Fengtao ZHOU, Sheng Huang, Xiang Zhu, Yi Zhang, Bo Liu",
        "abstract": "Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin.The code is available at: https://github.com/DearCaat/RRT-MIL.",
        "page": "http://arxiv.org/abs/2402.17228",
        "pdf": "http://arxiv.org/pdf/2402.17228.pdf"
    },
    {
        "title": "Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications",
        "author": "Junyi Ma, Xieyuanli Chen, Jiawei Huang, Jingyi Xu, Zhen Luo, Jintao Xu, Weihao Gu, Rui Ai, Hesheng Wang",
        "abstract": "Understanding how the surrounding environment changes is crucial for performing downstream tasks safely and reliably in autonomous driving applications. Recent occupancy estimation techniques using only camera images as input can provide dense occupancy representations of large-scale scenes based on the current observation. However, they are mostly limited to representing the current 3D space and do not consider the future state of surrounding objects along the time axis. To extend camera-only occupancy estimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark for camera-only 4D occupancy forecasting, evaluating the surrounding scene changes in a near future. We build our benchmark based on multiple publicly available datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5, which provides sequential occupancy states of general movable and static objects, as well as their 3D backward centripetal flow. To establish this benchmark for future research with comprehensive comparisons, we introduce four baseline types from diverse camera-based perception and prediction implementations, including a static-world occupancy model, voxelization of point cloud prediction, 2D-3D instance-based prediction, and our proposed novel end-to-end 4D occupancy forecasting network. Furthermore, the standardized evaluation protocol for preset multiple tasks is also provided to compare the performance of all the proposed baselines on present and future occupancy estimation with respect to objects of interest in autonomous driving scenarios. The dataset and our implementation of all four baselines in the proposed Cam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.",
        "page": "http://arxiv.org/abs/2311.17663",
        "pdf": "http://arxiv.org/pdf/2311.17663.pdf"
    },
    {
        "title": "Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching",
        "author": "Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, Zhiqiang Shen",
        "abstract": "The lightweight \"local-match-global\" matching introduced by SRe2L successfully creates a distilled dataset with comprehensive information on the full 224x224 ImageNet-1k. However, this one-sided approach is limited to a particular backbone, layer, and statistics, which limits the improvement of the generalization of a distilled dataset. We suggest that sufficient and various \"local-match-global\" matching are more precise and effective than a single one and has the ability to create a distilled dataset with richer information and better generalization. We call this perspective \"generalized matching\" and propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the complete dataset across various backbones, layers, and statistics. As experimentally demonstrated, G-VBSM is the first algorithm to obtain strong performance across both small-scale and large-scale datasets. Specifically, G-VBSM achieves a performance of 38.7% on CIFAR-100 with 128-width ConvNet, 47.6% on Tiny-ImageNet with ResNet18, and 31.4% on the full 224x224 ImageNet-1k with ResNet18, under images per class (IPC) 10, 50, and 10, respectively. These results surpass all SOTA methods by margins of 3.9%, 6.5%, and 10.1%, respectively.",
        "page": "http://arxiv.org/abs/2311.17950",
        "pdf": "http://arxiv.org/pdf/2311.17950.pdf"
    },
    {
        "title": "MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos",
        "author": "Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, DING ZHAO, Bo Li, Lijuan Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UniDepth: Universal Monocular Metric Depth Estimation",
        "author": "Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, Fisher Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation",
        "author": "Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LMDrive: Closed-Loop End-to-End Driving  with Large Language Models",
        "author": "Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven L. Waslander, Yu Liu, Hongsheng Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation",
        "author": "Qingping SUN, Yanjun Wang, Ailing Zeng, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiy Mei, Chi LEUNG, Ziwei Liu, Lei Yang, Zhongang Cai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud",
        "author": "WENCAN CHENG, WENCAN CHENG, Hao Tang, Luc Van Gool, Jong Hwan Ko",
        "abstract": "Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDiff.",
        "page": "http://arxiv.org/abs/2404.03159",
        "pdf": "http://arxiv.org/pdf/2404.03159.pdf"
    },
    {
        "title": "Open-World Human-Object Interaction Detection via Multi-modal Prompts",
        "author": "Jie Yang, Bingliang Li, Ailing Zeng, Ailing Zeng, Lei Zhang, Ruimao Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fourier-basis functions to bridge augmentation gap: Rethinking frequency augmentation in image classification",
        "author": "Mei Vaish, Puru Vaish, Shunxin Wang, Nicola Strisciuglio",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ProMotion: Prototypes As Motion Learners",
        "author": "Yawen Lu, Dongfang Liu, Qifan Wang, Cheng Han, Yiming Cui, Yiming Cui, Zhiwen Cao, Xueling Zhang, Yingjie Victor Chen, Heng Fan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Make Pixels Dance: High-Dynamic Video Generation",
        "author": "Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, Yuchen Zhang, Hang Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation",
        "author": "Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diffeomorphic Template Registration for Atmospheric Turbulence Mitigation",
        "author": "Dong Lao, Congli Wang, Congli Wang, Alex Wong, Stefano Soatto, Stefano Soatto",
        "abstract": "We describe a method for recovering the irradiance underlying a collection of images corrupted by atmospheric turbulence. Since supervised data is often technically impossible to obtain, assumptions and biases have to be imposed to solve this inverse problem, and we choose to model them explicitly. Rather than initializing a latent irradiance (\"template\") by heuristics to estimate deformation, we select one of the images as a reference, and model the deformation in this image by the aggregation of the optical flow from it to other images, exploiting a prior imposed by Central Limit Theorem. Then with a novel flow inversion module, the model registers each image TO the template but WITHOUT the template, avoiding artifacts related to poor template initialization. To illustrate the robustness of the method, we simply (i) select the first frame as the reference and (ii) use the simplest optical flow to estimate the warpings, yet the improvement in registration is decisive in the final reconstruction, as we achieve state-of-the-art performance despite its simplicity. The method establishes a strong baseline that can be further improved by integrating it seamlessly into more sophisticated pipelines, or with domain-specific methods if so desired.",
        "page": "http://arxiv.org/abs/2405.03662",
        "pdf": "http://arxiv.org/pdf/2405.03662.pdf"
    },
    {
        "title": "Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning",
        "author": "Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Soren Pirk, Soeren Pirk, ARIE KAUFMAN",
        "abstract": "Multi-view diffusion models, obtained by applying Supervised Finetuning (SFT) to text-to-image diffusion models, have driven recent breakthroughs in text-to-3D research. However, due to the limited size and quality of existing 3D datasets, they still suffer from multi-view inconsistencies and Neural Radiance Field (NeRF) reconstruction artifacts. We argue that multi-view diffusion models can benefit from further Reinforcement Learning Finetuning (RLFT), which allows models to learn from the data generated by themselves and improve beyond their dataset limitations during SFT. To this end, we introduce Carve3D, an improved RLFT algorithm coupled with a novel Multi-view Reconstruction Consistency (MRC) metric, to enhance the consistency of multi-view diffusion models. To measure the MRC metric on a set of multi-view images, we compare them with their corresponding NeRF renderings at the same camera viewpoints. The resulting model, which we denote as Carve3DM, demonstrates superior multi-view consistency and NeRF reconstruction quality than existing models. Our results suggest that pairing SFT with Carve3D's RLFT is essential for developing multi-view-consistent diffusion models, mirroring the standard Large Language Model (LLM) alignment pipeline. Our code, training and testing data, and video results are available at: https://desaixie.github.io/carve-3d.",
        "page": "http://arxiv.org/abs/2312.13980",
        "pdf": "http://arxiv.org/pdf/2312.13980.pdf"
    },
    {
        "title": "PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models",
        "author": "Fei Deng, Qifei Wang, Qifei Wang, Wei Wei, Tingbo Hou, Matthias Grundmann",
        "abstract": "Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail.",
        "page": "http://arxiv.org/abs/2402.08714",
        "pdf": "http://arxiv.org/pdf/2402.08714.pdf"
    },
    {
        "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "author": "Shengbang Tong, Zhuang Liu, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie",
        "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",
        "page": "http://arxiv.org/abs/2401.06209",
        "pdf": "http://arxiv.org/pdf/2401.06209.pdf"
    },
    {
        "title": "Language-driven Grasp Detection",
        "author": "An Dinh Vuong, Minh Nhat VU, Baoru Huang, Nghia Nguyen, Hieu Le, Thieu Vo, Thieu Vo, Anh Nguyen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection",
        "author": "Trevine Oorloff, Surya Koppisetti, Surya Koppisetti, Nicolo Bonettini, Divyaraj Solanki, Ben Colman, Yaser Yacoob, Ali Shahriyari, Gaurav Bharaj",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UniGS: Unified Representation for Image Generation and Segmentation",
        "author": "Lu Qi, Lehan Yang, Lehan Yang, Weidong Guo, Yu Xu, Bo Du, Varun Jampani, Ming-Hsuan Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Convolutional Prompting meets Language Models for Continual Learning",
        "author": "ANURAG Roy, Riddhiman Moulick, Vinay Verma, Vinay Verma Verma, Saptarshi Ghosh, Abir Das",
        "abstract": "Continual Learning (CL) enables machine learning models to learn from continuously shifting new training data in absence of data from old tasks. Recently, pretrained vision transformers combined with prompt tuning have shown promise for overcoming catastrophic forgetting in CL. These approaches rely on a pool of learnable prompts which can be inefficient in sharing knowledge across tasks leading to inferior performance. In addition, the lack of fine-grained layer specific prompts does not allow these to fully express the strength of the prompts for CL. We address these limitations by proposing ConvPrompt, a novel convolutional prompt creation mechanism that maintains layer-wise shared embeddings, enabling both layer-specific learning and better concept transfer across tasks. The intelligent use of convolution enables us to maintain a low parameter overhead without compromising performance. We further leverage Large Language Models to generate fine-grained text descriptions of each category which are used to get task similarity and dynamically decide the number of prompts to be learned. Extensive experiments demonstrate the superiority of ConvPrompt and improves SOTA by ~3% with significantly less parameter overhead. We also perform strong ablation over various modules to disentangle the importance of different components.",
        "page": "http://arxiv.org/abs/2403.20317",
        "pdf": "http://arxiv.org/pdf/2403.20317.pdf"
    },
    {
        "title": "POPDG: Popular 3D Dance Generation with PopDanceSet",
        "author": "Zhenye Luo, ZhenYe Luo, Min Ren, Xuecai Hu, Yongzhen Huang, Li Yao",
        "abstract": "Generating dances that are both lifelike and well-aligned with music continues to be a challenging task in the cross-modal domain. This paper introduces PopDanceSet, the first dataset tailored to the preferences of young audiences, enabling the generation of aesthetically oriented dances. And it surpasses the AIST++ dataset in music genre diversity and the intricacy and depth of dance movements. Moreover, the proposed POPDG model within the iDDPM framework enhances dance diversity and, through the Space Augmentation Algorithm, strengthens spatial physical connections between human body joints, ensuring that increased diversity does not compromise generation quality. A streamlined Alignment Module is also designed to improve the temporal alignment between dance and music. Extensive experiments show that POPDG achieves SOTA results on two datasets. Furthermore, the paper also expands on current evaluation metrics. The dataset and code are available at https://github.com/Luke-Luo1/POPDG.",
        "page": "http://arxiv.org/abs/2405.03178",
        "pdf": "http://arxiv.org/pdf/2405.03178.pdf"
    },
    {
        "title": "CapsFusion: Rethinking Image-Text Data at Scale",
        "author": "Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, Jingjing Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generative Multimodal Models are In-Context Learners",
        "author": "Quan Sun, Yufeng Cui, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang",
        "abstract": "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.",
        "page": "http://arxiv.org/abs/2312.13286",
        "pdf": "http://arxiv.org/pdf/2312.13286.pdf"
    },
    {
        "title": "TIGER: Time-Varying Denoising Model for 3D Point Cloud Generation with Diffusion Process",
        "author": "Zhiyuan Ren, Minchul Kim, Feng Liu, Feng Liu, Xiaoming Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FreeMan: Towards benchmarking 3D human pose estimation under Real-World Conditions",
        "author": "Jiong WANG, Fengyu Yang, Bingliang Li, Wenbo Gou, Danqi Yan, Ailing Zeng, Ailing Zeng, Yijun Gao, Junle Wang, Yanqing Jing, Ruimao Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TULIP: Transformer for Upsampling of LiDAR Point Cloud",
        "author": "Bin Yang, Bin Yang, Patrick Pfreundschuh, Roland Siegwart, Marco Hutter, Peyman Moghadam, Vaishakh Patil",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "KeyPoint Relative Position Encoding for Face Recognition",
        "author": "Minchul Kim, Feng Liu, Feng Liu, Yiyang Su, Anil Jain, Xiaoming Liu",
        "abstract": "In this paper, we address the challenge of making ViT models more robust to unseen affine transformations. Such robustness becomes useful in various recognition tasks such as face recognition when image alignment failures occur. We propose a novel method called KP-RPE, which leverages key points (e.g.~facial landmarks) to make ViT more resilient to scale, translation, and pose variations. We begin with the observation that Relative Position Encoding (RPE) is a good way to bring affine transform generalization to ViTs. RPE, however, can only inject the model with prior knowledge that nearby pixels are more important than far pixels. Keypoint RPE (KP-RPE) is an extension of this principle, where the significance of pixels is not solely dictated by their proximity but also by their relative positions to specific keypoints within the image. By anchoring the significance of pixels around keypoints, the model can more effectively retain spatial relationships, even when those relationships are disrupted by affine transformations. We show the merit of KP-RPE in face and gait recognition. The experimental results demonstrate the effectiveness in improving face recognition performance from low-quality images, particularly where alignment is prone to failure. Code and pre-trained models are available.",
        "page": "http://arxiv.org/abs/2403.14852",
        "pdf": "http://arxiv.org/pdf/2403.14852.pdf"
    },
    {
        "title": "Relightful Harmonization: Lighting-aware Portrait Background Replacement",
        "author": "Mengwei Ren, Wei Xiong, Jae Shin Yoon, Zhixin Shu, Jianming Zhang, HyunJoon Jung, Guido Gerig, He Zhang",
        "abstract": "Portrait harmonization aims to composite a subject into a new background, adjusting its lighting and color to ensure harmony with the background scene. Existing harmonization techniques often only focus on adjusting the global color and brightness of the foreground and ignore crucial illumination cues from the background such as apparent lighting direction, leading to unrealistic compositions. We introduce Relightful Harmonization, a lighting-aware diffusion model designed to seamlessly harmonize sophisticated lighting effect for the foreground portrait using any background image. Our approach unfolds in three stages. First, we introduce a lighting representation module that allows our diffusion model to encode lighting information from target image background. Second, we introduce an alignment network that aligns lighting features learned from image background with lighting features learned from panorama environment maps, which is a complete representation for scene illumination. Last, to further boost the photorealism of the proposed method, we introduce a novel data simulation pipeline that generates synthetic training pairs from a diverse range of natural images, which are used to refine the model. Our method outperforms existing benchmarks in visual fidelity and lighting coherence, showing superior generalization in real-world testing scenarios, highlighting its versatility and practicality.",
        "page": "http://arxiv.org/abs/2312.06886",
        "pdf": "http://arxiv.org/pdf/2312.06886.pdf"
    },
    {
        "title": "Visual Layout Composer: Image-Vector Dual Diffusion Model for Design Layout Generation",
        "author": "Mohammad Amin Shabani, Zhaowen Wang, Difan Liu, Nanxuan Zhao, Jimei Yang, Jimei Yang, Yasutaka Furukawa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Preserving Fairness Generalization in Deepfake Detection",
        "author": "Li Lin, Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu",
        "abstract": "Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection. The code is available at https://github.com/Purdue-M2/Fairness-Generalization",
        "page": "http://arxiv.org/abs/2402.17229",
        "pdf": "http://arxiv.org/pdf/2402.17229.pdf"
    },
    {
        "title": "In Search of a Data Transformation That Accelerates Neural Field Training",
        "author": "Junwon Seo, Sangyoon Lee, Kwang In Kim, Jaeho Lee",
        "abstract": "Neural field is an emerging paradigm in data representation that trains a neural network to approximate the given signal. A key obstacle that prevents its widespread adoption is the encoding speed-generating neural fields requires an overfitting of a neural network, which can take a significant number of SGD steps to reach the desired fidelity level. In this paper, we delve into the impacts of data transformations on the speed of neural field training, specifically focusing on how permuting pixel locations affect the convergence speed of SGD. Counterintuitively, we find that randomly permuting the pixel locations can considerably accelerate the training. To explain this phenomenon, we examine the neural field training through the lens of PSNR curves, loss landscapes, and error patterns. Our analyses suggest that the random pixel permutations remove the easy-to-fit patterns, which facilitate easy optimization in the early stage but hinder capturing fine details of the signal.",
        "page": "http://arxiv.org/abs/2311.17094",
        "pdf": "http://arxiv.org/pdf/2311.17094.pdf"
    },
    {
        "title": "PlatoNeRF: 3D Reconstruction in Plato\u2019s Cave via Single-View Two-Bounce Lidar",
        "author": "Tzofi Klinghoffer, Xiaoyu Xiang, Siddharth Somasundaram, Yuchen Fan, Christian Richardt, Ramesh Raskar, Rakesh Ranjan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Stratified Avatar Generation from Sparse Observations",
        "author": "Han Feng, Wenchao Ma, Quankai Gao, Xianwei Zheng, Nan Xue, Huijuan Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Semantic Correspondence with Viewpoint-Guided Spherical Maps",
        "author": "Octave Mariotti, Oisin Mac Aodha, Hakan Bilen",
        "abstract": "Recent progress in self-supervised representation learning has resulted in models that are capable of extracting image features that are not only effective at encoding image level, but also pixel-level, semantics. These features have been shown to be effective for dense visual semantic correspondence estimation, even outperforming fully-supervised methods. Nevertheless, current self-supervised approaches still fail in the presence of challenging image characteristics such as symmetries and repeated parts. To address these limitations, we propose a new approach for semantic correspondence estimation that supplements discriminative self-supervised features with 3D understanding via a weak geometric spherical prior. Compared to more involved 3D pipelines, our model only requires weak viewpoint information, and the simplicity of our spherical representation enables us to inject informative geometric priors into the model during training. We propose a new evaluation metric that better accounts for repeated part and symmetry-induced mistakes. We present results on the challenging SPair-71k dataset, where we show that our approach demonstrates is capable of distinguishing between symmetric views and repeated parts across many object categories, and also demonstrate that we can generalize to unseen classes on the AwA dataset.",
        "page": "http://arxiv.org/abs/2312.13216",
        "pdf": "http://arxiv.org/pdf/2312.13216.pdf"
    },
    {
        "title": "Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation",
        "author": "Ming Xu, Stephen Gould",
        "abstract": "We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state-of-the-art results for the unsupervised video action segmentation task.",
        "page": "http://arxiv.org/abs/2404.01518",
        "pdf": "http://arxiv.org/pdf/2404.01518.pdf"
    },
    {
        "title": "FMA-Net: Flow Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring",
        "author": "Geunhyuk Youk, Jihyong Oh, Munchurl Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Producing and Leveraging Online Map Uncertainty in Trajectory Prediction",
        "author": "Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, Boris Ivanovic",
        "abstract": "High-definition (HD) maps have played an integral role in the development of modern autonomous vehicle (AV) stacks, albeit with high associated labeling and maintenance costs. As a result, many recent works have proposed methods for estimating HD maps online from sensor data, enabling AVs to operate outside of previously-mapped regions. However, current online map estimation approaches are developed in isolation of their downstream tasks, complicating their integration in AV stacks. In particular, they do not produce uncertainty or confidence estimates. In this work, we extend multiple state-of-the-art online map estimation methods to additionally estimate uncertainty and show how this enables more tightly integrating online mapping with trajectory forecasting. In doing so, we find that incorporating uncertainty yields up to 50% faster training convergence and up to 15% better prediction performance on the real-world nuScenes driving dataset.",
        "page": "http://arxiv.org/abs/2403.16439",
        "pdf": "http://arxiv.org/pdf/2403.16439.pdf"
    },
    {
        "title": "Seeing the World through Your Eyes",
        "author": "Hadi Alzayer, Kevin Zhang, Brandon Y. Feng, Christopher Metzler, Jia-Bin Huang",
        "abstract": "The reflective nature of the human eye is an underappreciated source of information about what the world around us looks like. By imaging the eyes of a moving person, we can collect multiple views of a scene outside the camera's direct line of sight through the reflections in the eyes. In this paper, we reconstruct a 3D scene beyond the camera's line of sight using portrait images containing eye reflections. This task is challenging due to 1) the difficulty of accurately estimating eye poses and 2) the entangled appearance of the eye iris and the scene reflections. Our method jointly refines the cornea poses, the radiance field depicting the scene, and the observer's eye iris texture. We further propose a simple regularization prior on the iris texture pattern to improve reconstruction quality. Through various experiments on synthetic and real-world captures featuring people with varied eye colors, we demonstrate the feasibility of our approach to recover 3D scenes using eye reflections.",
        "page": "http://arxiv.org/abs/2306.09348",
        "pdf": "http://arxiv.org/pdf/2306.09348.pdf"
    },
    {
        "title": "Deep Generative Model based Rate-Distortion for Image Downscaling Assessment",
        "author": "yuanbang liang, Bhavesh Garg, Paul L. Rosin, Yipeng Qin",
        "abstract": "In this paper, we propose Image Downscaling Assessment by Rate-Distortion (IDA-RD), a novel measure to quantitatively evaluate image downscaling algorithms. In contrast to image-based methods that measure the quality of downscaled images, ours is process-based that draws ideas from rate-distortion theory to measure the distortion incurred during downscaling. Our main idea is that downscaling and super-resolution (SR) can be viewed as the encoding and decoding processes in the rate-distortion model, respectively, and that a downscaling algorithm that preserves more details in the resulting low-resolution (LR) images should lead to less distorted high-resolution (HR) images in SR. In other words, the distortion should increase as the downscaling algorithm deteriorates. However, it is non-trivial to measure this distortion as it requires the SR algorithm to be blind and stochastic. Our key insight is that such requirements can be met by recent SR algorithms based on deep generative models that can find all matching HR images for a given LR image on their learned image manifolds. Extensive experimental results show the effectiveness of our IDA-RD measure.",
        "page": "http://arxiv.org/abs/2403.15139",
        "pdf": "http://arxiv.org/pdf/2403.15139.pdf"
    },
    {
        "title": "Analyzing and Improving the Training Dynamics of Diffusion Models",
        "author": "Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, Samuli Laine",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Objects as volumes: A stochastic geometry view of opaque solids",
        "author": "Bailey Miller, Hanyu Chen, Alice Lai, Ioannis Gkioulekas",
        "abstract": "We develop a theory for the representation of opaque solids as volumes. Starting from a stochastic representation of opaque solids as random indicator functions, we prove the conditions under which such solids can be modeled using exponential volumetric transport. We also derive expressions for the volumetric attenuation coefficient as a functional of the probability distributions of the underlying indicator functions. We generalize our theory to account for isotropic and anisotropic scattering at different parts of the solid, and for representations of opaque solids as stochastic implicit surfaces. We derive our volumetric representation from first principles, which ensures that it satisfies physical constraints such as reciprocity and reversibility. We use our theory to explain, compare, and correct previous volumetric representations, as well as propose meaningful extensions that lead to improved performance in 3D reconstruction tasks.",
        "page": "http://arxiv.org/abs/2312.15406",
        "pdf": "http://arxiv.org/pdf/2312.15406.pdf"
    },
    {
        "title": "EGTR: Extracting Graph from Transformer for Scene Graph Generation",
        "author": "Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Robust Event-guided Low-Light Image Enhancement: A Large-Scale Real-World Event-Image Dataset and Novel Approach",
        "author": "Guoqiang Liang, Kanghao Chen, Hangyu Li, Yunfan Lu, Lin Wang",
        "abstract": "Event camera has recently received much attention for low-light image enhancement (LIE) thanks to their distinct advantages, such as high dynamic range. However, current research is prohibitively restricted by the lack of large-scale, real-world, and spatial-temporally aligned event-image datasets. To this end, we propose a real-world (indoor and outdoor) dataset comprising over 30K pairs of images and events under both low and normal illumination conditions. To achieve this, we utilize a robotic arm that traces a consistent non-linear trajectory to curate the dataset with spatial alignment precision under 0.03mm. We then introduce a matching alignment strategy, rendering 90% of our dataset with errors less than 0.01s. Based on the dataset, we propose a novel event-guided LIE approach, called EvLight, towards robust performance in real-world low-light scenes. Specifically, we first design the multi-scale holistic fusion branch to extract holistic structural and textural information from both events and images. To ensure robustness against variations in the regional illumination and noise, we then introduce a Signal-to-Noise-Ratio (SNR)-guided regional feature selection to selectively fuse features of images from regions with high SNR and enhance those with low SNR by extracting regional structure information from events. Extensive experiments on our dataset and the synthetic SDSD dataset demonstrate our EvLight significantly surpasses the frame-based methods. Code and datasets are available at https://vlislab22.github.io/eg-lowlight/.",
        "page": "http://arxiv.org/abs/2404.00834",
        "pdf": "http://arxiv.org/pdf/2404.00834.pdf"
    },
    {
        "title": "Style Aligned Image Generation via Shared Attention",
        "author": "Amir Hertz, Andrey Voynov, Shlomi Fruchter, Daniel Cohen-Or",
        "abstract": "Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.",
        "page": "http://arxiv.org/abs/2312.02133",
        "pdf": "http://arxiv.org/pdf/2312.02133.pdf"
    },
    {
        "title": "MetaCloak: Preventing Unauthorized Subject-driven Text-to-image Diffusion-based Synthesis via Meta-learning",
        "author": "Yixin Liu, Chenrui Fan, Yutong Dai, Xun Chen, Pan Zhou, Lichao Sun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Visual Anagrams: Synthesizing Multi-View Optical Illusions with Diffusion Models",
        "author": "Daniel Geng, Inbum Park, Andrew Owens",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Comparing the Decision-Making Mechanisms by Transformers and CNNs via Explanation Methods",
        "author": "Mingqi Jiang, Saeed Khorram, Li Fuxin",
        "abstract": "In order to gain insights about the decision-making of different visual recognition backbones, we propose two methodologies, sub-explanation counting and cross-testing, that systematically applies deep explanation algorithms on a dataset-wide basis, and compares the statistics generated from the amount and nature of the explanations. These methodologies reveal the difference among networks in terms of two properties called compositionality and disjunctivism. Transformers and ConvNeXt are found to be more compositional, in the sense that they jointly consider multiple parts of the image in building their decisions, whereas traditional CNNs and distilled transformers are less compositional and more disjunctive, which means that they use multiple diverse but smaller set of parts to achieve a confident prediction. Through further experiments, we pinpointed the choice of normalization to be especially important in the compositionality of a model, in that batch normalization leads to less compositionality while group and layer normalization lead to more. Finally, we also analyze the features shared by different backbones and plot a landscape of different models based on their feature-use similarity.",
        "page": "http://arxiv.org/abs/2212.06872",
        "pdf": "http://arxiv.org/pdf/2212.06872.pdf"
    },
    {
        "title": "DART: Implicit Doppler Tomography for Radar Novel View Synthesis",
        "author": "Tianshu Huang, John Miller, Akarsh Prabhakara, Tao Jin, Tarana Laroia, Zico Kolter, Anthony Rowe",
        "abstract": "Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images.",
        "page": "http://arxiv.org/abs/2403.03896",
        "pdf": "http://arxiv.org/pdf/2403.03896.pdf"
    },
    {
        "title": "RoHM: Robust Human Motion Reconstruction via Diffusion",
        "author": "Siwei Zhang, Bharat Lal Bhatnagar, Yuanlu Xu, Alexander Winkler, Petr Kadlecek, Siyu Tang, Federica Bogo",
        "abstract": "We propose RoHM, an approach for robust 3D human motion reconstruction from monocular RGB(-D) videos in the presence of noise and occlusions. Most previous approaches either train neural networks to directly regress motion in 3D or learn data-driven motion priors and combine them with optimization at test time. The former do not recover globally coherent motion and fail under occlusions; the latter are time-consuming, prone to local minima, and require manual tuning. To overcome these shortcomings, we exploit the iterative, denoising nature of diffusion models. RoHM is a novel diffusion-based motion model that, conditioned on noisy and occluded input data, reconstructs complete, plausible motions in consistent global coordinates. Given the complexity of the problem -- requiring one to address different tasks (denoising and infilling) in different solution spaces (local and global motion) -- we decompose it into two sub-tasks and learn two models, one for global trajectory and one for local motion. To capture the correlations between the two, we then introduce a novel conditioning module, combining it with an iterative inference scheme. We apply RoHM to a variety of tasks -- from motion reconstruction and denoising to spatial and temporal infilling. Extensive experiments on three popular datasets show that our method outperforms state-of-the-art approaches qualitatively and quantitatively, while being faster at test time. The code is available at https://sanweiliti.github.io/ROHM/ROHM.html.",
        "page": "http://arxiv.org/abs/2401.08570",
        "pdf": "http://arxiv.org/pdf/2401.08570.pdf"
    },
    {
        "title": "Tri-Perspective View Decomposition for Geometry-Aware Depth Completion",
        "author": "Zhiqiang Yan, Yuankai Lin, Kun Wang, Yupeng Zheng, Yufei Wang, Zhenyu Zhang, Jun Li, Jian Yang",
        "abstract": "Depth completion is a vital task for autonomous driving, as it involves reconstructing the precise 3D geometry of a scene from sparse and noisy depth measurements. However, most existing methods either rely only on 2D depth representations or directly incorporate raw 3D point clouds for compensation, which are still insufficient to capture the fine-grained 3D geometry of the scene. To address this challenge, we introduce Tri-Perspective view Decomposition (TPVD), a novel framework that can explicitly model 3D geometry. In particular, (1) TPVD ingeniously decomposes the original point cloud into three 2D views, one of which corresponds to the sparse depth input. (2) We design TPV Fusion to update the 2D TPV features through recurrent 2D-3D-2D aggregation, where a Distance-Aware Spherical Convolution (DASC) is applied. (3) By adaptively choosing TPV affinitive neighbors, the newly proposed Geometric Spatial Propagation Network (GSPN) further improves the geometric consistency. As a result, our TPVD outperforms existing methods on KITTI, NYUv2, and SUN RGBD. Furthermore, we build a novel depth completion dataset named TOFDC, which is acquired by the time-of-flight (TOF) sensor and the color camera on smartphones. Project page: https://yanzq95.github.io/projectpage/TOFDC/index.html",
        "page": "http://arxiv.org/abs/2403.15008",
        "pdf": "http://arxiv.org/pdf/2403.15008.pdf"
    },
    {
        "title": "Bilateral Event Mining and Complementary for Event Stream Super-Resolution",
        "author": "Zhilin Huang, Quanmin Liang, Yijie Yu, Chujun Qin, Xiawu Zheng, Kai Huang, Zikun Zhou, Wenming Yang",
        "abstract": "Event Stream Super-Resolution (ESR) aims to address the challenge of insufficient spatial resolution in event streams, which holds great significance for the application of event cameras in complex scenarios. Previous works for ESR often process positive and negative events in a mixed paradigm. This paradigm limits their ability to effectively model the unique characteristics of each event and mutually refine each other by considering their correlations. In this paper, we propose a bilateral event mining and complementary network (BMCNet) to fully leverage the potential of each event and capture the shared information to complement each other simultaneously. Specifically, we resort to a two-stream network to accomplish comprehensive mining of each type of events individually. To facilitate the exchange of information between two streams, we propose a bilateral information exchange (BIE) module. This module is layer-wisely embedded between two streams, enabling the effective propagation of hierarchical global information while alleviating the impact of invalid information brought by inherent characteristics of events. The experimental results demonstrate that our approach outperforms the previous state-of-the-art methods in ESR, achieving performance improvements of over 11\\% on both real and synthetic datasets. Moreover, our method significantly enhances the performance of event-based downstream tasks such as object recognition and video reconstruction. Our code is available at https://github.com/Lqm26/BMCNet-ESR.",
        "page": "http://arxiv.org/abs/2405.10037",
        "pdf": "http://arxiv.org/pdf/2405.10037.pdf"
    },
    {
        "title": "pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction",
        "author": "David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, Vincent Sitzmann",
        "abstract": "We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field.",
        "page": "http://arxiv.org/abs/2312.12337",
        "pdf": "http://arxiv.org/pdf/2312.12337.pdf"
    },
    {
        "title": "FineParser: A Fine-grained Spatio-temporal Action Parser for Human-centric Action Quality Assessment",
        "author": "Jinglin Xu, Sibo Yin, Guohao Zhao, Zishuo Wang, Yuxin Peng",
        "abstract": "Existing action quality assessment (AQA) methods mainly learn deep representations at the video level for scoring diverse actions. Due to the lack of a fine-grained understanding of actions in videos, they harshly suffer from low credibility and interpretability, thus insufficient for stringent applications, such as Olympic diving events. We argue that a fine-grained understanding of actions requires the model to perceive and parse actions in both time and space, which is also the key to the credibility and interpretability of the AQA technique. Based on this insight, we propose a new fine-grained spatial-temporal action parser named \\textbf{FineParser}. It learns human-centric foreground action representations by focusing on target action regions within each frame and exploiting their fine-grained alignments in time and space to minimize the impact of invalid backgrounds during the assessment. In addition, we construct fine-grained annotations of human-centric foreground action masks for the FineDiving dataset, called \\textbf{FineDiving-HM}. With refined annotations on diverse target action procedures, FineDiving-HM can promote the development of real-world AQA systems. Through extensive experiments, we demonstrate the effectiveness of FineParser, which outperforms state-of-the-art methods while supporting more tasks of fine-grained action understanding. Data and code are available at \\url{https://github.com/PKU-ICST-MIPL/FineParser_CVPR2024}.",
        "page": "http://arxiv.org/abs/2405.06887",
        "pdf": "http://arxiv.org/pdf/2405.06887.pdf"
    },
    {
        "title": "Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations",
        "author": "Sangmin Lee, Bolin Lai, Fiona Ryan, Bikram Boote, James Rehg",
        "abstract": "Understanding social interactions involving both verbal and non-verbal cues is essential for effectively interpreting social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not aligned to utterances in multi-party environments. Consequently, they are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates concurrently capturing verbal and non-verbal cues pertinent to social reasoning. Experiments demonstrate the effectiveness of the proposed approach with densely aligned multimodal representations in modeling fine-grained social interactions. Project website: https://sangmin-git.github.io/projects/MMSI.",
        "page": "http://arxiv.org/abs/2403.02090",
        "pdf": "http://arxiv.org/pdf/2403.02090.pdf"
    },
    {
        "title": "Learning to Produce Semi-dense Correspondences for Visual Localization",
        "author": "Khang Truong Giang, Soohwan Song, Sungho Jo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GPLD3D: Latent Diffusion of 3D Shape Generative Models by Enforcing Geometric and Physical Priors",
        "author": "Yuan Dong, Qi Zuo, Xiaodong Gu, Weihao Yuan, zhengyi zhao, Zilong Dong, Liefeng Bo, Qixing Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BioCLIP: A Vision Foundation Model for the Tree of Life",
        "author": "Samuel Stevens, Jiaman Wu, Matthew Thompson, Elizabeth Campolongo, Chan Hee Song, David Carlyn, Li Dong, Wasila Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su",
        "abstract": "Images of the natural world, collected by a variety of cameras, from drones to individual phones, are increasingly abundant sources of biological information. There is an explosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. Yet most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. A vision model for general organismal biology questions on images is of timely need. To approach this, we curate and release TreeOfLife-10M, the largest and most diverse ML-ready dataset of biology images. We then develop BioCLIP, a foundation model for the tree of life, leveraging the unique properties of biology captured by TreeOfLife-10M, namely the abundance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. We rigorously benchmark our approach on diverse fine-grained biology classification tasks and find that BioCLIP consistently and substantially outperforms existing baselines (by 16% to 17% absolute). Intrinsic evaluation reveals that BioCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability. https://imageomics.github.io/bioclip has models, data and code.",
        "page": "http://arxiv.org/abs/2311.18803",
        "pdf": "http://arxiv.org/pdf/2311.18803.pdf"
    },
    {
        "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
        "author": "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qing-Long Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai",
        "abstract": "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models. Code and models are available at https://github.com/OpenGVLab/InternVL.",
        "page": "http://arxiv.org/abs/2312.14238",
        "pdf": "http://arxiv.org/pdf/2312.14238.pdf"
    },
    {
        "title": "Neural Redshift: Random Networks are not Random Functions",
        "author": "Damien Teney, Armand Nicolicioiu, Valentin Hartmann, Ehsan Abbasnejad",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection",
        "author": "Gang Zhang, Chen Junnan, Guohuan Gao, Jianmin Li, Si Liu, Xiaolin Hu",
        "abstract": "LiDAR-based 3D object detection plays an essential role in autonomous driving. Existing high-performing 3D object detectors usually build dense feature maps in the backbone network and prediction head. However, the computational costs introduced by the dense feature maps grow quadratically as the perception range increases, making these models hard to scale up to long-range detection. Some recent works have attempted to construct fully sparse detectors to solve this issue; nevertheless, the resulting models either rely on a complex multi-stage pipeline or exhibit inferior performance. In this work, we propose SAFDNet, a straightforward yet highly effective architecture, tailored for fully sparse 3D object detection. In SAFDNet, an adaptive feature diffusion strategy is designed to address the center feature missing problem. We conducted extensive experiments on Waymo Open, nuScenes, and Argoverse2 datasets. SAFDNet performed slightly better than the previous SOTA on the first two datasets but much better on the last dataset, which features long-range detection, verifying the efficacy of SAFDNet in scenarios where long-range detection is required. Notably, on Argoverse2, SAFDNet surpassed the previous best hybrid detector HEDNet by 2.6% mAP while being 2.1x faster, and yielded 2.1% mAP gains over the previous best sparse detector FSDv2 while being 1.3x faster. The code will be available at https://github.com/zhanggang001/HEDNet.",
        "page": "http://arxiv.org/abs/2403.05817",
        "pdf": "http://arxiv.org/pdf/2403.05817.pdf"
    },
    {
        "title": "NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using Heuristics-Guided Segmentation",
        "author": "Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rich Human Feedback for Text-to-Image Generation",
        "author": "Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dvijotham, Katherine Collins, Yiwen Luo, Yang Li, Kai Kohlhoff, Deepak Ramachandran, Vidhya Navalpakkam",
        "abstract": "Recent Text-to-Image (T2I) generation models such as Stable Diffusion and Imagen have made significant progress in generating high-resolution images based on text descriptions. However, many generated images still suffer from issues such as artifacts/implausibility, misalignment with text descriptions, and low aesthetic quality. Inspired by the success of Reinforcement Learning with Human Feedback (RLHF) for large language models, prior works collected human-provided scores as feedback on generated images and trained a reward model to improve the T2I generation. In this paper, we enrich the feedback signal by (i) marking image regions that are implausible or misaligned with the text, and (ii) annotating which words in the text prompt are misrepresented or missing on the image. We collect such rich human feedback on 18K generated images (RichHF-18K) and train a multimodal transformer to predict the rich feedback automatically. We show that the predicted rich human feedback can be leveraged to improve image generation, for example, by selecting high-quality training data to finetune and improve the generative models, or by creating masks with predicted heatmaps to inpaint the problematic regions. Notably, the improvements generalize to models (Muse) beyond those used to generate the images on which human feedback data were collected (Stable Diffusion variants). The RichHF-18K data set will be released in our GitHub repository: https://github.com/google-research/google-research/tree/master/richhf_18k.",
        "page": "http://arxiv.org/abs/2312.10240",
        "pdf": "http://arxiv.org/pdf/2312.10240.pdf"
    },
    {
        "title": "URHand: Universal Relightable Hands",
        "author": "Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, He Wen, Lucas Evans, Bo Peng, Julia Buffalini, Autumn Trimble, Kevyn McPhail, Melissa Schoeller, Shoou-I Yu, Javier Romero, Michael Zollhoefer, Yaser Sheikh, Ziwei Liu, Shunsuke Saito",
        "abstract": "Existing photorealistic relightable hand models require extensive identity-specific observations in different views, poses, and illuminations, and face challenges in generalizing to natural illuminations and novel identities. To bridge this gap, we present URHand, the first universal relightable hand model that generalizes across viewpoints, poses, illuminations, and identities. Our model allows few-shot personalization using images captured with a mobile phone, and is ready to be photorealistically rendered under novel illuminations. To simplify the personalization process while retaining photorealism, we build a powerful universal relightable prior based on neural relighting from multi-view images of hands captured in a light stage with hundreds of identities. The key challenge is scaling the cross-identity training while maintaining personalized fidelity and sharp details without compromising generalization under natural illuminations. To this end, we propose a spatially varying linear lighting model as the neural renderer that takes physics-inspired shading as input feature. By removing non-linear activations and bias, our specifically designed lighting model explicitly keeps the linearity of light transport. This enables single-stage training from light-stage data while generalizing to real-time rendering under arbitrary continuous illuminations across diverse identities. In addition, we introduce the joint learning of a physically based model and our neural relighting model, which further improves fidelity and generalization. Extensive experiments show that our approach achieves superior performance over existing methods in terms of both quality and generalizability. We also demonstrate quick personalization of URHand from a short phone scan of an unseen identity.",
        "page": "http://arxiv.org/abs/2401.05334",
        "pdf": "http://arxiv.org/pdf/2401.05334.pdf"
    },
    {
        "title": "Learning to Segment Referred Objects from Narrated Egocentric Videos",
        "author": "Yuhan Shen, Huiyu Wang, Xitong Yang, Matt Feiszli, Ehsan Elhamifar, Lorenzo Torresani, Effrosyni Mavroudi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Transcriptomics-guided Slide Representation Learning in Computational Pathology",
        "author": "Guillaume Jaume, Lukas Oldenburg, Anurag Vaidya, Richard J. Chen, Drew F. K. Williamson, Thomas Peeters, Andrew Song, Faisal Mahmood",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffusionLight: Light Probes for Free by Painting a Chrome Ball",
        "author": "Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Varun Jampani, Amit Raj, Pramook Khungurn, Supasorn Suwajanakorn",
        "abstract": "We present a simple yet effective technique to estimate lighting in a single input image. Current techniques rely heavily on HDR panorama datasets to train neural networks to regress an input with limited field-of-view to a full environment map. However, these approaches often struggle with real-world, uncontrolled settings due to the limited diversity and size of their datasets. To address this problem, we leverage diffusion models trained on billions of standard images to render a chrome ball into the input image. Despite its simplicity, this task remains challenging: the diffusion models often insert incorrect or inconsistent objects and cannot readily generate images in HDR format. Our research uncovers a surprising relationship between the appearance of chrome balls and the initial diffusion noise map, which we utilize to consistently generate high-quality chrome balls. We further fine-tune an LDR diffusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure bracketing for HDR light estimation. Our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios.",
        "page": "http://arxiv.org/abs/2312.09168",
        "pdf": "http://arxiv.org/pdf/2312.09168.pdf"
    },
    {
        "title": "Describing Differences in Image Sets with Natural Language",
        "author": "Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph Gonzalez, Serena Yeung",
        "abstract": "How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two $\\textbf{sets}$ of images, which we term Set Difference Captioning. This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.",
        "page": "http://arxiv.org/abs/2312.02974",
        "pdf": "http://arxiv.org/pdf/2312.02974.pdf"
    },
    {
        "title": "Rethinking Inductive Biases for Surface Normal Estimation",
        "author": "Gwangbin Bae, Andrew J. Davison",
        "abstract": "Despite the growing demand for accurate surface normal estimation models, existing methods use general-purpose dense prediction models, adopting the same inductive biases as other tasks. In this paper, we discuss the inductive biases needed for surface normal estimation and propose to (1) utilize the per-pixel ray direction and (2) encode the relationship between neighboring surface normals by learning their relative rotation. The proposed method can generate crisp - yet, piecewise smooth - predictions for challenging in-the-wild images of arbitrary resolution and aspect ratio. Compared to a recent ViT-based state-of-the-art model, our method shows a stronger generalization ability, despite being trained on an orders of magnitude smaller dataset. The code is available at https://github.com/baegwangbin/DSINE.",
        "page": "http://arxiv.org/abs/2403.00712",
        "pdf": "http://arxiv.org/pdf/2403.00712.pdf"
    },
    {
        "title": "Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation",
        "author": "Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, Konrad Schindler",
        "abstract": "Monocular depth estimation is a fundamental computer vision task. Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough. The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures. Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains. This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation. We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge. The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data. It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases. Project page: https://marigoldmonodepth.github.io.",
        "page": "http://arxiv.org/abs/2312.02145",
        "pdf": "http://arxiv.org/pdf/2312.02145.pdf"
    },
    {
        "title": "IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection",
        "author": "Junbo Yin, Wenguan Wang, Runnan Chen, Wei Li, Ruigang Yang, Pascal Frossard, Jianbing Shen",
        "abstract": "Bird's eye view (BEV) representation has emerged as a dominant solution for describing 3D space in autonomous driving scenarios. However, objects in the BEV representation typically exhibit small sizes, and the associated point cloud context is inherently sparse, which leads to great challenges for reliable 3D perception. In this paper, we propose IS-Fusion, an innovative multimodal fusion framework that jointly captures the Instance- and Scene-level contextual information. IS-Fusion essentially differs from existing approaches that only focus on the BEV scene-level fusion by explicitly incorporating instance-level multimodal information, thus facilitating the instance-centric tasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF) module and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid and Grid-to-Region transformers to capture the multimodal scene context at different granularities. IGF mines instance candidates, explores their relationships, and aggregates the local multimodal context for each instance. These instances then serve as guidance to enhance the scene feature and yield an instance-aware BEV representation. On the challenging nuScenes benchmark, IS-Fusion outperforms all the published multimodal works to date. Code is available at: https://github.com/yinjunbo/IS-Fusion.",
        "page": "http://arxiv.org/abs/2403.15241",
        "pdf": "http://arxiv.org/pdf/2403.15241.pdf"
    },
    {
        "title": "The Manga Whisperer: Automatically Generating Transcriptions for Comics",
        "author": "Ragav Sachdeva, Andrew Zisserman",
        "abstract": "In the past few decades, Japanese comics, commonly referred to as Manga, have transcended both cultural and linguistic boundaries to become a true worldwide sensation. Yet, the inherent reliance on visual cues and illustration within manga renders it largely inaccessible to individuals with visual impairments. In this work, we seek to address this substantial barrier, with the aim of ensuring that manga can be appreciated and actively engaged by everyone. Specifically, we tackle the problem of diarisation i.e. generating a transcription of who said what and when, in a fully automatic way. To this end, we make the following contributions: (1) we present a unified model, Magi, that is able to (a) detect panels, text boxes and character boxes, (b) cluster characters by identity (without knowing the number of clusters apriori), and (c) associate dialogues to their speakers; (2) we propose a novel approach that is able to sort the detected text boxes in their reading order and generate a dialogue transcript; (3) we annotate an evaluation benchmark for this task using publicly available [English] manga pages. The code, evaluation datasets and the pre-trained model can be found at: https://github.com/ragavsachdeva/magi.",
        "page": "http://arxiv.org/abs/2401.10224",
        "pdf": "http://arxiv.org/pdf/2401.10224.pdf"
    },
    {
        "title": "PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics",
        "author": "Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CORE-MPI: Consistency Object Removal with Embedding MultiPlane Image",
        "author": "Donggeun Yoon, Donghyeon Cho",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Open-World Semantic Segmentation Including Class Similarity",
        "author": "Matteo Sodano, Federico Magistri, Lucas Nunes, Jens Behley, Cyrill Stachniss",
        "abstract": "Interpreting camera data is key for autonomously acting systems, such as autonomous vehicles. Vision systems that operate in real-world environments must be able to understand their surroundings and need the ability to deal with novel situations. This paper tackles open-world semantic segmentation, i.e., the variant of interpreting image data in which objects occur that have not been seen during training. We propose a novel approach that performs accurate closed-world semantic segmentation and, at the same time, can identify new categories without requiring any additional training data. Our approach additionally provides a similarity measure for every newly discovered class in an image to a known category, which can be useful information in downstream tasks such as planning or mapping. Through extensive experiments, we show that our model achieves state-of-the-art results on classes known from training data as well as for anomaly segmentation and can distinguish between different unknown classes.",
        "page": "http://arxiv.org/abs/2403.07532",
        "pdf": "http://arxiv.org/pdf/2403.07532.pdf"
    },
    {
        "title": "Multi-View Attentive Contextualization for Multi-View 3D Object Detection",
        "author": "Xianpeng Liu, Ce Zheng, Ming Qian, Nan Xue, Chen Chen, Zhebin Zhang, Chen Li, Tianfu Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OCAI: Improving Optical Flow Estimation by Occlusion and Consistency Aware Interpolation",
        "author": "Jisoo Jeong, Hong Cai, Risheek Garrepalli, Jamie Lin, Munawar Hayat, Fatih Porikli",
        "abstract": "The scarcity of ground-truth labels poses one major challenge in developing optical flow estimation models that are both generalizable and robust. While current methods rely on data augmentation, they have yet to fully exploit the rich information available in labeled video sequences. We propose OCAI, a method that supports robust frame interpolation by generating intermediate video frames alongside optical flows in between. Utilizing a forward warping approach, OCAI employs occlusion awareness to resolve ambiguities in pixel values and fills in missing values by leveraging the forward-backward consistency of optical flows. Additionally, we introduce a teacher-student style semi-supervised learning method on top of the interpolated frames. Using a pair of unlabeled frames and the teacher model's predicted optical flow, we generate interpolated frames and flows to train a student model. The teacher's weights are maintained using Exponential Moving Averaging of the student. Our evaluations demonstrate perceptually superior interpolation quality and enhanced optical flow accuracy on established benchmarks such as Sintel and KITTI.",
        "page": "http://arxiv.org/abs/2403.18092",
        "pdf": "http://arxiv.org/pdf/2403.18092.pdf"
    },
    {
        "title": "One-Shot Open Affordance Learning with Foundation Models",
        "author": "Gen Li, Deqing Sun, Laura Sevilla-Lara, Varun Jampani",
        "abstract": "We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances. While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances. To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess the potential for data-limited affordance learning. We then propose a vision-language framework with simple and effective designs that boost the alignment between visual features and affordance text embeddings. Experiments on two affordance segmentation benchmarks show that the proposed method outperforms state-of-the-art models with less than 1% of the full training data, and exhibits reasonable generalization capability on unseen objects and affordances.",
        "page": "http://arxiv.org/abs/2311.17776",
        "pdf": "http://arxiv.org/pdf/2311.17776.pdf"
    },
    {
        "title": "DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling",
        "author": "Linqi Zhou, Andy Shih, Chenlin Meng, Stefano Ermon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adaptive Softassign via Hadamard-Equipped Sinkhorn",
        "author": "Binrui Shen, Qiang Niu, Shengxin Zhu",
        "abstract": "Softassign is a pivotal method in graph matching and other learning tasks. Many softassign-based algorithms exhibit performance sensitivity to a parameter in the softassign. However, tuning the parameter is challenging and almost done empirically. This paper proposes an adaptive softassign method for graph matching by analyzing the relationship between the objective score and the parameter. This method can automatically tune the parameter based on a given error bound to guarantee accuracy. The Hadamard-Equipped Sinkhorn formulas introduced in this study significantly enhance the efficiency and stability of the adaptive softassign. Moreover, these formulas can also be used in optimal transport problems. The resulting adaptive softassign graph matching algorithm enjoys significantly higher accuracy than previous state-of-the-art large graph matching algorithms while maintaining comparable efficiency.",
        "page": "http://arxiv.org/abs/2309.13855",
        "pdf": "http://arxiv.org/pdf/2309.13855.pdf"
    },
    {
        "title": "DisCo: Disentangled Control for Realistic Human Dance Generation",
        "author": "Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diffusion Time-step Curriculum for One Image to 3D Generation",
        "author": "YI Xuanyu, Zike Wu, Qingshan Xu, Pan Zhou, Joo Lim, Hanwang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior",
        "author": "Zike Wu, Pan Zhou, YI Xuanyu, Xiaoding Yuan, Hanwang Zhang",
        "abstract": "Score distillation sampling (SDS) and its variants have greatly boosted the development of text-to-3D generation, but are vulnerable to geometry collapse and poor textures yet. To solve this issue, we first deeply analyze the SDS and find that its distillation sampling process indeed corresponds to the trajectory sampling of a stochastic differential equation (SDE): SDS samples along an SDE trajectory to yield a less noisy sample which then serves as a guidance to optimize a 3D model. However, the randomness in SDE sampling often leads to a diverse and unpredictable sample which is not always less noisy, and thus is not a consistently correct guidance, explaining the vulnerability of SDS. Since for any SDE, there always exists an ordinary differential equation (ODE) whose trajectory sampling can deterministically and consistently converge to the desired target point as the SDE, we propose a novel and effective \"Consistent3D\" method that explores the ODE deterministic sampling prior for text-to-3D generation. Specifically, at each training iteration, given a rendered image by a 3D model, we first estimate its desired 3D score function by a pre-trained 2D diffusion model, and build an ODE for trajectory sampling. Next, we design a consistency distillation sampling loss which samples along the ODE trajectory to generate two adjacent samples and uses the less noisy sample to guide another more noisy one for distilling the deterministic prior into the 3D model. Experimental results show the efficacy of our Consistent3D in generating high-fidelity and diverse 3D objects and large-scale scenes, as shown in Fig. 1. The codes are available at https://github.com/sail-sg/Consistent3D.",
        "page": "http://arxiv.org/abs/2401.09050",
        "pdf": "http://arxiv.org/pdf/2401.09050.pdf"
    },
    {
        "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
        "author": "Leigang Qu, Wenjie Wang, Yongqi Li, Hanwang Zhang, Liqiang Nie, Tat-seng Chua",
        "abstract": "Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models.",
        "page": "http://arxiv.org/abs/2403.04321",
        "pdf": "http://arxiv.org/pdf/2403.04321.pdf"
    },
    {
        "title": "Doubly Abductive Counterfactual Inference for Text-based Image Editing",
        "author": "Xue Song, Jiequan Cui, Hanwang Zhang, Jingjing Chen, Richang Hong, Yu-Gang Jiang",
        "abstract": "We study text-based image editing (TBIE) of a single image by counterfactual inference because it is an elegant formulation to precisely address the requirement: the edited image should retain the fidelity of the original one. Through the lens of the formulation, we find that the crux of TBIE is that existing techniques hardly achieve a good trade-off between editability and fidelity, mainly due to the overfitting of the single-image fine-tuning. To this end, we propose a Doubly Abductive Counterfactual inference framework (DAC). We first parameterize an exogenous variable as a UNet LoRA, whose abduction can encode all the image details. Second, we abduct another exogenous variable parameterized by a text encoder LoRA, which recovers the lost editability caused by the overfitted first abduction. Thanks to the second abduction, which exclusively encodes the visual transition from post-edit to pre-edit, its inversion -- subtracting the LoRA -- effectively reverts pre-edit back to post-edit, thereby accomplishing the edit. Through extensive experiments, our DAC achieves a good trade-off between editability and fidelity. Thus, we can support a wide spectrum of user editing intents, including addition, removal, manipulation, replacement, style transfer, and facial change, which are extensively validated in both qualitative and quantitative evaluations. Codes are in https://github.com/xuesong39/DAC.",
        "page": "http://arxiv.org/abs/2403.02981",
        "pdf": "http://arxiv.org/pdf/2403.02981.pdf"
    },
    {
        "title": "Brain Decodes Deep Nets",
        "author": "Huzheng Yang, James Gee, Jianbo Shi",
        "abstract": "We developed a tool for visualizing and analyzing large pre-trained vision models by mapping them onto the brain, thus exposing their hidden inside. Our innovation arises from a surprising usage of brain encoding: predicting brain fMRI measurements in response to images. We report two findings. First, explicit mapping between the brain and deep-network features across dimensions of space, layers, scales, and channels is crucial. This mapping method, FactorTopy, is plug-and-play for any deep-network; with it, one can paint a picture of the network onto the brain (literally!). Second, our visualization shows how different training methods matter: they lead to remarkable differences in hierarchical organization and scaling behavior, growing with more data or network capacity. It also provides insight into fine-tuning: how pre-trained models change when adapting to small datasets. We found brain-like hierarchically organized network suffer less from catastrophic forgetting after fine-tuned.",
        "page": "http://arxiv.org/abs/2312.01280",
        "pdf": "http://arxiv.org/pdf/2312.01280.pdf"
    },
    {
        "title": "Question Aware Vision Transformer for Multimodal Reasoning",
        "author": "Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, Ron Litman",
        "abstract": "Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.",
        "page": "http://arxiv.org/abs/2402.05472",
        "pdf": "http://arxiv.org/pdf/2402.05472.pdf"
    },
    {
        "title": "Instance-aware Contrastive Learning for Occluded Human Mesh Reconstruction",
        "author": "Mi-Gyeong Gwon, Gi-Mun Um, Won-Sik Cheong, Wonjun Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PointOBB: Learning Oriented Object Detection via Single Point Supervision",
        "author": "Junwei Luo, Xue Yang, Yi Yu, Qingyun Li, Junchi Yan, Yansheng Li",
        "abstract": "Single point-supervised object detection is gaining attention due to its cost-effectiveness. However, existing approaches focus on generating horizontal bounding boxes (HBBs) while ignoring oriented bounding boxes (OBBs) commonly used for objects in aerial images. This paper proposes PointOBB, the first single Point-based OBB generation method, for oriented object detection. PointOBB operates through the collaborative utilization of three distinctive views: an original view, a resized view, and a rotated/flipped (rot/flp) view. Upon the original view, we leverage the resized and rot/flp views to build a scale augmentation module and an angle acquisition module, respectively. In the former module, a Scale-Sensitive Consistency (SSC) loss is designed to enhance the deep network's ability to perceive the object scale. For accurate object angle predictions, the latter module incorporates self-supervised learning to predict angles, which is associated with a scale-guided Dense-to-Sparse (DS) matching strategy for aggregating dense angles corresponding to sparse objects. The resized and rot/flp views are switched using a progressive multi-view switching strategy during training to achieve coupled optimization of scale and angle. Experimental results on the DIOR-R and DOTA-v1.0 datasets demonstrate that PointOBB achieves promising performance, and significantly outperforms potential point-supervised baselines.",
        "page": "http://arxiv.org/abs/2311.14757",
        "pdf": "http://arxiv.org/pdf/2311.14757.pdf"
    },
    {
        "title": "HomoFormer: Homogenized Transformer for Image Shadow Removal",
        "author": "Jie Xiao, Xueyang Fu, Yurui Zhu, Dong Li, Jie Huang, Kai Zhu, Zheng-Jun Zha",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LiDAR-based Person Re-identification",
        "author": "Wenxuan Guo, Zhiyu Pan, Yingping Liang, Ziheng Xi, Zhi Chen Zhong, Jianjiang Feng, Jie Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Task-Customized Mixture of Adapters for General Image Fusion",
        "author": "Pengfei Zhu, Yang Sun, Bing Cao, Qinghua Hu",
        "abstract": "General image fusion aims at integrating important information from multi-source images. However, due to the significant cross-task gap, the respective fusion mechanism varies considerably in practice, resulting in limited performance across subtasks. To handle this problem, we propose a novel task-customized mixture of adapters (TC-MoA) for general image fusion, adaptively prompting various fusion tasks in a unified model. We borrow the insight from the mixture of experts (MoE), taking the experts as efficient tuning adapters to prompt a pre-trained foundation model. These adapters are shared across different tasks and constrained by mutual information regularization, ensuring compatibility with different tasks while complementarity for multi-source images. The task-specific routing networks customize these adapters to extract task-specific information from different sources with dynamic dominant intensity, performing adaptive visual feature prompt fusion. Notably, our TC-MoA controls the dominant intensity bias for different fusion tasks, successfully unifying multiple fusion tasks in a single model. Extensive experiments show that TC-MoA outperforms the competing approaches in learning commonalities while retaining compatibility for general image fusion (multi-modal, multi-exposure, and multi-focus), and also demonstrating striking controllability on more generalization experiments. The code is available at https://github.com/YangSun22/TC-MoA .",
        "page": "http://arxiv.org/abs/2403.12494",
        "pdf": "http://arxiv.org/pdf/2403.12494.pdf"
    },
    {
        "title": "Boosting Image Restoration via Priors from Pre-trained Models",
        "author": "Xiaogang Xu, Shu Kong, Tao Hu, Zhe Liu, Hujun Bao",
        "abstract": "Pre-trained models with large-scale training data, such as CLIP and Stable Diffusion, have demonstrated remarkable performance in various high-level computer vision tasks such as image understanding and generation from language descriptions. Yet, their potential for low-level tasks such as image restoration remains relatively unexplored. In this paper, we explore such models to enhance image restoration. As off-the-shelf features (OSF) from pre-trained models do not directly serve image restoration, we propose to learn an additional lightweight module called Pre-Train-Guided Refinement Module (PTG-RM) to refine restoration results of a target restoration network with OSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention (PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations, while PTG-CSA enhances spatial-channel attention for restoration-related learning. Extensive experiments demonstrate that PTG-RM, with its compact size ($<$1M parameters), effectively enhances restoration performance of various models across different tasks, including low-light enhancement, deraining, deblurring, and denoising.",
        "page": "http://arxiv.org/abs/2403.06793",
        "pdf": "http://arxiv.org/pdf/2403.06793.pdf"
    },
    {
        "title": "SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery",
        "author": "Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, HUIMEI HE, Jian Wang, Jingdong Chen, Ming Yang, Yongjun Zhang, Yansheng Li",
        "abstract": "Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense potential towards a generic model for Earth Observation. Nevertheless, these works primarily focus on a single modality without temporal and geo-context modeling, hampering their capabilities for diverse tasks. In this study, we present SkySense, a generic billion-scale model, pre-trained on a curated multi-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal sequences. SkySense incorporates a factorized multi-modal spatiotemporal encoder taking temporal sequences of optical and Synthetic Aperture Radar (SAR) data as input. This encoder is pre-trained by our proposed Multi-Granularity Contrastive Learning to learn representations across different modal and spatial granularities. To further enhance the RSI representations by the geo-context clue, we introduce Geo-Context Prototype Learning to learn region-aware prototypes upon RSI's multi-modal spatiotemporal features. To our best knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules can be flexibly combined or used individually to accommodate various tasks. It demonstrates remarkable generalization capabilities on a thorough evaluation encompassing 16 datasets over 7 tasks, from single- to multi-modal, static to temporal, and classification to localization. SkySense surpasses 18 recent RSFMs in all test scenarios. Specifically, it outperforms the latest models such as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and 3.61% on average respectively. We will release the pre-trained weights to facilitate future research and Earth Observation applications.",
        "page": "http://arxiv.org/abs/2312.10115",
        "pdf": "http://arxiv.org/pdf/2312.10115.pdf"
    },
    {
        "title": "Selective, Interpretable and Motion Consistent Privacy Attribute Obfuscation for Action Recognition",
        "author": "Filip Ilic, He Zhao, Thomas Pock, Richard P. Wildes",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Recipe for Scaling up Text-to-Video Generation with Text-free Videos",
        "author": "Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, Nong Sang",
        "abstract": "Diffusion-based text-to-video generation has witnessed impressive progress in the past year yet still falls behind text-to-image generation. One of the key reasons is the limited scale of publicly available data (e.g., 10M video-text pairs in WebVid10M vs. 5B image-text pairs in LAION), considering the high cost of video captioning. Instead, it could be far easier to collect unlabeled clips from video platforms like YouTube. Motivated by this, we come up with a novel text-to-video generation framework, termed TF-T2V, which can directly learn with text-free videos. The rationale behind is to separate the process of text decoding from that of temporal modeling. To this end, we employ a content branch and a motion branch, which are jointly optimized with weights shared. Following such a pipeline, we study the effect of doubling the scale of training set (i.e., video-only WebVid10M) with some randomly collected text-free videos and are encouraged to observe the performance improvement (FID from 9.67 to 8.19 and FVD from 484 to 441), demonstrating the scalability of our approach. We also find that our model could enjoy sustainable performance gain (FID from 8.19 to 7.64 and FVD from 441 to 366) after reintroducing some text labels for training. Finally, we validate the effectiveness and generalizability of our ideology on both native text-to-video generation and compositional video synthesis paradigms. Code and models will be publicly available at https://tf-t2v.github.io/.",
        "page": "http://arxiv.org/abs/2312.15770",
        "pdf": "http://arxiv.org/pdf/2312.15770.pdf"
    },
    {
        "title": "3D Facial Expressions through Analysis-by-Neural-Synthesis",
        "author": "George Retsinas, Panagiotis Filntisis, Radek Danecek, Victoria Abrevaya, Anastasios Roussos, Timo Bolkart, Petros Maragos",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "InstructVideo: Instructing Video Diffusion Models with Human Feedback",
        "author": "Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni",
        "abstract": "Diffusion models have emerged as the de facto paradigm for video generation. However, their reliance on web-scale data of varied quality often yields results that are visually unappealing and misaligned with the textual prompts. To tackle this problem, we propose InstructVideo to instruct text-to-video diffusion models with human feedback by reward fine-tuning. InstructVideo has two key ingredients: 1) To ameliorate the cost of reward fine-tuning induced by generating through the full DDIM sampling chain, we recast reward fine-tuning as editing. By leveraging the diffusion process to corrupt a sampled video, InstructVideo requires only partial inference of the DDIM sampling chain, reducing fine-tuning cost while improving fine-tuning efficiency. 2) To mitigate the absence of a dedicated video reward model for human preferences, we repurpose established image reward models, e.g., HPSv2. To this end, we propose Segmental Video Reward, a mechanism to provide reward signals based on segmental sparse sampling, and Temporally Attenuated Reward, a method that mitigates temporal modeling degradation during fine-tuning. Extensive experiments, both qualitative and quantitative, validate the practicality and efficacy of using image reward models in InstructVideo, significantly enhancing the visual quality of generated videos without compromising generalization capabilities. Code and models will be made publicly available.",
        "page": "http://arxiv.org/abs/2312.12490",
        "pdf": "http://arxiv.org/pdf/2312.12490.pdf"
    },
    {
        "title": "Are Conventional SNNs Really Efficient? A Perspective from Network Quantization",
        "author": "Guobin Shen, Dongcheng Zhao, Tenglong Li, Jindong Li, Yi Zeng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DREAM: Diffusion Rectification and Estimation-Adaptive Models",
        "author": "Jinxin Zhou, Tianyu Ding, Tianyi Chen, Jiachen Jiang, Ilya Zharkov, Zhihui Zhu, Luming Liang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NOPE: Novel Object Pose Estimation from a Single Image",
        "author": "Van Nguyen Nguyen, Thibault Groueix, Georgy Ponimatkin, Yinlin Hu, Renaud Marlet, Mathieu Salzmann, Vincent Lepetit",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence",
        "author": "Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, Vincent Lepetit",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach",
        "author": "Wei Dong, Xing Zhang, Bihui Chen, Dawei Yan, Zhijun Lin, Qingsen Yan, Peng Wang, Yang Yang",
        "abstract": "Parameter-efficient fine-tuning for pre-trained Vision Transformers aims to adeptly tailor a model to downstream tasks by learning a minimal set of new adaptation parameters while preserving the frozen majority of pre-trained parameters. Striking a balance between retaining the generalizable representation capacity of the pre-trained model and acquiring task-specific features poses a key challenge. Currently, there is a lack of focus on guiding this delicate trade-off. In this study, we approach the problem from the perspective of Singular Value Decomposition (SVD) of pre-trained parameter matrices, providing insights into the tuning dynamics of existing methods. Building upon this understanding, we propose a Residual-based Low-Rank Rescaling (RLRR) fine-tuning strategy. This strategy not only enhances flexibility in parameter tuning but also ensures that new parameters do not deviate excessively from the pre-trained model through a residual design. Extensive experiments demonstrate that our method achieves competitive performance across various downstream image classification tasks, all while maintaining comparable new parameters. We believe this work takes a step forward in offering a unified perspective for interpreting existing methods and serves as motivation for the development of new approaches that move closer to effectively considering the crucial trade-off mentioned above. Our code is available at \\href{https://github.com/zstarN70/RLRR.git}{https://github.com/zstarN70/RLRR.git}.",
        "page": "http://arxiv.org/abs/2403.19067",
        "pdf": "http://arxiv.org/pdf/2403.19067.pdf"
    },
    {
        "title": "Mind The Edge: Refining Depth Edges in Sparsely-Supervised Monocular Depth Estimation",
        "author": "Lior Talker, Aviad Cohen, Erez Yosef, Alexandra Dana, Michael Dinerstein",
        "abstract": "Monocular Depth Estimation (MDE) is a fundamental problem in computer vision with numerous applications. Recently, LIDAR-supervised methods have achieved remarkable per-pixel depth accuracy in outdoor scenes. However, significant errors are typically found in the proximity of depth discontinuities, i.e., depth edges, which often hinder the performance of depth-dependent applications that are sensitive to such inaccuracies, e.g., novel view synthesis and augmented reality. Since direct supervision for the location of depth edges is typically unavailable in sparse LIDAR-based scenes, encouraging the MDE model to produce correct depth edges is not straightforward. To the best of our knowledge this paper is the first attempt to address the depth edges issue for LIDAR-supervised scenes. In this work we propose to learn to detect the location of depth edges from densely-supervised synthetic data, and use it to generate supervision for the depth edges in the MDE training. To quantitatively evaluate our approach, and due to the lack of depth edges GT in LIDAR-based scenes, we manually annotated subsets of the KITTI and the DDAD datasets with depth edges ground truth. We demonstrate significant gains in the accuracy of the depth edges with comparable per-pixel depth accuracy on several challenging datasets. Code and datasets are available at \\url{https://github.com/liortalker/MindTheEdge}.",
        "page": "http://arxiv.org/abs/2212.05315",
        "pdf": "http://arxiv.org/pdf/2212.05315.pdf"
    },
    {
        "title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding",
        "author": "Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou",
        "abstract": "This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long-form video comprehension tasks and satisfy realistic user requirements.",
        "page": "http://arxiv.org/abs/2312.02051",
        "pdf": "http://arxiv.org/pdf/2312.02051.pdf"
    },
    {
        "title": "RCBEVDet: Radar-camera Fusion in Bird\u2019s Eye View for 3D Object Detection",
        "author": "Zhiwei Lin, Zhe Liu, Zhongyu Xia, Xinhao Wang, Yongtao Wang, Shengxiang Qi, Yang Dong, Nan Dong, Le Zhang, Ce Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image",
        "author": "Minje Kim, Tae-Kyun Kim",
        "abstract": "Creating personalized hand avatars is important to offer a realistic experience to users on AR / VR platforms. While most prior studies focused on reconstructing 3D hand shapes, some recent work has tackled the reconstruction of hand textures on top of shapes. However, these methods are often limited to capturing pixels on the visible side of a hand, requiring diverse views of the hand in a video or multiple images as input. In this paper, we propose a novel method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the first end-to-end trainable method for relightable, pose-free texture reconstruction of two interacting hands taking only a single RGB image, by three novel components: 1) bi-directional (left $\\leftrightarrow$ right) texture reconstruction using the texture symmetry of left / right hands, 2) utilizing a texture parametric model for hand texture recovery, and 3) the overall coarse-to-fine stage pipeline for reconstructing personalized texture of two interacting hands. BiTT first estimates the scene light condition and albedo image from an input image, then reconstructs the texture of both hands through the texture parametric model and bi-directional texture reconstructor. In experiments using InterHand2.6M and RGB2Hands datasets, our method significantly outperforms state-of-the-art hand texture reconstruction methods quantitatively and qualitatively. The code is available at https://github.com/yunminjin2/BiTT",
        "page": "http://arxiv.org/abs/2403.08262",
        "pdf": "http://arxiv.org/pdf/2403.08262.pdf"
    },
    {
        "title": "Commonsense Prototype for Outdoor Unsupervised 3D Object Detection",
        "author": "Hai Wu, Shijia Zhao, Xun Huang, Chenglu Wen, Xin Li, Cheng Wang",
        "abstract": "The prevalent approaches of unsupervised 3D object detection follow cluster-based pseudo-label generation and iterative self-training processes. However, the challenge arises due to the sparsity of LiDAR scans, which leads to pseudo-labels with erroneous size and position, resulting in subpar detection performance. To tackle this problem, this paper introduces a Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object detection. CPD first constructs Commonsense Prototype (CProto) characterized by high-quality bounding box and dense points, based on commonsense intuition. Subsequently, CPD refines the low-quality pseudo-labels by leveraging the size prior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely scanned objects by the geometric knowledge from CProto. CPD outperforms state-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD), PandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD and testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on easy and moderate car classes, respectively. These achievements position CPD in close proximity to fully supervised detectors, highlighting the significance of our method. The code will be available at https://github.com/hailanyi/CPD.",
        "page": "http://arxiv.org/abs/2404.16493",
        "pdf": "http://arxiv.org/pdf/2404.16493.pdf"
    },
    {
        "title": "SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation",
        "author": "Zhixuan Liu, Peter Schaldenbrand, Beverley-Claire Okogwu, Wenxuan Peng, Youngsik Yun, Andrew Hundt, Jihie Kim, Jean Oh",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AUEditNet: Dual-Branch Facial Action Unit Intensity Manipulation with Implicit Disentanglement",
        "author": "Shiwei Jin, Zhen Wang, Lei Wang, Peng Liu, Ning Bi, Truong Nguyen",
        "abstract": "Facial action unit (AU) intensity plays a pivotal role in quantifying fine-grained expression behaviors, which is an effective condition for facial expression manipulation. However, publicly available datasets containing intensity annotations for multiple AUs remain severely limited, often featuring a restricted number of subjects. This limitation places challenges to the AU intensity manipulation in images due to disentanglement issues, leading researchers to resort to other large datasets with pretrained AU intensity estimators for pseudo labels. In addressing this constraint and fully leveraging manual annotations of AU intensities for precise manipulation, we introduce AUEditNet. Our proposed model achieves impressive intensity manipulation across 12 AUs, trained effectively with only 18 subjects. Utilizing a dual-branch architecture, our approach achieves comprehensive disentanglement of facial attributes and identity without necessitating additional loss functions or implementing with large batch sizes. This approach offers a potential solution to achieve desired facial attribute editing despite the dataset's limited subject count. Our experiments demonstrate AUEditNet's superior accuracy in editing AU intensities, affirming its capability in disentangling facial attributes and identity within a limited subject pool. AUEditNet allows conditioning by either intensity values or target images, eliminating the need for constructing AU combinations for specific facial expression synthesis. Moreover, AU intensity estimation, as a downstream task, validates the consistency between real and edited images, confirming the effectiveness of our proposed AU intensity manipulation method.",
        "page": "http://arxiv.org/abs/2404.05063",
        "pdf": "http://arxiv.org/pdf/2404.05063.pdf"
    },
    {
        "title": "Stationary Representations: Optimally Approximating Compatibility and Implications for Improved Model Replacements",
        "author": "Niccol\u00f2 Biondi, Federico Pernici, Simone Ricci, Alberto Del Bimbo",
        "abstract": "Learning compatible representations enables the interchangeable use of semantic features as models are updated over time. This is particularly relevant in search and retrieval systems where it is crucial to avoid reprocessing of the gallery images with the updated model. While recent research has shown promising empirical evidence, there is still a lack of comprehensive theoretical understanding about learning compatible representations. In this paper, we demonstrate that the stationary representations learned by the $d$-Simplex fixed classifier optimally approximate compatibility representation according to the two inequality constraints of its formal definition. This not only establishes a solid foundation for future works in this line of research but also presents implications that can be exploited in practical learning scenarios. An exemplary application is the now-standard practice of downloading and fine-tuning new pre-trained models. Specifically, we show the strengths and critical issues of stationary representations in the case in which a model undergoing sequential fine-tuning is asynchronously replaced by downloading a better-performing model pre-trained elsewhere. Such a representation enables seamless delivery of retrieval service (i.e., no reprocessing of gallery images) and offers improved performance without operational disruptions during model replacement. Code available at: https://github.com/miccunifi/iamcl2r.",
        "page": "http://arxiv.org/abs/2405.02581",
        "pdf": "http://arxiv.org/pdf/2405.02581.pdf"
    },
    {
        "title": "OneLLM: One Framework to Align All Modalities with Language",
        "author": "Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation",
        "author": "Keqi Chen, vinkle srivastav, Nicolas Padoy",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neighbor Relations Matter in Video Scene Detection",
        "author": "Jiawei Tan, Hongxing Wang, Jiaxin Li, Zhilong Ou, Zhangbin Qian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VS: Reconstructing Clothed 3D Human from Single Image via Vertex Shift",
        "author": "Leyuan Liu, Yuhan Li, Yunqi Gao, Changxin Gao, Yuanyuan Liu, Jingying Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Theoretically Achieving Continuous Representation of Oriented Bounding Boxes",
        "author": "Zikai Xiao, Guo-Ye Yang, Xue Yang, Tai-Jiang Mu, Junchi Yan, Shi-Min Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis",
        "author": "Simon Niedermayr, Josef Stumpfegger, r\u00fcdiger westermann",
        "abstract": "Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian splat representation has been introduced for novel view synthesis from sparse image sets. Making such representations suitable for applications like network streaming and rendering on low-power devices requires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity-aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to $31\\times$ on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to $4\\times$ higher framerates than reported via an optimized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach.",
        "page": "http://arxiv.org/abs/2401.02436",
        "pdf": "http://arxiv.org/pdf/2401.02436.pdf"
    },
    {
        "title": "Pixel-level Semantic Correspondence through Layout-aware Representation Learning and Multi-scale Matching Integration",
        "author": "Yixuan Sun, Zhangyue Yin, Haibo Wang, Yan Wang, Xipeng Qiu, Weifeng Ge, Wenqiang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes",
        "author": "Xingyi Li, Zhiguo Cao, Yizheng Wu, Kewei Wang, Ke Xian, Zhe Wang, Guosheng Lin",
        "abstract": "Current 3D stylization methods often assume static scenes, which violates the dynamic nature of our real world. To address this limitation, we present S-DyRF, a reference-based spatio-temporal stylization method for dynamic neural radiance fields. However, stylizing dynamic 3D scenes is inherently challenging due to the limited availability of stylized reference images along the temporal axis. Our key insight lies in introducing additional temporal cues besides the provided reference. To this end, we generate temporal pseudo-references from the given stylized reference. These pseudo-references facilitate the propagation of style information from the reference to the entire dynamic 3D scene. For coarse style transfer, we enforce novel views and times to mimic the style details present in pseudo-references at the feature level. To preserve high-frequency details, we create a collection of stylized temporal pseudo-rays from temporal pseudo-references. These pseudo-rays serve as detailed and explicit stylization guidance for achieving fine style transfer. Experiments on both synthetic and real-world datasets demonstrate that our method yields plausible stylized results of space-time view synthesis on dynamic 3D scenes.",
        "page": "http://arxiv.org/abs/2403.06205",
        "pdf": "http://arxiv.org/pdf/2403.06205.pdf"
    },
    {
        "title": "Diffusion-based Blind Text Image Super-Resolution",
        "author": "Yuzhe Zhang, jiawei zhang, Hao Li, Zhouxia Wang, Luwei Hou, Dongqing Zou, Liheng Bian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning",
        "author": "Nikhil Singh, Chih-Wei Wu, Iroro Orife, Kalayeh",
        "abstract": "Audiovisual representation learning typically relies on the correspondence between sight and sound. However, there are often multiple audio tracks that can correspond with a visual scene. Consider, for example, different conversations on the same crowded street. The effect of such counterfactual pairs on audiovisual representation learning has not been previously explored. To investigate this, we use dubbed versions of movies to augment cross-modal contrastive learning. Our approach learns to represent alternate audio tracks, differing only in speech content, similarly to the same video. Our results show that dub-augmented training improves performance on a range of auditory and audiovisual tasks, without significantly affecting linguistic task performance overall. We additionally compare this approach to a strong baseline where we remove speech before pretraining, and find that dub-augmented training is more effective, including for paralinguistic and audiovisual tasks where speech removal leads to worse performance. These findings highlight the importance of considering speech variation when learning scene-level audiovisual correspondences and suggest that dubbed audio can be a useful augmentation technique for training audiovisual models toward more robust performance.",
        "page": "http://arxiv.org/abs/2304.05600",
        "pdf": "http://arxiv.org/pdf/2304.05600.pdf"
    },
    {
        "title": "Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates",
        "author": "Ka Chun SHUM, Jaeyeon Kim, Binh-Son Hua, Thanh Nguyen, Sai-Kit Yeung",
        "abstract": "Neural radiance field is an emerging rendering method that generates high-quality multi-view consistent images from a neural scene representation and volume rendering. Although neural radiance field-based techniques are robust for scene reconstruction, their ability to add or remove objects remains limited. This paper proposes a new language-driven approach for object manipulation with neural radiance fields through dataset updates. Specifically, to insert a new foreground object represented by a set of multi-view images into a background radiance field, we use a text-to-image diffusion model to learn and generate combined images that fuse the object of interest into the given background across views. These combined images are then used for refining the background radiance field so that we can render view-consistent images containing both the object and the background. To ensure view consistency, we propose a dataset updates strategy that prioritizes radiance field training with camera views close to the already-trained views prior to propagating the training to remaining views. We show that under the same dataset updates strategy, we can easily adapt our method for object insertion using data from text-to-3D models as well as object removal. Experimental results show that our method generates photorealistic images of the edited scenes, and outperforms state-of-the-art methods in 3D reconstruction and neural radiance field blending.",
        "page": "http://arxiv.org/abs/2309.11281",
        "pdf": "http://arxiv.org/pdf/2309.11281.pdf"
    },
    {
        "title": "Arbitrary Motion Style Transfer with Multi-condition Motion Latent Diffusion Model",
        "author": "Wenfeng Song, Xingliang Jin, Shuai Li, Chenglizhao Chen, Aimin Hao, Xia HOU, Ning Li, Hong Qin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations",
        "author": "Kewei Wang, Yizheng Wu, Jun Cen, Zhiyu Pan, Xingyi Li, Zhe Wang, Zhiguo Cao, Guosheng Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching",
        "author": "Rui Gong, Weide Liu, ZAIWANG GU, Xulei Yang, Jun Cheng",
        "abstract": "Geometric knowledge has been shown to be beneficial for the stereo matching task. However, prior attempts to integrate geometric insights into stereo matching algorithms have largely focused on geometric knowledge from single images while crucial cross-view factors such as occlusion and matching uniqueness have been overlooked. To address this gap, we propose a novel Intra-view and Cross-view Geometric knowledge learning Network (ICGNet), specifically crafted to assimilate both intra-view and cross-view geometric knowledge. ICGNet harnesses the power of interest points to serve as a channel for intra-view geometric understanding. Simultaneously, it employs the correspondences among these points to capture cross-view geometric relationships. This dual incorporation empowers the proposed ICGNet to leverage both intra-view and cross-view geometric knowledge in its learning process, substantially improving its ability to estimate disparities. Our extensive experiments demonstrate the superiority of the ICGNet over contemporary leading models.",
        "page": "http://arxiv.org/abs/2402.19270",
        "pdf": "http://arxiv.org/pdf/2402.19270.pdf"
    },
    {
        "title": "TokenCompose: Text-to-Image Diffusion with Token-level Supervision",
        "author": "Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, Zhuowen Tu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data",
        "author": "Mengqi Zhang, Yang Fu, Zheng Ding, Sifei Liu, Zhuowen Tu, Xiaolong Wang",
        "abstract": "3D hand-object interaction data is scarce due to the hardware constraints in scaling up the data collection process. In this paper, we propose HOIDiffusion for generating realistic and diverse 3D hand-object interaction data. Our model is a conditional diffusion model that takes both the 3D hand-object geometric structure and text description as inputs for image synthesis. This offers a more controllable and realistic synthesis as we can specify the structure and style inputs in a disentangled manner. HOIDiffusion is trained by leveraging a diffusion model pre-trained on large-scale natural images and a few 3D human demonstrations. Beyond controllable image synthesis, we adopt the generated 3D data for learning 6D object pose estimation and show its effectiveness in improving perception systems. Project page: https://mq-zhang1.github.io/HOIDiffusion",
        "page": "http://arxiv.org/abs/2403.12011",
        "pdf": "http://arxiv.org/pdf/2403.12011.pdf"
    },
    {
        "title": "FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition",
        "author": "SICHENG MO, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, Bolei Zhou",
        "abstract": "Recent approaches such as ControlNet offer users fine-grained spatial control over text-to-image (T2I) diffusion models. However, auxiliary modules have to be trained for each type of spatial condition, model architecture, and checkpoint, putting them at odds with the diverse intents and preferences a human designer would like to convey to the AI models during the content creation process. In this work, we present FreeControl, a training-free approach for controllable T2I generation that supports multiple conditions, architectures, and checkpoints simultaneously. FreeControl designs structure guidance to facilitate the structure alignment with a guidance image, and appearance guidance to enable the appearance sharing between images generated using the same seed. Extensive qualitative and quantitative experiments demonstrate the superior performance of FreeControl across a variety of pre-trained T2I models. In particular, FreeControl facilitates convenient training-free control over many different architectures and checkpoints, allows the challenging input conditions on which most of the existing training-free methods fail, and achieves competitive synthesis quality with training-based approaches.",
        "page": "http://arxiv.org/abs/2312.07536",
        "pdf": "http://arxiv.org/pdf/2312.07536.pdf"
    },
    {
        "title": "The Neglected Tails of Vision-Language Models",
        "author": "Shubham Parashar, Tian Liu, Zhiqiu Lin, Xiangjue Dong, Yanan Li, James Caverlee, Deva Ramanan, Shu Kong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Language Models as Black-Box Optimizers for Vision-Language Models",
        "author": "Shihong Liu, Samuel Yu, Zhiqiu Lin, Deepak Pathak, Deva Ramanan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians",
        "author": "Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, Yebin Liu",
        "abstract": "Creating high-fidelity 3D head avatars has always been a research hotspot, but there remains a great challenge under lightweight sparse view setups. In this paper, we propose Gaussian Head Avatar represented by controllable 3D Gaussians for high-fidelity head avatar modeling. We optimize the neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. Experiments show our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions.",
        "page": "http://arxiv.org/abs/2312.03029",
        "pdf": "http://arxiv.org/pdf/2312.03029.pdf"
    },
    {
        "title": "NeRF Analogies - Example-Based Visual Attribute Transfer for NeRFs",
        "author": "Michael Fischer, Zhengqin Li, Thu Nguyen-Phuoc, Alja\u017e Bo\u017ei\u010d, Zhao Dong, Carl Marshall, Tobias Ritschel",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering",
        "author": "Zhiwen Yan, Weng Fei Low, Yu Chen, Gim Hee Lee",
        "abstract": "3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering. Despite its high rendering quality and speed at high resolutions, they both deteriorate drastically when rendered at lower resolutions or from far away camera position. During low resolution or far away rendering, the pixel size of the image can fall below the Nyquist frequency compared to the screen size of each splatted 3D Gaussian and leads to aliasing effect. The rendering is also drastically slowed down by the sequential alpha blending of more splatted Gaussians per pixel. To address these issues, we propose a multi-scale 3D Gaussian splatting algorithm, which maintains Gaussians at different scales to represent the same scene. Higher-resolution images are rendered with more small Gaussians, and lower-resolution images are rendered with fewer larger Gaussians. With similar training time, our algorithm can achieve 13\\%-66\\% PSNR and 160\\%-2400\\% rendering speed improvement at 4$\\times$-128$\\times$ scale rendering on Mip-NeRF360 dataset compared to the single scale 3D Gaussian splitting. Our code and more results are available on our project website https://jokeryan.github.io/projects/ms-gs/",
        "page": "http://arxiv.org/abs/2311.17089",
        "pdf": "http://arxiv.org/pdf/2311.17089.pdf"
    },
    {
        "title": "Real-Time Exposure Correction via Collaborative Transformations and Adaptive Sampling",
        "author": "Ziwen Li, Feng Zhang, Meng Cao, Jinpu Zhang, Yuanjie Shao, Yuehuan Wang, Nong Sang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment",
        "author": "yiming ren, xiao han, Chengfeng Zhao, Jingya Wang, Lan Xu, Jingyi Yu, Yuexin Ma",
        "abstract": "For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon.",
        "page": "http://arxiv.org/abs/2402.17171",
        "pdf": "http://arxiv.org/pdf/2402.17171.pdf"
    },
    {
        "title": "iKUN: Speak to Trackers without Retraining",
        "author": "Yunhao Du, Cheng Lei, Zhicheng Zhao, Fei Su",
        "abstract": "Referring multi-object tracking (RMOT) aims to track multiple objects based on input textual descriptions. Previous works realize it by simply integrating an extra textual module into the multi-object tracker. However, they typically need to retrain the entire framework and have difficulties in optimization. In this work, we propose an insertable Knowledge Unification Network, termed iKUN, to enable communication with off-the-shelf trackers in a plug-and-play manner. Concretely, a knowledge unification module (KUM) is designed to adaptively extract visual features based on textual guidance. Meanwhile, to improve the localization accuracy, we present a neural version of Kalman filter (NKF) to dynamically adjust process noise and observation noise based on the current motion status. Moreover, to address the problem of open-set long-tail distribution of textual descriptions, a test-time similarity calibration method is proposed to refine the confidence score with pseudo frequency. Extensive experiments on Refer-KITTI dataset verify the effectiveness of our framework. Finally, to speed up the development of RMOT, we also contribute a more challenging dataset, Refer-Dance, by extending public DanceTrack dataset with motion and dressing descriptions. The codes and dataset are available at https://github.com/dyhBUPT/iKUN.",
        "page": "http://arxiv.org/abs/2312.16245",
        "pdf": "http://arxiv.org/pdf/2312.16245.pdf"
    },
    {
        "title": "Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization",
        "author": "Yujia Liu, Chenxi Yang, Dingquan Li, Jianhao Ding, Tingting Jiang",
        "abstract": "The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the quality score of an input image without additional information. NR-IQA models play a crucial role in the media industry, aiding in performance evaluation and optimization guidance. However, these models are found to be vulnerable to adversarial attacks, which introduce imperceptible perturbations to input images, resulting in significant changes in predicted scores. In this paper, we propose a defense method to improve the stability in predicted scores when attacked by small perturbations, thus enhancing the adversarial robustness of NR-IQA models. To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $\\ell_1$ norm of the model's gradient with respect to the input image. Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $\\ell_1$ norm of the gradient, thereby boosting the robustness of NR-IQA models. Experiments conducted on four NR-IQA baseline models demonstrate the effectiveness of our strategy in reducing score changes in the presence of adversarial attacks. To the best of our knowledge, this work marks the first attempt to defend against adversarial attacks on NR-IQA models. Our study offers valuable insights into the adversarial robustness of NR-IQA models and provides a foundation for future research in this area.",
        "page": "http://arxiv.org/abs/2403.11397",
        "pdf": "http://arxiv.org/pdf/2403.11397.pdf"
    },
    {
        "title": "PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection",
        "author": "Qihang Ma, Zhizhong Zhang, Xin Tan, Yanyun Qu, Chengwei Chen, Yuan Xie, Lizhuang Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning",
        "author": "Yun Li, Zhe Liu, Hang Chen, Lina Yao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes",
        "author": "Yichen Yao, Zimo Jiang, YUJING SUN, Zhencai Zhu, Xinge Zhu, Runnan Chen, Yuexin Ma",
        "abstract": "Human-centric 3D scene understanding has recently drawn increasing attention, driven by its critical impact on robotics. However, human-centric real-life scenarios are extremely diverse and complicated, and humans have intricate motions and interactions. With limited labeled data, supervised methods are difficult to generalize to general scenarios, hindering real-life applications. Mimicking human intelligence, we propose an unsupervised 3D detection method for human-centric scenarios by transferring the knowledge from synthetic human instances to real scenes. To bridge the gap between the distinct data representations and feature distributions of synthetic models and real point clouds, we introduce novel modules for effective instance-to-scene representation transfer and synthetic-to-real feature alignment. Remarkably, our method exhibits superior performance compared to current state-of-the-art techniques, achieving 87.8% improvement in mAP and closely approaching the performance of fully supervised methods (62.15 mAP vs. 69.02 mAP) on HuCenLife Dataset.",
        "page": "http://arxiv.org/abs/2403.02769",
        "pdf": "http://arxiv.org/pdf/2403.02769.pdf"
    },
    {
        "title": "DeconfuseTrack: Dealing with Confusion for Multi-Object Tracking",
        "author": "Cheng Huang, Shoudong Han, Mengyu He, Wenbo Zheng, Yuhao Wei",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Depth-aware Test-Time Training for Zero-shot Video Object Segmentation",
        "author": "Weihuang Liu, Xi Shen, Haolun Li, Xiuli Bi, Bo Liu, Chi-Man Pun, Xiaodong Cun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Masked Spatial Propagation Network for Sparsity-Adaptive Depth Refinement",
        "author": "Jinyoung Jun, Jae-Han Lee, Chang-Su Kim",
        "abstract": "The main function of depth completion is to compensate for an insufficient and unpredictable number of sparse depth measurements of hardware sensors. However, existing research on depth completion assumes that the sparsity -- the number of points or LiDAR lines -- is fixed for training and testing. Hence, the completion performance drops severely when the number of sparse depths changes significantly. To address this issue, we propose the sparsity-adaptive depth refinement (SDR) framework, which refines monocular depth estimates using sparse depth points. For SDR, we propose the masked spatial propagation network (MSPN) to perform SDR with a varying number of sparse depths effectively by gradually propagating sparse depth information throughout the entire depth map. Experimental results demonstrate that MPSN achieves state-of-the-art performance on both SDR and conventional depth completion scenarios.",
        "page": "http://arxiv.org/abs/2404.19294",
        "pdf": "http://arxiv.org/pdf/2404.19294.pdf"
    },
    {
        "title": "Dense Vision Transformer Compression with Few Samples",
        "author": "Hanxiao Zhang, Yifan Zhou, Guo-Hua Wang",
        "abstract": "Few-shot model compression aims to compress a large model into a more compact one with only a tiny training set (even without labels). Block-level pruning has recently emerged as a leading technique in achieving high accuracy and low latency in few-shot CNN compression. But, few-shot compression for Vision Transformers (ViT) remains largely unexplored, which presents a new challenge. In particular, the issue of sparse compression exists in traditional CNN few-shot methods, which can only produce very few compressed models of different model sizes. This paper proposes a novel framework for few-shot ViT compression named DC-ViT. Instead of dropping the entire block, DC-ViT selectively eliminates the attention module while retaining and reusing portions of the MLP module. DC-ViT enables dense compression, which outputs numerous compressed models that densely populate the range of model complexity. DC-ViT outperforms state-of-the-art few-shot compression methods by a significant margin of 10 percentage points, along with lower latency in the compression of ViT and its variants.",
        "page": "http://arxiv.org/abs/2403.18708",
        "pdf": "http://arxiv.org/pdf/2403.18708.pdf"
    },
    {
        "title": "Learning to Select Views for Efficient Multi-View Understanding",
        "author": "Yunzhong Hou, Stephen Gould, Liang Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Efficient Multi-scale Network with Learnable Discrete Wavelet Transform for Blind Motion Deblurring",
        "author": "Xin Gao, Tianheng Qiu, Xinyu Zhang, Hanlin Bai, Kang Liu, xuan huang, Hu Wei, Guoying Zhang, Huaping Liu",
        "abstract": "Coarse-to-fine schemes are widely used in traditional single-image motion deblur; however, in the context of deep learning, existing multi-scale algorithms not only require the use of complex modules for feature fusion of low-scale RGB images and deep semantics, but also manually generate low-resolution pairs of images that do not have sufficient confidence. In this work, we propose a multi-scale network based on single-input and multiple-outputs(SIMO) for motion deblurring. This simplifies the complexity of algorithms based on a coarse-to-fine scheme. To alleviate restoration defects impacting detail information brought about by using a multi-scale architecture, we combine the characteristics of real-world blurring trajectories with a learnable wavelet transform module to focus on the directional continuity and frequency features of the step-by-step transitions between blurred images to sharp images. In conclusion, we propose a multi-scale network with a learnable discrete wavelet transform (MLWNet), which exhibits state-of-the-art performance on multiple real-world deblurred datasets, in terms of both subjective and objective quality as well as computational efficiency.",
        "page": "http://arxiv.org/abs/2401.00027",
        "pdf": "http://arxiv.org/pdf/2401.00027.pdf"
    },
    {
        "title": "Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation",
        "author": "Junyan Wang, Zhenhong Sun, Stewart Tan, Xuanbai Chen, Weihua Chen, li, Cheng Zhang, Yang Song",
        "abstract": "Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls -- human-centric priors such as pose or depth maps -- during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross-attention layer. Extensive experiments show that our method largely improves over state-of-the-art text-to-image models to synthesize high-quality human images based on user-written prompts. Project page: \\url{https://hcplayercvpr2024.github.io}.",
        "page": "http://arxiv.org/abs/2403.05239",
        "pdf": "http://arxiv.org/pdf/2403.05239.pdf"
    },
    {
        "title": "TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion",
        "author": "Yu-Ying Yeh, Jia-Bin Huang, Changil Kim, Lei Xiao, Thu Nguyen-Phuoc, Numair Khan, Cheng Zhang, Manmohan Chandraker, Carl Marshall, Zhao Dong, Zhengqin Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MoST: Motion Style Transformer between Diverse Action Contents",
        "author": "Boeun Kim, Jungho Kim, Hyung Jin Chang, Jin Young Choi",
        "abstract": "While existing motion style transfer methods are effective between two motions with identical content, their performance significantly diminishes when transferring style between motions with different contents. This challenge lies in the lack of clear separation between content and style of a motion. To tackle this challenge, we propose a novel motion style transformer that effectively disentangles style from content and generates a plausible motion with transferred style from a source motion. Our distinctive approach to achieving the goal of disentanglement is twofold: (1) a new architecture for motion style transformer with `part-attentive style modulator across body parts' and `Siamese encoders that encode style and content features separately'; (2) style disentanglement loss. Our method outperforms existing methods and demonstrates exceptionally high quality, particularly in motion pairs with different contents, without the need for heuristic post-processing. Codes are available at https://github.com/Boeun-Kim/MoST.",
        "page": "http://arxiv.org/abs/2403.06225",
        "pdf": "http://arxiv.org/pdf/2403.06225.pdf"
    },
    {
        "title": "DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans",
        "author": "Akash Sengupta, Thiemo Alldieck, NIKOS KOLOTOUROS, Enric Corona, Andrei Zanfir, Cristian Sminchisescu",
        "abstract": "We present DiffHuman, a probabilistic method for photorealistic 3D human reconstruction from a single RGB image. Despite the ill-posed nature of this problem, most methods are deterministic and output a single solution, often resulting in a lack of geometric detail and blurriness in unseen or uncertain regions. In contrast, DiffHuman predicts a probability distribution over 3D reconstructions conditioned on an input 2D image, which allows us to sample multiple detailed 3D avatars that are consistent with the image. DiffHuman is implemented as a conditional diffusion model that denoises pixel-aligned 2D observations of an underlying 3D shape representation. During inference, we may sample 3D avatars by iteratively denoising 2D renders of the predicted 3D representation. Furthermore, we introduce a generator neural network that approximates rendering with considerably reduced runtime (55x speed up), resulting in a novel dual-branch diffusion framework. Our experiments show that DiffHuman can produce diverse and detailed reconstructions for the parts of the person that are unseen or uncertain in the input image, while remaining competitive with the state-of-the-art when reconstructing visible surfaces.",
        "page": "http://arxiv.org/abs/2404.00485",
        "pdf": "http://arxiv.org/pdf/2404.00485.pdf"
    },
    {
        "title": "LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation",
        "author": "Ke Guo, Zhenwei Miao, Wei Jing, Weiwei Liu, Weizi Li, Dayang Hao, Jia Pan",
        "abstract": "Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware of learner state distribution. Our method, applied to urban traffic simulation, demonstrates significant improvements over existing state-of-the-art baselines in both short-term microscopic and long-term macroscopic realism when evaluated on the real-world dataset pNEUMA.",
        "page": "http://arxiv.org/abs/2403.17601",
        "pdf": "http://arxiv.org/pdf/2403.17601.pdf"
    },
    {
        "title": "Unveiling Parts Beyond Objects: Towards Finer-Granularity Referring Expression Segmentation",
        "author": "Wenxuan Wang, Tongtian Yue, Yisi Zhang, Longteng Guo, Xingjian He, Xinlong Wang, Jing Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SPOT: Self-Training with Patch-Order Permutation for Object-Centric Learning with Autoregressive Transformers",
        "author": "Ioannis Kakogeorgiou, Spyros Gidaris, Konstantinos Karantzalos, Nikos Komodakis",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MoCha-Stereo: Motif Channel Attention Network for Stereo Matching",
        "author": "Ziyang Chen, Wei Long, He Yao, Yongjun Zhang, Bingshu Wang, Yongbin Qin, Jia Wu",
        "abstract": "Learning-based stereo matching techniques have made significant progress. However, existing methods inevitably lose geometrical structure information during the feature channel generation process, resulting in edge detail mismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network (MoCha-Stereo) is designed to address this problem. We provide the Motif Channel Correlation Volume (MCCV) to determine more accurate edge matching costs. MCCV is achieved by projecting motif channels, which capture common geometric structures in feature channels, onto feature maps and cost volumes. In addition, edge variations in %potential feature channels of the reconstruction error map also affect details matching, we propose the Reconstruction Error Motif Penalty (REMP) module to further refine the full-resolution disparity estimation. REMP integrates the frequency information of typical channel features from the reconstruction error. MoCha-Stereo ranks 1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure also shows excellent performance in Multi-View Stereo. Code is avaliable at https://github.com/ZYangChen/MoCha-Stereo.",
        "page": "http://arxiv.org/abs/2404.06842",
        "pdf": "http://arxiv.org/pdf/2404.06842.pdf"
    },
    {
        "title": "Attribute-Guided Pedestrian Retrieval: Bridging Person Re-ID with Internal Attribute Variability",
        "author": "Yan Huang, Zhang Zhang, Qiang Wu, yi zhong, Liang Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EFHQ: Multi-purpose ExtremePose-Face-HQ dataset",
        "author": "Trung Dao, Duc H Vu, Cuong Pham, Anh Tran",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Discriminability-Driven Channel Selection for Out-of-Distribution Detection",
        "author": "Yue Yuan, Rundong He, Yicong Dong, Zhongyi Han, Yilong Yin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection",
        "author": "Yichen Bai, Zongbo Han, Bing Cao, Xiaoheng Jiang, Qinghua Hu, Changqing Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Differentiable Display Photometric Stereo",
        "author": "Seokjun Choi, Seungwoo Yoon, Giljoo Nam, Seungyong Lee, Seung-Hwan Baek",
        "abstract": "Photometric stereo leverages variations in illumination conditions to reconstruct surface normals. Display photometric stereo, which employs a conventional monitor as an illumination source, has the potential to overcome limitations often encountered in bulky and difficult-to-use conventional setups. In this paper, we present differentiable display photometric stereo (DDPS), addressing an often overlooked challenge in display photometric stereo: the design of display patterns. Departing from using heuristic display patterns, DDPS learns the display patterns that yield accurate normal reconstruction for a target system in an end-to-end manner. To this end, we propose a differentiable framework that couples basis-illumination image formation with analytic photometric-stereo reconstruction. The differentiable framework facilitates the effective learning of display patterns via auto-differentiation. Also, for training supervision, we propose to use 3D printing for creating a real-world training dataset, enabling accurate reconstruction on the target real-world setup. Finally, we exploit that conventional LCD monitors emit polarized light, which allows for the optical separation of diffuse and specular reflections when combined with a polarization camera, leading to accurate normal reconstruction. Extensive evaluation of DDPS shows improved normal-reconstruction accuracy compared to heuristic patterns and demonstrates compelling properties such as robustness to pattern initialization, calibration errors, and simplifications in image formation and reconstruction.",
        "page": "http://arxiv.org/abs/2306.13325",
        "pdf": "http://arxiv.org/pdf/2306.13325.pdf"
    },
    {
        "title": "UnionFormer: Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization",
        "author": "Shuaibo Li, Wei Ma, Jianwei Guo, Shibiao Xu, Benchong Li, Xiaopeng Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CORES: Convolutional Response-based Score for Out-of-distribution Detection",
        "author": "Keke Tang, Chao Hou, Weilong Peng, Runnan Chen, Peican Zhu, Wenping Wang, Zhihong Tian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting",
        "author": "Haiwei Chen, Yajie Zhao",
        "abstract": "We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public benchmarks validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics.",
        "page": "http://arxiv.org/abs/2403.18186",
        "pdf": "http://arxiv.org/pdf/2403.18186.pdf"
    },
    {
        "title": "Building a Strong Pre-Training Baseline for Universal 3D Large-Scale Perception",
        "author": "Haoming Chen, Zhizhong Zhang, Yanyun Qu, Ruixin Zhang, Xin Tan, Yuan Xie",
        "abstract": "An effective pre-training framework with universal 3D representations is extremely desired in perceiving large-scale dynamic scenes. However, establishing such an ideal framework that is both task-generic and label-efficient poses a challenge in unifying the representation of the same primitive across diverse scenes. The current contrastive 3D pre-training methods typically follow a frame-level consistency, which focuses on the 2D-3D relationships in each detached image. Such inconsiderate consistency greatly hampers the promising path of reaching an universal pre-training framework: (1) The cross-scene semantic self-conflict, i.e., the intense collision between primitive segments of the same semantics from different scenes; (2) Lacking a globally unified bond that pushes the cross-scene semantic consistency into 3D representation learning. To address above challenges, we propose a CSC framework that puts a scene-level semantic consistency in the heart, bridging the connection of the similar semantic segments across various scenes. To achieve this goal, we combine the coherent semantic cues provided by the vision foundation model and the knowledge-rich cross-scene prototypes derived from the complementary multi-modality information. These allow us to train a universal 3D pre-training model that facilitates various downstream tasks with less fine-tuning efforts. Empirically, we achieve consistent improvements over SOTA pre-training approaches in semantic segmentation (+1.4% mIoU), object detection (+1.0% mAP), and panoptic segmentation (+3.0% PQ) using their task-specific 3D network on nuScenes. Code is released at https://github.com/chenhaomingbob/CSC, hoping to inspire future research.",
        "page": "http://arxiv.org/abs/2405.07201",
        "pdf": "http://arxiv.org/pdf/2405.07201.pdf"
    },
    {
        "title": "Learnable Earth Parser: Discovering 3D Prototypes in Aerial Scans",
        "author": "Romain Loiseau, Elliot Vincent, Mathieu Aubry, Loic Landrieu",
        "abstract": "We propose an unsupervised method for parsing large 3D scans of real-world scenes with easily-interpretable shapes. This work aims to provide a practical tool for analyzing 3D scenes in the context of aerial surveying and mapping, without the need for user annotations. Our approach is based on a probabilistic reconstruction model that decomposes an input 3D point cloud into a small set of learned prototypical 3D shapes. The resulting reconstruction is visually interpretable and can be used to perform unsupervised instance and low-shot semantic segmentation of complex scenes. We demonstrate the usefulness of our model on a novel dataset of seven large aerial LiDAR scans from diverse real-world scenarios. Our approach outperforms state-of-the-art unsupervised methods in terms of decomposition accuracy while remaining visually interpretable. Our code and dataset are available at https://romainloiseau.fr/learnable-earth-parser/",
        "page": "http://arxiv.org/abs/2304.09704",
        "pdf": "http://arxiv.org/pdf/2304.09704.pdf"
    },
    {
        "title": "Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision",
        "author": "Yi Yu, Xue Yang, Qingyun Li, Feipeng Da, Jifeng Dai, Yu Qiao, Junchi Yan",
        "abstract": "With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning rotated box (RBox) from the horizontal box (HBox) has attracted more and more attention. In this paper, we explore a more challenging yet label-efficient setting, namely single point-supervised OOD, and present our approach called Point2RBox. Specifically, we propose to leverage two principles: 1) Synthetic pattern knowledge combination: By sampling around each labeled point on the image, we spread the object feature to synthetic visual patterns with known boxes to provide the knowledge for box regression. 2) Transform self-supervision: With a transformed input image (e.g. scaled/rotated), the output RBoxes are trained to follow the same transformation so that the network can perceive the relative size/rotation between objects. The detector is further enhanced by a few devised techniques to cope with peripheral issues, e.g. the anchor/layer assignment as the size of the object is not available in our point supervision setting. To our best knowledge, Point2RBox is the first end-to-end solution for point-supervised OOD. In particular, our method uses a lightweight paradigm, yet it achieves a competitive performance among point-supervised alternatives, 41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.",
        "page": "http://arxiv.org/abs/2311.14758",
        "pdf": "http://arxiv.org/pdf/2311.14758.pdf"
    },
    {
        "title": "GROUNDHOG: Grounding Large Language Models to Holistic Segmentation",
        "author": "Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai",
        "abstract": "Most multimodal large language models (MLLMs) learn language-to-object grounding through causal language modeling where grounded objects are captured by bounding boxes as sequences of location tokens. This paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM developed by grounding Large Language Models to holistic segmentation. GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual entity tokens for the MLLM backbone, which then connects groundable phrases to unified grounding masks by retrieving and merging the entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by harvesting a collection of segmentation-grounded datasets with rich annotations. Our experimental results show that GROUNDHOG achieves superior performance on various language grounding tasks without task-specific fine-tuning, and significantly reduces object hallucination. GROUNDHOG also demonstrates better grounding towards complex forms of visual input and provides easy-to-understand diagnosis in failure cases.",
        "page": "http://arxiv.org/abs/2402.16846",
        "pdf": "http://arxiv.org/pdf/2402.16846.pdf"
    },
    {
        "title": "Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution",
        "author": "Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, Lei Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Poly Kernel Inception Network for Remote Sensing Detection",
        "author": "Xinhao Cai, Qiuxia Lai, Yuwei Wang, Wenguan Wang, Zeren Sun, Yazhou Yao",
        "abstract": "Object detection in remote sensing images (RSIs) often suffers from several increasing challenges, including the large variation in object scales and the diverse-ranging context. Prior methods tried to address these challenges by expanding the spatial receptive field of the backbone, either through large-kernel convolution or dilated convolution. However, the former typically introduces considerable background noise, while the latter risks generating overly sparse feature representations. In this paper, we introduce the Poly Kernel Inception Network (PKINet) to handle the above challenges. PKINet employs multi-scale convolution kernels without dilation to extract object features of varying scales and capture local context. In addition, a Context Anchor Attention (CAA) module is introduced in parallel to capture long-range contextual information. These two components work jointly to advance the performance of PKINet on four challenging remote sensing detection benchmarks, namely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.",
        "page": "http://arxiv.org/abs/2403.06258",
        "pdf": "http://arxiv.org/pdf/2403.06258.pdf"
    },
    {
        "title": "Video-Based Human Pose Regression via Decoupled Space-Time Aggregation",
        "author": "Jijie He, Wenwu Yang",
        "abstract": "By leveraging temporal dependency in video sequences, multi-frame human pose estimation algorithms have demonstrated remarkable results in complicated situations, such as occlusion, motion blur, and video defocus. These algorithms are predominantly based on heatmaps, resulting in high computation and storage requirements per frame, which limits their flexibility and real-time application in video scenarios, particularly on edge devices. In this paper, we develop an efficient and effective video-based human pose regression method, which bypasses intermediate representations such as heatmaps and instead directly maps the input to the output joint coordinates. Despite the inherent spatial correlation among adjacent joints of the human pose, the temporal trajectory of each individual joint exhibits relative independence. In light of this, we propose a novel Decoupled Space-Time Aggregation network (DSTA) to separately capture the spatial contexts between adjacent joints and the temporal cues of each individual joint, thereby avoiding the conflation of spatiotemporal dimensions. Concretely, DSTA learns a dedicated feature token for each joint to facilitate the modeling of their spatiotemporal dependencies. With the proposed joint-wise local-awareness attention mechanism, our method is capable of efficiently and flexibly utilizing the spatial dependency of adjacent joints and the temporal dependency of each joint itself. Extensive experiments demonstrate the superiority of our method. Compared to previous regression-based single-frame human pose estimation methods, DSTA significantly enhances performance, achieving an 8.9 mAP improvement on PoseTrack2017. Furthermore, our approach either surpasses or is on par with the state-of-the-art heatmap-based multi-frame human pose estimation methods. Project page: https://github.com/zgspose/DSTA.",
        "page": "http://arxiv.org/abs/2403.19926",
        "pdf": "http://arxiv.org/pdf/2403.19926.pdf"
    },
    {
        "title": "Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from Federated Learning",
        "author": "Joshua C. Zhao, Ahaan Dabholkar, Atul Sharma, Saurabh Bagchi",
        "abstract": "Federated learning is a decentralized learning paradigm introduced to preserve privacy of client data. Despite this, prior work has shown that an attacker at the server can still reconstruct the private training data using only the client updates. These attacks are known as data reconstruction attacks and fall into two major categories: gradient inversion (GI) and linear layer leakage attacks (LLL). However, despite demonstrating the effectiveness of these attacks in breaching privacy, prior work has not investigated the usefulness of the reconstructed data for downstream tasks. In this work, we explore data reconstruction attacks through the lens of training and improving models with leaked data. We demonstrate the effectiveness of both GI and LLL attacks in maliciously training models using the leaked data more accurately than a benign federated learning strategy. Counter-intuitively, this bump in training quality can occur despite limited reconstruction quality or a small total number of leaked images. Finally, we show the limitations of these attacks for downstream training, individually for GI attacks and for LLL attacks.",
        "page": "http://arxiv.org/abs/2403.18144",
        "pdf": "http://arxiv.org/pdf/2403.18144.pdf"
    },
    {
        "title": "3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos",
        "author": "Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing",
        "abstract": "Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\\\"ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2403.01444",
        "pdf": "http://arxiv.org/pdf/2403.01444.pdf"
    },
    {
        "title": "Estimating Extreme 3D Image Rotations using Cascaded Attention",
        "author": "Shay Dekel, Yosi Keller, Martin \u010cad\u00edk",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Federated Online Adaptation for Deep Stereo",
        "author": "Matteo Poggi, Fabio Tosi",
        "abstract": "We introduce a novel approach for adapting deep stereo networks in a collaborative manner. By building over principles of federated learning, we develop a distributed framework allowing for demanding the optimization process to a number of clients deployed in different environments. This makes it possible, for a deep stereo network running on resourced-constrained devices, to capitalize on the adaptation process carried out by other instances of the same architecture, and thus improve its accuracy in challenging environments even when it cannot carry out adaptation on its own. Experimental results show how federated adaptation performs equivalently to on-device adaptation, and even better when dealing with challenging environments.",
        "page": "http://arxiv.org/abs/2405.14873",
        "pdf": "http://arxiv.org/pdf/2405.14873.pdf"
    },
    {
        "title": "KVQ: Kwai Video Quality Assessment for Short-form Videos",
        "author": "Yiting Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, Yunpeng Qu, Ming Sun, Chao Zhou, Zhibo Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PTM-VQA: Efficient Video Quality Assessment Leveraging Diverse PreTrained Models from the Wild",
        "author": "Kun Yuan, Hongbo Liu, Mading Li, Muyi Sun, Ming Sun, Jiachao Gong, Jinhua Hao, Chao Zhou, Yansong Tang",
        "abstract": "Video quality assessment (VQA) is a challenging problem due to the numerous factors that can affect the perceptual quality of a video, \\eg, content attractiveness, distortion type, motion pattern, and level. However, annotating the Mean opinion score (MOS) for videos is expensive and time-consuming, which limits the scale of VQA datasets, and poses a significant obstacle for deep learning-based methods. In this paper, we propose a VQA method named PTM-VQA, which leverages PreTrained Models to transfer knowledge from models pretrained on various pre-tasks, enabling benefits for VQA from different aspects. Specifically, we extract features of videos from different pretrained models with frozen weights and integrate them to generate representation. Since these models possess various fields of knowledge and are often trained with labels irrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility (ICID) loss to impose constraints on features extracted by multiple pretrained models. The intra-consistency constraint ensures that features extracted by different pretrained models are in the same unified quality-aware latent space, while the inter-divisibility introduces pseudo clusters based on the annotation of samples and tries to separate features of samples from different clusters. Furthermore, with a constantly growing number of pretrained models, it is crucial to determine which models to use and how to use them. To address this problem, we propose an efficient scheme to select suitable candidates. Models with better clustering performance on VQA datasets are chosen to be our candidates. Extensive experiments demonstrate the effectiveness of the proposed method.",
        "page": "http://arxiv.org/abs/2405.17765",
        "pdf": "http://arxiv.org/pdf/2405.17765.pdf"
    },
    {
        "title": "Learning Triangular Distribution in Visual World",
        "author": "Ping Chen, Xingpeng Zhang, Chengtao Zhou, dichao Fan, Peng Tu, Le Zhang, Yanlin Qian",
        "abstract": "Convolution neural network is successful in pervasive vision tasks, including label distribution learning, which usually takes the form of learning an injection from the non-linear visual features to the well-defined labels. However, how the discrepancy between features is mapped to the label discrepancy is ambient, and its correctness is not guaranteed.To address these problems, we study the mathematical connection between feature and its label, presenting a general and simple framework for label distribution learning. We propose a so-called Triangular Distribution Transform (TDT) to build an injective function between feature and label, guaranteeing that any symmetric feature discrepancy linearly reflects the difference between labels. The proposed TDT can be used as a plug-in in mainstream backbone networks to address different label distribution learning tasks. Experiments on Facial Age Recognition, Illumination Chromaticity Estimation, and Aesthetics assessment show that TDT achieves on-par or better results than the prior arts.",
        "page": "http://arxiv.org/abs/2311.18605",
        "pdf": "http://arxiv.org/pdf/2311.18605.pdf"
    },
    {
        "title": "InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models",
        "author": "Jiun Tian Hoe, Xudong Jiang, Chee Seng Chan, Yap-peng Tan, Weipeng Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SAI3D: Segment Any Instance in 3D Scenes",
        "author": "Yingda Yin, Yuzheng Liu, Yang Xiao, Daniel Cohen-Or, Jingwei Huang, Baoquan Chen",
        "abstract": "Advancements in 3D instance segmentation have traditionally been tethered to the availability of annotated datasets, limiting their application to a narrow spectrum of object categories. Recent efforts have sought to harness vision-language models like CLIP for open-set semantic reasoning, yet these methods struggle to distinguish between objects of the same categories and rely on specific prompts that are not universally applicable. In this paper, we introduce SAI3D, a novel zero-shot 3D instance segmentation approach that synergistically leverages geometric priors and semantic cues derived from Segment Anything Model (SAM). Our method partitions a 3D scene into geometric primitives, which are then progressively merged into 3D instance segmentations that are consistent with the multi-view SAM masks. Moreover, we design a hierarchical region-growing algorithm with a dynamic thresholding mechanism, which largely improves the robustness of finegrained 3D scene parsing.Empirical evaluations on ScanNet, Matterport3D and the more challenging ScanNet++ datasets demonstrate the superiority of our approach. Notably, SAI3D outperforms existing open-vocabulary baselines and even surpasses fully-supervised methods in class-agnostic segmentation on ScanNet++. Our project page is at https://yd-yin.github.io/SAI3D.",
        "page": "http://arxiv.org/abs/2312.11557",
        "pdf": "http://arxiv.org/pdf/2312.11557.pdf"
    },
    {
        "title": "Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving",
        "author": "Junhao Zheng, Chenhao Lin, Jiahao Sun, Zhengyu Zhao, Qian Li, Chao Shen",
        "abstract": "Deep learning-based monocular depth estimation (MDE), extensively applied in autonomous driving, is known to be vulnerable to adversarial attacks. Previous physical attacks against MDE models rely on 2D adversarial patches, so they only affect a small, localized region in the MDE map but fail under various viewpoints. To address these limitations, we propose 3D Depth Fool (3D$^2$Fool), the first 3D texture-based adversarial attack against MDE models. 3D$^2$Fool is specifically optimized to generate 3D adversarial textures agnostic to model types of vehicles and to have improved robustness in bad weather conditions, such as rain and fog. Experimental results validate the superior performance of our 3D$^2$Fool across various scenarios, including vehicles, MDE models, weather conditions, and viewpoints. Real-world experiments with printed 3D textures on physical vehicle models further demonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters.",
        "page": "http://arxiv.org/abs/2403.17301",
        "pdf": "http://arxiv.org/pdf/2403.17301.pdf"
    },
    {
        "title": "Modality-Collaborative Test-Time Adaptation for Action Recognition",
        "author": "Baochen Xiong, Xiaoshan Yang, Yaguang Song, Yaowei Wang, Changsheng Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection",
        "author": "Jiayi Zhu, Qing Guo, Felix Juefei Xu, Yihao Huang, Yang Liu, Geguang Pu",
        "abstract": "Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly.",
        "page": "http://arxiv.org/abs/2403.18554",
        "pdf": "http://arxiv.org/pdf/2403.18554.pdf"
    },
    {
        "title": "Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation",
        "author": "Zhekai Du, Xinyao Li, Fengling Li, Ke Lu, Lei Zhu, Jingjing Li",
        "abstract": "Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between domains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pre-trained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flexibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way. Meanwhile, visual prompts are imposed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings. These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches.",
        "page": "http://arxiv.org/abs/2403.02899",
        "pdf": "http://arxiv.org/pdf/2403.02899.pdf"
    },
    {
        "title": "AAMDM: Accelerated Auto-regressive Motion Diffusion Model",
        "author": "Tianyu Li, Calvin Zhuhan Qiao, Ren Guanqiao, KangKang Yin, Sehoon Ha",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Language-only Training of Zero-shot Composed Image Retrieval",
        "author": "Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, Sangdoo Yun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CAT-Seg: Cost Aggregation for Open-vocabulary Semantic Segmentation",
        "author": "Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, Seungryong Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding",
        "author": "Zhihao Yuan, Jinke Ren, Chun-Mei Feng, Hengshuang Zhao, Shuguang Cui, Zhen Li",
        "abstract": "3D Visual Grounding (3DVG) aims at localizing 3D object based on textual descriptions. Conventional supervised methods for 3DVG often necessitate extensive annotations and a predefined vocabulary, which can be restrictive. To address this issue, we propose a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs). Our approach begins with a unique dialog-based method, engaging with LLMs to establish a foundational understanding of zero-shot 3DVG. Building on this, we design a visual program that consists of three types of modules, i.e., view-independent, view-dependent, and functional modules. These modules, specifically tailored for 3D scenarios, work collaboratively to perform complex reasoning and inference. Furthermore, we develop an innovative language-object correlation module to extend the scope of existing 3D object detectors into open-vocabulary scenarios. Extensive experiments demonstrate that our zero-shot approach can outperform some supervised baselines, marking a significant stride towards effective 3DVG.",
        "page": "http://arxiv.org/abs/2311.15383",
        "pdf": "http://arxiv.org/pdf/2311.15383.pdf"
    },
    {
        "title": "Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation",
        "author": "Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, Nong Sang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CAMixerSR: Only Details Need More \"Attention\"",
        "author": "Yan Wang, Yi Liu, Shijie Zhao, Junlin Li, Li zhang",
        "abstract": "To satisfy the rapidly increasing demands on the large image (2K-8K) super-resolution (SR), prevailing methods follow two independent tracks: 1) accelerate existing networks by content-aware routing, and 2) design better super-resolution networks via token mixer refining. Despite directness, they encounter unavoidable defects (e.g., inflexible route or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To erase the drawbacks, we integrate these schemes by proposing a content-aware mixer (CAMixer), which assigns convolution for simple contexts and additional deformable window-attention for sparse textures. Specifically, the CAMixer uses a learnable predictor to generate multiple bootstraps, including offsets for windows warping, a mask for classifying windows, and convolutional attentions for endowing convolution with the dynamic property, which modulates attention to include more useful textures self-adaptively and improves the representation capability of convolution. We further introduce a global classification loss to improve the accuracy of predictors. By simply stacking CAMixers, we obtain CAMixerSR which achieves superior performance on large-image SR, lightweight SR, and omnidirectional-image SR.",
        "page": "http://arxiv.org/abs/2402.19289",
        "pdf": "http://arxiv.org/pdf/2402.19289.pdf"
    },
    {
        "title": "Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay",
        "author": "Yuhang Zhou, Zhongyun Hua",
        "abstract": "Deep neural networks have demonstrated susceptibility to adversarial attacks. Adversarial defense techniques often focus on one-shot setting to maintain robustness against attack. However, new attacks can emerge in sequences in real-world deployment scenarios. As a result, it is crucial for a defense model to constantly adapt to new attacks, but the adaptation process can lead to catastrophic forgetting of previously defended against attacks. In this paper, we discuss for the first time the concept of continual adversarial defense under a sequence of attacks, and propose a lifelong defense baseline called Anisotropic \\& Isotropic Replay (AIR), which offers three advantages: (1) Isotropic replay ensures model consistency in the neighborhood distribution of new data, indirectly aligning the output preference between old and new tasks. (2) Anisotropic replay enables the model to learn a compromise data manifold with fresh mixed semantics for further replay constraints and potential future attacks. (3) A straightforward regularizer mitigates the 'plasticity-stability' trade-off by aligning model output between new and old tasks. Experiment results demonstrate that AIR can approximate or even exceed the empirical performance upper bounds achieved by Joint Training.",
        "page": "http://arxiv.org/abs/2404.01828",
        "pdf": "http://arxiv.org/pdf/2404.01828.pdf"
    },
    {
        "title": "Denoising Point Clouds in Latent Space via Graph Convolution and Invertible Neural Network",
        "author": "Aihua Mao, Biao Yan, Zijing Ma, Ying He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Permutation Equivariance of Transformers and Its Applications",
        "author": "Hengyuan Xu, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, Baochun Li",
        "abstract": "Revolutionizing the field of deep learning, Transformer-based models have achieved remarkable performance in many tasks. Recent research has recognized these models are robust to shuffling but are limited to inter-token permutation in the forward propagation. In this work, we propose our definition of permutation equivariance, a broader concept covering both inter- and intra- token permutation in the forward and backward propagation of neural networks. We rigorously proved that such permutation equivariance property can be satisfied on most vanilla Transformer-based models with almost no adaptation. We examine the property over a range of state-of-the-art models including ViT, Bert, GPT, and others, with experimental validations. Further, as a proof-of-concept, we explore how real-world applications including privacy-enhancing split learning, and model authorization, could exploit the permutation equivariance property, which implicates wider, intriguing application scenarios.",
        "page": "http://arxiv.org/abs/2304.07735",
        "pdf": "http://arxiv.org/pdf/2304.07735.pdf"
    },
    {
        "title": "Generalized Event Cameras",
        "author": "Varun Sundar, Matthew Dutson, Andrei Ardelean, Claudio Bruschini, Edoardo Charbon, Mohit Gupta",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution",
        "author": "Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding, Ping Luo",
        "abstract": "Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent trajectories from high-level instructions remains challenging, especially for long-range composition tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. This allows generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior following abstract instructions across diverse tasks. Experiments on multi-task robotic manipulation benchmarks like Meta-World and LOReL demonstrate state-of-the-art performance and human-interpretable skill representations from SkillDiffuser. More visualization results and information could be found on our website.",
        "page": "http://arxiv.org/abs/2312.11598",
        "pdf": "http://arxiv.org/pdf/2312.11598.pdf"
    },
    {
        "title": "Dual-scale Transformer for Large-scale Single-Pixel Imaging",
        "author": "Gang Qu, Ping Wang, Xin Yuan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PH-Net: Semi-Supervised Breast Lesion Segmentation via Patch-wise Hardness",
        "author": "Siyao Jiang, Huisi Wu, Junyang Chen, Qin Zhang, Jing Qin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generative Multi-modal Models are Good Class Incremental Learners",
        "author": "Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Task-Adaptive Saliency Guidance for Exemplar-free Class Incremental Learning",
        "author": "Xialei Liu, Jiang-Tian Zhai, Andrew Bagdanov, Ke Li, Ming-Ming Cheng",
        "abstract": "Exemplar-free Class Incremental Learning (EFCIL) aims to sequentially learn tasks with access only to data from the current one. EFCIL is of interest because it mitigates concerns about privacy and long-term storage of data, while at the same time alleviating the problem of catastrophic forgetting in incremental learning. In this work, we introduce task-adaptive saliency for EFCIL and propose a new framework, which we call Task-Adaptive Saliency Supervision (TASS), for mitigating the negative effects of saliency drift between different tasks. We first apply boundary-guided saliency to maintain task adaptivity and \\textit{plasticity} on model attention. Besides, we introduce task-agnostic low-level signals as auxiliary supervision to increase the \\textit{stability} of model attention. Finally, we introduce a module for injecting and recovering saliency noise to increase the robustness of saliency preservation. Our experiments demonstrate that our method can better preserve saliency maps across tasks and achieve state-of-the-art results on the CIFAR-100, Tiny-ImageNet, and ImageNet-Subset EFCIL benchmarks. Code is available at \\url{https://github.com/scok30/tass}.",
        "page": "http://arxiv.org/abs/2212.08251",
        "pdf": "http://arxiv.org/pdf/2212.08251.pdf"
    },
    {
        "title": "ViewFusion: Towards Multi-View Consistency via Interpolated Denoising",
        "author": "Xianghui Yang, Gil Avraham, Yan Zuo, Sameera Ramasinghe, Loris Bazzani, Anton van den Hengel",
        "abstract": "Novel-view synthesis through diffusion models has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained diffusion models. Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a diffusion process that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional fine-tuning. Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views.",
        "page": "http://arxiv.org/abs/2402.18842",
        "pdf": "http://arxiv.org/pdf/2402.18842.pdf"
    },
    {
        "title": "Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models",
        "author": "Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, Nenghai Yu",
        "abstract": "Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a diffusion model watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked diffusion model. Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising Diffusion Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable Diffusion, and the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness.",
        "page": "http://arxiv.org/abs/2404.04956",
        "pdf": "http://arxiv.org/pdf/2404.04956.pdf"
    },
    {
        "title": "Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts",
        "author": "Jiayi Chen, Benteng Ma, Hengfei Cui, Kwang-Ting Cheng, Yong Xia",
        "abstract": "Federated learning facilitates the collaborative learning of a global model across multiple distributed medical institutions without centralizing data. Nevertheless, the expensive cost of annotation on local clients remains an obstacle to effectively utilizing local data. To mitigate this issue, federated active learning methods suggest leveraging local and global model predictions to select a relatively small amount of informative local data for annotation. However, existing methods mainly focus on all local data sampled from the same domain, making them unreliable in realistic medical scenarios with domain shifts among different clients. In this paper, we make the first attempt to assess the informativeness of local data derived from diverse domains and propose a novel methodology termed Federated Evidential Active Learning (FEAL) to calibrate the data evaluation under domain shift. Specifically, we introduce a Dirichlet prior distribution in both local and global models to treat the prediction as a distribution over the probability simplex and capture both aleatoric and epistemic uncertainties by using the Dirichlet-based evidential model. Then we employ the epistemic uncertainty to calibrate the aleatoric uncertainty. Afterward, we design a diversity relaxation strategy to reduce data redundancy and maintain data diversity. Extensive experiments and analysis on five real multi-center medical image datasets demonstrate the superiority of FEAL over the state-of-the-art active learning methods in federated scenarios with domain shifts. The code will be available at https://github.com/JiayiChen815/FEAL.",
        "page": "http://arxiv.org/abs/2312.02567",
        "pdf": "http://arxiv.org/pdf/2312.02567.pdf"
    },
    {
        "title": "HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images",
        "author": "Xihe Yang, Xingyu Chen, Daiheng Gao, Finn Wong, Xiaoguang Han, Baoyuan Wang",
        "abstract": "As for human avatar reconstruction, contemporary techniques commonly necessitate the acquisition of costly data and struggle to achieve satisfactory results from a small number of casual images. In this paper, we investigate this task from a few-shot unconstrained photo album. The reconstruction of human avatars from such data sources is challenging because of limited data amount and dynamic articulated poses. For handling dynamic data, we integrate a skinning mechanism with deep marching tetrahedra (DMTet) to form a drivable tetrahedral representation, which drives arbitrary mesh topologies generated by the DMTet for the adaptation of unconstrained images. To effectively mine instructive information from few-shot data, we devise a two-phase optimization method with few-shot reference and few-shot guidance. The former focuses on aligning avatar identity with reference images, while the latter aims to generate plausible appearances for unseen regions. Overall, our framework, called HaveFun, can undertake avatar reconstruction, rendering, and animation. Extensive experiments on our developed benchmarks demonstrate that HaveFun exhibits substantially superior performance in reconstructing the human body and hand. Project website: https://seanchenxy.github.io/HaveFunWeb/.",
        "page": "http://arxiv.org/abs/2311.15672",
        "pdf": "http://arxiv.org/pdf/2311.15672.pdf"
    },
    {
        "title": "PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis",
        "author": "Zhengyao Lv, Yuxiang Wei, Wangmeng Zuo, Kwan-Yee K. Wong",
        "abstract": "Recent advancements in large-scale pre-trained text-to-image models have led to remarkable progress in semantic image synthesis. Nevertheless, synthesizing high-quality images with consistent semantics and layout remains a challenge. In this paper, we propose the adaPtive LAyout-semantiC fusion modulE (PLACE) that harnesses pre-trained models to alleviate the aforementioned issues. Specifically, we first employ the layout control map to faithfully represent layouts in the feature space. Subsequently, we combine the layout and semantic features in a timestep-adaptive manner to synthesize images with realistic details. During fine-tuning, we propose the Semantic Alignment (SA) loss to further enhance layout alignment. Additionally, we introduce the Layout-Free Prior Preservation (LFP) loss, which leverages unlabeled data to maintain the priors of pre-trained models, thereby improving the visual quality and semantic consistency of synthesized images. Extensive experiments demonstrate that our approach performs favorably in terms of visual quality, semantic consistency, and layout alignment. The source code and model are available at https://github.com/cszy98/PLACE/tree/main.",
        "page": "http://arxiv.org/abs/2403.01852",
        "pdf": "http://arxiv.org/pdf/2403.01852.pdf"
    },
    {
        "title": "Learning Degradation Independent Representations for Camera ISP Pipelines",
        "author": "Yanhui Guo, Fangzhou Luo, Xiaolin Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NARUTO: Neural Active Reconstruction from Uncertain Target Observations",
        "author": "Ziyue Feng, Huangying Zhan, Zheng Chen, Qingan Yan, Xiangyu Xu, Changjiang Cai, Bing Li, Qilun Zhu, Yi Xu",
        "abstract": "We present NARUTO, a neural active reconstruction system that combines a hybrid neural representation with uncertainty learning, enabling high-fidelity surface reconstruction. Our approach leverages a multi-resolution hash-grid as the mapping backbone, chosen for its exceptional convergence speed and capacity to capture high-frequency local features.The centerpiece of our work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment. By harnessing learned uncertainty, we propose a novel uncertainty aggregation strategy for goal searching and efficient path planning. Our system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity. We also demonstrate the utility of this uncertainty-aware approach by enhancing SOTA neural SLAM systems through an active ray sampling strategy. Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction, as evidenced by its impressive results on benchmark datasets like Replica and MP3D.",
        "page": "http://arxiv.org/abs/2402.18771",
        "pdf": "http://arxiv.org/pdf/2402.18771.pdf"
    },
    {
        "title": "SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation",
        "author": "Kejia Yin, Varshanth Rao, Ruowei Jiang, Xudong Liu, Parham Aarabi, David B. Lindell",
        "abstract": "Self-supervised landmark estimation is a challenging task that demands the formation of locally distinct feature representations to identify sparse facial landmarks in the absence of annotated data. To tackle this task, existing state-of-the-art (SOTA) methods (1) extract coarse features from backbones that are trained with instance-level self-supervised learning (SSL) paradigms, which neglect the dense prediction nature of the task, (2) aggregate them into memory-intensive hypercolumn formations, and (3) supervise lightweight projector networks to naively establish full local correspondences among all pairs of spatial features. In this paper, we introduce SCE-MAE, a framework that (1) leverages the MAE, a region-level SSL method that naturally better suits the landmark prediction task, (2) operates on the vanilla feature map instead of on expensive hypercolumns, and (3) employs a Correspondence Approximation and Refinement Block (CARB) that utilizes a simple density peak clustering algorithm and our proposed Locality-Constrained Repellence Loss to directly hone only select local correspondences. We demonstrate through extensive experiments that SCE-MAE is highly effective and robust, outperforming existing SOTA methods by large margins of approximately 20%-44% on the landmark matching and approximately 9%-15% on the landmark detection tasks.",
        "page": "http://arxiv.org/abs/2405.18322",
        "pdf": "http://arxiv.org/pdf/2405.18322.pdf"
    },
    {
        "title": "HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video",
        "author": "Zicong Fan, Maria Parelli, Maria Kadoglou, Xu Chen, Muhammed Kocabas, Michael J. Black, Otmar Hilliges",
        "abstract": "Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an articulated hand and object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hand and object from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and consequently the reconstruction quality. Our method does not rely on 3D hand-object annotations while outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualitatively show its robustness in reconstructing from in-the-wild videos. Code: https://github.com/zc-alexfan/hold",
        "page": "http://arxiv.org/abs/2311.18448",
        "pdf": "http://arxiv.org/pdf/2311.18448.pdf"
    },
    {
        "title": "Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving",
        "author": "Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, Zhaoxiang Zhang",
        "abstract": "In autonomous driving, predicting future events in advance and evaluating the foreseeable risks empowers autonomous vehicles to better plan their actions, enhancing safety and efficiency on the road. To this end, we propose Drive-WM, the first driving world model compatible with existing end-to-end planning models. Through a joint spatial-temporal modeling facilitated by view factorization, our model generates high-fidelity multiview videos in driving scenes. Building on its powerful generation ability, we showcase the potential of applying the world model for safe driving planning for the first time. Particularly, our Drive-WM enables driving into multiple futures based on distinct driving maneuvers, and determines the optimal trajectory according to the image-based rewards. Evaluation on real-world driving datasets verifies that our method could generate high-quality, consistent, and controllable multiview videos, opening up possibilities for real-world simulations and safe planning.",
        "page": "http://arxiv.org/abs/2311.17918",
        "pdf": "http://arxiv.org/pdf/2311.17918.pdf"
    },
    {
        "title": "Loopy-SLAM: Dense Neural SLAM with Loop Closures",
        "author": "Lorenzo Liso, Erik Sandstr\u00f6m, Vladimir Yugay, Luc Van Gool, Martin R. Oswald",
        "abstract": "Neural RGBD SLAM techniques have shown promise in dense Simultaneous Localization And Mapping (SLAM), yet face challenges such as error accumulation during camera tracking resulting in distorted maps. In response, we introduce Loopy-SLAM that globally optimizes poses and the dense 3D model. We use frame-to-model tracking using a data-driven point-based submap generation method and trigger loop closures online by performing global place recognition. Robust pose graph optimization is used to rigidly align the local submaps. As our representation is point based, map corrections can be performed efficiently without the need to store the entire history of input frames used for mapping as typically required by methods employing a grid based mapping structure. Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet datasets demonstrate competitive or superior performance in tracking, mapping, and rendering accuracy when compared to existing dense neural RGBD SLAM methods. Project page: notchla.github.io/Loopy-SLAM.",
        "page": "http://arxiv.org/abs/2402.09944",
        "pdf": "http://arxiv.org/pdf/2402.09944.pdf"
    },
    {
        "title": "Robust Depth Enhancement via Polarization Prompt Fusion Tuning",
        "author": "Kei IKEMURA, Yiming Huang, Felix Heide, Zhaoxiang Zhang, Qifeng Chen, Chenyang Lei",
        "abstract": "Existing depth sensors are imperfect and may provide inaccurate depth values in challenging scenarios, such as in the presence of transparent or reflective objects. In this work, we present a general framework that leverages polarization imaging to improve inaccurate depth measurements from various depth sensors. Previous polarization-based depth enhancement methods focus on utilizing pure physics-based formulas for a single sensor. In contrast, our method first adopts a learning-based strategy where a neural network is trained to estimate a dense and complete depth map from polarization data and a sensor depth map from different sensors. To further improve the performance, we propose a Polarization Prompt Fusion Tuning (PPFT) strategy to effectively utilize RGB-based models pre-trained on large-scale datasets, as the size of the polarization dataset is limited to train a strong model from scratch. We conducted extensive experiments on a public dataset, and the results demonstrate that the proposed method performs favorably compared to existing depth enhancement baselines. Code and demos are available at https://lastbasket.github.io/PPFT/.",
        "page": "http://arxiv.org/abs/2404.04318",
        "pdf": "http://arxiv.org/pdf/2404.04318.pdf"
    },
    {
        "title": "Link-Context Learning for Multimodal LLMs",
        "author": "Yan Tai, Weichen Fan, Zhao Zhang, Ziwei Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels",
        "author": "Tianming Liang, Chaolei Tan, Beihao Xia, Wei-Shi Zheng, Jian-Fang Hu",
        "abstract": "This paper focuses on open-ended video question answering, which aims to find the correct answers from a large answer set in response to a video-related question. This is essentially a multi-label classification task, since a question may have multiple answers. However, due to annotation costs, the labels in existing benchmarks are always extremely insufficient, typically one answer per question. As a result, existing works tend to directly treat all the unlabeled answers as negative labels, leading to limited ability for generalization. In this work, we introduce a simple yet effective ranking distillation framework (RADI) to mitigate this problem without additional manual annotation. RADI employs a teacher model trained with incomplete labels to generate rankings for potential answers, which contain rich knowledge about label priority as well as label-associated visual cues, thereby enriching the insufficient labeling information. To avoid overconfidence in the imperfect teacher model, we further present two robust and parameter-free ranking distillation approaches: a pairwise approach which introduces adaptive soft margins to dynamically refine the optimization constraints on various pairwise rankings, and a listwise approach which adopts sampling-based partial listwise learning to resist the bias in teacher ranking. Extensive experiments on five popular benchmarks consistently show that both our pairwise and listwise RADIs outperform state-of-the-art methods. Further analysis demonstrates the effectiveness of our methods on the insufficient labeling problem.",
        "page": "http://arxiv.org/abs/2403.14430",
        "pdf": "http://arxiv.org/pdf/2403.14430.pdf"
    },
    {
        "title": "Text-Driven Image Editing via Learnable Regions",
        "author": "Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu Jiang, Ming-Hsuan Yang",
        "abstract": "Language has emerged as a natural interface for image editing. In this paper, we introduce a method for region-based image editing driven by textual prompts, without the need for user-provided masks or sketches. Specifically, our approach leverages an existing pre-trained text-to-image model and introduces a bounding box generator to identify the editing regions that are aligned with the textual prompts. We show that this simple approach enables flexible editing that is compatible with current image generation models, and is able to handle complex prompts featuring multiple objects, complex sentences, or lengthy paragraphs. We conduct an extensive user study to compare our method against state-of-the-art methods. The experiments demonstrate the competitive performance of our method in manipulating images with high fidelity and realism that correspond to the provided language descriptions. Our project webpage can be found at: https://yuanze-lin.me/LearnableRegions_page.",
        "page": "http://arxiv.org/abs/2311.16432",
        "pdf": "http://arxiv.org/pdf/2311.16432.pdf"
    },
    {
        "title": "Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis",
        "author": "Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, Sergey Tulyakov",
        "abstract": "Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us to efficiently train a text-to-video model with billions of parameters for the first time, reach state-of-the-art results on a number of benchmarks, and generate videos with substantially higher quality, temporal consistency, and motion complexity. The user studies showed that our model was favored by a large margin over the most recent methods. See our website at https://snap-research.github.io/snapvideo/.",
        "page": "http://arxiv.org/abs/2402.14797",
        "pdf": "http://arxiv.org/pdf/2402.14797.pdf"
    },
    {
        "title": "ERMVP: Communication-Efficient and Collaboration-Robust Multi-Vehicle Perception in Challenging Environments",
        "author": "Jingyu Zhang, Kun Yang, Yilei Wang, Hanqi Wang, Peng Sun, Liang Song",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions",
        "author": "Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, Yifeng Shi",
        "abstract": "Although Vision Transformer (ViT) has achieved significant success in computer vision, it does not perform well in dense prediction tasks due to the lack of inner-patch information interaction and the limited diversity of feature scale. Most existing studies are devoted to designing vision-specific transformers to solve the above problems, which introduce additional pre-training costs. Therefore, we present a plain, pre-training-free, and feature-enhanced ViT backbone with Convolutional Multi-scale feature interaction, named ViT-CoMer, which facilitates bidirectional interaction between CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has the following advantages: (1) We inject spatial pyramid multi-receptive field convolutional features into the ViT architecture, which effectively alleviates the problems of limited local information interaction and single-feature representation in ViT. (2) We propose a simple and efficient CNN-Transformer bidirectional fusion interaction module that performs multi-scale fusion across hierarchical features, which is beneficial for handling dense prediction tasks. (3) We evaluate the performance of ViT-CoMer across various dense prediction tasks, different frameworks, and multiple advanced pre-training. Notably, our ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and 62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art methods. We hope ViT-CoMer can serve as a new backbone for dense prediction tasks to facilitate future research. The code will be released at https://github.com/Traffic-X/ViT-CoMer.",
        "page": "http://arxiv.org/abs/2403.07392",
        "pdf": "http://arxiv.org/pdf/2403.07392.pdf"
    },
    {
        "title": "Simple Semantic-Aided Few-Shot Learning",
        "author": "Hai Zhang, Junzhe Xu, Shanlin Jiang, Zhenan He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation",
        "author": "Ba Hung Ngo, Nhat-Tuong Do-Tran, Tuan-Ngoc Nguyen, Hae-Gon Jeon, Tae Jong Choi",
        "abstract": "Most domain adaptation (DA) methods are based on either a convolutional neural networks (CNNs) or a vision transformers (ViTs). They align the distribution differences between domains as encoders without considering their unique characteristics. For instance, ViT excels in accuracy due to its superior ability to capture global representations, while CNN has an advantage in capturing local representations. This fact has led us to design a hybrid method to fully take advantage of both ViT and CNN, called Explicitly Class-specific Boundaries (ECB). ECB learns CNN on ViT to combine their distinct strengths. In particular, we leverage ViT's properties to explicitly find class-specific decision boundaries by maximizing the discrepancy between the outputs of the two classifiers to detect target samples far from the source support. In contrast, the CNN encoder clusters target features based on the previously defined class-specific boundaries by minimizing the discrepancy between the probabilities of the two classifiers. Finally, ViT and CNN mutually exchange knowledge to improve the quality of pseudo labels and reduce the knowledge discrepancies of these models. Compared to conventional DA methods, our ECB achieves superior performance, which verifies its effectiveness in this hybrid model. The project website can be found https://dotrannhattuong.github.io/ECB/website.",
        "page": "http://arxiv.org/abs/2403.18360",
        "pdf": "http://arxiv.org/pdf/2403.18360.pdf"
    },
    {
        "title": "ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting",
        "author": "Yankai Jiang, Zhongzhen Huang, Rongzhao Zhang, Xiaofan Zhang, Shaoting Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection",
        "author": "Xiaotian Li, Baojie Fan, Jiandong Tian, Huijie Fan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Can I Trust Your Answer? Visually Grounded Video Question Answering",
        "author": "Junbin Xiao, Angela Yao, Yicong Li, Tat-seng Chua",
        "abstract": "We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with 10.5$K$ temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy, we further explore and propose a grounded-QA method via Gaussian mask optimization and cross-modal learning. Experiments with different backbones demonstrate that this grounding mechanism improves both grounding and QA. With these efforts, we aim to push towards trustworthy VLMs in VQA systems. Our dataset and code are available at https://github.com/doc-doc/NExT-GQA.",
        "page": "http://arxiv.org/abs/2309.01327",
        "pdf": "http://arxiv.org/pdf/2309.01327.pdf"
    },
    {
        "title": "GenZI: Zero-Shot 3D Human-Scene Interaction Generation",
        "author": "Lei Li, Angela Dai",
        "abstract": "Can we synthesize 3D humans interacting with scenes without learning from any 3D human-scene interaction data? We propose GenZI, the first zero-shot approach to generating 3D human-scene interactions. Key to GenZI is our distillation of interaction priors from large vision-language models (VLMs), which have learned a rich semantic space of 2D human-scene compositions. Given a natural language description and a coarse point location of the desired interaction in a 3D scene, we first leverage VLMs to imagine plausible 2D human interactions inpainted into multiple rendered views of the scene. We then formulate a robust iterative optimization to synthesize the pose and shape of a 3D human model in the scene, guided by consistency with the 2D interaction hypotheses. In contrast to existing learning-based approaches, GenZI circumvents the conventional need for captured 3D interaction data, and allows for flexible control of the 3D interaction synthesis with easy-to-use text prompts. Extensive experiments show that our zero-shot approach has high flexibility and generality, making it applicable to diverse scene types, including both indoor and outdoor environments.",
        "page": "http://arxiv.org/abs/2311.17737",
        "pdf": "http://arxiv.org/pdf/2311.17737.pdf"
    },
    {
        "title": "XFibrosis: Explicit Vessel-Fiber Modeling for Fibrosis Staging from Liver Pathology Images",
        "author": "CHONG YIN, Siqi Liu, Fei Lyu, Jiahao Lu, Sune Darkner, Vincent Wong, Pong C. Yuen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Prompting Vision Foundation Models for Pathology Image Analysis",
        "author": "CHONG YIN, Siqi Liu, Kaiyang Zhou, Vincent Wong, Pong C. Yuen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "One-Class Face Anti-spoofing via Spoof Cue Map-Guided Feature Learning",
        "author": "Pei-Kai Huang, Cheng-Hsuan Chiang, Tzu-Hsien Chen, Jun-Xiong Chong, Tyng-Luh Liu, Chiou-Ting Hsu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ProTeCt: Prompt Tuning for Taxonomic Open Set Classification",
        "author": "Tz-Ying Wu, Chih-Hui Ho, Nuno Vasconcelos",
        "abstract": "Visual-language foundation models, like CLIP, learn generalized representations that enable zero-shot open-set classification. Few-shot adaptation methods, based on prompt tuning, have been shown to further improve performance on downstream datasets. However, these methods do not fare well in the taxonomic open set (TOS) setting, where the classifier is asked to make predictions from label sets across different levels of semantic granularity. Frequently, they infer incorrect labels at coarser taxonomic class levels, even when the inference at the leaf level (original class labels) is correct. To address this problem, we propose a prompt tuning technique that calibrates the hierarchical consistency of model predictions. A set of metrics of hierarchical consistency, the Hierarchical Consistent Accuracy (HCA) and the Mean Treecut Accuracy (MTA), are first proposed to evaluate TOS model performance. A new Prompt Tuning for Hierarchical Consistency (ProTeCt) technique is then proposed to calibrate classification across label set granularities. Results show that ProTeCt can be combined with existing prompt tuning methods to significantly improve TOS classification without degrading the leaf level classification performance.",
        "page": "http://arxiv.org/abs/2306.02240",
        "pdf": "http://arxiv.org/pdf/2306.02240.pdf"
    },
    {
        "title": "Representing Part-Whole Hierarchies in Foundation Models by Learning Localizability, Composability, and Decomposability from Anatomy via Self-Supervision",
        "author": "Mohammad Reza Hosseinzadeh Taher, Michael Gotway, Jianming Liang",
        "abstract": "Humans effortlessly interpret images by parsing them into part-whole hierarchies; deep learning excels in learning multi-level feature spaces, but they often lack explicit coding of part-whole relations, a prominent property of medical imaging. To overcome this limitation, we introduce Adam-v2, a new self-supervised learning framework extending Adam [79] by explicitly incorporating part-whole hierarchies into its learning objectives through three key branches: (1) Localizability, acquiring discriminative representations to distinguish different anatomical patterns; (2) Composability, learning each anatomical structure in a parts-to-whole manner; and (3) Decomposability, comprehending each anatomical structure in a whole-to-parts manner. Experimental results across 10 tasks, compared to 11 baselines in zero-shot, few-shot transfer, and full fine-tuning settings, showcase Adam-v2's superior performance over large-scale medical models and existing SSL methods across diverse downstream tasks. The higher generality and robustness of Adam-v2's representations originate from its explicit construction of hierarchies for distinct anatomical structures from unlabeled medical images. Adam-v2 preserves a semantic balance of anatomical diversity and harmony in its embedding, yielding representations that are both generic and semantically meaningful, yet overlooked in existing SSL methods. All code and pretrained models are available at https://github.com/JLiangLab/Eden.",
        "page": "http://arxiv.org/abs/2404.15672",
        "pdf": "http://arxiv.org/pdf/2404.15672.pdf"
    },
    {
        "title": "OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition",
        "author": "Yuchen Pan, Junjun Jiang, Kui Jiang, Zhihao Wu, Keyuan Yu, Xianming Liu",
        "abstract": "Depression Recognition (DR) poses a considerable challenge, especially in the context of the growing concerns surrounding privacy. Traditional automatic diagnosis of DR technology necessitates the use of facial images, undoubtedly expose the patient identity features and poses privacy risks. In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images, we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features. It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR. More specifically, we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens, which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner. These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models, and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014, respectively.",
        "page": "http://arxiv.org/abs/2402.18786",
        "pdf": "http://arxiv.org/pdf/2402.18786.pdf"
    },
    {
        "title": "A Stealthy Wrongdoer: Feature-Oriented Reconstruction Attack against Split Learning",
        "author": "Xiaoyang Xu, Mengda Yang, Wenzhe Yi, Ziang Li, Juan Wang, Hongxin Hu, Yong ZHUANG, Yaxin Liu",
        "abstract": "Split Learning (SL) is a distributed learning framework renowned for its privacy-preserving features and minimal computational requirements. Previous research consistently highlights the potential privacy breaches in SL systems by server adversaries reconstructing training data. However, these studies often rely on strong assumptions or compromise system utility to enhance attack performance. This paper introduces a new semi-honest Data Reconstruction Attack on SL, named Feature-Oriented Reconstruction Attack (FORA). In contrast to prior works, FORA relies on limited prior knowledge, specifically that the server utilizes auxiliary samples from the public without knowing any client's private information. This allows FORA to conduct the attack stealthily and achieve robust performance. The key vulnerability exploited by FORA is the revelation of the model representation preference in the smashed data output by victim client. FORA constructs a substitute client through feature-level transfer learning, aiming to closely mimic the victim client's representation preference. Leveraging this substitute client, the server trains the attack model to effectively reconstruct private data. Extensive experiments showcase FORA's superior performance compared to state-of-the-art methods. Furthermore, the paper systematically evaluates the proposed method's applicability across diverse settings and advanced defense strategies.",
        "page": "http://arxiv.org/abs/2405.04115",
        "pdf": "http://arxiv.org/pdf/2405.04115.pdf"
    },
    {
        "title": "MS-DETR: Efficient DETR Training  with Mixed Supervision",
        "author": "Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, Jingdong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning to Control Camera Exposure via Reinforcement Learning",
        "author": "Kyunghyun Lee, Ukcheol Shin, Byeong-Uk Lee",
        "abstract": "Adjusting camera exposure in arbitrary lighting conditions is the first step to ensure the functionality of computer vision applications. Poorly adjusted camera exposure often leads to critical failure and performance degradation. Traditional camera exposure control methods require multiple convergence steps and time-consuming processes, making them unsuitable for dynamic lighting conditions. In this paper, we propose a new camera exposure control framework that rapidly controls camera exposure while performing real-time processing by exploiting deep reinforcement learning. The proposed framework consists of four contributions: 1) a simplified training ground to simulate real-world's diverse and dynamic lighting changes, 2) flickering and image attribute-aware reward design, along with lightweight state design for real-time processing, 3) a static-to-dynamic lighting curriculum to gradually improve the agent's exposure-adjusting capability, and 4) domain randomization techniques to alleviate the limitation of the training ground and achieve seamless generalization in the wild.As a result, our proposed method rapidly reaches a desired exposure level within five steps with real-time processing (1 ms). Also, the acquired images are well-exposed and show superiority in various computer vision tasks, such as feature extraction and object detection.",
        "page": "http://arxiv.org/abs/2404.01636",
        "pdf": "http://arxiv.org/pdf/2404.01636.pdf"
    },
    {
        "title": "RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection",
        "author": "Ximiao Zhang, Min Xu, Xiuzhuang Zhou",
        "abstract": "Self-supervised feature reconstruction methods have shown promising advances in industrial image anomaly detection and localization. Despite this progress, these methods still face challenges in synthesizing realistic and diverse anomaly samples, as well as addressing the feature redundancy and pre-training bias of pre-trained feature. In this work, we introduce RealNet, a feature reconstruction network with realistic synthetic anomaly and adaptive feature selection. It is incorporated with three key innovations: First, we propose Strength-controllable Diffusion Anomaly Synthesis (SDAS), a diffusion process-based synthesis strategy capable of generating samples with varying anomaly strengths that mimic the distribution of real anomalous samples. Second, we develop Anomaly-aware Features Selection (AFS), a method for selecting representative and discriminative pre-trained feature subsets to improve anomaly detection performance while controlling computational costs. Third, we introduce Reconstruction Residuals Selection (RRS), a strategy that adaptively selects discriminative residuals for comprehensive identification of anomalous regions across multiple levels of granularity. We assess RealNet on four benchmark datasets, and our results demonstrate significant improvements in both Image AUROC and Pixel AUROC compared to the current state-o-the-art methods. The code, data, and models are available at https://github.com/cnulab/RealNet.",
        "page": "http://arxiv.org/abs/2403.05897",
        "pdf": "http://arxiv.org/pdf/2403.05897.pdf"
    },
    {
        "title": "Neural Video Compression with Feature Modulation",
        "author": "Jiahao Li, Bin Li, Yan Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image",
        "author": "Yunhao Li, Xiaodong Wang, Ping Wang, Xin Yuan, Peidong Liu",
        "abstract": "In this paper, we explore the potential of Snapshot Compressive Imaging (SCI) technique for recovering the underlying 3D scene representation from a single temporal compressed image. SCI is a cost-effective method that enables the recording of high-dimensional data, such as hyperspectral or temporal information, into a single image using low-cost 2D imaging sensors. To achieve this, a series of specially designed 2D masks are usually employed, which not only reduces storage requirements but also offers potential privacy protection. Inspired by this, to take one step further, our approach builds upon the powerful 3D scene representation capabilities of neural radiance fields (NeRF). Specifically, we formulate the physical imaging process of SCI as part of the training of NeRF, allowing us to exploit its impressive performance in capturing complex scene structures. To assess the effectiveness of our method, we conduct extensive evaluations using both synthetic data and real data captured by our SCI system. Extensive experimental results demonstrate that our proposed approach surpasses the state-of-the-art methods in terms of image reconstruction and novel view image synthesis. Moreover, our method also exhibits the ability to restore high frame-rate multi-view consistent images by leveraging SCI and the rendering capabilities of NeRF. The code is available at https://github.com/WU-CVGL/SCINeRF.",
        "page": "http://arxiv.org/abs/2403.20018",
        "pdf": "http://arxiv.org/pdf/2403.20018.pdf"
    },
    {
        "title": "Day-Night Cross-domain Vehicle Re-identification",
        "author": "Hongchao Li, Jingong Chen, AIHUA ZHENG, Yong Wu, YongLong Luo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Training-free Pretrained Model Merging",
        "author": "Zhengqi Xu, Ke Yuan, Huiqiong Wang, Yong Wang, Mingli Song, Jie Song",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Weakly Misalignment-free Adaptive Feature Alignment for UAVs-based Multimodal Object Detection",
        "author": "Chen Chen, Jiahao Qi, Xingyue Liu, Kangcheng Bin, Ruigang Fu, Xikun Hu, Ping Zhong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Open-Vocabulary Video Anomaly Detection",
        "author": "Peng Wu, Xuerong Zhou, Guansong Pang, Yujia Sun, Jing Liu, Peng Wang, Yanning Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BlockGCN: Redefine Topology Awareness for Skeleton-Based Action Recognition",
        "author": "Yuxuan Zhou, Xudong Yan, Zhi-Qi Cheng, Yan Yan, Qi Dai, Xian-Sheng Hua",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fine-grained Prototypical Voting with Heterogeneous Mixup for Semi-supervised 2D-3D Cross-modal Retrieval",
        "author": "Fan Zhang, Xian-Sheng Hua, Chong Chen, Xiao Luo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes",
        "author": "Yihua Huang, Yangtian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transfomers",
        "author": "Sheng Yang, Jiawang Bai, Kuofeng Gao, Yong Yang, Yiming Li, Shu-Tao Xia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Weakly Supervised Monocular 3D Detection with a Single-View Image",
        "author": "Xueying Jiang, Sheng Jin, Lewei Lu, Xiaoqin Zhang, Shijian Lu",
        "abstract": "Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods.",
        "page": "http://arxiv.org/abs/2402.19144",
        "pdf": "http://arxiv.org/pdf/2402.19144.pdf"
    },
    {
        "title": "Coherent Temporal Synthesis for Incremental Action Segmentation",
        "author": "Guodong Ding, Hans Golong, Angela Yao",
        "abstract": "Data replay is a successful incremental learning technique for images. It prevents catastrophic forgetting by keeping a reservoir of previous data, original or synthesized, to ensure the model retains past knowledge while adapting to novel concepts. However, its application in the video domain is rudimentary, as it simply stores frame exemplars for action recognition. This paper presents the first exploration of video data replay techniques for incremental action segmentation, focusing on action temporal modeling. We propose a Temporally Coherent Action (TCA) model, which represents actions using a generative model instead of storing individual frames. The integration of a conditioning variable that captures temporal coherence allows our model to understand the evolution of action features over time. Therefore, action segments generated by TCA for replay are diverse and temporally coherent. In a 10-task incremental setup on the Breakfast dataset, our approach achieves significant increases in accuracy for up to 22% compared to the baselines.",
        "page": "http://arxiv.org/abs/2403.06102",
        "pdf": "http://arxiv.org/pdf/2403.06102.pdf"
    },
    {
        "title": "Domain Prompt Learning with Quaternion Networks",
        "author": "Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, Xiaokang Yang",
        "abstract": "Prompt learning has emerged as an effective and data-efficient technique in large Vision-Language Models (VLMs). However, when adapting VLMs to specialized domains such as remote sensing and medical imaging, domain prompt learning remains underexplored. While large-scale domain-specific foundation models can help tackle this challenge, their concentration on a single vision level makes it challenging to prompt both vision and language modalities. To overcome this, we propose to leverage domain-specific knowledge from domain-specific foundation models to transfer the robust recognition ability of VLMs from generalized to specialized domains, using quaternion networks. Specifically, the proposed method involves using domain-specific vision features from domain-specific foundation models to guide the transformation of generalized contextual embeddings from the language branch into a specialized space within the quaternion networks. Moreover, we present a hierarchical approach that generates vision prompt features by analyzing intermodal relationships between hierarchical language prompt features and domain-specific vision features. In this way, quaternion networks can effectively mine the intermodal relationships in the specific domain, facilitating domain-specific vision-language contrastive learning. Extensive experiments on domain-specific datasets show that our proposed method achieves new state-of-the-art results in prompt learning.",
        "page": "http://arxiv.org/abs/2312.08878",
        "pdf": "http://arxiv.org/pdf/2312.08878.pdf"
    },
    {
        "title": "Inversion-Free Image Editing with Language-Guided Diffusion Models",
        "author": "Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, Joyce Chai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors",
        "author": "Dave Zhenyu Chen, Haoxuan Li, Hsin-Ying Lee, Sergey Tulyakov, Matthias Nie\u00dfner",
        "abstract": "We propose SceneTex, a novel method for effectively generating high-quality and style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike previous methods that either iteratively warp 2D views onto a mesh surface or distillate diffusion latent features without accurate geometric and style cues, SceneTex formulates the texture synthesis task as an optimization problem in the RGB space where style and geometry consistency are properly reflected. At its core, SceneTex proposes a multiresolution texture field to implicitly encode the mesh appearance. We optimize the target texture via a score-distillation-based objective function in respective RGB renderings. To further secure the style consistency across views, we introduce a cross-attention decoder to predict the RGB values by cross-attending to the pre-sampled reference locations in each instance. SceneTex enables various and accurate texture synthesis for 3D-FRONT scenes, demonstrating significant improvements in visual quality and prompt fidelity over the prior texture generation methods.",
        "page": "http://arxiv.org/abs/2311.17261",
        "pdf": "http://arxiv.org/pdf/2311.17261.pdf"
    },
    {
        "title": "MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding",
        "author": "Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can Cui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang, James Rehg, chao zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SuperPrimitive: Scene Reconstruction at a Primitive Level",
        "author": "Kirill Mazur, Gwangbin Bae, Andrew J. Davison",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection",
        "author": "Boyang Peng, Sanqing Qu, Yong Wu, Tianpei Zou, Lianghua He, Alois Knoll, Guang Chen, Changjun Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World",
        "author": "Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, Yu Qiao",
        "abstract": "Being able to map the activities of others into one's own point of view is one fundamental human skill even from a very early age. Taking a step toward understanding this human ability, we introduce EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by demonstration videos. Focusing on the potential applications in daily assistance and professional support, EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories. Along with the videos we record high-quality gaze data and provide detailed multimodal annotations, formulating a playground for modeling the human ability to bridge asynchronous procedural actions from different viewpoints. To this end, we present benchmarks such as cross-view association, cross-view action planning, and cross-view referenced skill assessment, along with detailed analysis. We expect EgoExoLearn can serve as an important resource for bridging the actions across views, thus paving the way for creating AI agents capable of seamlessly learning by observing humans in the real world. Code and data can be found at: https://github.com/OpenGVLab/EgoExoLearn",
        "page": "http://arxiv.org/abs/2403.16182",
        "pdf": "http://arxiv.org/pdf/2403.16182.pdf"
    },
    {
        "title": "Object Recognition as Next Token Prediction",
        "author": "Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim",
        "abstract": "We present an approach to pose object recognition as next token prediction. The idea is to apply a language decoder that auto-regressively predicts the text tokens from image embeddings to form labels. To ground this prediction process in auto-regression, we customize a non-causal attention mask for the decoder, incorporating two key features: modeling tokens from different labels to be independent, and treating image tokens as a prefix. This masking mechanism inspires an efficient method - one-shot sampling - to simultaneously sample tokens of multiple labels in parallel and rank generated labels by their probabilities during inference. To further enhance the efficiency, we propose a simple strategy to construct a compact decoder by simply discarding the intermediate blocks of a pretrained language model. This approach yields a decoder that matches the full model's performance while being notably more efficient. The code is available at https://github.com/kaiyuyue/nxtp",
        "page": "http://arxiv.org/abs/2312.02142",
        "pdf": "http://arxiv.org/pdf/2312.02142.pdf"
    },
    {
        "title": "Garment Recovery with Shape and Deformation Priors",
        "author": "Ren Li, Corentin Dumery, Beno\u00eet Guillard, Pascal Fua",
        "abstract": "While modeling people wearing tight-fitting clothing has made great strides in recent years, loose-fitting clothing remains a challenge. We propose a method that delivers realistic garment models from real-world images, regardless of garment shape or deformation. To this end, we introduce a fitting approach that utilizes shape and deformation priors learned from synthetic data to accurately capture garment shapes and deformations, including large ones. Not only does our approach recover the garment geometry accurately, it also yields models that can be directly used by downstream applications such as animation and simulation.",
        "page": "http://arxiv.org/abs/2311.10356",
        "pdf": "http://arxiv.org/pdf/2311.10356.pdf"
    },
    {
        "title": "Learning to Rank Patches for Unbiased Image Redundancy Reduction",
        "author": "Yang Luo, Zhineng Chen, Peng Zhou, Zuxuan Wu, Xieping Gao, Yu-Gang Jiang",
        "abstract": "Images suffer from heavy spatial redundancy because pixels in neighboring regions are spatially correlated. Existing approaches strive to overcome this limitation by reducing less meaningful image regions. However, current leading methods rely on supervisory signals. They may compel models to preserve content that aligns with labeled categories and discard content belonging to unlabeled categories. This categorical inductive bias makes these methods less effective in real-world scenarios. To address this issue, we propose a self-supervised framework for image redundancy reduction called Learning to Rank Patches (LTRP). We observe that image reconstruction of masked image modeling models is sensitive to the removal of visible patches when the masking ratio is high (e.g., 90\\%). Building upon it, we implement LTRP via two steps: inferring the semantic density score of each patch by quantifying variation between reconstructions with and without this patch, and learning to rank the patches with the pseudo score. The entire process is self-supervised, thus getting out of the dilemma of categorical inductive bias. We design extensive experiments on different datasets and tasks. The results demonstrate that LTRP outperforms both supervised and other self-supervised methods due to the fair assessment of image content.",
        "page": "http://arxiv.org/abs/2404.00680",
        "pdf": "http://arxiv.org/pdf/2404.00680.pdf"
    },
    {
        "title": "Multi-Scale Video Anomaly Detection by Multi-Grained Spatio-Temporal Representation Learning",
        "author": "Menghao Zhang, Jingyu Wang, Qi Qi, Haifeng Sun, Zirui Zhuang, Pengfei Ren, Ruilong Ma, Jianxin Liao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Consistent Prompting for Rehearsal-Free Continual Learning",
        "author": "Zhanxin Gao, Jun Cen, Xiaobin Chang",
        "abstract": "Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifier consistency learning. In addition, prompt consistency learning is proposed to enhance prediction robustness and boost prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based counterparts and achieves state-of-the-art performance on multiple continual learning benchmarks. Detailed analysis shows that improvements come from more consistent training and testing.",
        "page": "http://arxiv.org/abs/2403.08568",
        "pdf": "http://arxiv.org/pdf/2403.08568.pdf"
    },
    {
        "title": "TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models",
        "author": "Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, Xianglong Liu",
        "abstract": "The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\\{1, \\ldots, T\\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TFMQ) framework building upon a Temporal Information Block which is just related to the time-step $t$ and unrelated to the sampling data. Powered by the pioneering block design, we devise temporal information aware reconstruction (TIAR) and finite set calibration (FSC) to align the full-precision temporal features in a limited time. Equipped with the framework, we can maintain the most temporal information and ensure the end-to-end generation quality. Extensive experiments on various datasets and diffusion models prove our state-of-the-art results. Remarkably, our quantization approach, for the first time, achieves model performance nearly on par with the full-precision model under 4-bit weight quantization. Additionally, our method incurs almost no extra computational cost and accelerates quantization time by $2.0 \\times$ on LSUN-Bedrooms $256 \\times 256$ compared to previous works. Our code is publicly available at https://github.com/ModelTC/TFMQ-DM.",
        "page": "http://arxiv.org/abs/2311.16503",
        "pdf": "http://arxiv.org/pdf/2311.16503.pdf"
    },
    {
        "title": "PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution",
        "author": "Honghao Chen, Xiangxiang Chu, Renyongjian, Xin Zhao, Kaiqi Huang",
        "abstract": "Recently, some large kernel convnets strike back with appealing performance and efficiency. However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem. Due to these issues, current CNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e., 51x5 + 5x51) and start to saturate as the kernel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human vision, we propose a human-like peripheral convolution that efficiently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large. Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K^2) to O(logK) without backfiring performance. Built on this, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on MS COCO. For the first time, we successfully scale up the kernel size of CNNs to an unprecedented 101x101 and demonstrate consistent improvements.",
        "page": "http://arxiv.org/abs/2403.07589",
        "pdf": "http://arxiv.org/pdf/2403.07589.pdf"
    },
    {
        "title": "Towards Better Vision-Inspired Vision-Language Models",
        "author": "Yun-Hao Cao, Kaixiang Ji, Ziyuan Huang, Chuanyang Zheng, Jiajia Liu, Jian Wang, Jingdong Chen, Ming Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Bidirectional Autoregessive Diffusion Model for Dance Generation",
        "author": "Canyu Zhang, Youbao Tang, NING Zhang, Ruei-Sung Lin, Mei Han, Jing Xiao, Song Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold",
        "author": "Guangyu Wang, Jinzhi Zhang, Fan Wang, Ruqi Huang, Lu Fang",
        "abstract": "We propose XScale-NVS for high-fidelity cross-scale novel view synthesis of real-world large-scale scenes. Existing representations based on explicit surface suffer from discretization resolution or UV distortion, while implicit volumetric representations lack scalability for large scenes due to the dispersed weight distribution and surface ambiguity. In light of the above challenges, we introduce hash featurized manifold, a novel hash-based featurization coupled with a deferred neural rendering framework. This approach fully unlocks the expressivity of the representation by explicitly concentrating the hash entries on the 2D manifold, thus effectively representing highly detailed contents independent of the discretization resolution. We also introduce a novel dataset, namely GigaNVS, to benchmark cross-scale, high-resolution novel view synthesis of realworld large-scale scenes. Our method significantly outperforms competing baselines on various real-world scenes, yielding an average LPIPS that is 40% lower than prior state-of-the-art on the challenging GigaNVS benchmark. Please see our project page at: xscalenvs.github.io.",
        "page": "http://arxiv.org/abs/2403.19517",
        "pdf": "http://arxiv.org/pdf/2403.19517.pdf"
    },
    {
        "title": "Neural Refinement for Absolute Pose Regression with Feature Synthesis",
        "author": "Shuai Chen, Yash Bhalgat, Xinghui Li, Jia-Wang Bian, Kejie Li, Zirui Wang, Victor Adrian Prisacariu",
        "abstract": "Absolute Pose Regression (APR) methods use deep neural networks to directly regress camera poses from RGB images. However, the predominant APR architectures only rely on 2D operations during inference, resulting in limited accuracy of pose estimation due to the lack of 3D geometry constraints or priors. In this work, we propose a test-time refinement pipeline that leverages implicit geometric constraints using a robust feature field to enhance the ability of APR methods to use 3D information during inference. We also introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D geometric features during training and directly renders dense novel view features at test time to refine APR methods. To enhance the robustness of our model, we introduce a feature fusion module and a progressive training strategy. Our proposed method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets.",
        "page": "http://arxiv.org/abs/2303.10087",
        "pdf": "http://arxiv.org/pdf/2303.10087.pdf"
    },
    {
        "title": "TextNeRF: A Novel Scene-Text Image Synthesis Method based on Neural Radiance Fields",
        "author": "Jialei Cui, Jianwei Du, Wenzhuo Liu, Zhouhui Lian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "No More Ambiguity in 360$^\\circ$ Room Layout via Bi-Layout Estimation",
        "author": "Yu-Ju Tsai, Jin-Cheng Jhang, JINGJING ZHENG, Wei Wang, Albert Chen, Min Sun, Cheng-Hao Kuo, Ming-Hsuan Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing",
        "author": "Li Maomao, Yu Li, Tianyu Yang, Yunfei Liu, Dongxu Yue, Zhihui Lin, Dong Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis",
        "author": "Yuming Gu, Hongyi Xu, You Xie, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, Linjie Luo",
        "abstract": "We present DiffPortrait3D, a conditional diffusion model that is capable of synthesizing 3D-consistent photo-realistic novel views from as few as a single in-the-wild portrait. Specifically, given a single RGB input, we aim to synthesize plausible but consistent facial details rendered from novel camera views with retained both identity and facial expression. In lieu of time-consuming optimization and fine-tuning, our zero-shot method generalizes well to arbitrary face portraits with unposed camera views, extreme facial expressions, and diverse artistic depictions. At its core, we leverage the generative prior of 2D diffusion models pre-trained on large-scale image datasets as our rendering backbone, while the denoising is guided with disentangled attentive control of appearance and camera pose. To achieve this, we first inject the appearance context from the reference image into the self-attention layers of the frozen UNets. The rendering view is then manipulated with a novel conditional control module that interprets the camera pose by watching a condition image of a crossed subject from the same view. Furthermore, we insert a trainable cross-view attention module to enhance view consistency, which is further strengthened with a novel 3D-aware noise generation process during inference. We demonstrate state-of-the-art results both qualitatively and quantitatively on our challenging in-the-wild and multi-view benchmarks.",
        "page": "http://arxiv.org/abs/2312.13016",
        "pdf": "http://arxiv.org/pdf/2312.13016.pdf"
    },
    {
        "title": "Learning Group Activity Features Through Person Attribute Prediction",
        "author": "Chihiro Nakatani, Hiroaki Kawashima, Norimichi Ukita",
        "abstract": "This paper proposes Group Activity Feature (GAF) learning in which features of multi-person activity are learned as a compact latent vector. Unlike prior work in which the manual annotation of group activities is required for supervised learning, our method learns the GAF through person attribute prediction without group activity annotations. By learning the whole network in an end-to-end manner so that the GAF is required for predicting the person attributes of people in a group, the GAF is trained as the features of multi-person activity. As a person attribute, we propose to use a person's action class and appearance features because the former is easy to annotate due to its simpleness, and the latter requires no manual annotation. In addition, we introduce a location-guided attribute prediction to disentangle the complex GAF for extracting the features of each target person properly. Various experimental results validate that our method outperforms SOTA methods quantitatively and qualitatively on two public datasets. Visualization of our GAF also demonstrates that our method learns the GAF representing fined-grained group activity classes. Code: https://github.com/chihina/GAFL-CVPR2024.",
        "page": "http://arxiv.org/abs/2403.02753",
        "pdf": "http://arxiv.org/pdf/2403.02753.pdf"
    },
    {
        "title": "Egocentric Full Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement",
        "author": "Jian Wang, Zhe Cao, Diogo Luvizon, Lingjie Liu, Kripasindhu Sarkar, Danhang Tang, Thabo Beeler, Christian Theobalt",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Revamping Federated Learning Security from a Defender's Perspective: A Unified Defense with Homomorphic Encrypted Data Space",
        "author": "Naveen Kumar Kummari, Reshmi Mitra, Krishna Mohan Chalavadi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Transferable Negative Prompts for Out-of-Distribution Detection",
        "author": "Tianqi Li, Guansong Pang, wenjun miao, Xiao Bai, Jin Zheng",
        "abstract": "Existing prompt learning methods have shown certain capabilities in Out-of-Distribution (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories, resulting in a high false positive rate. To address this issue, we introduce a novel OOD detection method, named 'NegPrompt', to learn a set of negative prompts, each representing a negative connotation of a given class label, for delineating the boundaries between ID and OOD images. It learns such negative prompts with ID data only, without any reliance on external outlier data. Further, current methods assume the availability of samples of all ID classes, rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training. In contrast, our learned negative prompts are transferable to novel class labels. Experiments on various ImageNet benchmarks show that NegPrompt surpasses state-of-the-art prompt-learning-based OOD detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios. Code is available at https://github.com/mala-lab/negprompt.",
        "page": "http://arxiv.org/abs/2404.03248",
        "pdf": "http://arxiv.org/pdf/2404.03248.pdf"
    },
    {
        "title": "Class Incremental Learning with Multi-Teacher Distillation",
        "author": "Haitao Wen, Lili Pan, Yu Dai, Heqian Qiu, Lanxiao Wang, Qingbo Wu, Hongliang Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CLIP-Driven Open-Vocabulary 3D Scene Graph Generation via Cross-Modality Contrastive Learning",
        "author": "Lianggangxu Chen, Xuejiao Wang, Jiale Lu, Shaohui Lin, Changbo Wang, Gaoqi He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Self-Distilled Masked Auto-Encoders are Efficient  Video Anomaly Detectors",
        "author": "Nicolae Ristea, Florinel Croitoru, Radu Tudor Ionescu, Marius Popescu, Fahad Shahbaz Khan, Mubarak Shah",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling",
        "author": "Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking",
        "author": "Jialin Li, Qiang Nie, Weifu Fu, Yuhuan Lin, Guangpin Tao, Yong Liu, Chengjie Wang",
        "abstract": "Deep learning models, particularly those based on transformers, often employ numerous stacked structures, which possess identical architectures and perform similar functions. While effective, this stacking paradigm leads to a substantial increase in the number of parameters, posing challenges for practical applications. In today's landscape of increasingly large models, stacking depth can even reach dozens, further exacerbating this issue. To mitigate this problem, we introduce LORS (LOw-rank Residual Structure). LORS allows stacked modules to share the majority of parameters, requiring a much smaller number of unique ones per module to match or even surpass the performance of using entirely distinct ones, thereby significantly reducing parameter usage. We validate our method by applying it to the stacked decoders of a query-based object detector, and conduct extensive experiments on the widely used MS COCO dataset. Experimental results demonstrate the effectiveness of our method, as even with a 70\\% reduction in the parameters of the decoder, our method still enables the model to achieve comparable or",
        "page": "http://arxiv.org/abs/2403.04303",
        "pdf": "http://arxiv.org/pdf/2403.04303.pdf"
    },
    {
        "title": "A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models",
        "author": "Julio Silva-Rodr\u00edguez, Sina Hajimiri, Ismail Ben Ayed, Jose Dolz",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FedMef: Towards Memory-efficient Federated Dynamic Pruning",
        "author": "Hong Huang, Weiming Zhuang, Chen Chen, Lingjuan Lyu",
        "abstract": "Federated learning (FL) promotes decentralized training while prioritizing data confidentiality. However, its application on resource-constrained devices is challenging due to the high demand for computation and memory resources to train deep learning models. Neural network pruning techniques, such as dynamic pruning, could enhance model efficiency, but directly adopting them in FL still poses substantial challenges, including post-pruning performance degradation, high activation memory usage, etc. To address these challenges, we propose FedMef, a novel and memory-efficient federated dynamic pruning framework. FedMef comprises two key components. First, we introduce the budget-aware extrusion that maintains pruning efficiency while preserving post-pruning performance by salvaging crucial information from parameters marked for pruning within a given budget. Second, we propose scaled activation pruning to effectively reduce activation memory footprints, which is particularly beneficial for deploying FL to memory-limited devices. Extensive experiments demonstrate the effectiveness of our proposed FedMef. In particular, it achieves a significant reduction of 28.5% in memory footprint compared to state-of-the-art methods while obtaining superior accuracy.",
        "page": "http://arxiv.org/abs/2403.14737",
        "pdf": "http://arxiv.org/pdf/2403.14737.pdf"
    },
    {
        "title": "3D Human Pose Perception from Egocentric Stereo Videos",
        "author": "Hiroyasu Akada, Jian Wang, Vladislav Golyanik, Christian Theobalt",
        "abstract": "While head-mounted devices are becoming more compact, they provide egocentric views with significant self-occlusions of the device user. Hence, existing methods often fail to accurately estimate complex 3D poses from egocentric views. In this work, we propose a new transformer-based framework to improve egocentric stereo 3D human pose estimation, which leverages the scene information and temporal context of egocentric stereo videos. Specifically, we utilize 1) depth features from our 3D scene reconstruction module with uniformly sampled windows of egocentric stereo frames, and 2) human joint queries enhanced by temporal features of the video inputs. Our method is able to accurately estimate human poses even in challenging scenarios, such as crouching and sitting. Furthermore, we introduce two new benchmark datasets, i.e., UnrealEgo2 and UnrealEgo-RW (RealWorld). The proposed datasets offer a much larger number of egocentric stereo views with a wider variety of human motions than the existing datasets, allowing comprehensive evaluation of existing and upcoming methods. Our extensive experiments show that the proposed approach significantly outperforms previous methods. We will release UnrealEgo2, UnrealEgo-RW, and trained models on our project page.",
        "page": "http://arxiv.org/abs/2401.00889",
        "pdf": "http://arxiv.org/pdf/2401.00889.pdf"
    },
    {
        "title": "Super-Resolution Reconstruction from Bayer-Pattern Spike Streams",
        "author": "Yanchen Dong, Ruiqin Xiong, Jian Zhang, Zhaofei Yu, Xiaopeng Fan, Shuyuan Zhu, Tiejun Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Vision Check-up for Language Models",
        "author": "Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Using Human Feedback to Fine-tune Diffusion Models  without Any Reward Model",
        "author": "Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, Xiu Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation",
        "author": "Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, Shuqiang Jiang",
        "abstract": "Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.",
        "page": "http://arxiv.org/abs/2404.01943",
        "pdf": "http://arxiv.org/pdf/2404.01943.pdf"
    },
    {
        "title": "UniMODE: Unified Monocular 3D Object Detection",
        "author": "Zhuoling Li, Xiaogang Xu, Ser-Nam Lim, Hengshuang Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Dynamic Kernel Prior Model for Unsupervised Blind Image Super-Resolution",
        "author": "Zhixiong Yang, Jingyuan Xia, Shengxi Li, Xinghua Huang, Shuanghui Zhang, Zhen Liu, Yaowen Fu, Yongxiang Liu",
        "abstract": "Deep learning-based methods have achieved significant successes on solving the blind super-resolution (BSR) problem. However, most of them request supervised pre-training on labelled datasets. This paper proposes an unsupervised kernel estimation model, named dynamic kernel prior (DKP), to realize an unsupervised and pre-training-free learning-based algorithm for solving the BSR problem. DKP can adaptively learn dynamic kernel priors to realize real-time kernel estimation, and thereby enables superior HR image restoration performances. This is achieved by a Markov chain Monte Carlo sampling process on random kernel distributions. The learned kernel prior is then assigned to optimize a blur kernel estimation network, which entails a network-based Langevin dynamic optimization strategy. These two techniques ensure the accuracy of the kernel estimation. DKP can be easily used to replace the kernel estimation models in the existing methods, such as Double-DIP and FKP-DIP, or be added to the off-the-shelf image restoration model, such as diffusion model. In this paper, we incorporate our DKP model with DIP and diffusion model, referring to DIP-DKP and Diff-DKP, for validations. Extensive simulations on Gaussian and motion kernel scenarios demonstrate that the proposed DKP model can significantly improve the kernel estimation with comparable runtime and memory usage, leading to state-of-the-art BSR results. The code is available at https://github.com/XYLGroup/DKP.",
        "page": "http://arxiv.org/abs/2404.15620",
        "pdf": "http://arxiv.org/pdf/2404.15620.pdf"
    },
    {
        "title": "Audio-Visual Segmentation via Unlabeled Frame Exploitation",
        "author": "Jinxiang Liu, Yikun Liu, Ferenas, Chen Ju, Ya Zhang, Yanfeng Wang",
        "abstract": "Audio-visual segmentation (AVS) aims to segment the sounding objects in video frames. Although great progress has been witnessed, we experimentally reveal that current methods reach marginal performance gain within the use of the unlabeled frames, leading to the underutilization issue. To fully explore the potential of the unlabeled frames for AVS, we explicitly divide them into two categories based on their temporal characteristics, i.e., neighboring frame (NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame, often contain rich motion information that assists in the accurate localization of sounding objects. Contrary to NFs, DFs have long temporal distances from the labeled frame, which share semantic-similar objects with appearance variations. Considering their unique characteristics, we propose a versatile framework that effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the motion cues as the dynamic guidance to improve the objectness localization. Besides, we exploit the semantic cues in DFs by treating them as valid augmentations to the labeled frames, which are then used to enrich data diversity in a self-training manner. Extensive experimental results demonstrate the versatility and superiority of our method, unleashing the power of the abundant unlabeled frames.",
        "page": "http://arxiv.org/abs/2403.11074",
        "pdf": "http://arxiv.org/pdf/2403.11074.pdf"
    },
    {
        "title": "Editable Scene Simulation for Autonomous Driving via LLM-Agent Collaboration",
        "author": "Yuxi Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing Liu, Hao Zhao, Siheng Chen, Yanfeng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs",
        "author": "Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael Ryoo, Tsung-Yu Lin",
        "abstract": "Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA). However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.",
        "page": "http://arxiv.org/abs/2404.07449",
        "pdf": "http://arxiv.org/pdf/2404.07449.pdf"
    },
    {
        "title": "Enhancing Quality of Compressed Images by Mitigating Enhancement Bias Towards Compression Domain",
        "author": "Qunliang Xing, Mai Xu, Shengxi Li, Xin Deng, Meisong Zheng, huaida liu, Ying Chen",
        "abstract": "Existing quality enhancement methods for compressed images focus on aligning the enhancement domain with the raw domain to yield realistic images. However, these methods exhibit a pervasive enhancement bias towards the compression domain, inadvertently regarding it as more realistic than the raw domain. This bias makes enhanced images closely resemble their compressed counterparts, thus degrading their perceptual quality. In this paper, we propose a simple yet effective method to mitigate this bias and enhance the quality of compressed images. Our method employs a conditional discriminator with the compressed image as a key condition, and then incorporates a domain-divergence regularization to actively distance the enhancement domain from the compression domain. Through this dual strategy, our method enables the discrimination against the compression domain, and brings the enhancement domain closer to the raw domain. Comprehensive quality evaluations confirm the superiority of our method over other state-of-the-art methods without incurring inference overheads.",
        "page": "http://arxiv.org/abs/2402.17200",
        "pdf": "http://arxiv.org/pdf/2402.17200.pdf"
    },
    {
        "title": "Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo",
        "author": "Zongrui Li, Zhan Lu, Haojie Yan, Boxin Shi, Gang Pan, Qian Zheng, Xudong Jiang",
        "abstract": "Natural Light Uncalibrated Photometric Stereo (NaUPS) relieves the strict environment and light assumptions in classical Uncalibrated Photometric Stereo (UPS) methods. However, due to the intrinsic ill-posedness and high-dimensional ambiguities, addressing NaUPS is still an open question. Existing works impose strong assumptions on the environment lights and objects' material, restricting the effectiveness in more general scenarios. Alternatively, some methods leverage supervised learning with intricate models while lacking interpretability, resulting in a biased estimation. In this work, we proposed Spin Light Uncalibrated Photometric Stereo (Spin-UP), an unsupervised method to tackle NaUPS in various environment lights and objects. The proposed method uses a novel setup that captures the object's images on a rotatable platform, which mitigates NaUPS's ill-posedness by reducing unknowns and provides reliable priors to alleviate NaUPS's ambiguities. Leveraging neural inverse rendering and the proposed training strategies, Spin-UP recovers surface normals, environment light, and isotropic reflectance under complex natural light with low computational cost. Experiments have shown that Spin-UP outperforms other supervised / unsupervised NaUPS methods and achieves state-of-the-art performance on synthetic and real-world datasets. Codes and data are available at https://github.com/LMozart/CVPR2024-SpinUP.",
        "page": "http://arxiv.org/abs/2404.01612",
        "pdf": "http://arxiv.org/pdf/2404.01612.pdf"
    },
    {
        "title": "EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation",
        "author": "Chanyoung Kim, Woojung Han, Dayun Ju, Seong Jae Hwang",
        "abstract": "Semantic segmentation has innately relied on extensive pixel-level annotated data, leading to the emergence of unsupervised methodologies. Among them, leveraging self-supervised Vision Transformers for unsupervised semantic segmentation (USS) has been making steady progress with expressive deep features. Yet, for semantically segmenting images with complex objects, a predominant challenge remains: the lack of explicit object-level semantic encoding in patch-level features. This technical limitation often leads to inadequate segmentation of complex objects with diverse structures. To address this gap, we present a novel approach, EAGLE, which emphasizes object-centric representation learning for unsupervised semantic segmentation. Specifically, we introduce EiCue, a spectral technique providing semantic and structural cues through an eigenbasis derived from the semantic similarity matrix of deep image features and color affinity from an image. Further, by incorporating our object-centric contrastive loss with EiCue, we guide our model to learn object-level representations with intra- and inter-image object-feature consistency, thereby enhancing semantic accuracy. Extensive experiments on COCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art USS results of EAGLE with accurate and consistent semantic segmentation across complex scenes.",
        "page": "http://arxiv.org/abs/2403.01482",
        "pdf": "http://arxiv.org/pdf/2403.01482.pdf"
    },
    {
        "title": "A Physics-informed Low-rank Deep Neural Network for Blind and Universal Lens Aberration Correction",
        "author": "Jin Gong, Runzhao Yang, Weihang Zhang, Jinli Suo, Qionghai Dai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Data Poisoning based Backdoor Attacks to Contrastive Learning",
        "author": "Jinghuai Zhang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A2XP: Towards Private Domain Generalization",
        "author": "Geunhyeok Yu, Hyoseok Hwang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks",
        "author": "Kai Han, Yunhe Wang, Jianyuan Guo, Enhua Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "An Empirical Study of Scaling Law for Scene Text Recognition",
        "author": "Miao Rang, Zhenni Bi, Chuanjian Liu, Yunhe Wang, Kai Han",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Deep Imbalanced Regression via Hierarchical Classification Adjustment",
        "author": "Haipeng Xiong, Angela Yao",
        "abstract": "Regression tasks in computer vision, such as age estimation or counting, are often formulated into classification by quantizing the target space into classes. Yet real-world data is often imbalanced -- the majority of training samples lie in a head range of target values, while a minority of samples span a usually larger tail range. By selecting the class quantization, one can adjust imbalanced regression targets into balanced classification outputs, though there are trade-offs in balancing classification accuracy and quantization error. To improve regression performance over the entire range of data, we propose to construct hierarchical classifiers for solving imbalanced regression tasks. The fine-grained classifiers limit the quantization error while being modulated by the coarse predictions to ensure high accuracy. Standard hierarchical classification approaches, however, when applied to the regression problem, fail to ensure that predicted ranges remain consistent across the hierarchy. As such, we propose a range-preserving distillation process that can effectively learn a single classifier from the set of hierarchical classifiers. Our novel hierarchical classification adjustment (HCA) for imbalanced regression shows superior results on three diverse tasks: age estimation, crowd counting and depth estimation. We will release the source code upon acceptance.",
        "page": "http://arxiv.org/abs/2310.17154",
        "pdf": "http://arxiv.org/pdf/2310.17154.pdf"
    },
    {
        "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
        "author": "Eric Slyman, Stefan Lee, Scott Cohen, Kushal Kafle",
        "abstract": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
        "page": "http://arxiv.org/abs/2404.16123",
        "pdf": "http://arxiv.org/pdf/2404.16123.pdf"
    },
    {
        "title": "Modular Blind Video Quality Assessment",
        "author": "Wen Wen, Mu Li, Yabin ZHANG, Yiting Liao, Junlin Li, Li zhang, Kede Ma",
        "abstract": "Blind video quality assessment (BVQA) plays a pivotal role in evaluating and improving the viewing experience of end-users across a wide range of video-based platforms and services. Contemporary deep learning-based models primarily analyze video content in its aggressively subsampled format, while being blind to the impact of the actual spatial resolution and frame rate on video quality. In this paper, we propose a modular BVQA model and a method of training it to improve its modularity. Our model comprises a base quality predictor, a spatial rectifier, and a temporal rectifier, responding to the visual content and distortion, spatial resolution, and frame rate changes on video quality, respectively. During training, spatial and temporal rectifiers are dropped out with some probabilities to render the base quality predictor a standalone BVQA model, which should work better with the rectifiers. Extensive experiments on both professionally-generated content and user-generated content video databases show that our quality model achieves superior or comparable performance to current methods. Additionally, the modularity of our model offers an opportunity to analyze existing video quality databases in terms of their spatial and temporal complexity.",
        "page": "http://arxiv.org/abs/2402.19276",
        "pdf": "http://arxiv.org/pdf/2402.19276.pdf"
    },
    {
        "title": "Open-vocabulary object 6D pose estimation",
        "author": "Jaime Corsetti, Davide Boscaini, Changjae Oh, Andrea Cavallaro, Fabio Poiesi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting",
        "author": "Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin",
        "abstract": "3D editing plays a crucial role in many areas such as gaming and virtual reality. Traditional 3D editing methods, which rely on representations like meshes and point clouds, often fall short in realistically depicting complex scenes. On the other hand, methods based on implicit 3D representations, like Neural Radiance Field (NeRF), render complex scenes effectively but suffer from slow processing speeds and limited control over specific scene areas. In response to these challenges, our paper presents GaussianEditor, an innovative and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D representation. GaussianEditor enhances precision and control in editing through our proposed Gaussian semantic tracing, which traces the editing target throughout the training process. Additionally, we propose Hierarchical Gaussian splatting (HGS) to achieve stabilized and fine results under stochastic generative guidance from 2D diffusion models. We also develop editing strategies for efficient object removal and integration, a challenging task for existing methods. Our comprehensive experiments demonstrate GaussianEditor's superior control, efficacy, and rapid performance, marking a significant advancement in 3D editing. Project Page: https://buaacyw.github.io/gaussian-editor/",
        "page": "http://arxiv.org/abs/2311.14521",
        "pdf": "http://arxiv.org/pdf/2311.14521.pdf"
    },
    {
        "title": "MoST: Multi-modality Scene Tokenization for Motion Prediction",
        "author": "Norman Mu, Jingwei Ji, Zhenpei Yang, Nathan Harada, Haotian Tang, Kan Chen, Charles R. Qi, Runzhou Ge, Kratarth Goel, Zoey Yang, Scott Ettinger, Rami Al-Rfou, Dragomir Anguelov, Yin Zhou",
        "abstract": "Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights. This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions). An alternative paradigm is end-to-end learning from raw sensors. However, this approach suffers from the lack of interpretability and requires significantly more training resources. In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner. The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information. Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures. To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings. Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art.",
        "page": "http://arxiv.org/abs/2404.19531",
        "pdf": "http://arxiv.org/pdf/2404.19531.pdf"
    },
    {
        "title": "RankED: Addressing Imbalance and Uncertainty in Edge Detection Using Ranking-based Losses",
        "author": "bedrettin cetinkaya, Sinan Kalkan, Emre Akbas",
        "abstract": "Detecting edges in images suffers from the problems of (P1) heavy imbalance between positive and negative classes as well as (P2) label uncertainty owing to disagreement between different annotators. Existing solutions address P1 using class-balanced cross-entropy loss and dice loss and P2 by only predicting edges agreed upon by most annotators. In this paper, we propose RankED, a unified ranking-based approach that addresses both the imbalance problem (P1) and the uncertainty problem (P2). RankED tackles these two problems with two components: One component which ranks positive pixels over negative pixels, and the second which promotes high confidence edge pixels to have more label certainty. We show that RankED outperforms previous studies and sets a new state-of-the-art on NYUD-v2, BSDS500 and Multi-cue datasets. Code is available at https://ranked-cvpr24.github.io.",
        "page": "http://arxiv.org/abs/2403.01795",
        "pdf": "http://arxiv.org/pdf/2403.01795.pdf"
    },
    {
        "title": "Efficient Model Stealing Defense with Noise Transition Matrix",
        "author": "Dong-Dong Wu, Chilin Fu, Weichang Wu, Wenwen Xia, Xiaolu Zhang, JUN ZHOU, Min-Ling Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Streaming Dense Video Captioning",
        "author": "Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, Cordelia Schmid",
        "abstract": "An ideal model for dense video captioning -- predicting captions localized temporally in a video -- should be able to handle long input videos, predict rich, detailed textual descriptions, and be able to produce outputs before processing the entire video. Current state-of-the-art models, however, process a fixed number of downsampled frames, and make a single full prediction after seeing the whole video. We propose a streaming dense video captioning model that consists of two novel components: First, we propose a new memory module, based on clustering incoming tokens, which can handle arbitrarily long videos as the memory is of a fixed size. Second, we develop a streaming decoding algorithm that enables our model to make predictions before the entire video has been processed. Our model achieves this streaming ability, and significantly improves the state-of-the-art on three dense video captioning benchmarks: ActivityNet, YouCook2 and ViTT. Our code is released at https://github.com/google-research/scenic.",
        "page": "http://arxiv.org/abs/2404.01297",
        "pdf": "http://arxiv.org/pdf/2404.01297.pdf"
    },
    {
        "title": "End-to-End Spatio-Temporal Action Localisation with Video Transformers",
        "author": "Alexey Gritsenko, Xuehan Xiong, Josip Djolonga, Mostafa Dehghani, Chen Sun, Mario Lu\u010di\u0107, Cordelia Schmid, Anurag Arnab",
        "abstract": "The most performant spatio-temporal action localisation models use external person proposals and complex external memory banks. We propose a fully end-to-end, purely-transformer based model that directly ingests an input video, and outputs tubelets -- a sequence of bounding boxes and the action classes at each frame. Our flexible model can be trained with either sparse bounding-box supervision on individual frames, or full tubelet annotations. And in both cases, it predicts coherent tubelets as the output. Moreover, our end-to-end model requires no additional pre-processing in the form of proposals, or post-processing in terms of non-maximal suppression. We perform extensive ablation experiments, and significantly advance the state-of-the-art results on four different spatio-temporal action localisation benchmarks with both sparse keyframes and full tubelet annotations.",
        "page": "http://arxiv.org/abs/2304.12160",
        "pdf": "http://arxiv.org/pdf/2304.12160.pdf"
    },
    {
        "title": "Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction",
        "author": "Guillaume Jaume, Anurag Vaidya, Richard J. Chen, Drew F. K. Williamson, Paul Pu Liang, Faisal Mahmood",
        "abstract": "Integrating whole-slide images (WSIs) and bulk transcriptomics for predicting patient survival can improve our understanding of patient prognosis. However, this multimodal task is particularly challenging due to the different nature of these data: WSIs represent a very high-dimensional spatial description of a tumor, while bulk transcriptomics represent a global description of gene expression levels within that tumor. In this context, our work aims to address two key challenges: (1) how can we tokenize transcriptomics in a semantically meaningful and interpretable way?, and (2) how can we capture dense multimodal interactions between these two modalities? Specifically, we propose to learn biological pathway tokens from transcriptomics that can encode specific cellular functions. Together with histology patch tokens that encode the different morphological patterns in the WSI, we argue that they form appropriate reasoning units for downstream interpretability analyses. We propose fusing both modalities using a memory-efficient multimodal Transformer that can model interactions between pathway and histology patch tokens. Our proposed model, SURVPATH, achieves state-of-the-art performance when evaluated against both unimodal and multimodal baselines on five datasets from The Cancer Genome Atlas. Our interpretability framework identifies key multimodal prognostic factors, and, as such, can provide valuable insights into the interaction between genotype and phenotype, enabling a deeper understanding of the underlying biological mechanisms at play. We make our code public at: https://github.com/ajv012/SurvPath.",
        "page": "http://arxiv.org/abs/2304.06819",
        "pdf": "http://arxiv.org/pdf/2304.06819.pdf"
    },
    {
        "title": "Morphological Prototyping for Unsupervised Slide Representation Learning in Computational Pathology",
        "author": "Andrew Song, Richard J. Chen, Tong Ding, Drew F. K. Williamson, Guillaume Jaume, Faisal Mahmood",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision",
        "author": "Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong, Gang Hua, Tianyi Zhang, Bedrich Benes, Aniket Bera",
        "abstract": "We have witnessed significant progress in deep learning-based 3D vision, ranging from neural radiance field (NeRF) based 3D representation learning to applications in novel view synthesis (NVS). However, existing scene-level datasets for deep learning-based 3D vision, limited to either synthetic environments or a narrow selection of real-world scenes, are quite insufficient. This insufficiency not only hinders a comprehensive benchmark of existing methods but also caps what could be explored in deep learning-based 3D analysis. To address this critical gap, we present DL3DV-10K, a large-scale scene dataset, featuring 51.2 million frames from 10,510 videos captured from 65 types of point-of-interest (POI) locations, covering both bounded and unbounded scenes, with different levels of reflection, transparency, and lighting. We conducted a comprehensive benchmark of recent NVS methods on DL3DV-10K, which revealed valuable insights for future research in NVS. In addition, we have obtained encouraging results in a pilot study to learn generalizable NeRF from DL3DV-10K, which manifests the necessity of a large-scale scene-level dataset to forge a path toward a foundation model for learning 3D representation. Our DL3DV-10K dataset, benchmark results, and models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.",
        "page": "http://arxiv.org/abs/2312.16256",
        "pdf": "http://arxiv.org/pdf/2312.16256.pdf"
    },
    {
        "title": "ZONE: Zero-Shot Instruction-Guided Local Editing",
        "author": "Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xuhui Liu, Jiaming Liu, Li Lin, Xu Tang, Yao Hu, Jianzhuang Liu, Baochang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving",
        "author": "Chen Min, Dawei Zhao, Liang Xiao, Jian Zhao, Xinli Xu, Zheng Zhu, Lei Jin, Jianshu Li, Yulan Guo, Junliang Xing, Liping Jing, Yiming Nie, Bin Dai",
        "abstract": "Vision-centric autonomous driving has recently raised wide attention due to its lower cost. Pre-training is essential for extracting a universal representation. However, current vision-centric pre-training typically relies on either 2D or 3D pre-text tasks, overlooking the temporal characteristics of autonomous driving as a 4D scene understanding task. In this paper, we address this challenge by introducing a world model-based autonomous driving 4D representation learning framework, dubbed \\emph{DriveWorld}, which is capable of pre-training from multi-camera driving videos in a spatio-temporal fashion. Specifically, we propose a Memory State-Space Model for spatio-temporal modelling, which consists of a Dynamic Memory Bank module for learning temporal-aware latent dynamics to predict future changes and a Static Scene Propagation module for learning spatial-aware latent statics to offer comprehensive scene contexts. We additionally introduce a Task Prompt to decouple task-aware features for various downstream tasks. The experiments demonstrate that DriveWorld delivers promising results on various autonomous driving tasks. When pre-trained with the OpenScene dataset, DriveWorld achieves a 7.5% increase in mAP for 3D object detection, a 3.0% increase in IoU for online mapping, a 5.0% increase in AMOTA for multi-object tracking, a 0.1m decrease in minADE for motion forecasting, a 3.0% increase in IoU for occupancy prediction, and a 0.34m reduction in average L2 error for planning.",
        "page": "http://arxiv.org/abs/2405.04390",
        "pdf": "http://arxiv.org/pdf/2405.04390.pdf"
    },
    {
        "title": "Shadows Don\u2019t Lie and Lines Can't Bend! Generative Models don't know Projective Geometry...for now",
        "author": "Ayush Sarkar, Hanlin Mai, Amitabh Mahapatra, David Forsyth, Svetlana Lazebnik, Anand Bhattad",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models",
        "author": "Jingyao Xu, Yuetong Lu, Yandong Li, Siyang Lu, Dongdong Wang, Xiang Wei",
        "abstract": "Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples. However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability. We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs). The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images. We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models. Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner.",
        "page": "http://arxiv.org/abs/2404.15081",
        "pdf": "http://arxiv.org/pdf/2404.15081.pdf"
    },
    {
        "title": "Learned representation-guided diffusion models for large-image generation",
        "author": "Alexandros Graikos, Srikar Yellapragada, Minh-Quan Le, Saarthak Kapse, Prateek Prasanna, Joel Saltz, Dimitris Samaras",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Desigen: A Pipeline for Controllable Design Template Generation",
        "author": "Haohan Weng, Danqing Huang, YU QIAO, Hu Zheng, Chin-Yew Lin, Tong Zhang, C. L. Philip Chen",
        "abstract": "Templates serve as a good starting point to implement a design (e.g., banner, slide) but it takes great effort from designers to manually create. In this paper, we present Desigen, an automatic template creation pipeline which generates background images as well as harmonious layout elements over the background. Different from natural images, a background image should preserve enough non-salient space for the overlaying layout elements. To equip existing advanced diffusion-based models with stronger spatial control, we propose two simple but effective techniques to constrain the saliency distribution and reduce the attention weight in desired regions during the background generation process. Then conditioned on the background, we synthesize the layout with a Transformer-based autoregressive generator. To achieve a more harmonious composition, we propose an iterative inference strategy to adjust the synthesized background and layout in multiple rounds. We constructed a design dataset with more than 40k advertisement banners to verify our approach. Extensive experiments demonstrate that the proposed pipeline generates high-quality templates comparable to human designers. More than a single-page design, we further show an application of presentation generation that outputs a set of theme-consistent slides. The data and code are available at https://whaohan.github.io/desigen.",
        "page": "http://arxiv.org/abs/2403.09093",
        "pdf": "http://arxiv.org/pdf/2403.09093.pdf"
    },
    {
        "title": "From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations",
        "author": "Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, Alexander Richard",
        "abstract": "We present a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction. Given speech audio, we output multiple possibilities of gestural motion for an individual, including face, body, and hands. The key behind our method is in combining the benefits of sample diversity from vector quantization with the high-frequency details obtained through diffusion to generate more dynamic, expressive motion. We visualize the generated motion using highly photorealistic avatars that can express crucial nuances in gestures (e.g. sneers and smirks). To facilitate this line of research, we introduce a first-of-its-kind multi-view conversational dataset that allows for photorealistic reconstruction. Experiments show our model generates appropriate and diverse gestures, outperforming both diffusion- and VQ-only methods. Furthermore, our perceptual evaluation highlights the importance of photorealism (vs. meshes) in accurately assessing subtle motion details in conversational gestures. Code and dataset available online.",
        "page": "http://arxiv.org/abs/2401.01885",
        "pdf": "http://arxiv.org/pdf/2401.01885.pdf"
    },
    {
        "title": "Multiview Aerial Visual RECognition (MAVREC) Dataset: Can Multi-view Improve Aerial Visual Perception?",
        "author": "Aritra Dutta, Srijan Das, Jacob Nielsen, RAJATSUBHRA CHAKRABORTY, Mubarak Shah",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions",
        "author": "Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, Adriana Romero-Soriano",
        "abstract": "Curation methods for massive vision-language datasets trade off between dataset size and quality. However, even the highest quality of available curated captions are far too short to capture the rich visual detail in an image. To show the value of dense and highly-aligned image-text pairs, we collect the Densely Captioned Images (DCI) dataset, containing 8012 natural images human-annotated with mask-aligned descriptions averaging above 1000 words each. With precise and reliable captions associated with specific parts of an image, we can evaluate vision-language models' (VLMs) understanding of image content with a novel task that matches each caption with its corresponding subcrop. As current models are often limited to 77 text tokens, we also introduce a summarized version (sDCI) in which each caption length is limited. We show that modern techniques that make progress on standard benchmarks do not correspond with significant improvement on our sDCI based benchmark. Lastly, we finetune CLIP using sDCI and show significant improvements over the baseline despite a small training set. By releasing the first human annotated dense image captioning dataset, we hope to enable the development of new benchmarks or fine-tuning recipes for the next generation of VLMs to come.",
        "page": "http://arxiv.org/abs/2312.08578",
        "pdf": "http://arxiv.org/pdf/2312.08578.pdf"
    },
    {
        "title": "4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling",
        "author": "Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, David B. Lindell",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment",
        "author": "Muhammad Sohail Danish, Muhammad Haris Khan, Muhammad Akhtar Munir, M. Sarfraz, Mohsen Ali",
        "abstract": "In this work, we tackle the problem of domain generalization for object detection, specifically focusing on the scenario where only a single source domain is available. We propose an effective approach that involves two key steps: diversifying the source domain and aligning detections based on class prediction confidence and localization. Firstly, we demonstrate that by carefully selecting a set of augmentations, a base detector can outperform existing methods for single domain generalization by a good margin. This highlights the importance of domain diversification in improving the performance of object detectors. Secondly, we introduce a method to align detections from multiple views, considering both classification and localization outputs. This alignment procedure leads to better generalized and well-calibrated object detector models, which are crucial for accurate decision-making in safety-critical applications. Our approach is detector-agnostic and can be seamlessly applied to both single-stage and two-stage detectors. To validate the effectiveness of our proposed methods, we conduct extensive experiments and ablations on challenging domain-shift scenarios. The results consistently demonstrate the superiority of our approach compared to existing methods. Our code and models are available at: https://github.com/msohaildanish/DivAlign",
        "page": "http://arxiv.org/abs/2405.14497",
        "pdf": "http://arxiv.org/pdf/2405.14497.pdf"
    },
    {
        "title": "MeshPose: Unifying DensePose and 3D Body Mesh reconstruction",
        "author": "Eric-Tuan Le, Antonios Kakolyris, Petros Koutras, Himmy Tam, Efstratios Skordos, George Papandreou, Riza Alp Guler, Iasonas Kokkinos",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation",
        "author": "Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, Sarah Parisot",
        "abstract": "Text-to-image generation has achieved astonishing results, yet precise spatial controllability and prompt fidelity remain highly challenging. This limitation is typically addressed through cumbersome prompt engineering, scene layout conditioning, or image editing techniques which often require hand drawn masks. Nonetheless, pre-existing works struggle to take advantage of the natural instance-level compositionality of scenes due to the typically flat nature of rasterized RGB output images. Towards adressing this challenge, we introduce MuLAn: a novel dataset comprising over 44K MUlti-Layer ANnotations of RGB images as multilayer, instance-wise RGBA decompositions, and over 100K instance images. To build MuLAn, we developed a training free pipeline which decomposes a monocular RGB image into a stack of RGBA layers comprising of background and isolated instances. We achieve this through the use of pretrained general-purpose models, and by developing three modules: image decomposition for instance discovery and extraction, instance completion to reconstruct occluded areas, and image re-assembly. We use our pipeline to create MuLAn-COCO and MuLAn-LAION datasets, which contain a variety of image decompositions in terms of style, composition and complexity. With MuLAn, we provide the first photorealistic resource providing instance decomposition and occlusion information for high quality images, opening up new avenues for text-to-image generative AI research. With this, we aim to encourage the development of novel generation and editing technology, in particular layer-wise solutions. MuLAn data resources are available at https://MuLAn-dataset.github.io/.",
        "page": "http://arxiv.org/abs/2404.02790",
        "pdf": "http://arxiv.org/pdf/2404.02790.pdf"
    },
    {
        "title": "Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark",
        "author": "Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard",
        "abstract": "We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities. The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms. We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data. In our evaluation, we thoroughly assessed existing audio and audio-visual models against multiple criteria and proposed settings to enhance their performance on real-world data. We also conducted experiments to investigate the impact of incorporating visual data (i.e., images and depth) into neural acoustic field models. Additionally, we demonstrated the effectiveness of a simple sim2real approach, where a model is pre-trained with simulated data and fine-tuned with sparse real-world data, resulting in significant improvements in the few-shot learning approach. RAF is the first dataset to provide densely captured room acoustic data, making it an ideal resource for researchers working on audio and audio-visual neural acoustic field modeling techniques. Demos and datasets are available on our project page: https://facebookresearch.github.io/real-acoustic-fields/",
        "page": "http://arxiv.org/abs/2403.18821",
        "pdf": "http://arxiv.org/pdf/2403.18821.pdf"
    },
    {
        "title": "Practical Measurements of Translucent Materials with Inter-Pixel Translucency Prior",
        "author": "Zhenyu Chen, Jie Guo, Shuichang Lai, Ruoyu Fu, mengxun kong, Chen Wang, Hongyu Sun, Zhebin Zhang, Chen Li, Yanwen Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsupervised Universal Image Segmentation",
        "author": "Xudong Wang, Dantong Niu, Xinyang Han, Long Lian, Roei Herzig, Trevor Darrell",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Gated Fields: Learning Scene Reconstruction from Gated Videos",
        "author": "Andrea Ramazzina, Stefanie Walz, Pragyan Dahal, Mario Bijelic, Felix Heide",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FADES: Fair Disentanglement with Sensitive Relevance",
        "author": "Taeuk Jang, Xiaoqian Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MonoNPHM: Dynamic Head Reconstruction from Monocular Videos",
        "author": "Simon Giebenhain, Tobias Kirschstein, Markos Georgopoulos, Martin R\u00fcnz, Lourdes Agapito, Matthias Nie\u00dfner",
        "abstract": "We present Monocular Neural Parametric Head Models (MonoNPHM) for dynamic 3D head reconstructions from monocular RGB videos. To this end, we propose a latent appearance space that parameterizes a texture field on top of a neural parametric model. We constrain predicted color values to be correlated with the underlying geometry such that gradients from RGB effectively influence latent geometry codes during inverse rendering. To increase the representational capacity of our expression space, we augment our backward deformation field with hyper-dimensions, thus improving color and geometry representation in topologically challenging expressions. Using MonoNPHM as a learned prior, we approach the task of 3D head reconstruction using signed distance field based volumetric rendering. By numerically inverting our backward deformation field, we incorporated a landmark loss using facial anchor points that are closely tied to our canonical geometry representation. To evaluate the task of dynamic face reconstruction from monocular RGB videos we record 20 challenging Kinect sequences under casual conditions. MonoNPHM outperforms all baselines with a significant margin, and makes an important step towards easily accessible neural parametric face models through RGB tracking.",
        "page": "http://arxiv.org/abs/2312.06740",
        "pdf": "http://arxiv.org/pdf/2312.06740.pdf"
    },
    {
        "title": "Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling",
        "author": "Baoquan Zhang, Huaibin Wang, Luo Chuyao, Xutao Li, Guotao liang, Yunming Ye, joeq, Yao He",
        "abstract": "Vector-Quantized Image Modeling (VQIM) is a fundamental research problem in image synthesis, which aims to represent an image with a discrete token sequence. Existing studies effectively address this problem by learning a discrete codebook from scratch and in a code-independent manner to quantize continuous representations into discrete tokens. However, learning a codebook from scratch and in a code-independent manner is highly challenging, which may be a key reason causing codebook collapse, i.e., some code vectors can rarely be optimized without regard to the relationship between codes and good codebook priors such that die off finally. In this paper, inspired by pretrained language models, we find that these language models have actually pretrained a superior codebook via a large number of text corpus, but such information is rarely exploited in VQIM. To this end, we propose a novel codebook transfer framework with part-of-speech, called VQCT, which aims to transfer a well-trained codebook from pretrained language models to VQIM for robust codebook learning. Specifically, we first introduce a pretrained codebook from language models and part-of-speech knowledge as priors. Then, we construct a vision-related codebook with these priors for achieving codebook transfer. Finally, a novel codebook transfer network is designed to exploit abundant semantic relationships between codes contained in pretrained codebooks for robust VQIM codebook learning. Experimental results on four datasets show that our VQCT method achieves superior VQIM performance over previous state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2403.10071",
        "pdf": "http://arxiv.org/pdf/2403.10071.pdf"
    },
    {
        "title": "TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding",
        "author": "Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, Li Yi",
        "abstract": "Humans commonly work with multiple objects in daily life and can intuitively transfer manipulation skills to novel objects by understanding object functional regularities. However, existing technical approaches for analyzing and synthesizing hand-object manipulation are mostly limited to handling a single hand and object due to the lack of data support. To address this, we construct TACO, an extensive bimanual hand-object-interaction dataset spanning a large variety of tool-action-object compositions for daily human activities. TACO contains 2.5K motion sequences paired with third-person and egocentric views, precise hand-object 3D meshes, and action labels. To rapidly expand the data scale, we present a fully automatic data acquisition pipeline combining multi-view sensing with an optical motion capture system. With the vast research fields provided by TACO, we benchmark three generalizable hand-object-interaction tasks: compositional action recognition, generalizable hand-object motion forecasting, and cooperative grasp synthesis. Extensive experiments reveal new insights, challenges, and opportunities for advancing the studies of generalizable hand-object motion analysis and synthesis. Our data and code are available at https://taco2024.github.io.",
        "page": "http://arxiv.org/abs/2401.08399",
        "pdf": "http://arxiv.org/pdf/2401.08399.pdf"
    },
    {
        "title": "ProxyCap: Real-time Monocular Full-body Capture in World Space via Human-Centric Proxy-to-Motion Learning",
        "author": "Yuxiang Zhang, Hongwen Zhang, Liangxiao Hu, Jiajun Zhang, Hongwei Yi, Shengping Zhang, Yebin Liu",
        "abstract": "Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner. However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space. In this work, we introduce ProxyCap, a human-centric proxy-to-motion learning scheme to learn world-space motions from a proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy data enables us to build a learning-based network with accurate world-space supervision while also mitigating the generalization issues. For more accurate and physically plausible predictions in world space, our network is designed to learn human motions from a human-centric perspective, which enables the understanding of the same motion captured with different camera trajectories. Moreover, a contact-aware neural motion descent module is proposed in our network so that it can be aware of foot-ground contact and motion misalignment with the proxy observations. With the proposed learning-based solution, we demonstrate the first real-time monocular full-body capture system with plausible foot-ground contact in world space even using hand-held moving cameras. Our project page is https://zhangyux15.github.io/ProxyCapV2.",
        "page": "http://arxiv.org/abs/2307.01200",
        "pdf": "http://arxiv.org/pdf/2307.01200.pdf"
    },
    {
        "title": "HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation",
        "author": "Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying Feng, Yebin Liu, Qing Wang",
        "abstract": "Recent text-to-3D methods employing diffusion models have made significant advancements in 3D human generation. However, these approaches face challenges due to the limitations of text-to-image diffusion models, which lack an understanding of 3D structures. Consequently, these methods struggle to achieve high-quality human generation, resulting in smooth geometry and cartoon-like appearances. In this paper, we propose HumanNorm, a novel approach for high-quality and realistic 3D human generation. The main idea is to enhance the model's 2D perception of 3D geometry by learning a normal-adapted diffusion model and a normal-aligned diffusion model. The normal-adapted diffusion model can generate high-fidelity normal maps corresponding to user prompts with view-dependent and body-aware text. The normal-aligned diffusion model learns to generate color images aligned with the normal maps, thereby transforming physical geometry details into realistic appearance. Leveraging the proposed normal diffusion model, we devise a progressive geometry generation strategy and a multi-step Score Distillation Sampling (SDS) loss to enhance the performance of 3D human generation. Comprehensive experiments substantiate HumanNorm's ability to generate 3D humans with intricate geometry and realistic appearances. HumanNorm outperforms existing text-to-3D methods in both geometry and texture quality. The project page of HumanNorm is https://humannorm.github.io/.",
        "page": "http://arxiv.org/abs/2310.01406",
        "pdf": "http://arxiv.org/pdf/2310.01406.pdf"
    },
    {
        "title": "Validating Privacy-Preserving Face Recognition under a Minimum Assumption",
        "author": "Hui Zhang, Xingbo Dong, YenLungLai, Ying Zhou, Xiaoyan ZHANG, Xingguo Lv, Zhe Jin, Xuejun Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multimodal Sense-Informed Prediction of 3D Human Motions",
        "author": "Zhenyu Lou, Qiongjie Cui, Haofan Wang, Xu Tang, Hong Zhou",
        "abstract": "Predicting future human pose is a fundamental application for machine intelligence, which drives robots to plan their behavior and paths ahead of time to seamlessly accomplish human-robot collaboration in real-world 3D scenarios. Despite encouraging results, existing approaches rarely consider the effects of the external scene on the motion sequence, leading to pronounced artifacts and physical implausibilities in the predictions. To address this limitation, this work introduces a novel multi-modal sense-informed motion prediction approach, which conditions high-fidelity generation on two modal information: external 3D scene, and internal human gaze, and is able to recognize their salience for future human activity. Furthermore, the gaze information is regarded as the human intention, and combined with both motion and scene features, we construct a ternary intention-aware attention to supervise the generation to match where the human wants to reach. Meanwhile, we introduce semantic coherence-aware attention to explicitly distinguish the salient point clouds and the underlying ones, to ensure a reasonable interaction of the generated sequence with the 3D scene. On two real-world benchmarks, the proposed method achieves state-of-the-art performance both in 3D human pose and trajectory prediction.",
        "page": "http://arxiv.org/abs/2405.02911",
        "pdf": "http://arxiv.org/pdf/2405.02911.pdf"
    },
    {
        "title": "MiKASA: Multi-Key-Anchor Scene-Aware Transformer for 3D Visual Grounding",
        "author": "Chun-Peng Chang, Shaoxiang Wang, Alain Pagani, Didier Stricker",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CLIP-KD: An Empirical Study of CLIP Model Distillation",
        "author": "Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, XinQiang Yu, Han Yang, boyu diao, Yongjun Xu",
        "abstract": "Contrastive Language-Image Pre-training (CLIP) has become a promising language-supervised visual pre-training framework. This paper aims to distill small CLIP models supervised by a large teacher CLIP model. We propose several distillation strategies, including relation, feature, gradient and contrastive paradigms, to examine the effectiveness of CLIP-Knowledge Distillation (KD). We show that a simple feature mimicry with Mean Squared Error loss works surprisingly well. Moreover, interactive contrastive learning across teacher and student encoders is also effective in performance improvement. We explain that the success of CLIP-KD can be attributed to maximizing the feature similarity between teacher and student. The unified method is applied to distill several student models trained on CC3M+12M. CLIP-KD improves student CLIP models consistently over zero-shot ImageNet classification and cross-modal retrieval benchmarks. When using ViT-L/14 pretrained on Laion-400M as the teacher, CLIP-KD achieves 57.5\\% and 55.4\\% zero-shot top-1 ImageNet accuracy over ViT-B/16 and ResNet-50, surpassing the original CLIP without KD by 20.5\\% and 20.1\\% margins, respectively. Our code is released on https://github.com/winycg/CLIP-KD.",
        "page": "http://arxiv.org/abs/2307.12732",
        "pdf": "http://arxiv.org/pdf/2307.12732.pdf"
    },
    {
        "title": "InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree Neural Radiance Fields",
        "author": "Dongqing Wang, Tong Zhang, Alaa Abboud, Sabine S\u00fcsstrunk",
        "abstract": "We propose InNeRF360, an automatic system that accurately removes text-specified objects from 360-degree Neural Radiance Fields (NeRF). The challenge is to effectively remove objects while inpainting perceptually consistent content for the missing regions, which is particularly demanding for existing NeRF models due to their implicit volumetric representation. Moreover, unbounded scenes are more prone to floater artifacts in the inpainted region than frontal-facing scenes, as the change of object appearance and background across views is more sensitive to inaccurate segmentations and inconsistent inpainting. With a trained NeRF and a text description, our method efficiently removes specified objects and inpaints visually consistent content without artifacts. We apply depth-space warping to enforce consistency across multiview text-encoded segmentations, and then refine the inpainted NeRF model using perceptual priors and 3D diffusion-based geometric priors to ensure visual plausibility. Through extensive experiments in segmentation and inpainting on 360-degree and frontal-facing NeRFs, we show that our approach is effective and enhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.",
        "page": "http://arxiv.org/abs/2305.15094",
        "pdf": "http://arxiv.org/pdf/2305.15094.pdf"
    },
    {
        "title": "Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction",
        "author": "Xiaoyang Lyu, Chirui Chang, Peng Dai, Yangtian Sun, Xiaojuan Qi",
        "abstract": "Scene reconstruction from multi-view images is a fundamental problem in computer vision and graphics. Recent neural implicit surface reconstruction methods have achieved high-quality results; however, editing and manipulating the 3D geometry of reconstructed scenes remains challenging due to the absence of naturally decomposed object entities and complex object/background compositions. In this paper, we present Total-Decom, a novel method for decomposed 3D reconstruction with minimal human interaction. Our approach seamlessly integrates the Segment Anything Model (SAM) with hybrid implicit-explicit neural surface representations and a mesh-based region-growing technique for accurate 3D object decomposition. Total-Decom requires minimal human annotations while providing users with real-time control over the granularity and quality of decomposition. We extensively evaluate our method on benchmark datasets and demonstrate its potential for downstream applications, such as animation and scene editing. The code is available at https://github.com/CVMI-Lab/Total-Decom.git.",
        "page": "http://arxiv.org/abs/2403.19314",
        "pdf": "http://arxiv.org/pdf/2403.19314.pdf"
    },
    {
        "title": "CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoor Object Detection from Multi-view Images",
        "author": "Guanlin Shen, Jingwei Huang, Zhihua Hu, Bin Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing",
        "author": "Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao",
        "abstract": "Generative 3D part assembly involves understanding part relationships and predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work often focus on the geometry of individual parts, neglecting part-whole hierarchies of objects. Leveraging two key observations: 1) super-part poses provide strong hints about part poses, and 2) predicting super-part poses is easier due to fewer superparts, we propose a part-whole-hierarchy message passing network for efficient 3D part assembly. We first introduce super-parts by grouping geometrically similar parts without any semantic labels. Then we employ a part-whole hierarchical encoder, wherein a super-part encoder predicts latent super-part poses based on input parts. Subsequently, we transform the point cloud using the latent poses, feeding it to the part encoder for aggregating super-part information and reasoning about part relationships to predict all part poses. In training, only ground-truth part poses are required. During inference, the predicted latent poses of super-parts enhance interpretability. Experimental results on the PartNet dataset show that our method achieves state-of-the-art performance in part and connectivity accuracy and enables an interpretable hierarchical part assembly. Code is available at https://github.com/pkudba/3DHPA.",
        "page": "http://arxiv.org/abs/2402.17464",
        "pdf": "http://arxiv.org/pdf/2402.17464.pdf"
    },
    {
        "title": "LightIt: Illumination Modeling and Control for Diffusion Models",
        "author": "Peter Kocsis, Kalyan Sunkavalli, Julien Philip, Matthias Nie\u00dfner, Yannick Hold-Geoffroy",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Test-Time Zero-Shot Temporal Action Localization",
        "author": "Benedetta Liberatori, Alessandro Conti, Paolo Rota, Yiming Wang, Elisa Ricci",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HIVE: Harnessing Human Feedback for Instructional Visual Editing",
        "author": "Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu",
        "abstract": "Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.",
        "page": "http://arxiv.org/abs/2303.09618",
        "pdf": "http://arxiv.org/pdf/2303.09618.pdf"
    },
    {
        "title": "NeRF Director: Revisiting View Selection in Neural Volume Rendering",
        "author": "Wenhui Xiao, Rodrigo Santa Cruz, David Ahmedt-Aristizabal, Olivier Salvado, Clinton Fookes, Leo Lebrat",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unbiased Faster R-CNN for Single-source Domain Generalized Object Detection",
        "author": "Yajing Liu, Shijun Zhou, Xiyao Liu, chunhui Hao, Baojie Fan, Jiandong Tian",
        "abstract": "Single-source domain generalization (SDG) for object detection is a challenging yet essential task as the distribution bias of the unseen domain degrades the algorithm performance significantly. However, existing methods attempt to extract domain-invariant features, neglecting that the biased data leads the network to learn biased features that are non-causal and poorly generalizable. To this end, we propose an Unbiased Faster R-CNN (UFR) for generalizable feature learning. Specifically, we formulate SDG in object detection from a causal perspective and construct a Structural Causal Model (SCM) to analyze the data bias and feature bias in the task, which are caused by scene confounders and object attribute confounders. Based on the SCM, we design a Global-Local Transformation module for data augmentation, which effectively simulates domain diversity and mitigates the data bias. Additionally, we introduce a Causal Attention Learning module that incorporates a designed attention invariance loss to learn image-level features that are robust to scene confounders. Moreover, we develop a Causal Prototype Learning module with an explicit instance constraint and an implicit prototype constraint, which further alleviates the negative impact of object attribute confounders. Experimental results on five scenes demonstrate the prominent generalization ability of our method, with an improvement of 3.9% mAP on the Night-Clear scene.",
        "page": "http://arxiv.org/abs/2405.15225",
        "pdf": "http://arxiv.org/pdf/2405.15225.pdf"
    },
    {
        "title": "NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis",
        "author": "Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, Leonidas Guibas",
        "abstract": "We address the problem of generating realistic 3D motions of humans interacting with objects in a scene. Our key idea is to create a neural interaction field attached to a specific object, which outputs the distance to the valid interaction manifold given a human pose as input. This interaction field guides the sampling of an object-conditioned human motion diffusion model, so as to encourage plausible contacts and affordance semantics. To support interactions with scarcely available data, we propose an automated synthetic data pipeline. For this, we seed a pre-trained motion model, which has priors for the basics of human movement, with interaction-specific anchor poses extracted from limited motion capture data. Using our guided diffusion model trained on generated synthetic data, we synthesize realistic motions for sitting and lifting with several objects, outperforming alternative approaches in terms of motion quality and successful action completion. We call our framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.",
        "page": "http://arxiv.org/abs/2307.07511",
        "pdf": "http://arxiv.org/pdf/2307.07511.pdf"
    },
    {
        "title": "The More You See in 2D, the More You Perceive in 3D",
        "author": "Xinyang Han, Zelin Gao, Angjoo Kanazawa, Shubham Goel, Yossi Gandelsman",
        "abstract": "Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning. The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis. We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods. We demonstrate our system on real images as well as standard synthetic benchmarks. Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding.",
        "page": "http://arxiv.org/abs/2404.03652",
        "pdf": "http://arxiv.org/pdf/2404.03652.pdf"
    },
    {
        "title": "SPAD: Spatially Aware Multiview Diffusers",
        "author": "Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning",
        "author": "Hyuck Lee, Heeyoung Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards More Unified In-context Visual Understanding",
        "author": "Dianmo Sheng, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, Jianmin Bao, Tao Gong, Bin Liu, Shengwei Xu, Nenghai Yu",
        "abstract": "The rapid advancement of large language models (LLMs) has accelerated the emergence of in-context learning (ICL) as a cutting-edge approach in the natural language processing domain. Recently, ICL has been employed in visual understanding tasks, such as semantic segmentation and image captioning, yielding promising results. However, existing visual ICL framework can not enable producing content across multiple modalities, which limits their potential usage scenarios. To address this issue, we present a new ICL framework for visual understanding with multi-modal output enabled. First, we quantize and embed both text and visual prompt into a unified representational space, structured as interleaved in-context sequences. Then a decoder-only sparse transformer architecture is employed to perform generative modeling on them, facilitating in-context learning. Thanks to this design, the model is capable of handling in-context vision understanding tasks with multimodal output in a unified pipeline.Experimental results demonstrate that our model achieves competitive performance compared with specialized models and previous ICL baselines. Overall, our research takes a further step toward unified multimodal in-context learning.",
        "page": "http://arxiv.org/abs/2312.02520",
        "pdf": "http://arxiv.org/pdf/2312.02520.pdf"
    },
    {
        "title": "PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF",
        "author": "Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, Yin Yang",
        "abstract": "We show that physics-based simulations can be seamlessly integrated with NeRF to generate high-quality elastodynamics of real-world objects. Unlike existing methods, we discretize nonlinear hyperelasticity in a meshless way, obviating the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed to capture nonlinear dynamics and large deformation on the implicit model. Such meshless integration enables versatile simulations of complex and codimensional shapes. We adaptively place the least-square kernels according to the NeRF density field to significantly reduce the complexity of the nonlinear simulation. As a result, physically realistic animations can be conveniently synthesized using our method for a wide range of hyperelastic materials at an interactive rate. For more information, please visit our project page at https://fytalon.github.io/pienerf/.",
        "page": "http://arxiv.org/abs/2311.13099",
        "pdf": "http://arxiv.org/pdf/2311.13099.pdf"
    },
    {
        "title": "Rotation-Agnostic Image Representation Learning for Digital Pathology",
        "author": "Saghir Alfasly, Abubakr Shafique, Peyman Nejat, Jibran Khan, Areej Alsaafin, Ghazal Alabtah, Hamid Tizhoosh",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking with Event Cameras",
        "author": "Sachin Shah, Matthew Chan, Haoming Cai, Jingxi Chen, Sakshum Kulshrestha, Chahat Deep Singh, Yiannis Aloimonos, Christopher Metzler",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective",
        "author": "Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla, James Rehg, Vamsi Krishna Ithapu, Ruohan Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LEDITS++: Limitless Image Editing using Text-to-Image Models",
        "author": "Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, Apolin\u00e1rio Passos",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SemCity: Semantic Scene Generation with Triplane Diffusion",
        "author": "Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Ju-hyeong Seon, Sung-Eui Yoon",
        "abstract": "We present \"SemCity,\" a 3D diffusion model for semantic scene generation in real-world outdoor environments. Most 3D diffusion models focus on generating a single object, synthetic indoor scenes, or synthetic outdoor scenes, while the generation of real-world outdoor scenes is rarely addressed. In this paper, we concentrate on generating a real-outdoor scene through learning a diffusion model on a real-world outdoor dataset. In contrast to synthetic data, real-outdoor datasets often contain more empty spaces due to sensor limitations, causing challenges in learning real-outdoor distributions. To address this issue, we exploit a triplane representation as a proxy form of scene distributions to be learned by our diffusion model. Furthermore, we propose a triplane manipulation that integrates seamlessly with our triplane diffusion model. The manipulation improves our diffusion model's applicability in a variety of downstream tasks related to outdoor scene generation such as scene inpainting, scene outpainting, and semantic scene completion refinements. In experimental results, we demonstrate that our triplane diffusion model shows meaningful generation results compared with existing work in a real-outdoor dataset, SemanticKITTI. We also show our triplane manipulation facilitates seamlessly adding, removing, or modifying objects within a scene. Further, it also enables the expansion of scenes toward a city-level scale. Finally, we evaluate our method on semantic scene completion refinements where our diffusion model enhances predictions of semantic scene completion networks by learning scene distribution. Our code is available at https://github.com/zoomin-lee/SemCity.",
        "page": "http://arxiv.org/abs/2403.07773",
        "pdf": "http://arxiv.org/pdf/2403.07773.pdf"
    },
    {
        "title": "TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models",
        "author": "Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei",
        "abstract": "Recent advances in text-to-video generation have demonstrated the utility of powerful diffusion models. Nevertheless, the problem is not trivial when shaping diffusion models to animate static image (i.e., image-to-video generation). The difficulty originates from the aspect that the diffusion process of subsequent animated frames should not only preserve the faithful alignment with the given image but also pursue temporal coherence among adjacent frames. To alleviate this, we present TRIP, a new recipe of image-to-video diffusion paradigm that pivots on image noise prior derived from static image to jointly trigger inter-frame relational reasoning and ease the coherent temporal modeling via temporal residual learning. Technically, the image noise prior is first attained through one-step backward diffusion process based on both static image and noised video latent codes. Next, TRIP executes a residual-like dual-path scheme for noise prediction: 1) a shortcut path that directly takes image noise prior as the reference noise of each frame to amplify the alignment between the first frame and subsequent frames; 2) a residual path that employs 3D-UNet over noised video and static image latent codes to enable inter-frame relational reasoning, thereby easing the learning of the residual noise for each frame. Furthermore, both reference and residual noise of each frame are dynamically merged via attention mechanism for final video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT datasets demonstrate the effectiveness of our TRIP for image-to-video generation. Please see our project page at https://trip-i2v.github.io/TRIP/.",
        "page": "http://arxiv.org/abs/2403.17005",
        "pdf": "http://arxiv.org/pdf/2403.17005.pdf"
    },
    {
        "title": "Boosting Diffusion Models with Moving Average Sampling in Frequency Domain",
        "author": "Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, Tao Mei",
        "abstract": "Diffusion models have recently brought a powerful revolution in image generation. Despite showing impressive generative capabilities, most of these models rely on the current sample to denoise the next one, possibly resulting in denoising instability. In this paper, we reinterpret the iterative denoising process as model optimization and leverage a moving average mechanism to ensemble all the prior samples. Instead of simply applying moving average to the denoised samples at different timesteps, we first map the denoised samples to data space and then perform moving average to avoid distribution shift across timesteps. In view that diffusion models evolve the recovery from low-frequency components to high-frequency details, we further decompose the samples into different frequency components and execute moving average separately on each component. We name the complete approach \"Moving Average Sampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into mainstream pre-trained diffusion models and sampling schedules. Extensive experiments on both unconditional and conditional diffusion models demonstrate that our MASF leads to superior performances compared to the baselines, with almost negligible additional complexity cost.",
        "page": "http://arxiv.org/abs/2403.17870",
        "pdf": "http://arxiv.org/pdf/2403.17870.pdf"
    },
    {
        "title": "Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution",
        "author": "Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, Tao Mei",
        "abstract": "Diffusion models are just at a tipping point for image super-resolution task. Nevertheless, it is not trivial to capitalize on diffusion models for video super-resolution which necessitates not only the preservation of visual appearance from low-resolution to high-resolution videos, but also the temporal consistency across video frames. In this paper, we propose a novel approach, pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video super-resolution. SATeCo pivots on learning spatial-temporal guidance from low-resolution videos to calibrate both latent-space high-resolution video denoising and pixel-space video reconstruction. Technically, SATeCo freezes all the parameters of the pre-trained UNet and VAE, and only optimizes two deliberately-designed spatial feature adaptation (SFA) and temporal feature alignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame features via adaptively estimating affine parameters for each pixel, guaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA delves into feature interaction within a 3D local window (tubelet) through self-attention, and executes cross-attention between tubelet and its low-resolution counterpart to guide temporal feature alignment. Extensive experiments conducted on the REDS4 and Vid4 datasets demonstrate the effectiveness of our approach.",
        "page": "http://arxiv.org/abs/2403.17000",
        "pdf": "http://arxiv.org/pdf/2403.17000.pdf"
    },
    {
        "title": "SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer",
        "author": "Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, Chang-Wen Chen",
        "abstract": "Diffusion Transformer (DiT) has emerged as the new trend of generative diffusion models on image generation. In view of extremely slow convergence in typical DiT, recent breakthroughs have been driven by mask strategy that significantly improves the training efficiency of DiT with additional intra-image contextual learning. Despite this progress, mask strategy still suffers from two inherent limitations: (a) training-inference discrepancy and (b) fuzzy relations between mask reconstruction & generative diffusion process, resulting in sub-optimal training of DiT. In this work, we address these limitations by novelly unleashing the self-supervised discrimination knowledge to boost DiT training. Technically, we frame our DiT in a teacher-student manner. The teacher-student discriminative pairs are built on the diffusion noises along the same Probability Flow Ordinary Differential Equation (PF-ODE). Instead of applying mask reconstruction loss over both DiT encoder and decoder, we decouple DiT encoder and decoder to separately tackle discriminative and generative objectives. In particular, by encoding discriminative pairs with student and teacher DiT encoders, a new discriminative loss is designed to encourage the inter-image alignment in the self-supervised embedding space. After that, student samples are fed into student DiT decoder to perform the typical generative diffusion task. Extensive experiments are conducted on ImageNet dataset, and our method achieves a competitive balance between training cost and generative capacity.",
        "page": "http://arxiv.org/abs/2403.17004",
        "pdf": "http://arxiv.org/pdf/2403.17004.pdf"
    },
    {
        "title": "VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation",
        "author": "Yang Chen, Yingwei Pan, haibo yang, Ting Yao, Tao Mei",
        "abstract": "Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at https://vp3d-cvpr24.github.io.",
        "page": "http://arxiv.org/abs/2403.17001",
        "pdf": "http://arxiv.org/pdf/2403.17001.pdf"
    },
    {
        "title": "VideoMAC: Video Masked Autoencoders Meet ConvNets",
        "author": "Gensheng Pei, Tao Chen, Xiruo Jiang, \u5218\u534e\u5cf0 Liu, Zeren Sun, Yazhou Yao",
        "abstract": "Recently, the advancement of self-supervised learning techniques, like masked autoencoders (MAE), has greatly influenced visual representation learning for images and videos. Nevertheless, it is worth noting that the predominant approaches in existing masked image / video modeling rely excessively on resource-intensive vision transformers (ViTs) as the feature encoder. In this paper, we propose a new approach termed as \\textbf{VideoMAC}, which combines video masked autoencoders with resource-friendly ConvNets. Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames. To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse convolutional operators as encoders. Simultaneously, we present a simple yet effective masked video modeling (MVM) approach, a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency in videos. Additionally, we demonstrate that VideoMAC, empowering classical (ResNet) / modern (ConvNeXt) convolutional encoders to harness the benefits of MVM, outperforms ViT-based approaches on downstream tasks, including video object segmentation (+\\textbf{5.2\\%} / \\textbf{6.4\\%} $\\mathcal{J}\\&\\mathcal{F}$), body part propagation (+\\textbf{6.3\\%} / \\textbf{3.1\\%} mIoU), and human pose tracking (+\\textbf{10.2\\%} / \\textbf{11.1\\%} PCK@0.1).",
        "page": "http://arxiv.org/abs/2402.19082",
        "pdf": "http://arxiv.org/pdf/2402.19082.pdf"
    },
    {
        "title": "GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding",
        "author": "Hao Li, Dingwen Zhang, Yalun Dai, Nian Liu, Lechao Cheng, Li Jingfeng, Jingdong Wang, Junwei Han",
        "abstract": "Applying NeRF to downstream perception tasks for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, \\textit{i.e.}, the \"label rendering\" task, to build semantic NeRFs. However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from unclear boundary segmentation and abnormal segmentation of pixels within an object. To solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception. To accomplish this goal, we introduce transformers to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields. In addition, we propose two self-distillation mechanisms, i.e., the Semantic Distill Loss and the Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality of the semantic field and the maintenance of geometric consistency. In evaluation, we conduct experimental comparisons under two perception tasks (\\textit{i.e.} semantic and instance segmentation) using both synthetic and real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\\%, 11.76\\%, and 8.47\\% on generalized semantic segmentation, finetuning semantic segmentation, and instance segmentation, respectively.",
        "page": "http://arxiv.org/abs/2311.11863",
        "pdf": "http://arxiv.org/pdf/2311.11863.pdf"
    },
    {
        "title": "Exploring Region-Word Alignment in Built-in Detector for Open-Vocabulary Object Detection",
        "author": "Heng Zhang, Qiuyu Zhao, Linyu Zheng, Hao Zeng, Zhiwei Ge, Tianhao Li, Sulong Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Low-Resource Vision Challenges for Foundation Models",
        "author": "Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures",
        "author": "Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao HU, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, Xiaoguang Han",
        "abstract": "In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while remarkable progress has been made with models trained on large-scale synthetic and real-captured object data like Objaverse and MVImgNet, a similar level of progress has not been observed in the domain of human-centric tasks partially due to the lack of a large-scale human dataset. Existing datasets of high-fidelity 3D human capture continue to be mid-sized due to the significant challenges in acquiring large-scale high-quality 3D human data. To bridge this gap, we present MVHumanNet, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using a multi-view human capture system, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot studies on view-consistent action recognition, human NeRF reconstruction, text-driven view-unconstrained human image generation, as well as 2D view-unconstrained human image and 3D avatar generation. Extensive experiments demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet data with annotations will foster further innovations in the domain of 3D human-centric tasks at scale.",
        "page": "http://arxiv.org/abs/2312.02963",
        "pdf": "http://arxiv.org/pdf/2312.02963.pdf"
    },
    {
        "title": "DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning",
        "author": "Sikai Bai, Jie ZHANG, Song Guo, Shuaicheng Li, Jingcai Guo, Jun Hou, Tao Han, Xiaocheng Lu",
        "abstract": "Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation. Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels.",
        "page": "http://arxiv.org/abs/2403.08506",
        "pdf": "http://arxiv.org/pdf/2403.08506.pdf"
    },
    {
        "title": "POCE: Primal Policy Optimization with Conservative Estimation for Multi-constraint Offline Reinforcement Learning",
        "author": "Jiayi Guan, Li Shen, Ao Zhou, Lusong Li, Han Hu, Xiaodong He, Guang Chen, Changjun Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Ensemble Diversity Facilitates Adversarial Transferability",
        "author": "Bowen Tang, Zheng Wang, Yi Bin, Qi Dou, Yang Yang, Heng Tao Shen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "From a Bird\u2019s Eye View to See: Joint Camera and Subject Registration without the Camera Calibration",
        "author": "Zekun Qian, Ruize Han, Wei Feng, Song Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Accept the Modality Gap: An Exploration in the Hyperbolic Space",
        "author": "Sameera Ramasinghe, Violetta Shevchenko, Gil Avraham, Thalaiyasingam Ajanthan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PHYSCENE: Physically Interactable 3D Scene Synthesis for Embodied AI",
        "author": "Yandan Yang, Baoxiong Jia, Peiyuan Zhi, Siyuan Huang",
        "abstract": "With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation. While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored. To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents. Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability. Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin. Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research. Project website: http://physcene.github.io.",
        "page": "http://arxiv.org/abs/2404.09465",
        "pdf": "http://arxiv.org/pdf/2404.09465.pdf"
    },
    {
        "title": "A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint",
        "author": "Xiaofeng Cong, Jie Gui, Jing Zhang, Junming Hou, Hao Shen",
        "abstract": "Existing research based on deep learning has extensively explored the problem of daytime image dehazing. However, few studies have considered the characteristics of nighttime hazy scenes. There are two distinctions between nighttime and daytime haze. First, there may be multiple active colored light sources with lower illumination intensity in nighttime scenes, which may cause haze, glow and noise with localized, coupled and frequency inconsistent characteristics. Second, due to the domain discrepancy between simulated and real-world data, unrealistic brightness may occur when applying a dehazing model trained on simulated data to real-world data. To address the above two issues, we propose a semi-supervised model for real-world nighttime dehazing. First, the spatial attention and frequency spectrum filtering are implemented as a spatial-frequency domain information interaction module to handle the first issue. Second, a pseudo-label-based retraining strategy and a local window-based brightness loss for semi-supervised training process is designed to suppress haze and glow while achieving realistic brightness. Experiments on public benchmarks validate the effectiveness of the proposed method and its superiority over state-of-the-art methods. The source code and Supplementary Materials are placed in the https://github.com/Xiaofeng-life/SFSNiD.",
        "page": "http://arxiv.org/abs/2403.18548",
        "pdf": "http://arxiv.org/pdf/2403.18548.pdf"
    },
    {
        "title": "A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation",
        "author": "Qucheng Peng, Ce Zheng, Chen Chen",
        "abstract": "3D human pose data collected in controlled laboratory settings present challenges for pose estimators that generalize across diverse scenarios. To address this, domain generalization is employed. Current methodologies in domain generalization for 3D human pose estimation typically utilize adversarial training to generate synthetic poses for training. Nonetheless, these approaches exhibit several limitations. First, the lack of prior information about the target domain complicates the application of suitable augmentation through a single pose augmentor, affecting generalization on target domains. Moreover, adversarial training's discriminator tends to enforce similarity between source and synthesized poses, impeding the exploration of out-of-source distributions. Furthermore, the pose estimator's optimization is not exposed to domain shifts, limiting its overall generalization ability. To address these limitations, we propose a novel framework featuring two pose augmentors: the weak and the strong augmentors. Our framework employs differential strategies for generation and discrimination processes, facilitating the preservation of knowledge related to source poses and the exploration of out-of-source distributions without prior information about target poses. Besides, we leverage meta-optimization to simulate domain shifts in the optimization process of the pose estimator, thereby improving its generalization ability. Our proposed approach significantly outperforms existing methods, as demonstrated through comprehensive experiments on various benchmark datasets.Our code will be released at \\url{https://github.com/davidpengucf/DAF-DG}.",
        "page": "http://arxiv.org/abs/2403.11310",
        "pdf": "http://arxiv.org/pdf/2403.11310.pdf"
    },
    {
        "title": "Hearing Anything Anywhere",
        "author": "Mason Wang, Ryosuke Sawata, Samuel Clarke, Ruohan Gao, Shangzhe Wu, Jiajun Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MICap: A Unified Model for Identity-aware Movie Descriptions",
        "author": "Haran Raajesh, Naveen Reddy Desanur, Zeeshan Khan, Makarand Tapaswi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SD2Event: Self-supervised Learning of Dynamic Detectors and Contextual Descriptors for Event Cameras",
        "author": "Yuan Gao, Yuqing Zhu, Xinjun Li, Yimin Du, Tianzhu Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MeaCap: Memory-Augmented Zero-shot Image Captioning",
        "author": "Zequn Zeng, Yan Xie, Hao Zhang, Chiyu Chen, Zhengjue Wang, Bo Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Re-thinking Data Availability Attacks Against Deep Neural Networks",
        "author": "Bin Fang, Bo Li, Shuang Wu, Shouhong Ding, Ran Yi, Lizhuang Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Self-Supervised Facial Representation Learning with Facial Region Awareness",
        "author": "Zheng Gao, Ioannis Patras",
        "abstract": "Self-supervised pre-training has been proved to be effective in learning transferable representations that benefit various visual tasks. This paper asks this question: can self-supervised pre-training learn general facial representations for various facial analysis tasks? Recent efforts toward this goal are limited to treating each face image as a whole, i.e., learning consistent facial representations at the image-level, which overlooks the consistency of local facial representations (i.e., facial regions like eyes, nose, etc). In this work, we make a first attempt to propose a novel self-supervised facial representation learning framework to learn consistent global and local facial representations, Facial Region Awareness (FRA). Specifically, we explicitly enforce the consistency of facial regions by matching the local facial representations across views, which are extracted with learned heatmaps highlighting the facial regions. Inspired by the mask prediction in supervised semantic segmentation, we obtain the heatmaps via cosine similarity between the per-pixel projection of feature maps and facial mask embeddings computed from learnable positional embeddings, which leverage the attention mechanism to globally look up the facial image for facial regions. To learn such heatmaps, we formulate the learning of facial mask embeddings as a deep clustering problem by assigning the pixel features from the feature maps to them. The transfer learning results on facial classification and regression tasks show that our FRA outperforms previous pre-trained models and more importantly, using ResNet as the unified backbone for various tasks, our FRA achieves comparable or even better performance compared with SOTA methods in facial analysis tasks.",
        "page": "http://arxiv.org/abs/2403.02138",
        "pdf": "http://arxiv.org/pdf/2403.02138.pdf"
    },
    {
        "title": "Abductive Ego-View Accident Video Understanding for Safe Driving Perception",
        "author": "Jianwu Fang, Lei-lei Li, Junfei Zhou, Junbin Xiao, Hongkai Yu, Chen Lv, Jianru Xue, Tat-seng Chua",
        "abstract": "We present MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each with temporally aligned text descriptions. We annotate over 2.23 million object boxes and 58,650 pairs of video-based accident reasons, covering 58 accident categories. MM-AU supports various accident understanding tasks, particularly multimodal video diffusion to understand accident cause-effect chains for safe driving. With MM-AU, we present an Abductive accident Video understanding framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven by an abductive CLIP model. This model involves a contrastive interaction loss to learn the pair co-occurrence of normal, near-accident, accident frames with the corresponding text descriptions, such as accident reasons, prevention advice, and accident categories. OAVD enforces the causal region learning while fixing the content of the original frame background in video generation, to find the dominant cause-effect chain for certain accidents. Extensive experiments verify the abductive ability of AdVersa-SD and the superiority of OAVD against the state-of-the-art diffusion models. Additionally, we provide careful benchmark evaluations for object detection and accident reason answering since AdVersa-SD relies on precise object and accident reason information.",
        "page": "http://arxiv.org/abs/2403.00436",
        "pdf": "http://arxiv.org/pdf/2403.00436.pdf"
    },
    {
        "title": "Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling",
        "author": "Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, Weidong Cai",
        "abstract": "Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the interference of distributional latent bias. Then, we incorporate a suite of critical geometric properties to impose proper constraints on the layout of constructed embedding space, which in turn minimizes the uncontrollable risk for unknown class learning and structuring. Furthermore, a spectral graph-theoretic method is devised to estimate the number of potential novel classes. It inherits two intriguing merits compared to existent approaches, namely high computational efficiency and flexibility for taxonomy-adaptive estimation. Extensive experiments across various biomedical scenarios substantiate the effectiveness and general applicability of our method.",
        "page": "http://arxiv.org/abs/2403.01053",
        "pdf": "http://arxiv.org/pdf/2403.01053.pdf"
    },
    {
        "title": "Segment Any Event Streams via Weighted Adaptation of Pivotal Tokens",
        "author": "Zhiwen Chen, Zhiyu Zhu, Yifan Zhang, Junhui Hou, Guangming Shi, Jinjian Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DETRs Beat YOLOs on Real-time Object Detection",
        "author": "Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen",
        "abstract": "The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: https://zhao-yian.github.io/RTDETR.",
        "page": "http://arxiv.org/abs/2304.08069",
        "pdf": "http://arxiv.org/pdf/2304.08069.pdf"
    },
    {
        "title": "Neural Clustering based Visual Representation Learning",
        "author": "Guikun Chen, Xia Li, Yi Yang, Wenguan Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions",
        "author": "Chengfeng Zhao, Juze Zhang, Jiashen Du, Ziwei Shan, Junye Wang, Jingyi Yu, Jingya Wang, Lan Xu",
        "abstract": "We are living in a world surrounded by diverse and \"smart\" devices with rich modalities of sensing ability. Conveniently capturing the interactions between us humans and these objects remains far-reaching. In this paper, we present I'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the human and object in a novel setting: using a minimal amount of RGB camera and object-mounted Inertial Measurement Unit (IMU). It combines general motion inference and category-aware refinement. For the former, we introduce a holistic human-object tracking method to fuse the IMU signals and the RGB stream and progressively recover the human motions and subsequently the companion object motions. For the latter, we tailor a category-aware motion diffusion model, which is conditioned on both the raw IMU observations and the results from the previous stage under over-parameterization representation. It significantly refines the initial results and generates vivid body, hand, and object motions. Moreover, we contribute a large dataset with ground truth human and object motions, dense RGB inputs, and rich object-mounted IMU measurements. Extensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid capture setting. Our dataset and code will be released to the community.",
        "page": "http://arxiv.org/abs/2312.08869",
        "pdf": "http://arxiv.org/pdf/2312.08869.pdf"
    },
    {
        "title": "AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond",
        "author": "Zixiang Zhou, Yu Wan, Baoyuan Wang",
        "abstract": "Large Language Models(LLMs) have shown remarkable emergent abilities in unifying almost all (if not every) NLP tasks. In the human motion-related realm, however, researchers still develop siloed models for each task. Inspired by InstuctGPT, and the generalist concept behind Gato, we introduce AvatarGPT, an All-in-One framework for motion understanding, planning, generations as well as other tasks such as motion in-between synthesis. AvatarGPT treats each task as one type of instruction fine-tuned on the shared LLM. All the tasks are seamlessly interconnected with language as the universal interface, constituting a closed-loop within the framework. To achieve this, human motion sequences are first encoded as discrete tokens, which serve as the extended vocabulary of LLM. Then, an unsupervised pipeline to generate natural language descriptions of human action sequences from in-the-wild videos is developed. Finally, all tasks are jointly trained. Extensive experiments show that AvatarGPT achieves SOTA on low-level tasks, and promising results on high-level tasks, demonstrating the effectiveness of our proposed All-in-One framework. Moreover, for the first time, AvatarGPT enables a principled approach by iterative traversal of the tasks within the closed-loop for unlimited long-motion synthesis.",
        "page": "http://arxiv.org/abs/2311.16468",
        "pdf": "http://arxiv.org/pdf/2311.16468.pdf"
    },
    {
        "title": "De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts",
        "author": "Yuzheng Wang, Dingkang Yang, Zhaoyu Chen, Yang Liu, Siao Liu, Wenqiang Zhang, Lihua Zhang, Lizhe Qi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OHTA: One-shot Hand Avatar via Data-driven Implicit Priors",
        "author": "Xiaozheng Zheng, Chao Wen, Zhuo Su, Zeran Xu, Zhaohu Li, Yang Zhao, Zhou Xue",
        "abstract": "In this paper, we delve into the creation of one-shot hand avatars, attaining high-fidelity and drivable hand representations swiftly from a single image. With the burgeoning domains of the digital human, the need for quick and personalized hand avatar creation has become increasingly critical. Existing techniques typically require extensive input data and may prove cumbersome or even impractical in certain scenarios. To enhance accessibility, we present a novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed hand avatars from merely one image. OHTA tackles the inherent difficulties of this data-limited problem by learning and utilizing data-driven hand priors. Specifically, we design a hand prior model initially employed for 1) learning various hand priors with available data and subsequently for 2) the inversion and fitting of the target identity with prior knowledge. OHTA demonstrates the capability to create high-fidelity hand avatars with consistent animatable quality, solely relying on a single image. Furthermore, we illustrate the versatility of OHTA through diverse applications, encompassing text-to-avatar conversion, hand editing, and identity latent space manipulation.",
        "page": "http://arxiv.org/abs/2402.18969",
        "pdf": "http://arxiv.org/pdf/2402.18969.pdf"
    },
    {
        "title": "Structure-Guided Adversarial Training of Diffusion Models",
        "author": "Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin CUI",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CMA: A Chromaticity Map Adapter for Robust Detection of Screen-Recapture Document Images",
        "author": "Changsheng Chen, Liangwei Lin, Yongqi Chen, Bin Li, Jishen Zeng, Jiwu Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
        "author": "Enxin Song, Wenhao Chai, Guanhong Wang, Haoyang Zhou, Feiyang Wu, Yucheng Zhang, Tian Ye, Haozhe Chi, Xun Guo, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, Gaoang Wang",
        "abstract": "Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific pre-defined vision tasks. Yet, existing systems can only handle videos with very few frames. For long videos, the computation complexity, memory cost, and long-term temporal connection impose additional challenges. Taking advantage of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with our specially designed memory mechanism, we propose the MovieChat to overcome these challenges. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video and 14K manual annotations for validation of the effectiveness of our method.",
        "page": "http://arxiv.org/abs/2307.16449",
        "pdf": "http://arxiv.org/pdf/2307.16449.pdf"
    },
    {
        "title": "Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning",
        "author": "Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, De-Chuan Zhan",
        "abstract": "Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones. Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes. As a result, it is desired to figure out a way of efficient model updating without harming former knowledge. In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces. These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces. As data evolves, the expanding subspaces render the old class classifiers incompatible with new-stage spaces. Correspondingly, we design a semantic-guided prototype complement strategy that synthesizes old classes' new features without using any old class instance. Extensive experiments on seven benchmark datasets verify EASE's state-of-the-art performance. Code is available at: https://github.com/sun-hailong/CVPR24-Ease",
        "page": "http://arxiv.org/abs/2403.12030",
        "pdf": "http://arxiv.org/pdf/2403.12030.pdf"
    },
    {
        "title": "Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation",
        "author": "Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander William Clegg, Eric Undersander, Angel Xuan Chang, Manolis Savva",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models",
        "author": "Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, Ying Shan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation",
        "author": "Yanhui Wang, Jianmin Bao, Wenming Weng, Ruoyu Feng, Dacheng Yin, Tao Yang, Jingxu Zhang, Qi Dai, Zhiyuan Zhao, Chunyu Wang, Kai Qiu, Yuhui Yuan, Xiaoyan Sun, Chong Luo, Baining Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Text-guided 3D Scene Composition",
        "author": "Qihang Zhang, Chaoyang Wang, Aliaksandr Siarohin, Peiye Zhuang, Yinghao Xu, Ceyuan Yang, Dahua Lin, Bolei Zhou, Sergey Tulyakov, Hsin-Ying Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM",
        "author": "Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, Jonathon Luiten",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Density-Guided Semi-Supervised 3D Semantic Segmentation with Dual-Space Hardness Sampling",
        "author": "Jianan Li, Qiulei Dong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "From Correspondences to Pose: Non-minimal Certifiably Optimal Relative Pose without Disambiguation",
        "author": "Javier Tirado-Gar\u00edn, Javier Civera",
        "abstract": "Estimating the relative camera pose from $n \\geq 5$ correspondences between two calibrated views is a fundamental task in computer vision. This process typically involves two stages: 1) estimating the essential matrix between the views, and 2) disambiguating among the four candidate relative poses that satisfy the epipolar geometry. In this paper, we demonstrate a novel approach that, for the first time, bypasses the second stage. Specifically, we show that it is possible to directly estimate the correct relative camera pose from correspondences without needing a post-processing step to enforce the cheirality constraint on the correspondences. Building on recent advances in certifiable non-minimal optimization, we frame the relative pose estimation as a Quadratically Constrained Quadratic Program (QCQP). By applying the appropriate constraints, we ensure the estimation of a camera pose that corresponds to a valid 3D geometry and that is globally optimal when certified. We validate our method through exhaustive synthetic and real-world experiments, confirming the efficacy, efficiency and accuracy of the proposed approach. Code is available at https://github.com/javrtg/C2P.",
        "page": "http://arxiv.org/abs/2312.05995",
        "pdf": "http://arxiv.org/pdf/2312.05995.pdf"
    },
    {
        "title": "SI-MIL: Taming Deep MIL for Self-Interpretability in Gigapixel Histopathology",
        "author": "Saarthak Kapse, Pushpak Pati, Srijan Das, Jingwei Zhang, Chao Chen, Maria Vakalopoulou, Joel Saltz, Dimitris Samaras, Rajarsi Gupta, Prateek Prasanna",
        "abstract": "Introducing interpretability and reasoning into Multiple Instance Learning (MIL) methods for Whole Slide Image (WSI) analysis is challenging, given the complexity of gigapixel slides. Traditionally, MIL interpretability is limited to identifying salient regions deemed pertinent for downstream tasks, offering little insight to the end-user (pathologist) regarding the rationale behind these selections. To address this, we propose Self-Interpretable MIL (SI-MIL), a method intrinsically designed for interpretability from the very outset. SI-MIL employs a deep MIL framework to guide an interpretable branch grounded on handcrafted pathological features, facilitating linear predictions. Beyond identifying salient regions, SI-MIL uniquely provides feature-level interpretations rooted in pathological insights for WSIs. Notably, SI-MIL, with its linear prediction constraints, challenges the prevalent myth of an inevitable trade-off between model interpretability and performance, demonstrating competitive results compared to state-of-the-art methods on WSI-level prediction tasks across three cancer types. In addition, we thoroughly benchmark the local and global-interpretability of SI-MIL in terms of statistical analysis, a domain expert study, and desiderata of interpretability, namely, user-friendliness and faithfulness.",
        "page": "http://arxiv.org/abs/2312.15010",
        "pdf": "http://arxiv.org/pdf/2312.15010.pdf"
    },
    {
        "title": "Low-Latency Neural Stereo Streaming",
        "author": "Qiqi Hou, Qiqi Hou, Farzad Farhadzadeh, Amir Said, Guillaume Sautiere, Hoang Le",
        "abstract": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
        "page": "http://arxiv.org/abs/2403.17879",
        "pdf": "http://arxiv.org/pdf/2403.17879.pdf"
    },
    {
        "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
        "author": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Fei Huang",
        "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.",
        "page": "http://arxiv.org/abs/2311.04257",
        "pdf": "http://arxiv.org/pdf/2311.04257.pdf"
    },
    {
        "title": "Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection",
        "author": "Huan Liu, Zichang Tan, Zichang Tan, Chuangchuang Tan, Yunchao Wei, Jingdong Wang, Yao Zhao",
        "abstract": "In this paper, we study the problem of generalizable synthetic image detection, aiming to detect forgery images from diverse generative methods, e.g., GANs and diffusion models. Cutting-edge solutions start to explore the benefits of pre-trained models, and mainly follow the fixed paradigm of solely training an attached classifier, e.g., combining frozen CLIP-ViT with a learnable linear layer in UniFD. However, our analysis shows that such a fixed paradigm is prone to yield detectors with insufficient learning regarding forgery representations. We attribute the key challenge to the lack of forgery adaptation, and present a novel forgery-aware adaptive transformer approach, namely FatFormer. Based on the pre-trained vision-language spaces of CLIP, FatFormer introduces two core designs for the adaption to build generalized forgery representations. First, motivated by the fact that both image and frequency analysis are essential for synthetic image detection, we develop a forgery-aware adapter to adapt image features to discern and integrate local forgery traces within image and frequency domains. Second, we find that considering the contrastive objectives between adapted image features and text prompt embeddings, a previously overlooked aspect, results in a nontrivial generalization improvement. Accordingly, we introduce language-guided alignment to supervise the forgery adaptation with image and text prompts in FatFormer. Experiments show that, by coupling these two designs, our approach tuned on 4-class ProGAN data attains a remarkable detection performance, achieving an average of 98% accuracy to unseen GANs, and surprisingly generalizes to unseen diffusion models with 95% accuracy.",
        "page": "http://arxiv.org/abs/2312.16649",
        "pdf": "http://arxiv.org/pdf/2312.16649.pdf"
    },
    {
        "title": "OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition",
        "author": "Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, Zhibo Yang",
        "abstract": "Recently, visually-situated text parsing (VsTP) has experienced notable advancements, driven by the increasing demand for automated document understanding and the emergence of Generative Large Language Models (LLMs) capable of processing document-based questions. Various methods have been proposed to address the challenging problem of VsTP. However, due to the diversified targets and heterogeneous schemas, previous works usually design task-specific architectures and objectives for individual tasks, which inadvertently leads to modal isolation and complex workflow. In this paper, we propose a unified paradigm for parsing visually-situated text across diverse scenarios. Specifically, we devise a universal model, called OmniParser, which can simultaneously handle three typical visually-situated text parsing tasks: text spotting, key information extraction, and table recognition. In OmniParser, all tasks share the unified encoder-decoder architecture, the unified objective: point-conditioned text generation, and the unified input & output representation: prompt & structured sequences. Extensive experiments demonstrate that the proposed OmniParser achieves state-of-the-art (SOTA) or highly competitive performances on 7 datasets for the three visually-situated text parsing tasks, despite its unified, concise design. The code is available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery.",
        "page": "http://arxiv.org/abs/2403.19128",
        "pdf": "http://arxiv.org/pdf/2403.19128.pdf"
    },
    {
        "title": "LowRankOcc: Tensor Decomposition and Low-Rank Recovery for Vision-based 3D Semantic Occupancy Prediction",
        "author": "Linqing Zhao, Xiuwei Xu, Ziwei Wang, Yunpeng Zhang, Borui Zhang, Wenzhao Zheng, Dalong Du, Jie Zhou, Jiwen Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Taming Mode Collapse in Score Distillation for Text-to-3D Generation",
        "author": "Peihao Wang, Dejia Xu, Dejia Xu, Zhiwen Fan, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, Vikas Chandra",
        "abstract": "Despite the remarkable performance of score distillation in text-to-3D generation, such techniques notoriously suffer from view inconsistency issues, also known as \"Janus\" artifact, where the generated objects fake each view with multiple front faces. Although empirically effective methods have approached this problem via score debiasing or prompt engineering, a more rigorous perspective to explain and tackle this problem remains elusive. In this paper, we reveal that the existing score distillation-based text-to-3D generation frameworks degenerate to maximal likelihood seeking on each view independently and thus suffer from the mode collapse problem, manifesting as the Janus artifact in practice. To tame mode collapse, we improve score distillation by re-establishing the entropy term in the corresponding variational objective, which is applied to the distribution of rendered images. Maximizing the entropy encourages diversity among different views in generated 3D assets, thereby mitigating the Janus problem. Based on this new objective, we derive a new update rule for 3D score distillation, dubbed Entropic Score Distillation (ESD). We theoretically reveal that ESD can be simplified and implemented by just adopting the classifier-free guidance trick upon variational score distillation. Although embarrassingly straightforward, our extensive experiments successfully demonstrate that ESD can be an effective treatment for Janus artifacts in score distillation.",
        "page": "http://arxiv.org/abs/2401.00909",
        "pdf": "http://arxiv.org/pdf/2401.00909.pdf"
    },
    {
        "title": "Sequential Modeling Enables Scalable Learning for Large Vision Models",
        "author": "Yutong Bai, Xinyang Geng, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan L. Yuille, Trevor Darrell, Jitendra Malik, Alexei A. Efros",
        "abstract": "We introduce a novel sequential modeling approach which enables learning a Large Vision Model (LVM) without making use of any linguistic data. To do this, we define a common format, \"visual sentences\", in which we can represent raw images and videos as well as annotated data sources such as semantic segmentations and depth reconstructions without needing any meta-knowledge beyond the pixels. Once this wide variety of visual data (comprising 420 billion tokens) is represented as sequences, the model can be trained to minimize a cross-entropy loss for next token prediction. By training across various scales of model architecture and data diversity, we provide empirical evidence that our models scale effectively. Many different vision tasks can be solved by designing suitable visual prompts at test time.",
        "page": "http://arxiv.org/abs/2312.00785",
        "pdf": "http://arxiv.org/pdf/2312.00785.pdf"
    },
    {
        "title": "Distraction is All You Need: Memory-Efficient Image Immunization against Diffusion-Based Image Editing",
        "author": "Ling Lo, Cheng Yeo, Hong-Han Shuai, Hong-Han Shuai, Wen-Huang Cheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OmniMedVQA: A New Large-Scale Comprehensive  Evaluation Benchmark for Medical LVLM",
        "author": "Yutao Hu, Yutao Hu, Tianbin, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UniPAD: A Universal Pre-training Paradigm for Autonomous Driving",
        "author": "Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, SHIXIANG TANG, Hengshuang Zhao, Qibo Qiu, Binbin Lin, Xiaofei He, Xiaofei He, Wanli Ouyang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation",
        "author": "Xu Zheng, Pengyuan Zhou, ATHANASIOS, Lin Wang",
        "abstract": "This paper addresses an interesting yet challenging problem -- source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation -- given only a pinhole image-trained model (i.e., source) and unlabeled panoramic images (i.e., target). Tackling this problem is nontrivial due to the semantic mismatches, style discrepancies, and inevitable distortion of panoramic images. To this end, we propose a novel method that utilizes Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) with a fixed FoV to mimic the pinhole images. Both projections are shown effective in extracting knowledge from the source model. However, the distinct projection discrepancies between source and target domains impede the direct knowledge transfer; thus, we propose a panoramic prototype adaptation module (PPAM) to integrate panoramic prototypes from the extracted knowledge for adaptation. We then impose the loss constraints on both predictions and prototypes and propose a cross-dual attention module (CDAM) at the feature level to better align the spatial and channel characteristics across the domains and projections. Both knowledge extraction and transfer processes are synchronously updated to reach the best performance. Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our method achieves significantly better performance than prior SFUDA methods for pinhole-to-panoramic adaptation.",
        "page": "http://arxiv.org/abs/2403.12505",
        "pdf": "http://arxiv.org/pdf/2403.12505.pdf"
    },
    {
        "title": "MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling",
        "author": "Xuzhe Zhang, Yuhao Wu, Elsa Angelini, Ang Li, Jia Guo, Jerod Rasmussen, Thomas O'Connor, Pathik Wadhwa, Andrea Jackowski, Hai Li, Jonathan Posner, Andrew Laine, YUN WANG, Yun Wang",
        "abstract": "Robust segmentation is critical for deriving quantitative measures from large-scale, multi-center, and longitudinal medical scans. Manually annotating medical scans, however, is expensive and labor-intensive and may not always be available in every domain. Unsupervised domain adaptation (UDA) is a well-studied technique that alleviates this label-scarcity problem by leveraging available labels from another domain. In this study, we introduce Masked Autoencoding and Pseudo-Labeling Segmentation (MAPSeg), a $\\textbf{unified}$ UDA framework with great versatility and superior performance for heterogeneous and volumetric medical image segmentation. To the best of our knowledge, this is the first study that systematically reviews and develops a framework to tackle four different domain shifts in medical image segmentation. More importantly, MAPSeg is the first framework that can be applied to $\\textbf{centralized}$, $\\textbf{federated}$, and $\\textbf{test-time}$ UDA while maintaining comparable performance. We compare MAPSeg with previous state-of-the-art methods on a private infant brain MRI dataset and a public cardiac CT-MRI dataset, and MAPSeg outperforms others by a large margin (10.5 Dice improvement on the private MRI dataset and 5.7 on the public CT-MRI dataset). MAPSeg poses great practical value and can be applied to real-world problems. GitHub: https://github.com/XuzheZ/MAPSeg/.",
        "page": "http://arxiv.org/abs/2303.09373",
        "pdf": "http://arxiv.org/pdf/2303.09373.pdf"
    },
    {
        "title": "Seeing Motion at Nighttime with an Event Camera",
        "author": "Haoyue Liu, Shihan Peng, Lin Zhu, Yi Chang, Hanyu Zhou, Luxin Yan",
        "abstract": "We focus on a very challenging task: imaging at nighttime dynamic scenes. Most previous methods rely on the low-light enhancement of a conventional RGB camera. However, they would inevitably face a dilemma between the long exposure time of nighttime and the motion blur of dynamic scenes. Event cameras react to dynamic changes with higher temporal resolution (microsecond) and higher dynamic range (120dB), offering an alternative solution. In this work, we present a novel nighttime dynamic imaging method with an event camera. Specifically, we discover that the event at nighttime exhibits temporal trailing characteristics and spatial non-stationary distribution. Consequently, we propose a nighttime event reconstruction network (NER-Net) which mainly includes a learnable event timestamps calibration module (LETC) to align the temporal trailing events and a non-uniform illumination aware module (NIAM) to stabilize the spatiotemporal distribution of events. Moreover, we construct a paired real low-light event dataset (RLED) through a co-axial imaging system, including 64,200 spatially and temporally aligned image GTs and low-light events. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art methods in terms of visual quality and generalization ability on real-world nighttime datasets. The project are available at: https://github.com/Liu-haoyue/NER-Net.",
        "page": "http://arxiv.org/abs/2404.11884",
        "pdf": "http://arxiv.org/pdf/2404.11884.pdf"
    },
    {
        "title": "Wavelet-based Fourier Information Interaction with Frequency Diffusion Adjustment for Underwater Image Restoration",
        "author": "Chen Zhao, Weiling Cai, Chenyu Dong, Chenyu Dong, Chengwei Hu",
        "abstract": "Underwater images are subject to intricate and diverse degradation, inevitably affecting the effectiveness of underwater visual tasks. However, most approaches primarily operate in the raw pixel space of images, which limits the exploration of the frequency characteristics of underwater images, leading to an inadequate utilization of deep models' representational capabilities in producing high-quality images. In this paper, we introduce a novel Underwater Image Enhancement (UIE) framework, named WF-Diff, designed to fully leverage the characteristics of frequency domain information and diffusion models. WF-Diff consists of two detachable networks: Wavelet-based Fourier information interaction network (WFI2-net) and Frequency Residual Diffusion Adjustment Module (FRDAM). With our full exploration of the frequency domain information, WFI2-net aims to achieve preliminary enhancement of frequency information in the wavelet space. Our proposed FRDAM can further refine the high- and low-frequency information of the initial enhanced images, which can be viewed as a plug-and-play universal module to adjust the detail of the underwater images. With the above techniques, our algorithm can show SOTA performance on real-world underwater image datasets, and achieves competitive performance in visual quality.",
        "page": "http://arxiv.org/abs/2311.16845",
        "pdf": "http://arxiv.org/pdf/2311.16845.pdf"
    },
    {
        "title": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning",
        "author": "Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, Hong-Han Shuai, Wen-Huang Cheng",
        "abstract": "Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions. This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding. In this work, we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts. Initially, we identify key visual clues critical to visual emotion recognition. Subsequently, we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data, effectively addressing the scarcity of annotated instruction data in this domain. Expanding on the groundwork established by InstructBLIP, our proposed EmoVIT architecture incorporates emotion-specific instruction data, leveraging the powerful capabilities of Large Language Models to enhance performance. Through extensive experiments, our model showcases its proficiency in emotion classification, adeptness in affective reasoning, and competence in comprehending humor. The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs, providing valuable insights and opening avenues for future exploration in this domain. Our code is available at \\url{https://github.com/aimmemotion/EmoVIT}.",
        "page": "http://arxiv.org/abs/2404.16670",
        "pdf": "http://arxiv.org/pdf/2404.16670.pdf"
    },
    {
        "title": "Segment and Caption Anything",
        "author": "Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, Zicheng Liu",
        "abstract": "We propose a method to efficiently equip the Segment Anything Model (SAM) with the ability to generate regional captions. SAM presents strong generalizability to segment anything while is short for semantic understanding. By introducing a lightweight query-based feature mixer, we align the region-specific features with the embedding space of language models for later caption generation. As the number of trainable parameters is small (typically in the order of tens of millions), it costs less computation, less memory usage, and less communication bandwidth, resulting in both fast and scalable training. To address the scarcity problem of regional caption data, we propose to first pre-train our model on objection detection and segmentation tasks. We call this step weak supervision pretraining since the pre-training data only contains category names instead of full-sentence descriptions. The weak supervision pretraining allows us to leverage many publicly available object detection and segmentation datasets. We conduct extensive experiments to demonstrate the superiority of our method and validate each design choice. This work serves as a stepping stone towards scaling up regional captioning data and sheds light on exploring efficient ways to augment SAM with regional semantics. The project page, along with the associated code, can be accessed via https://xk-huang.github.io/segment-caption-anything/.",
        "page": "http://arxiv.org/abs/2312.00869",
        "pdf": "http://arxiv.org/pdf/2312.00869.pdf"
    },
    {
        "title": "MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark",
        "author": "Sanghyun Woo, Kwanyong Park, Inkyu Shin, Myungchul Kim, In So Kweon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generative Latent Coding for Ultra-Low Bitrate Image Compression",
        "author": "Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, Yan Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation",
        "author": "Yunhao Ge, Yihe Tang, Jiashu Xu, Jiashu Xu, Cem Gokmen, Chengshu Li, Wensi Ai, Benjamin Martinez, Arman Aydin, Mona Anvari, Ayush Chakravarthy, Hong-Xing Yu, Josiah Wong, Sanjana Srivastava, Sharon Lee, Shengxin Zha, Laurent Itti, Yunzhu Li, Roberto Mart\u00edn-Mart\u00edn, Miao Liu, Pengchuan Zhang, Ruohan Zhang, Li Fei-Fei, Jiajun Wu",
        "abstract": "The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as \"filled\" and \"folded\"), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction. Project website: https://behavior-vision-suite.github.io/",
        "page": "http://arxiv.org/abs/2405.09546",
        "pdf": "http://arxiv.org/pdf/2405.09546.pdf"
    },
    {
        "title": "Event-based Visible and Infrared Fusion via Multi-task Collaboration",
        "author": "Mengyue Geng, Lin Zhu, Lizhi Wang, Wei Zhang, Ruiqin Xiong, Yonghong Tian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts",
        "author": "Mu Cai, Haotian Liu, Siva Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee, Yong Jae Lee",
        "abstract": "While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \"red bounding box\" or \"pointed arrow\". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.",
        "page": "http://arxiv.org/abs/2312.00784",
        "pdf": "http://arxiv.org/pdf/2312.00784.pdf"
    },
    {
        "title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models",
        "author": "Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Sriram Yenamandra, Mikael Henaff, Alexander Sax, Sneha Silwal, Paul McVay, Oleksandr Maksymets, Sergio Arnaud, Pranav Putta, Karmesh Yadav, Qiyang Li, Benjamin Newman, Mohit Sharma, Mohit Sharma, Vincent-Pierre Berges, Shiqi Zhang, Pulkit Agrawal, Dhruv Batra, Yonatan Bisk, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Aravind Rajeswaran",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BEVSpread: Spread Voxel Pooling for Bird\u2019s-Eye-View Representation in Vision-based Roadside 3D Object Detection",
        "author": "Wenjie Wang, Yehao Lu, Guangcong Zheng, Shuigenzhan, Xiaoqing Ye, Zichang Tan, Jingdong Wang, Gaoang Wang, Xi Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Memory-based Adapters for Online 3D Scene Perception",
        "author": "Xiuwei Xu, Chong Xia, Ziwei Wang, Linqing Zhao, Linqing Zhao, Yueqi Duan, Jie Zhou, Jiwen Lu",
        "abstract": "In this paper, we propose a new framework for online 3D scene perception. Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information. To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability. Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features. Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame. We further propose 3D-to-2D adapter to enhance image features with strong global context. Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs. \\href{https://xuxw98.github.io/Online3D/}{Project page}.",
        "page": "http://arxiv.org/abs/2403.06974",
        "pdf": "http://arxiv.org/pdf/2403.06974.pdf"
    },
    {
        "title": "MultiDiff: Consistent Novel View Synthesis from a Single Image",
        "author": "Norman M\u00fcller, Katja Schwarz, Katja Schwarz, Barbara Roessle, Lorenzo Porzi, Samuel Rota Bul\u00f2, Matthias Nie\u00dfner, Peter Kontschieder",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D Medical Image Analysis",
        "author": "Linshan Wu, Linshan Wu, Jia-Xin Zhuang, Hao Chen",
        "abstract": "Self-Supervised Learning (SSL) has demonstrated promising results in 3D medical image analysis. However, the lack of high-level semantics in pre-training still heavily hinders the performance of downstream tasks. We observe that 3D medical images contain relatively consistent contextual position information, i.e., consistent geometric relations between different organs, which leads to a potential way for us to learn consistent semantic representations in pre-training. In this paper, we propose a simple-yet-effective Volume Contrast (VoCo) framework to leverage the contextual position priors for pre-training. Specifically, we first generate a group of base crops from different regions while enforcing feature discrepancy among them, where we employ them as class assignments of different regions. Then, we randomly crop sub-volumes and predict them belonging to which class (located at which region) by contrasting their similarity to different base crops, which can be seen as predicting contextual positions of different sub-volumes. Through this pretext task, VoCo implicitly encodes the contextual position priors into model representations without the guidance of annotations, enabling us to effectively improve the performance of downstream tasks that require high-level semantics. Extensive experimental results on six downstream tasks demonstrate the superior effectiveness of VoCo. Code will be available at https://github.com/Luffy03/VoCo.",
        "page": "http://arxiv.org/abs/2402.17300",
        "pdf": "http://arxiv.org/pdf/2402.17300.pdf"
    },
    {
        "title": "Hallucination Augmented Contrastive Learning for Multimodal Large Language Model",
        "author": "Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, Fei Huang, Shikun Zhang",
        "abstract": "Multi-modal large language models (MLLMs) have been shown to efficiently integrate natural language with visual information to handle multi-modal tasks. However, MLLMs still face a fundamental limitation of hallucinations, where they tend to generate erroneous or fabricated information. In this paper, we address hallucinations in MLLMs from a novel perspective of representation learning. We first analyzed the representation distribution of textual and visual tokens in MLLM, revealing two important findings: 1) there is a significant gap between textual and visual representations, indicating unsatisfactory cross-modal representation alignment; 2) representations of texts that contain and do not contain hallucinations are entangled, making it challenging to distinguish them. These two observations inspire us with a simple yet effective method to mitigate hallucinations. Specifically, we introduce contrastive learning into MLLMs and use text with hallucination as hard negative examples, naturally bringing representations of non-hallucinative text and visual samples closer while pushing way representations of non-hallucinating and hallucinative text. We evaluate our method quantitatively and qualitatively, showing its effectiveness in reducing hallucination occurrences and improving performance across multiple benchmarks. On the MMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the baseline MiniGPT-4/LLaVA. Our code is available on https://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl.",
        "page": "http://arxiv.org/abs/2312.06968",
        "pdf": "http://arxiv.org/pdf/2312.06968.pdf"
    },
    {
        "title": "Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields",
        "author": "Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, Achuta Kadambi",
        "abstract": "3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework encounters significant challenges, notably the disparities in spatial resolution and channel consistency between RGB images and feature maps. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model. Project website at: https://feature-3dgs.github.io/",
        "page": "http://arxiv.org/abs/2312.03203",
        "pdf": "http://arxiv.org/pdf/2312.03203.pdf"
    },
    {
        "title": "SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking",
        "author": "Xiaojun Hou, Jiazheng Xing, Yijie Qian, Yaowei Guo, Shuo Xin, Junhao Chen, Kai Tang, Mengmeng Wang, Mengmeng Wang, Zhengkai Jiang, Liang Liu, Yong Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Edit One for All: Interactive Batch Image Editing",
        "author": "Thao Nguyen, Utkarsh Ojha, Yuheng Li, Haotian Liu, Yong Jae Lee, Yong Jae Lee",
        "abstract": "In recent years, image editing has advanced remarkably. With increased human control, it is now possible to edit an image in a plethora of ways; from specifying in text what we want to change, to straight up dragging the contents of the image in an interactive point-based manner. However, most of the focus has remained on editing single images at a time. Whether and how we can simultaneously edit large batches of images has remained understudied. With the goal of minimizing human supervision in the editing process, this paper presents a novel method for interactive batch image editing using StyleGAN as the medium. Given an edit specified by users in an example image (e.g., make the face frontal), our method can automatically transfer that edit to other test images, so that regardless of their initial state (pose), they all arrive at the same final state (e.g., all facing front). Extensive experiments demonstrate that edits performed using our method have similar visual quality to existing single-image-editing methods, while having more visual consistency and saving significant time and human effort.",
        "page": "http://arxiv.org/abs/2401.10219",
        "pdf": "http://arxiv.org/pdf/2401.10219.pdf"
    },
    {
        "title": "RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation",
        "author": "Peng Lu, Tao Jiang, Yining Li, Xiangtai Li, Kai Chen, Wenming Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Resolution Limit of Single-Photon LIDAR",
        "author": "Stanley H. Chan, Hashan K Weerasooriya, Weijian Zhang, Pamela Abshire, Istvan Gyongy, Robert Henderson",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neural Visibility Field for Uncertainty-Driven Active Mapping",
        "author": "Shangjie Xue, Jesse Dill, Pranay Mathur, Frank Dellaert, Panagiotis Tsiotras, Danfei Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification",
        "author": "Jiangbo Shi, Chen Li, Tieliang Gong, Yefeng Zheng, Huazhu Fu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FairCLIP: Harnessing Fairness in Vision-Language Learning",
        "author": "Yan Luo, MIN SHI, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, Yi Fang, Mengyu Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fusing Personal and Environmental Cues for Identification and Segmentation of First-Person Camera Wearers in Third-Person Views",
        "author": "Ziwei Zhao, Yuchen Wang, Chuhua Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CaDeT: a Causal Disentanglement Approach for Robust Trajectory Prediction in Autonomous Driving",
        "author": "Mozhgan Pourkeshavarz, Junrui Zhang, Amir Rasouli",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AlignMiF: Geometry-Aligned Multimodal Implicit Field for Enhanced LiDAR-Camera Joint Synthesis",
        "author": "Tao Tang, Guangrun Wang, Yixing Lao, Peng Chen, Jie Liu, Liang Lin, Kaicheng Yu, Xiaodan Liang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BodyMAP - Jointly Predicting Body Mesh and 3D Applied Pressure Map for People in Bed",
        "author": "Abhishek Tandon, Anujraaj Goyal, Henry M. Clever, Zackory Erickson",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Telling Left from Right: Identifying Geometry-Aware Semantic Correspondence",
        "author": "Junyi Zhang, Charles Herrmann, Junhwa Hur, Eric Chen, Varun Jampani, Deqing Sun, Ming-Hsuan Yang",
        "abstract": "While pre-trained large-scale vision models have shown significant promise for semantic correspondence, their features often struggle to grasp the geometry and orientation of instances. This paper identifies the importance of being geometry-aware for semantic correspondence and reveals a limitation of the features of current foundation models under simple post-processing. We show that incorporating this information can markedly enhance semantic correspondence performance with simple but effective solutions in both zero-shot and supervised settings. We also construct a new challenging benchmark for semantic correspondence built from an existing animal pose estimation dataset, for both pre-training validating models. Our method achieves a PCK@0.10 score of 65.4 (zero-shot) and 85.6 (supervised) on the challenging SPair-71k dataset, outperforming the state of the art by 5.5p and 11.0p absolute gains, respectively. Our code and datasets are publicly available at: https://telling-left-from-right.github.io/.",
        "page": "http://arxiv.org/abs/2311.17034",
        "pdf": "http://arxiv.org/pdf/2311.17034.pdf"
    },
    {
        "title": "Text-Guided 3D Face Synthesis - From Generation to Editing",
        "author": "Yunjie Wu, Yapeng Meng, Zhipeng Hu, Lincheng Li, Haoqian Wu, Kun Zhou, Weiwei Xu, Xin Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AssistGUI: Task-Oriented PC Graphical User Interface Automation",
        "author": "Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qin WU, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang, Luowei Zhou, Mike Zheng Shou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Don\u2019t drop your samples! Coherence-aware training benefits Conditional diffusion",
        "author": "Nicolas Dufour, Victor Besnier, Vicky Kalogeiton, David Picard",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions",
        "author": "Weijia Li, Haote Yang, Zhenghao Hu, Juepeng Zheng, Gui-Song Xia, Conghui He",
        "abstract": "3D building reconstruction from monocular remote sensing images is an important and challenging research problem that has received increasing attention in recent years, owing to its low cost of data acquisition and availability for large-scale applications. However, existing methods rely on expensive 3D-annotated samples for fully-supervised training, restricting their application to large-scale cross-city scenarios. In this work, we propose MLS-BRN, a multi-level supervised building reconstruction network that can flexibly utilize training samples with different annotation levels to achieve better reconstruction results in an end-to-end manner. To alleviate the demand on full 3D supervision, we design two new modules, Pseudo Building Bbox Calculator and Roof-Offset guided Footprint Extractor, as well as new tasks and training strategies for different types of samples. Experimental results on several public and new datasets demonstrate that our proposed MLS-BRN achieves competitive performance using much fewer 3D-annotated samples, and significantly improves the footprint extraction and 3D reconstruction performance compared with current state-of-the-art. The code and datasets of this work will be released at https://github.com/opendatalab/MLS-BRN.git.",
        "page": "http://arxiv.org/abs/2404.04823",
        "pdf": "http://arxiv.org/pdf/2404.04823.pdf"
    },
    {
        "title": "Improving Physics-Augmented Continuum Neural Radiance Field-Based Geometry-Agnostic System Identification with Lagrangian Particle Optimization",
        "author": "Takuhiro Kaneko",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning the 3D Fauna of the Web",
        "author": "Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, Jiajun Wu",
        "abstract": "Learning 3D models of all animals on the Earth requires massively scaling up existing solutions. With this ultimate goal in mind, we develop 3D-Fauna, an approach that learns a pan-category deformable 3D animal model for more than 100 animal species jointly. One crucial bottleneck of modeling animals is the limited availability of training data, which we overcome by simply learning from 2D Internet images. We show that prior category-specific attempts fail to generalize to rare species with limited training images. We address this challenge by introducing the Semantic Bank of Skinned Models (SBSM), which automatically discovers a small set of base animal shapes by combining geometric inductive priors with semantic knowledge implicitly captured by an off-the-shelf self-supervised feature extractor. To train such a model, we also contribute a new large-scale dataset of diverse animal species. At inference time, given a single image of any quadruped animal, our model reconstructs an articulated 3D mesh in a feed-forward fashion within seconds.",
        "page": "http://arxiv.org/abs/2401.02400",
        "pdf": "http://arxiv.org/pdf/2401.02400.pdf"
    },
    {
        "title": "Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers",
        "author": "Hongjie Wang, Bhishma Dedhia, Niraj Jha",
        "abstract": "Deployment of Transformer models on edge devices is becoming increasingly challenging due to the exponentially growing inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require computationally expensive fine-tuning, which is undesirable in many edge deployment cases. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. It leverages the attention graph of pre-trained Transformer models to produce an importance distribution for tokens via our proposed Weighted Page Rank (WPR) algorithm. This distribution further guides token partitioning for efficient similarity-based pruning. Due to the elimination of the fine-tuning overhead, Zero-TPrune can prune large models at negligible computational cost, switch between different pruning configurations at no computational cost, and perform hyperparameter tuning efficiently. We evaluate the performance of Zero-TPrune on vision tasks by applying it to various vision Transformer backbones and testing them on ImageNet. Without any fine-tuning, Zero-TPrune reduces the FLOPs cost of DeiT-S by 34.7% and improves its throughput by 45.3% with only 0.4% accuracy loss. Compared with state-of-the-art pruning methods that require fine-tuning, Zero-TPrune not only eliminates the need for fine-tuning after pruning but also does so with only 0.1% accuracy loss. Compared with state-of-the-art fine-tuning-free pruning methods, Zero-TPrune reduces accuracy loss by up to 49% with similar FLOPs budgets. Project webpage: https://jha-lab.github.io/zerotprune.",
        "page": "http://arxiv.org/abs/2305.17328",
        "pdf": "http://arxiv.org/pdf/2305.17328.pdf"
    },
    {
        "title": "Towards Robust 3D Object Detection with LiDAR and 4D Radar Fusion in Various Weather Conditions",
        "author": "Yujeong Chae, Hyeonseong Kim, Kuk-Jin Yoon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Generalizing to Unseen Domains with Few Labels",
        "author": "Chamuditha Jayanga Galappaththige, Sanoojan Baliah, Malitha Gunawardhana, Muhammad Haris Khan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Snapshot Lidar: Fourier embedding of amplitude and phase for single-image depth reconstruction",
        "author": "Sarah Friday, Yunzi Shi, Yaswanth Kumar Cherivirala, Vishwanath Saragadam, Adithya Pediredla",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework",
        "author": "Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-yee Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GLID: Pre-training a Generalist Encoder-Decoder Vision Model",
        "author": "Jihao Liu, Jinliang Zheng, Yu Liu, Hongsheng Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors",
        "author": "He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, Xun Cao",
        "abstract": "Foot contact is an important cue for human motion capture, understanding, and generation. Existing datasets tend to annotate dense foot contact using visual matching with thresholding or incorporating pressure signals. However, these approaches either suffer from low accuracy or are only designed for small-range and slow motion. There is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/.",
        "page": "http://arxiv.org/abs/2403.17610",
        "pdf": "http://arxiv.org/pdf/2403.17610.pdf"
    },
    {
        "title": "Control4D: Efficient 4D Portrait  Editing with Text",
        "author": "Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao ZHOU, Hongwen Zhang, Yebin Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion",
        "author": "Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, Hongdong Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis",
        "author": "Zhan Li, Zhang Chen, Zhong Li, Yi Xu",
        "abstract": "Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem. Despite recent advancements, simultaneously achieving high-resolution photorealistic results, real-time rendering, and compact storage remains a formidable task. To address these challenges, we propose Spacetime Gaussian Feature Splatting as a novel dynamic scene representation, composed of three pivotal components. First, we formulate expressive Spacetime Gaussians by enhancing 3D Gaussians with temporal opacity and parametric motion/rotation. This enables Spacetime Gaussians to capture static, dynamic, as well as transient content within a scene. Second, we introduce splatted feature rendering, which replaces spherical harmonics with neural features. These features facilitate the modeling of view- and time-dependent appearance while maintaining small size. Third, we leverage the guidance of training error and coarse depth to sample new Gaussians in areas that are challenging to converge with existing pipelines. Experiments on several established real-world datasets demonstrate that our method achieves state-of-the-art rendering quality and speed, while retaining compact storage. At 8K resolution, our lite-version model can render at 60 FPS on an Nvidia RTX 4090 GPU. Our code is available at https://github.com/oppo-us-research/SpacetimeGaussians.",
        "page": "http://arxiv.org/abs/2312.16812",
        "pdf": "http://arxiv.org/pdf/2312.16812.pdf"
    },
    {
        "title": "LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning",
        "author": "Siyuan Cheng, Guanhong Tao, Yingqi Liu, Guangyu Shen, Shengwei An, Shiwei Feng, Xiangzhe Xu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang",
        "abstract": "Backdoor attack poses a significant security threat to Deep Learning applications. Existing attacks are often not evasive to established backdoor detection techniques. This susceptibility primarily stems from the fact that these attacks typically leverage a universal trigger pattern or transformation function, such that the trigger can cause misclassification for any input. In response to this, recent papers have introduced attacks using sample-specific invisible triggers crafted through special transformation functions. While these approaches manage to evade detection to some extent, they reveal vulnerability to existing backdoor mitigation techniques. To address and enhance both evasiveness and resilience, we introduce a novel backdoor attack LOTUS. Specifically, it leverages a secret function to separate samples in the victim class into a set of partitions and applies unique triggers to different partitions. Furthermore, LOTUS incorporates an effective trigger focusing mechanism, ensuring only the trigger corresponding to the partition can induce the backdoor behavior. Extensive experimental results show that LOTUS can achieve high attack success rate across 4 datasets and 7 model structures, and effectively evading 13 backdoor detection and mitigation techniques. The code is available at https://github.com/Megum1/LOTUS.",
        "page": "http://arxiv.org/abs/2403.17188",
        "pdf": "http://arxiv.org/pdf/2403.17188.pdf"
    },
    {
        "title": "Long-Tailed Anomaly Detection with Learnable Class Names",
        "author": "Chih-Hui Ho, Kuan-Chuan Peng, Nuno Vasconcelos",
        "abstract": "Anomaly detection (AD) aims to identify defective images and localize their defects (if any). Ideally, AD models should be able to detect defects over many image classes; without relying on hard-coded class names that can be uninformative or inconsistent across datasets; learn without anomaly supervision; and be robust to the long-tailed distributions of real-world applications. To address these challenges, we formulate the problem of long-tailed AD by introducing several datasets with different levels of class imbalance and metrics for performance evaluation. We then propose a novel method, LTAD, to detect defects from multiple and long-tailed classes, without relying on dataset class names. LTAD combines AD by reconstruction and semantic AD modules. AD by reconstruction is implemented with a transformer-based reconstruction module. Semantic AD is implemented with a binary classifier, which relies on learned pseudo class names and a pretrained foundation model. These modules are learned over two phases. Phase 1 learns the pseudo-class names and a variational autoencoder (VAE) for feature synthesis that augments the training data to combat long-tails. Phase 2 then learns the parameters of the reconstruction and classification modules of LTAD. Extensive experiments using the proposed long-tailed datasets show that LTAD substantially outperforms the state-of-the-art methods for most forms of dataset imbalance. The long-tailed dataset split is available at https://zenodo.org/records/10854201 .",
        "page": "http://arxiv.org/abs/2403.20236",
        "pdf": "http://arxiv.org/pdf/2403.20236.pdf"
    },
    {
        "title": "Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors",
        "author": "Haoxuanye Ji, Pengpeng Liang, Erkang Cheng",
        "abstract": "Multi-camera-based 3D object detection has made notable progress in the past several years. However, we observe that there are cases (e.g. faraway regions) in which popular 2D object detectors are more reliable than state-of-the-art 3D detectors. In this paper, to improve the performance of query-based 3D object detectors, we present a novel query generating approach termed QAF2D, which infers 3D query anchors from 2D detection results. A 2D bounding box of an object in an image is lifted to a set of 3D anchors by associating each sampled point within the box with depth, yaw angle, and size candidates. Then, the validity of each 3D anchor is verified by comparing its projection in the image with its corresponding 2D box, and only valid anchors are kept and used to construct queries. The class information of the 2D bounding box associated with each query is also utilized to match the predicted boxes with ground truth for the set-based loss. The image feature extraction backbone is shared between the 3D detector and 2D detector by adding a small number of prompt parameters. We integrate QAF2D into three popular query-based 3D object detectors and carry out comprehensive evaluations on the nuScenes dataset. The largest improvement that QAF2D can bring about on the nuScenes validation subset is $2.3\\%$ NDS and $2.7\\%$ mAP. Code is available at https://github.com/nullmax-vision/QAF2D.",
        "page": "http://arxiv.org/abs/2403.06093",
        "pdf": "http://arxiv.org/pdf/2403.06093.pdf"
    },
    {
        "title": "Estimating Noisy Class Posterior with Part-level Labels for Noisy Label Learning",
        "author": "Rui Zhao, Bin Shi, Jianfei Ruan, Tianze Pan, Bo Dong",
        "abstract": "In noisy label learning, estimating noisy class posteriors plays a fundamental role for developing consistent classifiers, as it forms the basis for estimating clean class posteriors and the transition matrix. Existing methods typically learn noisy class posteriors by training a classification model with noisy labels. However, when labels are incorrect, these models may be misled to overemphasize the feature parts that do not reflect the instance characteristics, resulting in significant errors in estimating noisy class posteriors. To address this issue, this paper proposes to augment the supervised information with part-level labels, encouraging the model to focus on and integrate richer information from various parts. Specifically, our method first partitions features into distinct parts by cropping instances, yielding part-level labels associated with these various parts. Subsequently, we introduce a novel single-to-multiple transition matrix to model the relationship between the noisy and part-level labels, which incorporates part-level labels into a classifier-consistent framework. Utilizing this framework with part-level labels, we can learn the noisy class posteriors more precisely by guiding the model to integrate information from various parts, ultimately improving the classification performance. Our method is theoretically sound, while experiments show that it is empirically effective in synthetic and real-world noisy benchmarks.",
        "page": "http://arxiv.org/abs/2405.05714",
        "pdf": "http://arxiv.org/pdf/2405.05714.pdf"
    },
    {
        "title": "Unmixing Diffusion for Self-Supervised Hyperspectral Image Denoising",
        "author": "Haijin Zeng, Jiezhang Cao, Yongyong Chen, Kai Zhang, Hiep Luong, Wilfried Philips",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ManiFPT: Defining and Analyzing Fingerprints of Generative Models",
        "author": "Hae Jin Song, Mahyar Khayatkhoei, Wael AbdAlmageed",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation",
        "author": "Mingyu Lee, Jongwon Choi",
        "abstract": "We propose a text-guided variational image generation method to address the challenge of getting clean data for anomaly detection in industrial manufacturing. Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image. The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality. Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data. Our approach is validated through generalization tests across four baseline models and three distinct datasets. We present an additional analysis to enhance the effectiveness of anomaly detection models by utilizing the generated images.",
        "page": "http://arxiv.org/abs/2403.06247",
        "pdf": "http://arxiv.org/pdf/2403.06247.pdf"
    },
    {
        "title": "The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes",
        "author": "Myeongseob Ko, Feiyang Kang, Weiyan Shi, Ming Jin, Zhou Yu, Ruoxi Jia",
        "abstract": "Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models. In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches. We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analysis of memorization, mislabeled data detection, and tracing behavior in language models. Our code will be made available at https://github.com/ruoxi-jia-group/Forward-INF.",
        "page": "http://arxiv.org/abs/2402.08922",
        "pdf": "http://arxiv.org/pdf/2402.08922.pdf"
    },
    {
        "title": "Deep Equilibrium Diffusion Restoration with Parallel Sampling",
        "author": "Jiezhang Cao, Yue Shi, Kai Zhang, Yulun Zhang, Radu Timofte, Luc Van Gool",
        "abstract": "Diffusion model-based image restoration (IR) aims to use diffusion models to recover high-quality (HQ) images from degraded images, achieving promising performance. Due to the inherent property of diffusion models, most existing methods need long serial sampling chains to restore HQ images step-by-step, resulting in expensive sampling time and high computation costs. Moreover, such long sampling chains hinder understanding the relationship between inputs and restoration results since it is hard to compute the gradients in the whole chains. In this work, we aim to rethink the diffusion model-based IR models through a different perspective, i.e., a deep equilibrium (DEQ) fixed point system, called DeqIR. Specifically, we derive an analytical solution by modeling the entire sampling chain in these IR models as a joint multivariate fixed point system. Based on the analytical solution, we can conduct parallel sampling and restore HQ images without training. Furthermore, we compute fast gradients via DEQ inversion and found that initialization optimization can boost image quality and control the generation direction. Extensive experiments on benchmarks demonstrate the effectiveness of our method on typical IR tasks and real-world settings.",
        "page": "http://arxiv.org/abs/2311.11600",
        "pdf": "http://arxiv.org/pdf/2311.11600.pdf"
    },
    {
        "title": "Robust Distillation via Untargeted and Targeted Intermediate Adversarial Samples",
        "author": "Junhao Dong, Piotr Koniusz, Junxi Chen, Z. Wang, Yew-Soon Ong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multiway Point Cloud Mosaicking with Diffusion and Global Optimization",
        "author": "Shengze Jin, Iro Armeni, Marc Pollefeys, Daniel Barath",
        "abstract": "We introduce a novel framework for multiway point cloud mosaicking (named Wednesday), designed to co-align sets of partially overlapping point clouds -- typically obtained from 3D scanners or moving RGB-D cameras -- into a unified coordinate system. At the core of our approach is ODIN, a learned pairwise registration algorithm that iteratively identifies overlaps and refines attention scores, employing a diffusion-based process for denoising pairwise correlation matrices to enhance matching accuracy. Further steps include constructing a pose graph from all point clouds, performing rotation averaging, a novel robust algorithm for re-estimating translations optimally in terms of consensus maximization and translation optimization. Finally, the point cloud rotations and positions are optimized jointly by a diffusion-based approach. Tested on four diverse, large-scale datasets, our method achieves state-of-the-art pairwise and multiway registration results by a large margin on all benchmarks. Our code and models are available at https://github.com/jinsz/Multiway-Point-Cloud-Mosaicking-with-Diffusion-and-Global-Optimization.",
        "page": "http://arxiv.org/abs/2404.00429",
        "pdf": "http://arxiv.org/pdf/2404.00429.pdf"
    },
    {
        "title": "IDGuard: Robust, General, Identity-centric POI Proactive Defense Against Face Editing Abuse",
        "author": "Yunshu Dai, Jianwei Fei, Fangjun Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Single-Model and Any-Modality for Video Object Tracking",
        "author": "Zongwei Wu, Jilai Zheng, Xiangxuan Ren, Florin-Alexandru Vasluianu, Chao Ma, Danda Paudel, Luc Van Gool, Radu Timofte",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation",
        "author": "Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo",
        "abstract": "Generating vivid and emotional 3D co-speech gestures is crucial for virtual avatar animation in human-machine interaction applications. While the existing methods enable generating the gestures to follow a single emotion label, they overlook that long gesture sequence modeling with emotion transition is more practical in real scenes. In addition, the lack of large-scale available datasets with emotional transition speech and corresponding 3D human gestures also limits the addressing of this task. To fulfill this goal, we first incorporate the ChatGPT-4 and an audio inpainting approach to construct the high-fidelity emotion transition human speeches. Considering obtaining the realistic 3D pose annotations corresponding to the dynamically inpainted emotion transition audio is extremely difficult, we propose a novel weakly supervised training strategy to encourage authority gesture transitions. Specifically, to enhance the coordination of transition gestures w.r.t different emotional ones, we model the temporal association representation between two different emotional gesture sequences as style guidance and infuse it into the transition generation. We further devise an emotion mixture mechanism that provides weak supervision based on a learnable mixed emotion label for transition gestures. Last, we present a keyframe sampler to supply effective initial posture cues in long sequences, enabling us to generate diverse gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models constructed by adapting single emotion-conditioned counterparts on our newly defined emotion transition task and datasets. Our code and dataset will be released on the project page: https://xingqunqi-lab.github.io/Emo-Transition-Gesture/.",
        "page": "http://arxiv.org/abs/2311.17532",
        "pdf": "http://arxiv.org/pdf/2311.17532.pdf"
    },
    {
        "title": "Pose-Transformed Equivariant Network for  3D Point Trajectory Prediction",
        "author": "Ruixuan Yu, Jian Sun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval",
        "author": "Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim",
        "abstract": "There has been significant attention to the research on dense video captioning, which aims to automatically localize and caption all events within untrimmed video. Several studies introduce methods by designing dense video captioning as a multitasking problem of event localization and event captioning to consider inter-task relations. However, addressing both tasks using only visual input is challenging due to the lack of semantic content. In this study, we address this by proposing a novel framework inspired by the cognitive information processing of humans. Our model utilizes external memory to incorporate prior knowledge. The memory retrieval method is proposed with cross-modal video-to-text matching. To effectively incorporate retrieved text features, the versatile encoder and the decoder with visual and textual cross-attention modules are designed. Comparative experiments have been conducted to show the effectiveness of the proposed method on ActivityNet Captions and YouCook2 datasets. Experimental results show promising performance of our model without extensive pretraining from a large video dataset.",
        "page": "http://arxiv.org/abs/2404.07610",
        "pdf": "http://arxiv.org/pdf/2404.07610.pdf"
    },
    {
        "title": "Object Dynamics Modeling with Hierarchical Point Cloud-based Representations",
        "author": "Chanho Kim, Li Fuxin",
        "abstract": "Modeling object dynamics with a neural network is an important problem with numerous applications. Most recent work has been based on graph neural networks. However, physics happens in 3D space, where geometric information potentially plays an important role in modeling physical phenomena. In this work, we propose a novel U-net architecture based on continuous point convolution which naturally embeds information from 3D coordinates and allows for multi-scale feature representations with established downsampling and upsampling procedures. Bottleneck layers in the downsampled point clouds lead to better long-range interaction modeling. Besides, the flexibility of point convolutions allows our approach to generalize to sparsely sampled points from mesh vertices and dynamically generate features on important interaction points on mesh faces. Experimental results demonstrate that our approach significantly improves the state-of-the-art, especially in scenarios that require accurate gravity or collision reasoning.",
        "page": "http://arxiv.org/abs/2404.06044",
        "pdf": "http://arxiv.org/pdf/2404.06044.pdf"
    },
    {
        "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
        "author": "Jiantong Jiang, Zeyi Wen, Atif Mansoor, Ajmal Mian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MESA: Matching Everything by Segmenting Anything",
        "author": "Yesheng Zhang, Xu Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications",
        "author": "Yuwen Xiong, Zhiqi Li, Yuntao Chen, Feng Wang, Xizhou Zhu, Jiapeng Luo, Wenhai Wang, Tong Lu, Hongsheng Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai",
        "abstract": "We introduce Deformable Convolution v4 (DCNv4), a highly efficient and effective operator designed for a broad spectrum of vision applications. DCNv4 addresses the limitations of its predecessor, DCNv3, with two key enhancements: 1. removing softmax normalization in spatial aggregation to enhance its dynamic property and expressive power and 2. optimizing memory access to minimize redundant operations for speedup. These improvements result in a significantly faster convergence compared to DCNv3 and a substantial increase in processing speed, with DCNv4 achieving more than three times the forward speed. DCNv4 demonstrates exceptional performance across various tasks, including image classification, instance and semantic segmentation, and notably, image generation. When integrated into generative models like U-Net in the latent diffusion model, DCNv4 outperforms its baseline, underscoring its possibility to enhance generative models. In practical applications, replacing DCNv3 with DCNv4 in the InternImage model to create FlashInternImage results in up to 80% speed increase and further performance improvement without further modifications. The advancements in speed and efficiency of DCNv4, combined with its robust performance across diverse vision tasks, show its potential as a foundational building block for future vision models.",
        "page": "http://arxiv.org/abs/2401.06197",
        "pdf": "http://arxiv.org/pdf/2401.06197.pdf"
    },
    {
        "title": "What, when, and where? -- Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions",
        "author": "Brian Chen, Nina Shvetsova, Andrew Rouditchenko, Daniel Kondermann, Samuel Thomas, Shih-Fu Chang, Rogerio Feris, James Glass, Hilde Kuehne",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Accurate Spatial Gene Expression Prediction by Integrating Multi-Resolution Features",
        "author": "Youngmin Chung, Ji Hun Ha, Kyeong Chan Im, Joo Sang Lee",
        "abstract": "Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression analysis within tissue contexts. However, the high costs and methodological limitations of ST necessitate a more robust predictive model. In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization. By integrating these features through an effective fusion strategy, TRIPLEX achieves accurate gene expression prediction. Our comprehensive benchmark study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC). The model's predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX's potential in advancing cancer diagnosis and treatment.",
        "page": "http://arxiv.org/abs/2403.07592",
        "pdf": "http://arxiv.org/pdf/2403.07592.pdf"
    },
    {
        "title": "2S-UDF: A Novel Two-stage UDF Learning Method for Robust Non-watertight Model Reconstruction from Multi-view Images",
        "author": "Junkai Deng, Fei Hou, Xuhui Chen, Wencheng Wang, Ying He",
        "abstract": "Recently, building on the foundation of neural radiance field, various techniques have emerged to learn unsigned distance fields (UDF) to reconstruct 3D non-watertight models from multi-view images. Yet, a central challenge in UDF-based volume rendering is formulating a proper way to convert unsigned distance values into volume density, ensuring that the resulting weight function remains unbiased and sensitive to occlusions. Falling short on these requirements often results in incorrect topology or large reconstruction errors in resulting models. This paper addresses this challenge by presenting a novel two-stage algorithm, 2S-UDF, for learning a high-quality UDF from multi-view images. Initially, the method applies an easily trainable density function that, while slightly biased and transparent, aids in coarse reconstruction. The subsequent stage then refines the geometry and appearance of the object to achieve a high-quality reconstruction by directly adjusting the weight function used in volume rendering to ensure that it is unbiased and occlusion-aware. Decoupling density and weight in two stages makes our training stable and robust, distinguishing our technique from existing UDF learning approaches. Evaluations on the DeepFashion3D, DTU, and BlendedMVS datasets validate the robustness and effectiveness of our proposed approach. In both quantitative metrics and visual quality, the results indicate our superior performance over other UDF learning techniques in reconstructing 3D non-watertight models from multi-view images. Our code is available at https://bitbucket.org/jkdeng/2sudf/.",
        "page": "http://arxiv.org/abs/2303.15368",
        "pdf": "http://arxiv.org/pdf/2303.15368.pdf"
    },
    {
        "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback",
        "author": "Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun",
        "abstract": "Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.8%, outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs, and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code, model, and data at https://github.com/RLHF-V/RLHF-V.",
        "page": "http://arxiv.org/abs/2312.00849",
        "pdf": "http://arxiv.org/pdf/2312.00849.pdf"
    },
    {
        "title": "General Point Model Pretraining with Autoencoding and Autoregressive",
        "author": "Zhe Li, Zhangyang Gao, Cheng Tan, Bocheng Ren, Laurence Yang, Stan Z. Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Instance-Aware Correspondences for Robust Multi-Instance Point Cloud Registration in Cluttered Scenes",
        "author": "Zhiyuan Yu, Zheng Qin, lintao zheng, Kai Xu",
        "abstract": "Multi-instance point cloud registration estimates the poses of multiple instances of a model point cloud in a scene point cloud. Extracting accurate point correspondence is to the center of the problem. Existing approaches usually treat the scene point cloud as a whole, overlooking the separation of instances. Therefore, point features could be easily polluted by other points from the background or different instances, leading to inaccurate correspondences oblivious to separate instances, especially in cluttered scenes. In this work, we propose MIRETR, Multi-Instance REgistration TRansformer, a coarse-to-fine approach to the extraction of instance-aware correspondences. At the coarse level, it jointly learns instance-aware superpoint features and predicts per-instance masks. With instance masks, the influence from outside of the instance being concerned is minimized, such that highly reliable superpoint correspondences can be extracted. The superpoint correspondences are then extended to instance candidates at the fine level according to the instance masks. At last, an efficient candidate selection and refinement algorithm is devised to obtain the final registrations. Extensive experiments on three public benchmarks demonstrate the efficacy of our approach. In particular, MIRETR outperforms the state of the arts by 16.6 points on F1 score on the challenging ROBI benchmark. Code and models are available at https://github.com/zhiyuanYU134/MIRETR.",
        "page": "http://arxiv.org/abs/2404.04557",
        "pdf": "http://arxiv.org/pdf/2404.04557.pdf"
    },
    {
        "title": "GaussianAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh",
        "author": "Jing Wen, Xiaoming Zhao, Jason Ren, Alexander G. Schwing, Shenlong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities",
        "author": "Mingcheng Li, Dingkang Yang, Xiao Zhao, Shuaibing Wang, Yan Wang, Kun Yang, Mingyang Sun, Dongliang Kou, Qian, Lihua Zhang",
        "abstract": "Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data. Most MSA efforts are based on the assumption of modality completeness. However, in real-world applications, some practical factors cause uncertain modality missingness, which drastically degrades the model's performance. To this end, we propose a Correlation-decoupled Knowledge Distillation (CorrKD) framework for the MSA task under uncertain missing modalities. Specifically, we present a sample-level contrastive distillation mechanism that transfers comprehensive knowledge containing cross-sample correlations to reconstruct missing semantics. Moreover, a category-guided prototype distillation mechanism is introduced to capture cross-category correlations using category prototypes to align feature distributions and generate favorable joint representations. Eventually, we design a response-disentangled consistency distillation strategy to optimize the sentiment decision boundaries of the student network through response disentanglement and mutual information maximization. Comprehensive experiments on three datasets indicate that our framework can achieve favorable improvements compared with several baselines.",
        "page": "http://arxiv.org/abs/2404.16456",
        "pdf": "http://arxiv.org/pdf/2404.16456.pdf"
    },
    {
        "title": "ES$^3$: Evolving Self-Supervised Learning of Robust Audio-Visual Speech Representations",
        "author": "Yuanhang Zhang, Shuang Yang, Shiguang Shan, Xilin Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MSU-4S - The Michigan State University Four Seasons Dataset",
        "author": "Daniel Kent, Mohammed Alyaqoub, Xiaohu Lu, Sayed Khatounabadi, Kookjin Sung, Cole Scheller, Alexander Dalat, Xinwei Guo, Asma Bin Thabit, Roberto Muntaner Whitley, Hayder Radha",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Progressive Multi-Frequency Representation for Image Warping",
        "author": "Jun Xiao, Zihang Lyu, Cong Zhang, Yakun Ju, Changjian Shui, Kin-man Lam",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control",
        "author": "Jaskirat Singh, Jianming Zhang, Qing Liu, Cameron Smith, Zhe Lin, Liang Zheng",
        "abstract": "The field of generative image inpainting and object insertion has made significant progress with the recent advent of latent diffusion models. Utilizing a precise object mask can greatly enhance these applications. However, due to the challenges users encounter in creating high-fidelity masks, there is a tendency for these methods to rely on more coarse masks (e.g., bounding box) for these applications. This results in limited control and compromised background content preservation. To overcome these limitations, we introduce SmartMask, which allows any novice user to create detailed masks for precise object insertion. Combined with a ControlNet-Inpaint model, our experiments demonstrate that SmartMask achieves superior object insertion quality, preserving the background content more effectively than previous methods. Notably, unlike prior works the proposed approach can also be used even without user-mask guidance, which allows it to perform mask-free object insertion at diverse positions and scales. Furthermore, we find that when used iteratively with a novel instruction-tuning based planning model, SmartMask can be used to design detailed layouts from scratch. As compared with user-scribble based layout design, we observe that SmartMask allows for better quality outputs with layout-to-image generation methods. Project page is available at https://smartmask-gen.github.io",
        "page": "http://arxiv.org/abs/2312.05039",
        "pdf": "http://arxiv.org/pdf/2312.05039.pdf"
    },
    {
        "title": "Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology",
        "author": "Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Vasudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, Dominique Beaini, Maciej Sypetkowski, Chi Cheng, Kristen Morse, Maureen Makes, Ben Mabey, Berton Earnshaw",
        "abstract": "Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images. This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets. Our results show that ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5% relative improvement when recalling known biological relationships curated from public databases. Additionally, we develop a new channel-agnostic MAE architecture (CA-MAE) that allows for inputting images of different numbers and orders of channels at inference time. We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP) generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M). Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond.",
        "page": "http://arxiv.org/abs/2404.10242",
        "pdf": "http://arxiv.org/pdf/2404.10242.pdf"
    },
    {
        "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild",
        "author": "Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, Chao Dong",
        "abstract": "We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts.",
        "page": "http://arxiv.org/abs/2401.13627",
        "pdf": "http://arxiv.org/pdf/2401.13627.pdf"
    },
    {
        "title": "VOODOO 3D: VOlumetric pOrtrait Disentanglement fOr Online 3D head reenactment",
        "author": "Phong Tran, Egor Zakharov, Long Nhat Ho, Anh Tran, Liwen Hu, Hao Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning",
        "author": "Christopher Liao, Theodoros Tsiligkaridis, Brian Kulis",
        "abstract": "Over the past year, a large body of multimodal research has emerged around zero-shot evaluation using GPT descriptors. These studies boost the zero-shot accuracy of pretrained VL models with an ensemble of label-specific text generated by GPT. A recent study, WaffleCLIP, demonstrated that similar zero-shot accuracy can be achieved with an ensemble of random descriptors. However, both zero-shot methods are un-trainable and consequently sub-optimal when some few-shot out-of-distribution (OOD) training data is available. Inspired by these prior works, we present two more flexible methods called descriptor and word soups, which do not require an LLM at test time and can leverage training data to increase OOD target accuracy. Descriptor soup greedily selects a small set of textual descriptors using generic few-shot training data, then calculates robust class embeddings using the selected descriptors. Word soup greedily assembles a chain of words in a similar manner. Compared to existing few-shot soft prompt tuning methods, word soup requires fewer parameters by construction and less GPU memory, since it does not require backpropagation. Both soups outperform current published few-shot methods, even when combined with SoTA zero-shot methods, on cross-dataset and domain generalization benchmarks. Compared with SoTA prompt and descriptor ensembling methods, such as ProDA and WaffleCLIP, word soup achieves higher OOD accuracy with fewer ensemble members. Please checkout our code: github.com/Chris210634/word_soups",
        "page": "http://arxiv.org/abs/2311.13612",
        "pdf": "http://arxiv.org/pdf/2311.13612.pdf"
    },
    {
        "title": "Masked and Shuffled Blind Spot Denoising for Real-World Images",
        "author": "Hamadi Chihaoui, Paolo Favaro",
        "abstract": "We introduce a novel approach to single image denoising based on the Blind Spot Denoising principle, which we call MAsked and SHuffled Blind Spot Denoising (MASH). We focus on the case of correlated noise, which often plagues real images. MASH is the result of a careful analysis to determine the relationships between the level of blindness (masking) of the input and the (unknown) noise correlation. Moreover, we introduce a shuffling technique to weaken the local correlation of noise, which in turn yields an additional denoising performance improvement. We evaluate MASH via extensive experiments on real-world noisy image datasets. We demonstrate on par or better results compared to existing self-supervised denoising methods.",
        "page": "http://arxiv.org/abs/2404.09389",
        "pdf": "http://arxiv.org/pdf/2404.09389.pdf"
    },
    {
        "title": "ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing",
        "author": "Jun-Kun Chen, Samuel Rota Bul\u00f2, Norman M\u00fcller, Lorenzo Porzi, Peter Kontschieder, Yu-Xiong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks",
        "author": "Yuhao Liu, Zhanghan Ke, Fang Liu, Nanxuan Zhao, Rynson W.H. Lau",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation",
        "author": "Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, Gordon Wetzstein",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions",
        "author": "Zhen Liu, Hao Zhu, Qi Zhang, Jingde Fu, Weibing Deng, Zhan Ma, Yanwen Guo, Xun Cao",
        "abstract": "Implicit Neural Representation (INR), which utilizes a neural network to map coordinate inputs to corresponding attributes, is causing a revolution in the field of signal processing. However, current INR techniques suffer from a restricted capability to tune their supported frequency set, resulting in imperfect performance when representing complex signals with multiple frequencies. We have identified that this frequency-related problem can be greatly alleviated by introducing variable-periodic activation functions, for which we propose FINER. By initializing the bias of the neural network within different ranges, sub-functions with various frequencies in the variable-periodic function are selected for activation. Consequently, the supported frequency set of FINER can be flexibly tuned, leading to improved performance in signal representation. We demonstrate the capabilities of FINER in the contexts of 2D image fitting, 3D signed distance field representation, and 5D neural radiance fields optimization, and we show that it outperforms existing INRs.",
        "page": "http://arxiv.org/abs/2312.02434",
        "pdf": "http://arxiv.org/pdf/2312.02434.pdf"
    },
    {
        "title": "VidToMe: Video Token Merging for Zero-Shot Video Editing",
        "author": "Xirui Li, Chao Ma, Xiaokang Yang, Ming-Hsuan Yang",
        "abstract": "Diffusion models have made significant advances in generating high-quality images, but their application to video generation has remained challenging due to the complexity of temporal motion. Zero-shot video editing offers a solution by utilizing pre-trained image diffusion models to translate source videos into new ones. Nevertheless, existing methods struggle to maintain strict temporal consistency and efficient memory consumption. In this work, we propose a novel approach to enhance temporal consistency in generated videos by merging self-attention tokens across frames. By aligning and compressing temporally redundant tokens across frames, our method improves temporal coherence and reduces memory consumption in self-attention computations. The merging strategy matches and aligns tokens according to the temporal correspondence between frames, facilitating natural temporal consistency in generated video frames. To manage the complexity of video processing, we divide videos into chunks and develop intra-chunk local token merging and inter-chunk global token merging, ensuring both short-term video continuity and long-term content consistency. Our video editing approach seamlessly extends the advancements in image editing to video editing, rendering favorable results in temporal consistency over state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2312.10656",
        "pdf": "http://arxiv.org/pdf/2312.10656.pdf"
    },
    {
        "title": "Text-image Alignment for Diffusion-based Perception",
        "author": "Neehar Kondapaneni, Markus Marks, Manuel Knott, Rog\u00e9rio Guimar\u00e3es, Pietro Perona",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization",
        "author": "Jimyeong Kim, Jungwon Park, Wonjong Rhee",
        "abstract": "In text-to-image personalization, a timely and crucial challenge is the tendency of generated images overfitting to the biases present in the reference images. We initiate our study with a comprehensive categorization of the biases into background, nearby-object, tied-object, substance (in style re-contextualization), and pose biases. These biases manifest in the generated images due to their entanglement into the subject embedding. This undesired embedding entanglement not only results in the reflection of biases from the reference images into the generated images but also notably diminishes the alignment of the generated images with the given generation prompt. To address this challenge, we propose SID~(Selectively Informative Description), a text description strategy that deviates from the prevalent approach of only characterizing the subject's class identification. SID is generated utilizing multimodal GPT-4 and can be seamlessly integrated into optimization-based models. We present comprehensive experimental results along with analyses of cross-attention maps, subject-alignment, non-subject-disentanglement, and text-alignment.",
        "page": "http://arxiv.org/abs/2403.15330",
        "pdf": "http://arxiv.org/pdf/2403.15330.pdf"
    },
    {
        "title": "Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners",
        "author": "Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen",
        "abstract": "Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/",
        "page": "http://arxiv.org/abs/2402.17723",
        "pdf": "http://arxiv.org/pdf/2402.17723.pdf"
    },
    {
        "title": "Generating Handwritten Mathematical Expressions From Symbol Graphs: An End-to-End Pipeline",
        "author": "Yu chen, Fei Gao, YanguangZhang, Maoying Qiao, Nannan Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks",
        "author": "Yaxu Xie, Alain Pagani, Didier Stricker",
        "abstract": "Scene graphs have been recently introduced into 3D spatial understanding as a comprehensive representation of the scene. The alignment between 3D scene graphs is the first step of many downstream tasks such as scene graph aided point cloud registration, mosaicking, overlap checking, and robot navigation. In this work, we treat 3D scene graph alignment as a partial graph-matching problem and propose to solve it with a graph neural network. We reuse the geometric features learned by a point cloud registration method and associate the clustered point-level geometric features with the node-level semantic feature via our designed feature fusion module. Partial matching is enabled by using a learnable method to select the top-k similar node pairs. Subsequent downstream tasks such as point cloud registration are achieved by running a pre-trained registration network within the matched regions. We further propose a point-matching rescoring method, that uses the node-wise alignment of the 3D scene graph to reweight the matching candidates from a pre-trained point cloud registration method. It reduces the false point correspondences estimated especially in low-overlapping cases. Experiments show that our method improves the alignment accuracy by 10~20% in low-overlap and random transformation scenarios and outperforms the existing work in multiple downstream tasks.",
        "page": "http://arxiv.org/abs/2403.19474",
        "pdf": "http://arxiv.org/pdf/2403.19474.pdf"
    },
    {
        "title": "When Visual Grounding Meets Gigapixel-level Large-scale Scenes: Benchmark and Approach",
        "author": "TAO MA, Bing Bai, Haozhe Lin, Heyuan Wang, Yu Wang, Lin Luo, Lu Fang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Mitigating Motion Blur in Neural Radiance Fields with Events and Frames",
        "author": "Marco Cannici, Davide Scaramuzza",
        "abstract": "Neural Radiance Fields (NeRFs) have shown great potential in novel view synthesis. However, they struggle to render sharp images when the data used for training is affected by motion blur. On the other hand, event cameras excel in dynamic scenes as they measure brightness changes with microsecond resolution and are thus only marginally affected by blur. Recent methods attempt to enhance NeRF reconstructions under camera motion by fusing frames and events. However, they face challenges in recovering accurate color content or constrain the NeRF to a set of predefined camera poses, harming reconstruction quality in challenging conditions. This paper proposes a novel formulation addressing these issues by leveraging both model- and learning-based modules. We explicitly model the blur formation process, exploiting the event double integral as an additional model-based prior. Additionally, we model the event-pixel response using an end-to-end learnable response function, allowing our method to adapt to non-idealities in the real event-camera sensor. We show, on synthetic and real data, that the proposed approach outperforms existing deblur NeRFs that use only frames as well as those that combine frames and events by +6.13dB and +2.48dB, respectively.",
        "page": "http://arxiv.org/abs/2403.19780",
        "pdf": "http://arxiv.org/pdf/2403.19780.pdf"
    },
    {
        "title": "DemoFusion: Democratising High-Resolution Image Generation With No $$$",
        "author": "Ruoyi DU, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design",
        "author": "Seokju Yun, Youngmin Ro",
        "abstract": "Recently, efficient Vision Transformers have shown great performance with low latency on resource-constrained devices. Conventionally, they use 4x4 patch embeddings and a 4-stage structure at the macro level, while utilizing sophisticated attention with multi-head configuration at the micro level. This paper aims to address computational redundancy at all design levels in a memory-efficient manner. We discover that using larger-stride patchify stem not only reduces memory access costs but also achieves competitive performance by leveraging token representations with reduced spatial redundancy from the early stages. Furthermore, our preliminary analyses suggest that attention layers in the early stages can be substituted with convolutions, and several attention heads in the latter stages are computationally redundant. To handle this, we introduce a single-head attention module that inherently prevents head redundancy and simultaneously boosts accuracy by parallelly combining global and local information. Building upon our solutions, we introduce SHViT, a Single-Head Vision Transformer that obtains the state-of-the-art speed-accuracy tradeoff. For example, on ImageNet-1k, our SHViT-S4 is 3.3x, 8.1x, and 2.4x faster than MobileViTv2 x1.0 on GPU, CPU, and iPhone12 mobile device, respectively, while being 1.3% more accurate. For object detection and instance segmentation on MS COCO using Mask-RCNN head, our model achieves performance comparable to FastViT-SA12 while exhibiting 3.8x and 2.0x lower backbone latency on GPU and mobile device, respectively.",
        "page": "http://arxiv.org/abs/2401.16456",
        "pdf": "http://arxiv.org/pdf/2401.16456.pdf"
    },
    {
        "title": "Makeup Prior Models for 3D Facial Makeup Estimation and Applications",
        "author": "Xingchao Yang, Takafumi Taketomi, Yuki Endo, Yoshihiro Kanamori",
        "abstract": "In this work, we introduce two types of makeup prior models to extend existing 3D face prior models: PCA-based and StyleGAN2-based priors. The PCA-based prior model is a linear model that is easy to construct and is computationally efficient. However, it retains only low-frequency information. Conversely, the StyleGAN2-based model can represent high-frequency information with relatively higher computational cost than the PCA-based model. Although there is a trade-off between the two models, both are applicable to 3D facial makeup estimation and related applications. By leveraging makeup prior models and designing a makeup consistency module, we effectively address the challenges that previous methods faced in robustly estimating makeup, particularly in the context of handling self-occluded faces. In experiments, we demonstrate that our approach reduces computational costs by several orders of magnitude, achieving speeds up to 180 times faster. In addition, by improving the accuracy of the estimated makeup, we confirm that our methods are highly advantageous for various 3D facial makeup applications such as 3D makeup face reconstruction, user-friendly makeup editing, makeup transfer, and interpolation.",
        "page": "http://arxiv.org/abs/2403.17761",
        "pdf": "http://arxiv.org/pdf/2403.17761.pdf"
    },
    {
        "title": "Holoported Characters: Real-time Free-viewpoint Rendering of Humans from Sparse RGB Cameras",
        "author": "Ashwath Shetty, Marc Habermann, Guoxing Sun, Diogo Luvizon, Vladislav Golyanik, Christian Theobalt",
        "abstract": "We present the first approach to render highly realistic free-viewpoint videos of a human actor in general apparel, from sparse multi-view recording to display, in real-time at an unprecedented 4K resolution. At inference, our method only requires four camera views of the moving actor and the respective 3D skeletal pose. It handles actors in wide clothing, and reproduces even fine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and hand gestures. At training time, our learning-based approach expects dense multi-view video and a rigged static surface scan of the actor. Our method comprises three main stages. Stage 1 is a skeleton-driven neural approach for high-quality capture of the detailed dynamic mesh geometry. Stage 2 is a novel solution to create a view-dependent texture using four test-time camera views as input. Finally, stage 3 comprises a new image-based refinement network rendering the final 4K image given the output from the previous stages. Our approach establishes a new benchmark for real-time rendering resolution and quality using sparse input camera views, unlocking possibilities for immersive telepresence.",
        "page": "http://arxiv.org/abs/2312.07423",
        "pdf": "http://arxiv.org/pdf/2312.07423.pdf"
    },
    {
        "title": "Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle",
        "author": "Youtian Lin, Zuozhuo Dai, Siyu Zhu, Yao Yao",
        "abstract": "We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos. In contrast to the prevalent NeRF-based approaches hampered by slow training and rendering speeds, our approach harnesses recent advancements in point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain Deformation Model (DDDM) is proposed to explicitly model attribute deformations of each Gaussian point, where the time-dependent residual of each attribute is captured by a polynomial fitting in the time domain, and a Fourier series fitting in the frequency domain. The proposed DDDM is capable of modeling complex scene deformations across long video footage, eliminating the need for training separate 3DGS for each frame or introducing an additional implicit neural field to model 3D dynamics. Moreover, the explicit deformation modeling for discretized Gaussian points ensures ultra-fast training and rendering of a 4D scene, which is comparable to the original 3DGS designed for static 3D reconstruction. Our proposed approach showcases a substantial efficiency improvement, achieving a $5\\times$ faster training speed compared to the per-frame 3DGS modeling. In addition, quantitative results demonstrate that the proposed Gaussian-Flow significantly outperforms previous leading methods in novel view rendering quality. Project page: https://nju-3dv.github.io/projects/Gaussian-Flow",
        "page": "http://arxiv.org/abs/2312.03431",
        "pdf": "http://arxiv.org/pdf/2312.03431.pdf"
    },
    {
        "title": "MuRF: Multi-Baseline Radiance Fields",
        "author": "Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, Fisher Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Resource-Efficient Transformer Pruning for Finetuning of Large Models",
        "author": "Fatih Ilhan, Gong Su, Selim Tekin, Tiansheng Huang, Sihao Hu, Ling Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Referring Image Editing: Object-level Image Editing via Referring Expressions",
        "author": "Chang Liu, Xiangtai Li, Henghui Ding",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Cloud-Device Collaborative Learning for Multimodal Large Language Models",
        "author": "Guanqun Wang, Jiaming Liu, Chenxuan Li, Yuan Zhang, Ma Junpeng, Xinyu Wei, Kevin Zhang, Maurice Chong, Renrui Zhang, Yijiang Liu, Shanghang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes",
        "author": "Xuying Zhang, Bo-Wen Yin, yuming chen, Zheng Lin, Yunheng Li, Qibin Hou, Ming-Ming Cheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "X-3D: Explicit 3D Structure Modeling for Point Cloud Recognition",
        "author": "Shuofeng Sun, Yongming Rao, Jiwen Lu, Haibin Yan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BiPer: Binary Neural Networks using a Periodic Function",
        "author": "Edwin Vargas, Claudia Correa, Carlos Hinojosa, Henry Arguello",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Putting the Object Back into Video Object Segmentation",
        "author": "Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, Alexander G. Schwing",
        "abstract": "We present Cutie, a video object segmentation (VOS) network with object-level memory reading, which puts the object representation from memory back into the video object segmentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In contrast, Cutie performs top-down object-level memory reading by adapting a small set of object queries. Via those, it interacts with the bottom-up pixel features iteratively with a query-based object transformer (qt, hence Cutie). The object queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accurate segmentation. Together with foreground-background masked attention, Cutie cleanly separates the semantics of the foreground object from the background. On the challenging MOSE dataset, Cutie improves by 8.7 J&F over XMem with a similar running time and improves by 4.2 J&F over DeAOT while being three times faster. Code is available at: https://hkchengrex.github.io/Cutie",
        "page": "http://arxiv.org/abs/2310.12982",
        "pdf": "http://arxiv.org/pdf/2310.12982.pdf"
    },
    {
        "title": "CoDeF: Content Deformation Fields for Temporally Consistent Video Processing",
        "author": "Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, Yujun Shen",
        "abstract": "We present the content deformation field CoDeF as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis.Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline.We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e.g., the object shape) from the video.With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field.We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training.More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog.Project page can be found at https://qiuyu96.github.io/CoDeF/.",
        "page": "http://arxiv.org/abs/2308.07926",
        "pdf": "http://arxiv.org/pdf/2308.07926.pdf"
    },
    {
        "title": "BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning",
        "author": "Ruyang Liu, Chen Li, Yixiao Ge, Thomas H. Li, Ying Shan, Ge Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards 3D Vision with Low-Cost Single-Photon Cameras",
        "author": "Fangzhou Mu, Carter Sifferman, Sacha Jungerman, Yiquan Li, Zhiyue Han, Michael Gleicher, Mohit Gupta, Yin Li",
        "abstract": "We present a method for reconstructing 3D shape of arbitrary Lambertian objects based on measurements by miniature, energy-efficient, low-cost single-photon cameras. These cameras, operating as time resolved image sensors, illuminate the scene with a very fast pulse of diffuse light and record the shape of that pulse as it returns back from the scene at a high temporal resolution. We propose to model this image formation process, account for its non-idealities, and adapt neural rendering to reconstruct 3D geometry from a set of spatially distributed sensors with known poses. We show that our approach can successfully recover complex 3D shapes from simulated data. We further demonstrate 3D object reconstruction from real-world captures, utilizing measurements from a commodity proximity sensor. Our work draws a connection between image-based modeling and active range scanning and is a step towards 3D vision with single-photon cameras.",
        "page": "http://arxiv.org/abs/2403.17801",
        "pdf": "http://arxiv.org/pdf/2403.17801.pdf"
    },
    {
        "title": "Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models",
        "author": "Kota Sueyoshi, Takashi Matsubara",
        "abstract": "Diffusion models have achieved remarkable results in generating high-quality, diverse, and creative images. However, when it comes to text-based image generation, they often fail to capture the intended meaning presented in the text. For instance, a specified object may not be generated, an unnecessary object may be generated, and an adjective may alter objects it was not intended to modify. Moreover, we found that relationships indicating possession between objects are often overlooked. While users' intentions in text are diverse, existing methods tend to specialize in only some aspects of these. In this paper, we propose Predicated Diffusion, a unified framework to express users' intentions. We consider that the root of the above issues lies in the text encoder, which often focuses only on individual words and neglects the logical relationships between them. The proposed method does not solely rely on the text encoder, but instead, represents the intended meaning in the text as propositions using predicate logic and treats the pixels in the attention maps as the fuzzy predicates. This enables us to obtain a differentiable loss function that makes the image fulfill the proposition by minimizing it. When compared to several existing methods, we demonstrated that Predicated Diffusion can generate images that are more faithful to various text prompts, as verified by human evaluators and pretrained image-text models.",
        "page": "http://arxiv.org/abs/2311.16117",
        "pdf": "http://arxiv.org/pdf/2311.16117.pdf"
    },
    {
        "title": "MoReVQA: Exploring Modular Reasoning Models for Video Question Answering",
        "author": "Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Color Shift Estimation-and-Correction for Image Enhancement",
        "author": "Yiyu Li, Ke Xu, Gerhard Hancke, Rynson W.H. Lau",
        "abstract": "Images captured under sub-optimal illumination conditions may contain both over- and under-exposures. Current approaches mainly focus on adjusting image brightness, which may exacerbate the color tone distortion in under-exposed areas and fail to restore accurate colors in over-exposed regions. We observe that over- and under-exposed regions display opposite color tone distribution shifts with respect to each other, which may not be easily normalized in joint modeling as they usually do not have ``normal-exposed'' regions/pixels as reference. In this paper, we propose a novel method to enhance images with both over- and under-exposures by learning to estimate and correct such color shifts. Specifically, we first derive the color feature maps of the brightened and darkened versions of the input image via a UNet-based network, followed by a pseudo-normal feature generator to produce pseudo-normal color feature maps. We then propose a novel COlor Shift Estimation (COSE) module to estimate the color shifts between the derived brightened (or darkened) color feature maps and the pseudo-normal color feature maps. The COSE module corrects the estimated color shifts of the over- and under-exposed regions separately. We further propose a novel COlor MOdulation (COMO) module to modulate the separately corrected colors in the over- and under-exposed regions to produce the enhanced image. Comprehensive experiments show that our method outperforms existing approaches. Project webpage: https://github.com/yiyulics/CSEC.",
        "page": "http://arxiv.org/abs/2405.17725",
        "pdf": "http://arxiv.org/pdf/2405.17725.pdf"
    },
    {
        "title": "Dexterous Grasp Transformer",
        "author": "Guo-Hao Xu, Yi-Lin Wei, Dian Zheng, Xiao-Ming Wu, Wei-Shi Zheng",
        "abstract": "In this work, we propose a novel discriminative framework for dexterous grasp generation, named Dexterous Grasp TRansformer (DGTR), capable of predicting a diverse set of feasible grasp poses by processing the object point cloud with only one forward pass. We formulate dexterous grasp generation as a set prediction task and design a transformer-based grasping model for it. However, we identify that this set prediction paradigm encounters several optimization challenges in the field of dexterous grasping and results in restricted performance. To address these issues, we propose progressive strategies for both the training and testing phases. First, the dynamic-static matching training (DSMT) strategy is presented to enhance the optimization stability during the training phase. Second, we introduce the adversarial-balanced test-time adaptation (AB-TTA) with a pair of adversarial losses to improve grasping quality during the testing phase. Experimental results on the DexGraspNet dataset demonstrate the capability of DGTR to predict dexterous grasp poses with both high quality and diversity. Notably, while keeping high quality, the diversity of grasp poses predicted by DGTR significantly outperforms previous works in multiple metrics without any data pre-processing. Codes are available at https://github.com/iSEE-Laboratory/DGTR .",
        "page": "http://arxiv.org/abs/2404.18135",
        "pdf": "http://arxiv.org/pdf/2404.18135.pdf"
    },
    {
        "title": "Posterior Distillation Sampling",
        "author": "Juil Koo, Chanho Park, Minhyuk Sung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction",
        "author": "Inhwan Bae, Junoh Lee, Hae-Gon Jeon",
        "abstract": "Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language model in understanding and reasoning high-level knowledge, such as scene context and social relationships between pedestrians, we introduce an auxiliary multi-task question and answering. We then train a numerical tokenizer with the prompt data. We encourage the tokenizer to separate the integer and decimal parts well, and leverage it to capture correlations between the consecutive numbers in the language model. Lastly, we train the language model using the numerical tokenizer and all of the question-answer prompts. Here, we propose a beam-search-based most-likely prediction and a temperature-based multimodal prediction to implement both deterministic and stochastic inferences. Applying our LMTraj, we show that the language-based model can be a powerful pedestrian trajectory predictor, and outperforms existing numerical-based predictor methods. Code is publicly available at https://github.com/inhwanbae/LMTrajectory .",
        "page": "http://arxiv.org/abs/2403.18447",
        "pdf": "http://arxiv.org/pdf/2403.18447.pdf"
    },
    {
        "title": "C3Net: Compound Conditioned ControlNet for Multimodal Content Generation",
        "author": "Juntao Zhang, Yuehuai LIU, Yu-Wing Tai, Chi-Keung Tang",
        "abstract": "We present Compound Conditioned ControlNet, C3Net, a novel generative neural architecture taking conditions from multiple modalities and synthesizing multimodal contents simultaneously (e.g., image, text, audio). C3Net adapts the ControlNet architecture to jointly train and make inferences on a production-ready diffusion model and its trainable copies. Specifically, C3Net first aligns the conditions from multi-modalities to the same semantic latent space using modality-specific encoders based on contrastive training. Then, it generates multimodal outputs based on the aligned latent space, whose semantic information is combined using a ControlNet-like architecture called Control C3-UNet. Correspondingly, with this system design, our model offers an improved solution for joint-modality generation through learning and explaining multimodal conditions instead of simply taking linear interpolations on the latent space. Meanwhile, as we align conditions to a unified latent space, C3Net only requires one trainable Control C3-UNet to work on multimodal semantic information. Furthermore, our model employs unimodal pretraining on the condition alignment stage, outperforming the non-pretrained alignment even on relatively scarce training data and thus demonstrating high-quality compound condition generation. We contribute the first high-quality tri-modal validation set to validate quantitatively that C3Net outperforms or is on par with first and contemporary state-of-the-art multimodal generation. Our codes and tri-modal dataset will be released.",
        "page": "http://arxiv.org/abs/2311.17951",
        "pdf": "http://arxiv.org/pdf/2311.17951.pdf"
    },
    {
        "title": "Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation",
        "author": "Jiaming Liu, Ran Xu, Senqiao Yang, Renrui Zhang, Qizhe Zhang, Zehui Chen, Yandong Guo, Shanghang Zhang",
        "abstract": "Continual Test-Time Adaptation (CTTA) is proposed to migrate a source pre-trained model to continually changing target distributions, addressing real-world dynamism. Existing CTTA methods mainly rely on entropy minimization or teacher-student pseudo-labeling schemes for knowledge extraction in unlabeled target domains. However, dynamic data distributions cause miscalibrated predictions and noisy pseudo-labels in existing self-supervised learning methods, hindering the effective mitigation of error accumulation and catastrophic forgetting problems during the continual adaptation process. To tackle these issues, we propose a continual self-supervised method, Adaptive Distribution Masked Autoencoders (ADMA), which enhances the extraction of target domain knowledge while mitigating the accumulation of distribution shifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism to adaptively sample masked positions, followed by establishing consistency constraints between the masked target samples and the original target samples. Additionally, for masked tokens, we utilize an efficient decoder to reconstruct a hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients), leveraging its invariant properties to boost task-relevant representations. Through conducting extensive experiments on four widely recognized benchmarks, our proposed method attains state-of-the-art performance in both classification and segmentation CTTA tasks. Our project page: https://sites.google.com/view/continual-mae/home.",
        "page": "http://arxiv.org/abs/2312.12480",
        "pdf": "http://arxiv.org/pdf/2312.12480.pdf"
    },
    {
        "title": "HOI-M$^3$: Capture Multiple Humans and Objects Interaction within Contextual Environment",
        "author": "Juze Zhang, Jingyan Zhang, Zining Song, Zhanhe Shi, Chengfeng Zhao, Ye Shi, Jingyi Yu, Lan Xu, Jingya Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation",
        "author": "Jinfeng Xu, Siyuan Yang, Xianzhi Li, Yuan Tang, yixue Hao, Long Hu, Min Chen",
        "abstract": "Existing point cloud semantic segmentation networks cannot identify unknown classes and update their knowledge, due to a closed-set and static perspective of the real world, which would induce the intelligent agent to make bad decisions. To address this problem, we propose a Probability-Driven Framework (PDF) for open world semantic segmentation that includes (i) a lightweight U-decoder branch to identify unknown classes by estimating the uncertainties, (ii) a flexible pseudo-labeling scheme to supply geometry features along with probability distribution features of unknown classes by generating pseudo labels, and (iii) an incremental knowledge distillation strategy to incorporate novel classes into the existing knowledge base gradually. Our framework enables the model to behave like human beings, which could recognize unknown objects and incrementally learn them with the corresponding knowledge. Experimental results on the S3DIS and ScanNetv2 datasets demonstrate that the proposed PDF outperforms other methods by a large margin in both important tasks of open world semantic segmentation.",
        "page": "http://arxiv.org/abs/2404.00979",
        "pdf": "http://arxiv.org/pdf/2404.00979.pdf"
    },
    {
        "title": "Higher-order Relational Reasoning for Pedestrian Trajectory Prediction",
        "author": "Sungjune Kim, Hyung-gun Chi, Hyerin Lim, Karthik Ramani, Jinkyu Kim, Sangpil Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Discover and Mitigate Multiple Biased Subgroups in Image Classifiers",
        "author": "Zeliang Zhang, Mingqian Feng, Zhiheng Li, Chenliang Xu",
        "abstract": "Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications. Such subgroups are typically unknown due to the absence of subgroup labels. Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness. Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist. In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers. Our approach decomposes the image features into multiple components that represent multiple subgroups. This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier. We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using vision-language foundation models. Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate the effectiveness of DIM in discovering and mitigating multiple biased subgroups. Furthermore, DIM uncovers the failure modes of the classifier on Hard ImageNet, showcasing its broader applicability to understanding model bias in image classifiers. The code is available at https://github.com/ZhangAIPI/DIM.",
        "page": "http://arxiv.org/abs/2403.12777",
        "pdf": "http://arxiv.org/pdf/2403.12777.pdf"
    },
    {
        "title": "Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection",
        "author": "Jongha Kim, Jihwan Park, Jinyoung Park, Jinyoung Kim, Sehyung Kim, Hyunwoo J. Kim",
        "abstract": "Visual Relationship Detection (VRD) has seen significant advancements with Transformer-based architectures recently. However, we identify two key limitations in a conventional label assignment for training Transformer-based VRD models, which is a process of mapping a ground-truth (GT) to a prediction. Under the conventional assignment, an unspecialized query is trained since a query is expected to detect every relation, which makes it difficult for a query to specialize in specific relations. Furthermore, a query is also insufficiently trained since a GT is assigned only to a single prediction, therefore near-correct or even correct predictions are suppressed by being assigned no relation as a GT. To address these issues, we propose Groupwise Query Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise Query Specialization trains a specialized query by dividing queries and relations into disjoint groups and directing a query in a specific query group solely toward relations in the corresponding relation group. Quality-Aware Multi-Assignment further facilitates the training by assigning a GT to multiple predictions that are significantly close to a GT in terms of a subject, an object, and the relation in between. Experimental results and analyses show that SpeaQ effectively trains specialized queries, which better utilize the capacity of a model, resulting in consistent performance gains with zero additional inference cost across multiple VRD models and benchmarks. Code is available at https://github.com/mlvlab/SpeaQ.",
        "page": "http://arxiv.org/abs/2403.17709",
        "pdf": "http://arxiv.org/pdf/2403.17709.pdf"
    },
    {
        "title": "SeD: Semantic-Aware Discriminator for Image Super-Resolution",
        "author": "Bingchen Li, Xin Li, Hanxin Zhu, YEYING JIN, Ruoyu Feng, Zhizheng Zhang, Zhibo Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Robust Emotion Recognition in Context Debiasing",
        "author": "Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang, Lihua Zhang",
        "abstract": "Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capture the adverse direct effect caused by the context bias. During the inference, we eliminate the direct context effect from the total causal effect by comparing factual and counterfactual outcomes, resulting in bias mitigation and robust prediction. As a model-agnostic framework, CLEF can be readily integrated into existing methods, bringing consistent performance gains.",
        "page": "http://arxiv.org/abs/2403.05963",
        "pdf": "http://arxiv.org/pdf/2403.05963.pdf"
    },
    {
        "title": "Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds",
        "author": "Tianrui Lou, Xiaojun Jia, Jindong Gu, Li Liu, Siyuan Liang, Bangyan He, Xiaochun Cao",
        "abstract": "Adversarial attack methods based on point manipulation for 3D point cloud classification have revealed the fragility of 3D models, yet the adversarial examples they produce are easily perceived or defended against. The trade-off between the imperceptibility and adversarial strength leads most point attack methods to inevitably introduce easily detectable outlier points upon a successful attack. Another promising strategy, shape-based attack, can effectively eliminate outliers, but existing methods often suffer significant reductions in imperceptibility due to irrational deformations. We find that concealing deformation perturbations in areas insensitive to human eyes can achieve a better trade-off between imperceptibility and adversarial strength, specifically in parts of the object surface that are complex and exhibit drastic curvature changes. Therefore, we propose a novel shape-based adversarial attack method, HiT-ADV, which initially conducts a two-stage search for attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region using Gaussian kernel functions. Additionally, HiT-ADV is extendable to physical attack. We propose that by employing benign resampling and benign rigid transformations, we can further enhance physical adversarial strength with little sacrifice to imperceptibility. Extensive experiments have validated the superiority of our method in terms of adversarial and imperceptible properties in both digital and physical spaces. Our code is avaliable at: https://github.com/TRLou/HiT-ADV.",
        "page": "http://arxiv.org/abs/2403.05247",
        "pdf": "http://arxiv.org/pdf/2403.05247.pdf"
    },
    {
        "title": "TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint Video",
        "author": "Minye Wu, Zehao Wang, Georgios Kouros, Tinne Tuytelaars",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "On Scaling up a Multilingual Vision and Language Model",
        "author": "Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lu\u010di\u0107, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Understanding Video Transfomers via Universal Concept Discovery",
        "author": "Matthew Kowal, Achal Dave, Rares Andrei Ambrus, Adrien Gaidon, Kosta Derpanis, Pavel Tokmakov",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Kandinsky Conformal Prediction: Efficient Calibration of Image Segmentation Algorithms",
        "author": "Joren Brunekreef, Eric Marcus, Ray Sheombarsing, Jan-Jakob Sonke, Jonas Teuwen",
        "abstract": "Image segmentation algorithms can be understood as a collection of pixel classifiers, for which the outcomes of nearby pixels are correlated. Classifier models can be calibrated using Inductive Conformal Prediction, but this requires holding back a sufficiently large calibration dataset for computing the distribution of non-conformity scores of the model's predictions. If one only requires only marginal calibration on the image level, this calibration set consists of all individual pixels in the images available for calibration. However, if the goal is to attain proper calibration for each individual pixel classifier, the calibration set consists of individual images. In a scenario where data are scarce (such as the medical domain), it may not always be possible to set aside sufficiently many images for this pixel-level calibration. The method we propose, dubbed ``Kandinsky calibration'', makes use of the spatial structure present in the distribution of natural images to simultaneously calibrate the classifiers of ``similar'' pixels. This can be seen as an intermediate approach between marginal (imagewise) and conditional (pixelwise) calibration, where non-conformity scores are aggregated over similar image regions, thereby making more efficient use of the images available for calibration. We run experiments on segmentation algorithms trained and calibrated on subsets of the public MS-COCO and Medical Decathlon datasets, demonstrating that Kandinsky calibration method can significantly improve the coverage. When compared to both pixelwise and imagewise calibration on little data, the Kandinsky method achieves much lower coverage errors, indicating the data efficiency of the Kandinsky calibration.",
        "page": "http://arxiv.org/abs/2311.11837",
        "pdf": "http://arxiv.org/pdf/2311.11837.pdf"
    },
    {
        "title": "Generative Proxemics: A Prior for 3D Social Interaction from Images",
        "author": "Vickie Ye, Vickie Ye, Georgios Pavlakos, Michael J. Black, Angjoo Kanazawa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DIOD: Self-Distillation Meets Object Discovery",
        "author": "Sandra Kara, Hejer AMMAR, Julien Denize, Florian Chabot, Quoc Cuong PHAM",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Amodal Completion via Progressive Mixed Context Diffusion",
        "author": "Katherine Xu, Lingzhi Zhang, Jianbo Shi",
        "abstract": "Our brain can effortlessly recognize objects even when partially hidden from view. Seeing the visible of the hidden is called amodal completion; however, this task remains a challenge for generative AI despite rapid progress. We propose to sidestep many of the difficulties of existing approaches, which typically involve a two-step process of predicting amodal masks and then generating pixels. Our method involves thinking outside the box, literally! We go outside the object bounding box to use its context to guide a pre-trained diffusion inpainting model, and then progressively grow the occluded object and trim the extra background. We overcome two technical challenges: 1) how to be free of unwanted co-occurrence bias, which tends to regenerate similar occluders, and 2) how to judge if an amodal completion has succeeded. Our amodal completion method exhibits improved photorealistic completion results compared to existing approaches in numerous successful completion cases. And the best part? It doesn't require any special training or fine-tuning of models.",
        "page": "http://arxiv.org/abs/2312.15540",
        "pdf": "http://arxiv.org/pdf/2312.15540.pdf"
    },
    {
        "title": "FaceLift: Semi-supervised 3D Facial Landmark Localization",
        "author": "David Ferman, Pablo Garrido, Gaurav Bharaj",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Instance-aware Exploration-Verification-Exploitation for Instance ImageGoal Navigation",
        "author": "Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li",
        "abstract": "As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to navigate to a specified object depicted by a goal image in an unexplored environment. The main challenge of this task lies in identifying the target object from different viewpoints while rejecting similar distractors. Existing ImageGoal Navigation methods usually adopt the simple Exploration-Exploitation framework and ignore the identification of specific instance during navigation. In this work, we propose to imitate the human behaviour of ``getting closer to confirm\" when distinguishing objects from a distance. Specifically, we design a new modular navigation framework named Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level image goal navigation. Our method allows for active switching among the exploration, verification, and exploitation actions, thereby facilitating the agent in making reasonable decisions under different situations. On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our method surpasses previous state-of-the-art work, with a classical segmentation model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success)",
        "page": "http://arxiv.org/abs/2402.17587",
        "pdf": "http://arxiv.org/pdf/2402.17587.pdf"
    },
    {
        "title": "OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning",
        "author": "Geng Xinyu, Jiaming Wang, Jiawei Gong, yuerong xue, Jun Xu, Fanglin Chen, Xiaolin Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MorpheuS: Neural Dynamic 360$^{\\circ}$ Surface Reconstruction from Monocular RGB-D Video",
        "author": "Hengyi Wang, Jingwen Wang, Lourdes Agapito",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders",
        "author": "Soumen Basu, Mayuna Gupta, Chetan Madan, Pankaj Gupta, Chetan Arora",
        "abstract": "In recent years, automated Gallbladder Cancer (GBC) detection has gained the attention of researchers. Current state-of-the-art (SOTA) methodologies relying on ultrasound sonography (US) images exhibit limited generalization, emphasizing the need for transformative approaches. We observe that individual US frames may lack sufficient information to capture disease manifestation. This study advocates for a paradigm shift towards video-based GBC detection, leveraging the inherent advantages of spatiotemporal representations. Employing the Masked Autoencoder (MAE) for representation learning, we address shortcomings in conventional image-based methods. We propose a novel design called FocusMAE to systematically bias the selection of masking tokens from high-information regions, fostering a more refined representation of malignancy. Additionally, we contribute the most extensive US video dataset for GBC detection. We also note that, this is the first study on US video-based GBC detection. We validate the proposed methods on the curated dataset, and report a new state-of-the-art (SOTA) accuracy of 96.4% for the GBC detection problem, against an accuracy of 84% by current Image-based SOTA - GBCNet, and RadFormer, and 94.7% by Video-based SOTA - AdaMAE. We further demonstrate the generality of the proposed FocusMAE on a public CT-based Covid detection dataset, reporting an improvement in accuracy by 3.3% over current baselines. The source code and pretrained models are available at: https://gbc-iitd.github.io/focusmae",
        "page": "http://arxiv.org/abs/2403.08848",
        "pdf": "http://arxiv.org/pdf/2403.08848.pdf"
    },
    {
        "title": "ChatScene: Knowledge-Enabled Safety-Critical Scenario Generation for Autonomous Vehicles",
        "author": "Jiawei Zhang, Chejian Xu, Bo Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "WateRF: Robust Watermarks in Radiance Fields for Protection of Copyrights",
        "author": "Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SnAG: Scalable and Accurate Video Grounding",
        "author": "Fangzhou Mu, SICHENG MO, Yin Li",
        "abstract": "Temporal grounding of text descriptions in videos is a central problem in vision-language learning and video understanding. Existing methods often prioritize accuracy over scalability -- they have been optimized for grounding only a few text queries within short videos, and fail to scale up to long videos with hundreds of queries. In this paper, we study the effect of cross-modal fusion on the scalability of video grounding models. Our analysis establishes late fusion as a more cost-effective fusion scheme for long-form videos with many text queries. Moreover, it leads us to a novel, video-centric sampling scheme for efficient training. Based on these findings, we present SnAG, a simple baseline for scalable and accurate video grounding. Without bells and whistles, SnAG is 43% more accurate and 1.5x faster than CONE, a state of the art for long-form video grounding on the challenging MAD dataset, while achieving highly competitive results on short videos.",
        "page": "http://arxiv.org/abs/2404.02257",
        "pdf": "http://arxiv.org/pdf/2404.02257.pdf"
    },
    {
        "title": "PolarRec: Improving Radio Interferometric Data Reconstruction Using Polar Coordinates",
        "author": "Ruoqi Wang, Zhuoyang Chen, Jiayi Zhu, Qiong Luo, Feng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning to Predict Activity Progress by Self-Supervised Video Alignment",
        "author": "Gerard Donahue, Ehsan Elhamifar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NeLF-Pro: Neural Light Field Probes for Multi-Scale Novel View Synthesis",
        "author": "Zinuo You, Andreas Geiger, Anpei Chen",
        "abstract": "We present NeLF-Pro, a novel representation to model and reconstruct light fields in diverse natural scenes that vary in extent and spatial granularity. In contrast to previous fast reconstruction methods that represent the 3D scene globally, we model the light field of a scene as a set of local light field feature probes, parameterized with position and multi-channel 2D feature maps. Our central idea is to bake the scene's light field into spatially varying learnable representations and to query point features by weighted blending of probes close to the camera - allowing for mipmap representation and rendering. We introduce a novel vector-matrix-matrix (VMM) factorization technique that effectively represents the light field feature probes as products of core factors (i.e., VM) shared among local feature probes, and a basis factor (i.e., M) - efficiently encoding internal relationships and patterns within the scene. Experimentally, we demonstrate that NeLF-Pro significantly boosts the performance of feature grid-based representations, and achieves fast reconstruction with better rendering quality while maintaining compact modeling. Project webpage https://sinoyou.github.io/nelf-pro/.",
        "page": "http://arxiv.org/abs/2312.13328",
        "pdf": "http://arxiv.org/pdf/2312.13328.pdf"
    },
    {
        "title": "Region-Based Representations Revisited",
        "author": "Michal Shlapentokh-Rothman, Ansel Blume, Yao Xiao, Yuqun Wu, Sethuraman T V, Heyi Tao, Jae Yong Lee, Wilfredo Torres-Calderon, Yu-Xiong Wang, Derek Hoiem",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures",
        "author": "Lisa Mais, Peter Hirsch, Claire Managan, Ramya Kandarpa, Josef Rumberger, Annika Reinke, Lena Maier-Hein, Gudrun Ihrke, Dagmar Kainmueller",
        "abstract": "Instance segmentation of neurons in volumetric light microscopy images of nervous systems enables groundbreaking research in neuroscience by facilitating joint functional and morphological analyses of neural circuits at cellular resolution. Yet said multi-neuron light microscopy data exhibits extremely challenging properties for the task of instance segmentation: Individual neurons have long-ranging, thin filamentous and widely branching morphologies, multiple neurons are tightly inter-weaved, and partial volume effects, uneven illumination and noise inherent to light microscopy severely impede local disentangling as well as long-range tracing of individual neurons. These properties reflect a current key challenge in machine learning research, namely to effectively capture long-range dependencies in the data. While respective methodological research is buzzing, to date methods are typically benchmarked on synthetic datasets. To address this gap, we release the FlyLight Instance Segmentation Benchmark (FISBe) dataset, the first publicly available multi-neuron light microscopy dataset with pixel-wise annotations. In addition, we define a set of instance segmentation metrics for benchmarking that we designed to be meaningful with regard to downstream analyses. Lastly, we provide three baselines to kick off a competition that we envision to both advance the field of machine learning regarding methodology for capturing long-range data dependencies, and facilitate scientific discovery in basic neuroscience.",
        "page": "http://arxiv.org/abs/2404.00130",
        "pdf": "http://arxiv.org/pdf/2404.00130.pdf"
    },
    {
        "title": "ToNNO: Tomographic Reconstruction of a Neural Network\u2019s Output for Weakly Supervised Segmentation of 3D Medical Images",
        "author": "Marius Schmidt-Mengin, Alexis Benichoux, Shibeshih Belachew, Nikos Komodakis, Nikos Paragios",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Physics-aware Hand-object Interaction Denoising",
        "author": "Haowen Luo, Yunze Liu, Li Yi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Disentangled Pre-training for Human-Object Interaction Detection",
        "author": "Zhuolong Li, Xingao Li, Changxing Ding, Xiangmin Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement",
        "author": "Hao Wu, Huabin Liu, Yu Qiao, Xiao Sun",
        "abstract": "We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for dense video captioning (DVC), that elaborates on improving the quality of the generated event captions and their associated pseudo event boundaries from unlabeled videos. By leveraging the capabilities of diverse large language models (LLMs), we generate rich DVC-oriented caption candidates and optimize the corresponding pseudo boundaries under several meticulously designed objectives, considering diversity, event-centricity, temporal ordering, and coherence. Moreover, we further introduce a novel online boundary refinement strategy that iteratively improves the quality of pseudo boundaries during training. Comprehensive experiments have been conducted to examine the effectiveness of the proposed technique components. By leveraging a substantial amount of unlabeled video data, such as HowTo100M, we achieve a remarkable advancement on standard DVC datasets like YouCook2 and ActivityNet. We outperform the previous state-of-the-art Vid2Seq across a majority of metrics, achieving this with just 0.4% of the unlabeled video data used for pre-training by Vid2Seq.",
        "page": "http://arxiv.org/abs/2404.02755",
        "pdf": "http://arxiv.org/pdf/2404.02755.pdf"
    },
    {
        "title": "CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification",
        "author": "Haoran Lai, Qingsong Yao, Zihang Jiang, Rongsheng Wang, Zhiyang He, Xiaodong Tao, S Kevin Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Deep-TROJ: An Inference Stage Trojan Insertion Algorithm through Efficient Weight Replacement Attack",
        "author": "Sabbir Ahmed, RANYANG ZHOU, Shaahin Angizi, Adnan Rakin Rakin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SignGraph: A Sign Sequence is Worth Graphs of Nodes",
        "author": "Shiwei Gan, Yafeng Yin, Zhiwei Jiang, Hongkai Wen, Lei Xie, Sanglu Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiLiGenRT: A Photometric Stereo Dataset with Quantified Roughness and Translucency",
        "author": "Heng Guo, Jieji Ren, Feishi Wang, Boxin Shi, Mingjun Ren, Yasuyuki Matsushita",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation",
        "author": "Fahimeh Hosseini Noohdani, Parsa Hosseini, Aryan Yazdan Parast, Hamidreza Araghi, Mahdieh Baghshah",
        "abstract": "While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especially in datapoints on which models have a high confidence). In fact, according to the amount of spurious correlation and the easiness of classification based on the causal or non-causal components, the model usually attends to one of these more (on samples with high confidence). Following this, we first try to identify the causal components of images using class activation maps of models trained with ERM. Afterward, we intervene on images by combining them and retraining the model on the augmented data, including the counterfactual ones. Along with its high interpretability, this work proposes a group-balancing method by intervening on images without requiring group labels or information regarding the spurious features during training. The method has an overall better worst group accuracy compared to previous methods with the same amount of supervision on the group labels in correlation shift.",
        "page": "http://arxiv.org/abs/2402.18919",
        "pdf": "http://arxiv.org/pdf/2402.18919.pdf"
    },
    {
        "title": "TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models",
        "author": "Haomiao Ni, Bernhard Egger, Suhas Lohit, Anoop Cherian, Ye Wang, Toshiaki Koike-Akino, Sharon X. Huang, Tim Marks",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Coherence As Texture -- Passive Textureless 3D Reconstruction by Self-interference",
        "author": "Wei-Yu Chen, Aswin C. Sankaranarayanan, Anat Levin, Matthew O\u2019Toole",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Hunting Attributes:  Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation",
        "author": "Feilong Tang, Zhongxing Xu, Zhaojun QU, Wei Feng, xingjian jiang, Zongyuan Ge",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Space-time Diffusion Features for Zero-shot Text-driven Motion Transfer",
        "author": "Rafail Fridman, Danah Yatim, Omer Bar-Tal, Yoni Kasten, Tali Dekel",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Retrieval-Augmented Open-Vocabulary Object Detection",
        "author": "Jooyeon Kim, Eulrang Cho, Sehyung Kim, Hyunwoo J. Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NTO3D: Neural Target Object 3D Reconstruction with Segment Anything",
        "author": "Xiaobao Wei, Renrui Zhang, Jiarui Wu, Jiaming Liu, Ming Lu, Yandong Guo, Shanghang Zhang",
        "abstract": "Neural 3D reconstruction from multi-view images has recently attracted increasing attention from the community. Existing methods normally learn a neural field for the whole scene, while it is still under-explored how to reconstruct a target object indicated by users. Considering the Segment Anything Model (SAM) has shown effectiveness in segmenting any 2D images, in this paper, we propose NTO3D, a novel high-quality Neural Target Object 3D (NTO3D) reconstruction method, which leverages the benefits of both neural field and SAM. We first propose a novel strategy to lift the multi-view 2D segmentation masks of SAM into a unified 3D occupancy field. The 3D occupancy field is then projected into 2D space and generates the new prompts for SAM. This process is iterative until convergence to separate the target object from the scene. After this, we then lift the 2D features of the SAM encoder into a 3D feature field in order to improve the reconstruction quality of the target object. NTO3D lifts the 2D masks and features of SAM into the 3D neural field for high-quality neural target object 3D reconstruction. We conduct detailed experiments on several benchmark datasets to demonstrate the advantages of our method. The code will be available at: https://github.com/ucwxb/NTO3D.",
        "page": "http://arxiv.org/abs/2309.12790",
        "pdf": "http://arxiv.org/pdf/2309.12790.pdf"
    },
    {
        "title": "MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models",
        "author": "Yanting Wang, Hongye Fu, Wei Zou, Jinyuan Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Mudslide: A Universal Nuclear Instance Segmentation Method",
        "author": "Jun Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World",
        "author": "Wen Yin, Jian Lou, Pan Zhou, Yulai Xie, Dan Feng, Yuhua Sun, Tailai Zhang, Lichao Sun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "G3DR: Generative 3D Reconstruction in ImageNet",
        "author": "Pradyumna Reddy, Ismail Elezi, Jiankang Deng",
        "abstract": "We introduce a novel 3D generative method, Generative 3D Reconstruction (G3DR) in ImageNet, capable of generating diverse and high-quality 3D objects from single images, addressing the limitations of existing methods. At the heart of our framework is a novel depth regularization technique that enables the generation of scenes with high-geometric fidelity. G3DR also leverages a pretrained language-vision model, such as CLIP, to enable reconstruction in novel views and improve the visual realism of generations. Additionally, G3DR designs a simple but effective sampling procedure to further improve the quality of generations. G3DR offers diverse and efficient 3D asset generation based on class or text conditioning. Despite its simplicity, G3DR is able to beat state-of-theart methods, improving over them by up to 22% in perceptual metrics and 90% in geometry scores, while needing only half of the training time. Code is available at https://github.com/preddy5/G3DR",
        "page": "http://arxiv.org/abs/2403.00939",
        "pdf": "http://arxiv.org/pdf/2403.00939.pdf"
    },
    {
        "title": "Harnessing Meta-Learning for Improving Full-Frame Video Stabilization",
        "author": "Muhammad Kashif Ali, Eun Woo Im, Dongjin Kim, Tae Hyun Kim",
        "abstract": "Video stabilization is a longstanding computer vision problem, particularly pixel-level synthesis solutions for video stabilization which synthesize full frames add to the complexity of this task. These techniques aim to stabilize videos by synthesizing full frames while enhancing the stability of the considered video. This intensifies the complexity of the task due to the distinct mix of unique motion profiles and visual content present in each video sequence, making robust generalization with fixed parameters difficult. In our study, we introduce a novel approach to enhance the performance of pixel-level synthesis solutions for video stabilization by adapting these models to individual input video sequences. The proposed adaptation exploits low-level visual cues accessible during test-time to improve both the stability and quality of resulting videos. We highlight the efficacy of our methodology of \"test-time adaptation\" through simple fine-tuning of one of these models, followed by significant stability gain via the integration of meta-learning techniques. Notably, significant improvement is achieved with only a single adaptation step. The versatility of the proposed algorithm is demonstrated by consistently improving the performance of various pixel-level synthesis models for video stabilization in real-world scenarios.",
        "page": "http://arxiv.org/abs/2403.03662",
        "pdf": "http://arxiv.org/pdf/2403.03662.pdf"
    },
    {
        "title": "PEM: Prototype-based Efficient MaskFormer for Image Segmentation",
        "author": "Niccol\u00f2 Cavagnero, Gabriele Rosi, Claudia Cuttano, Francesca Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli",
        "abstract": "Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high semantic content in an efficient way, thanks to the combination of deformable convolutions and context-based self-modulation. We benchmark the proposed PEM architecture on two tasks, semantic and panoptic segmentation, evaluated on two different datasets, Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task and dataset, outperforming task-specific architectures while being comparable and even better than computationally-expensive baselines.",
        "page": "http://arxiv.org/abs/2402.19422",
        "pdf": "http://arxiv.org/pdf/2402.19422.pdf"
    },
    {
        "title": "Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation",
        "author": "Hyunwoo Ryu, Jiwoo Kim, Hyunseok An, Junwoo Chang, Joohwan Seo, Taehan Kim, Yubin Kim, Chaewon Hwang, Jongeun Choi, Roberto Horowitz",
        "abstract": "Diffusion generative modeling has become a promising approach for learning robotic manipulation tasks from stochastic human demonstrations. In this paper, we present Diffusion-EDFs, a novel SE(3)-equivariant diffusion-based approach for visual robotic manipulation tasks. We show that our proposed method achieves remarkable data efficiency, requiring only 5 to 10 human demonstrations for effective end-to-end training in less than an hour. Furthermore, our benchmark experiments demonstrate that our approach has superior generalizability and robustness compared to state-of-the-art methods. Lastly, we validate our methods with real hardware experiments. Project Website: https://sites.google.com/view/diffusion-edfs/home",
        "page": "http://arxiv.org/abs/2309.02685",
        "pdf": "http://arxiv.org/pdf/2309.02685.pdf"
    },
    {
        "title": "BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics",
        "author": "Wenqian Zhang, Molin Huang, Yuxuan Zhou, Juze Zhang, Jingyi Yu, Jingya Wang, Lan Xu",
        "abstract": "The recently emerging text-to-motion advances have spired numerous attempts for convenient and interactive human motion generation. Yet, existing methods are largely limited to generating body motions only without considering the rich two-hand motions, let alone handling various conditions like body dynamics or texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal dataset for two-hand motion generation. Our dataset includes accurate motion tracking for the human body and hands and provides pair-wised finger-level hand annotations and body descriptions. We further provide a strong baseline method, BOTH2Hands, for the novel task: generating vivid two-hand motions from both implicit body dynamics and explicit text prompts. We first warm up two parallel body-to-hand and text-to-hand diffusion models and then utilize the cross-attention transformer for motion blending. Extensive experiments and cross-validations demonstrate the effectiveness of our approach and dataset for generating convincing two-hand motions from the hybrid body-and-textual conditions. Our dataset and code will be disseminated to the community for future research.",
        "page": "http://arxiv.org/abs/2312.07937",
        "pdf": "http://arxiv.org/pdf/2312.07937.pdf"
    },
    {
        "title": "DriveTrack: A Benchmark for Long-Range Point Tracking in Real-World Videos",
        "author": "Arjun Balasingam, Joseph Chandler, Chenning Li, Zhoutong Zhang, Hari Balakrishnan",
        "abstract": "This paper presents DriveTrack, a new benchmark and data generation framework for long-range keypoint tracking in real-world videos. DriveTrack is motivated by the observation that the accuracy of state-of-the-art trackers depends strongly on visual attributes around the selected keypoints, such as texture and lighting. The problem is that these artifacts are especially pronounced in real-world videos, but these trackers are unable to train on such scenes due to a dearth of annotations. DriveTrack bridges this gap by building a framework to automatically annotate point tracks on autonomous driving datasets. We release a dataset consisting of 1 billion point tracks across 24 hours of video, which is seven orders of magnitude greater than prior real-world benchmarks and on par with the scale of synthetic benchmarks. DriveTrack unlocks new use cases for point tracking in real-world videos. First, we show that fine-tuning keypoint trackers on DriveTrack improves accuracy on real-world scenes by up to 7%. Second, we analyze the sensitivity of trackers to visual artifacts in real scenes and motivate the idea of running assistive keypoint selectors alongside trackers.",
        "page": "http://arxiv.org/abs/2312.09523",
        "pdf": "http://arxiv.org/pdf/2312.09523.pdf"
    },
    {
        "title": "Can Biases in ImageNet Models Explain Generalization?",
        "author": "Paul Gavrikov, Janis Keuper",
        "abstract": "The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different training methods to understand how and if these biases - including shape bias, spectral biases, and critical bands - interact with generalization. Our extensive study results reveal that contrary to previous findings, these biases are insufficient to accurately predict the generalization of a model holistically. We provide access to all checkpoints and evaluation code at https://github.com/paulgavrikov/biases_vs_generalization",
        "page": "http://arxiv.org/abs/2404.01509",
        "pdf": "http://arxiv.org/pdf/2404.01509.pdf"
    },
    {
        "title": "Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline",
        "author": "Anas Al-lahham, Muhammad Zaigham Zaheer, Nurbek Tastan, Karthik Nandakumar",
        "abstract": "Unsupervised (US) video anomaly detection (VAD) in surveillance applications is gaining more popularity recently due to its practical real-world applications. As surveillance videos are privacy sensitive and the availability of large-scale video data may enable better US-VAD systems, collaborative learning can be highly rewarding in this setting. However, due to the extremely challenging nature of the US-VAD task, where learning is carried out without any annotations, privacy-preserving collaborative learning of US-VAD systems has not been studied yet. In this paper, we propose a new baseline for anomaly detection capable of localizing anomalous events in complex surveillance videos in a fully unsupervised fashion without any labels on a privacy-preserving participant-based distributed training configuration. Additionally, we propose three new evaluation protocols to benchmark anomaly detection approaches on various scenarios of collaborations and data availability. Based on these protocols, we modify existing VAD datasets to extensively evaluate our approach as well as existing US SOTA methods on two large-scale datasets including UCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits, and codes are available here: https://github.com/AnasEmad11/CLAP",
        "page": "http://arxiv.org/abs/2404.00847",
        "pdf": "http://arxiv.org/pdf/2404.00847.pdf"
    },
    {
        "title": "Exploring Efficient Asymmetric Blind-Spots for Self-Supervised Denoising in Real-World Scenarios",
        "author": "Shiyan Chen, Jiyuan Zhang, Zhaofei Yu, Tiejun Huang",
        "abstract": "Self-supervised denoising has attracted widespread attention due to its ability to train without clean images. However, noise in real-world scenarios is often spatially correlated, which causes many self-supervised algorithms that assume pixel-wise independent noise to perform poorly. Recent works have attempted to break noise correlation with downsampling or neighborhood masking. However, denoising on downsampled subgraphs can lead to aliasing effects and loss of details due to a lower sampling rate. Furthermore, the neighborhood masking methods either come with high computational complexity or do not consider local spatial preservation during inference. Through the analysis of existing methods, we point out that the key to obtaining high-quality and texture-rich results in real-world self-supervised denoising tasks is to train at the original input resolution structure and use asymmetric operations during training and inference. Based on this, we propose Asymmetric Tunable Blind-Spot Network (AT-BSN), where the blind-spot size can be freely adjusted, thus better balancing noise correlation suppression and image local spatial destruction during training and inference. In addition, we regard the pre-trained AT-BSN as a meta-teacher network capable of generating various teacher networks by sampling different blind-spots. We propose a blind-spot based multi-teacher distillation strategy to distill a lightweight network, significantly improving performance. Experimental results on multiple datasets prove that our method achieves state-of-the-art, and is superior to other self-supervised algorithms in terms of computational overhead and visual effects.",
        "page": "http://arxiv.org/abs/2303.16783",
        "pdf": "http://arxiv.org/pdf/2303.16783.pdf"
    },
    {
        "title": "Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic Treatment based on Anthropic Prior Knowledge",
        "author": "Bo Zou, Shaofeng Wang, Hao Liu, Gaoyue Sun, Yajie Wang, Zuo FeiFei, Chengbin Quan, Youjian Zhao",
        "abstract": "Teeth localization, segmentation, and labeling in 2D images have great potential in modern dentistry to enhance dental diagnostics, treatment planning, and population-based studies on oral health. However, general instance segmentation frameworks are incompetent due to 1) the subtle differences between some teeth' shapes (e.g., maxillary first premolar and second premolar), 2) the teeth's position and shape variation across subjects, and 3) the presence of abnormalities in the dentition (e.g., caries and edentulism). To address these problems, we propose a ViT-based framework named TeethSEG, which consists of stacked Multi-Scale Aggregation (MSA) blocks and an Anthropic Prior Knowledge (APK) layer. Specifically, to compose the two modules, we design 1) a unique permutation-based upscaler to ensure high efficiency while establishing clear segmentation boundaries with 2) multi-head self/cross-gating layers to emphasize particular semantics meanwhile maintaining the divergence between token embeddings. Besides, we collect 3) the first open-sourced intraoral image dataset IO150K, which comprises over 150k intraoral photos, and all photos are annotated by orthodontists using a human-machine hybrid algorithm. Experiments on IO150K demonstrate that our TeethSEG outperforms the state-of-the-art segmentation models on dental image segmentation.",
        "page": "http://arxiv.org/abs/2404.01013",
        "pdf": "http://arxiv.org/pdf/2404.01013.pdf"
    },
    {
        "title": "Multi-Level Neural Scene Graphs for Dynamic Urban Environments",
        "author": "Tobias Fischer, Lorenzo Porzi, Samuel Rota Bul\u00f2, Marc Pollefeys, Peter Kontschieder",
        "abstract": "We estimate the radiance field of large-scale dynamic areas from multiple vehicle captures under varying environmental conditions. Previous works in this domain are either restricted to static environments, do not scale to more than a single short video, or struggle to separately represent dynamic object instances. To this end, we present a novel, decomposable radiance field approach for dynamic urban environments. We propose a multi-level neural scene graph representation that scales to thousands of images from dozens of sequences with hundreds of fast-moving objects. To enable efficient training and rendering of our representation, we develop a fast composite ray sampling and rendering scheme. To test our approach in urban driving scenarios, we introduce a new, novel view synthesis benchmark. We show that our approach outperforms prior art by a significant margin on both established and our proposed benchmark while being faster in training and rendering.",
        "page": "http://arxiv.org/abs/2404.00168",
        "pdf": "http://arxiv.org/pdf/2404.00168.pdf"
    },
    {
        "title": "PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding",
        "author": "Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, Ying Shan",
        "abstract": "Recent advances in text-to-image generation have made remarkable progress in synthesizing realistic human photos conditioned on given text prompts. However, existing personalized generation methods cannot simultaneously satisfy the requirements of high efficiency, promising identity (ID) fidelity, and flexible text controllability. In this work, we introduce PhotoMaker, an efficient personalized text-to-image generation method, which mainly encodes an arbitrary number of input ID images into a stack ID embedding for preserving ID information. Such an embedding, serving as a unified ID representation, can not only encapsulate the characteristics of the same input ID comprehensively, but also accommodate the characteristics of different IDs for subsequent integration. This paves the way for more intriguing and practically valuable applications. Besides, to drive the training of our PhotoMaker, we propose an ID-oriented data construction pipeline to assemble the training data. Under the nourishment of the dataset constructed through the proposed pipeline, our PhotoMaker demonstrates better ID preservation ability than test-time fine-tuning based methods, yet provides significant speed improvements, high-quality generation results, strong generalization capabilities, and a wide range of applications. Our project page is available at https://photo-maker.github.io/",
        "page": "http://arxiv.org/abs/2312.04461",
        "pdf": "http://arxiv.org/pdf/2312.04461.pdf"
    },
    {
        "title": "Improving Spectral Snapshot Reconstruction with Spectral-Spatial Rectification",
        "author": "Jiancheng Zhang, Haijin Zeng, Yongyong Chen, Dengxiu Yu, Yinping Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MAGICK: A Large-scale Captioned Dataset from Matting Generated Images using Chroma Keying",
        "author": "Ryan Burgert, Brian Price, Jason Kuen, Yijun Li, Michael Ryoo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Self-Supervised Multi-Object Tracking with Path Consistency",
        "author": "Zijia Lu, Bing Shuai, Yanbei Chen, Zhenlin Xu, Davide Modolo",
        "abstract": "In this paper, we propose a novel concept of path consistency to learn robust object matching without using manual object identity supervision. Our key idea is that, to track a object through frames, we can obtain multiple different association results from a model by varying the frames it can observe, i.e., skipping frames in observation. As the differences in observations do not alter the identities of objects, the obtained association results should be consistent. Based on this rationale, we generate multiple observation paths, each specifying a different set of frames to be skipped, and formulate the Path Consistency Loss that enforces the association results are consistent across different observation paths. We use the proposed loss to train our object matching model with only self-supervision. By extensive experiments on three tracking datasets (MOT17, PersonPath22, KITTI), we demonstrate that our method outperforms existing unsupervised methods with consistent margins on various evaluation metrics, and even achieves performance close to supervised methods.",
        "page": "http://arxiv.org/abs/2404.05136",
        "pdf": "http://arxiv.org/pdf/2404.05136.pdf"
    },
    {
        "title": "3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling",
        "author": "Chaokang Jiang, Guangming Wang, Jiuming Liu, Hesheng Wang, Zhuang Ma, Zhenqiang Liu, LIANG, Yi Shan, Dalong Du",
        "abstract": "Learning 3D scene flow from LiDAR point clouds presents significant difficulties, including poor generalization from synthetic datasets to real scenes, scarcity of real-world 3D labels, and poor performance on real sparse LiDAR point clouds. We present a novel approach from the perspective of auto-labelling, aiming to generate a large number of 3D scene flow pseudo labels for real-world LiDAR point clouds. Specifically, we employ the assumption of rigid body motion to simulate potential object-level rigid movements in autonomous driving scenarios. By updating different motion attributes for multiple anchor boxes, the rigid motion decomposition is obtained for the whole scene. Furthermore, we developed a novel 3D scene flow data augmentation method for global and local motion. By perfectly synthesizing target point clouds based on augmented motion parameters, we easily obtain lots of 3D scene flow labels in point clouds highly consistent with real scenarios. On multiple real-world datasets including LiDAR KITTI, nuScenes, and Argoverse, our method outperforms all previous supervised and unsupervised methods without requiring manual labelling. Impressively, our method achieves a tenfold reduction in EPE3D metric on the LiDAR KITTI dataset, reducing it from $0.190m$ to a mere $0.008m$ error.",
        "page": "http://arxiv.org/abs/2402.18146",
        "pdf": "http://arxiv.org/pdf/2402.18146.pdf"
    },
    {
        "title": "SPECAT: SPatial-spEctral Cumulative-Attention Transformer for High-Resolution Hyperspectral Image Reconstruction",
        "author": "Zhiyang Yao, Shuyang Liu, Xiaoyun Yuan, Lu Fang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Online Task-Free Continual Generative and Discriminative Learning via Dynamic Cluster Memory",
        "author": "\u98de \u53f6, Adrian Bors",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CPR-Coach: Recognizing Composite Error Actions based on Single-class Training",
        "author": "Shunli Wang, Shuaibing Wang, Dingkang Yang, Mingcheng Li, Haopeng Kuang, Xiao Zhao, Liuzhen Su, Peng Zhai, Lihua Zhang",
        "abstract": "The fine-grained medical action analysis task has received considerable attention from pattern recognition communities recently, but it faces the problems of data and algorithm shortage. Cardiopulmonary Resuscitation (CPR) is an essential skill in emergency treatment. Currently, the assessment of CPR skills mainly depends on dummies and trainers, leading to high training costs and low efficiency. For the first time, this paper constructs a vision-based system to complete error action recognition and skill assessment in CPR. Specifically, we define 13 types of single-error actions and 74 types of composite error actions during external cardiac compression and then develop a video dataset named CPR-Coach. By taking the CPR-Coach as a benchmark, this paper thoroughly investigates and compares the performance of existing action recognition models based on different data modalities. To solve the unavoidable Single-class Training & Multi-class Testing problem, we propose a humancognition-inspired framework named ImagineNet to improve the model's multierror recognition performance under restricted supervision. Extensive experiments verify the effectiveness of the framework. We hope this work could advance research toward fine-grained medical action analysis and skill assessment. The CPR-Coach dataset and the code of ImagineNet are publicly available on Github.",
        "page": "http://arxiv.org/abs/2309.11718",
        "pdf": "http://arxiv.org/pdf/2309.11718.pdf"
    },
    {
        "title": "DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting",
        "author": "Demin Yu, Xutao Li, Yunming Ye, Baoquan Zhang, Luo Chuyao, Kuai Dai, wangrui, Chenxunlai",
        "abstract": "Precipitation nowcasting is an important spatio-temporal prediction task to predict the radar echoes sequences based on current observations, which can serve both meteorological science and smart city applications. Due to the chaotic evolution nature of the precipitation systems, it is a very challenging problem. Previous studies address the problem either from the perspectives of deterministic modeling or probabilistic modeling. However, their predictions suffer from the blurry, high-value echoes fading away and position inaccurate issues. The root reason of these issues is that the chaotic evolutionary precipitation systems are not appropriately modeled. Inspired by the nature of the systems, we propose to decompose and model them from the perspective of global deterministic motion and local stochastic variations with residual mechanism. A unified and flexible framework that can equip any type of spatio-temporal models is proposed based on residual diffusion, which effectively tackles the shortcomings of previous methods. Extensive experimental results on four publicly available radar datasets demonstrate the effectiveness and superiority of the proposed framework, compared to state-of-the-art techniques. Our code is publicly available at https://github.com/DeminYu98/DiffCast.",
        "page": "http://arxiv.org/abs/2312.06734",
        "pdf": "http://arxiv.org/pdf/2312.06734.pdf"
    },
    {
        "title": "Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under Manhattan World Assumption",
        "author": "Nobuhiko Wakai, Satoshi Sato, Yasunori Ishii, Takayoshi Yamashita",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking Boundary Discontinuity Problem for Oriented Object Detection",
        "author": "Hang Xu, Xinyuan Liu, Haonan Xu, Yike Ma, Zunjie Zhu, Chenggang Yan, Feng Dai",
        "abstract": "Oriented object detection has been developed rapidly in the past few years, where rotation equivariance is crucial for detectors to predict rotated boxes. It is expected that the prediction can maintain the corresponding rotation when objects rotate, but severe mutation in angular prediction is sometimes observed when objects rotate near the boundary angle, which is well-known boundary discontinuity problem. The problem has been long believed to be caused by the sharp loss increase at the angular boundary, and widely used joint-optim IoU-like methods deal with this problem by loss-smoothing. However, we experimentally find that even state-of-the-art IoU-like methods actually fail to solve the problem. On further analysis, we find that the key to solution lies in encoding mode of the smoothing function rather than in joint or independent optimization. In existing IoU-like methods, the model essentially attempts to fit the angular relationship between box and object, where the break point at angular boundary makes the predictions highly unstable.To deal with this issue, we propose a dual-optimization paradigm for angles. We decouple reversibility and joint-optim from single smoothing function into two distinct entities, which for the first time achieves the objectives of both correcting angular boundary and blending angle with other parameters.Extensive experiments on multiple datasets show that boundary discontinuity problem is well-addressed. Moreover, typical IoU-like methods are improved to the same level without obvious performance gap. The code is available at https://github.com/hangxu-cv/cvpr24acm.",
        "page": "http://arxiv.org/abs/2305.10061",
        "pdf": "http://arxiv.org/pdf/2305.10061.pdf"
    },
    {
        "title": "Restoration by Generation with Constrained Priors",
        "author": "Zheng Ding, Xuaner Zhang, Zhuowen Tu, Zhihao Xia",
        "abstract": "The inherent generative power of denoising diffusion models makes them well-suited for image restoration tasks where the objective is to find the optimal high-quality image within the generative space that closely resembles the input image. We propose a method to adapt a pretrained diffusion model for image restoration by simply adding noise to the input image to be restored and then denoise. Our method is based on the observation that the space of a generative model needs to be constrained. We impose this constraint by finetuning the generative model with a set of anchor images that capture the characteristics of the input image. With the constrained space, we can then leverage the sampling strategy used for generation to do image restoration. We evaluate against previous methods and show superior performances on multiple real-world restoration datasets in preserving identity and image quality. We also demonstrate an important and practical application on personalized restoration, where we use a personal album as the anchor images to constrain the generative space. This approach allows us to produce results that accurately preserve high-frequency details, which previous works are unable to do. Project webpage: https://gen2res.github.io.",
        "page": "http://arxiv.org/abs/2312.17161",
        "pdf": "http://arxiv.org/pdf/2312.17161.pdf"
    },
    {
        "title": "Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge",
        "author": "Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim",
        "abstract": "The goal of the multi-sound source localization task is to localize sound sources from the mixture individually. While recent multi-sound source localization methods have shown improved performance, they face challenges due to their reliance on prior information about the number of objects to be separated. In this paper, to overcome this limitation, we present a novel multi-sound source localization method that can perform localization without prior knowledge of the number of sound sources. To achieve this goal, we propose an iterative object identification (IOI) module, which can recognize sound-making objects in an iterative manner. After finding the regions of sound-making objects, we devise object similarity-aware clustering (OSC) loss to guide the IOI module to effectively combine regions of the same object but also distinguish between different objects and backgrounds. It enables our method to perform accurate localization of sound-making objects without any prior knowledge. Extensive experimental results on the MUSIC and VGGSound benchmarks show the significant performance improvements of the proposed method over the existing methods for both single and multi-source. Our code is available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL",
        "page": "http://arxiv.org/abs/2403.17420",
        "pdf": "http://arxiv.org/pdf/2403.17420.pdf"
    },
    {
        "title": "OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning",
        "author": "Noor Ahmed, Anna Kukleva, Bernt Schiele",
        "abstract": "Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which the problem space expands with limited data. FSCIL methods inherently face the challenge of catastrophic forgetting as data arrives incrementally, making models susceptible to overwriting previously acquired knowledge. Moreover, given the scarcity of labeled samples available at any given time, models may be prone to overfitting and find it challenging to strike a balance between extensive pretraining and the limited incremental data. To address these challenges, we propose the OrCo framework built on two core principles: features' orthogonality in the representation space, and contrastive learning. In particular, we improve the generalization of the embedding space by employing a combination of supervised and self-supervised contrastive losses during the pretraining phase. Additionally, we introduce OrCo loss to address challenges arising from data limitations during incremental sessions. Through feature space perturbations and orthogonality between classes, the OrCo loss maximizes margins and reserves space for the following incremental data. This, in turn, ensures the accommodation of incoming classes in the feature space without compromising previously acquired knowledge. Our experimental results showcase state-of-the-art performance across three benchmark datasets, including mini-ImageNet, CIFAR100, and CUB datasets. Code is available at https://github.com/noorahmedds/OrCo",
        "page": "http://arxiv.org/abs/2403.18550",
        "pdf": "http://arxiv.org/pdf/2403.18550.pdf"
    },
    {
        "title": "CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update",
        "author": "Zhi Gao, Yuntao Du., Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, Qing Li",
        "abstract": "Utilizing large language models (LLMs) to compose off-the-shelf visual tools represents a promising avenue of research for developing robust visual assistants capable of addressing diverse visual tasks. However, these methods often overlook the potential for continual learning, typically by freezing the utilized tools, thus limiting their adaptation to environments requiring new knowledge. To tackle this challenge, we propose CLOVA, a Closed-Loop Visual Assistant, which operates within a framework encompassing inference, reflection, and learning phases. During the inference phase, LLMs generate programs and execute corresponding tools to complete assigned tasks. In the reflection phase, a multimodal global-local reflection scheme analyzes human feedback to determine which tools require updating. Lastly, the learning phase employs three flexible approaches to automatically gather training data and introduces a novel prompt tuning scheme to update the tools, allowing CLOVA to efficiently acquire new knowledge. Experimental findings demonstrate that CLOVA surpasses existing tool-usage methods by 5% in visual question answering and multiple-image reasoning, by 10% in knowledge tagging, and by 20% in image editing. These results underscore the significance of the continual learning capability in general visual assistants.",
        "page": "http://arxiv.org/abs/2312.10908",
        "pdf": "http://arxiv.org/pdf/2312.10908.pdf"
    },
    {
        "title": "NeuRAD: Neural Rendering for Autonomous Driving",
        "author": "Adam Tonderski, Carl Lindstr\u00f6m, Georg Hess, William Ljungbergh, Lennart Svensson, Christoffer Petersson",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learned Lossless Image Compression based on Bit Plane Slicing",
        "author": "Zhe Zhang, Huairui Wang, Zhenzhong Chen, Shan Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning",
        "author": "Hongwei Zheng, Linyuan Zhou, Han Li, Jinming Su, Xiaoming Wei, Xu Xiaoming",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement",
        "author": "Ziyu Wang, Yue Xu, Cewu Lu, Yonglu Li",
        "abstract": "Recently, dataset distillation has paved the way towards efficient machine learning, especially for image datasets. However, the distillation for videos, characterized by an exclusive temporal dimension, remains an underexplored domain. In this work, we provide the first systematic study of video distillation and introduce a taxonomy to categorize temporal compression. Our investigation reveals that the temporal information is usually not well learned during distillation, and the temporal dimension of synthetic data contributes little. The observations motivate our unified framework of disentangling the dynamic and static information in the videos. It first distills the videos into still images as static memory and then compensates the dynamic and motion information with a learnable dynamic memory block. Our method achieves state-of-the-art on video datasets at different scales, with a notably smaller memory storage budget. Our code is available at https://github.com/yuz1wan/video_distillation.",
        "page": "http://arxiv.org/abs/2312.00362",
        "pdf": "http://arxiv.org/pdf/2312.00362.pdf"
    },
    {
        "title": "NeRFDeformer: NeRF Transformation from a Single View via 3D Scene Flows",
        "author": "Zhenggang Tang, Jason Ren, Xiaoming Zhao, Bowen Wen, Jonathan Tremblay, Stan Birchfield, Alexander G. Schwing",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation",
        "author": "Suraj Patni, Aradhye Agarwal, Chetan Arora",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CosmicMan: A Text-to-Image Foundation Model for Humans",
        "author": "Shikai Li, Jianglin Fu, Kaiyuan Liu, Wentao Wang, Kwan-Yee Lin, Wayne Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Language-conditioned Detection Transformer",
        "author": "Jang Hyun Cho, Philipp Kr\u00e4henb\u00fchl",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Distilled Datamodel with Reverse Gradient Matching",
        "author": "Jingwen Ye, Ruonan Yu, Songhua Liu, Xinchao Wang",
        "abstract": "The proliferation of large-scale AI models trained on extensive datasets has revolutionized machine learning. With these models taking on increasingly central roles in various applications, the need to understand their behavior and enhance interpretability has become paramount. To investigate the impact of changes in training data on a pre-trained model, a common approach is leave-one-out retraining. This entails systematically altering the training dataset by removing specific samples to observe resulting changes within the model. However, retraining the model for each altered dataset presents a significant computational challenge, given the need to perform this operation for every dataset variation. In this paper, we introduce an efficient framework for assessing data impact, comprising offline training and online evaluation stages. During the offline training phase, we approximate the influence of training data on the target model through a distilled synset, formulated as a reversed gradient matching problem. For online evaluation, we expedite the leave-one-out process using the synset, which is then utilized to compute the attribution matrix based on the evaluation objective. Experimental evaluations, including training data attribution and assessments of data quality, demonstrate that our proposed method achieves comparable model behavior evaluation while significantly speeding up the process compared to the direct retraining method.",
        "page": "http://arxiv.org/abs/2404.14006",
        "pdf": "http://arxiv.org/pdf/2404.14006.pdf"
    },
    {
        "title": "Digital Life Project: Autonomous 3D Characters with Social Intelligence",
        "author": "Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiy Mei, Chen Wei, Wang Ruisi, Wanqi Yin, Liang Pan, Xiangyu Fan, Han Du, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, YuKun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu",
        "abstract": "In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: https://digital-life-project.com/",
        "page": "http://arxiv.org/abs/2312.04547",
        "pdf": "http://arxiv.org/pdf/2312.04547.pdf"
    },
    {
        "title": "Continual Self-supervised Learning: Towards Universal Multi-modal Medical Data Representation Learning",
        "author": "Yiwen Ye, Yutong Xie, Jianpeng Zhang, Ziyang Chen, Qi Wu, Yong Xia",
        "abstract": "Self-supervised learning is an efficient pre-training method for medical image analysis. However, current research is mostly confined to specific-modality data pre-training, consuming considerable time and resources without achieving universality across different modalities. A straightforward solution is combining all modality data for joint self-supervised pre-training, which poses practical challenges. Firstly, our experiments reveal conflicts in representation learning as the number of modalities increases. Secondly, multi-modal data collected in advance cannot cover all real-world scenarios. In this paper, we reconsider versatile self-supervised learning from the perspective of continual learning and propose MedCoSS, a continuous self-supervised learning approach for multi-modal medical data. Unlike joint self-supervised learning, MedCoSS assigns different modality data to different training stages, forming a multi-stage pre-training process. To balance modal conflicts and prevent catastrophic forgetting, we propose a rehearsal-based continual learning method. We introduce the k-means sampling strategy to retain data from previous modalities and rehearse it when learning new modalities. Instead of executing the pretext task on buffer data, a feature distillation strategy and an intra-modal mixup strategy are applied to these data for knowledge retention. We conduct continuous self-supervised pre-training on a large-scale multi-modal unlabeled dataset, including clinical reports, X-rays, CT scans, MRI scans, and pathological images. Experimental results demonstrate MedCoSS's exceptional generalization ability across nine downstream datasets and its significant scalability in integrating new modality data. Code and pre-trained weight are available at https://github.com/yeerwen/MedCoSS.",
        "page": "http://arxiv.org/abs/2311.17597",
        "pdf": "http://arxiv.org/pdf/2311.17597.pdf"
    },
    {
        "title": "A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark",
        "author": "Jakub Paplham, Vojtech Franc",
        "abstract": "Comparing different age estimation methods poses a challenge due to the unreliability of published results stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. This paper identifies two trivial, yet persistent issues with the currently used evaluation protocol and describes how to resolve them. We offer an extensive comparative analysis for state-of-the-art facial age estimation methods. Surprisingly, we find that the performance differences between the methods are negligible compared to the effect of other factors, such as facial alignment, facial coverage, image resolution, model architecture, or the amount of data used for pretraining. We use the gained insights to propose using FaRL as the backbone model and demonstrate its effectiveness on all public datasets. We make the source code and exact data splits public on GitHub.",
        "page": "http://arxiv.org/abs/2307.04570",
        "pdf": "http://arxiv.org/pdf/2307.04570.pdf"
    },
    {
        "title": "Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging",
        "author": "Bhargav Ghanekar, Salman Siddique Khan, Pranav Sharma, Shreyas Singh, Vivek Boominathan, Kaushik Mitra, Ashok Veeraraghavan",
        "abstract": "Passive, compact, single-shot 3D sensing is useful in many application areas such as microscopy, medical imaging, surgical navigation, and autonomous driving where form factor, time, and power constraints can exist. Obtaining RGB-D scene information over a short imaging distance, in an ultra-compact form factor, and in a passive, snapshot manner is challenging. Dual-pixel (DP) sensors are a potential solution to achieve the same. DP sensors collect light rays from two different halves of the lens in two interleaved pixel arrays, thus capturing two slightly different views of the scene, like a stereo camera system. However, imaging with a DP sensor implies that the defocus blur size is directly proportional to the disparity seen between the views. This creates a trade-off between disparity estimation vs. deblurring accuracy. To improve this trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which we use a coded aperture in the imaging lens along with a DP sensor. In our approach, we jointly learn an optimal coded pattern and the reconstruction algorithm in an end-to-end optimization setting. Our resulting CADS imaging system demonstrates improvement of >1.5dB PSNR in all-in-focus (AIF) estimates and 5-6% in depth estimation quality over naive DP sensing for a wide range of aperture settings. Furthermore, we build the proposed CADS prototypes for DSLR photography settings and in an endoscope and a dermoscope form factor. Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D reconstruction results in simulations and real-world experiments in a passive, snapshot, and compact manner.",
        "page": "http://arxiv.org/abs/2402.18102",
        "pdf": "http://arxiv.org/pdf/2402.18102.pdf"
    },
    {
        "title": "You Only Need Less Attention Each Stage in Vision Transformers",
        "author": "Shuoxi Zhang, Hanpeng Liu, Stephen Lin, Kun He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Self-Calibrating Vicinal Risk Minimisation for Model Calibration",
        "author": "Jiawei Liu, Changkun Ye, Ruikai Cui, Nick Barnes",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection",
        "author": "Chuangchuang Tan, Huan Liu, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, Yunchao Wei",
        "abstract": "Recently, the proliferation of highly realistic synthetic images, facilitated through a variety of GANs and Diffusions, has significantly heightened the susceptibility to misuse. While the primary focus of deepfake detection has traditionally centered on the design of detection algorithms, an investigative inquiry into the generator architectures has remained conspicuously absent in recent years. This paper contributes to this lacuna by rethinking the architectures of CNN-based generators, thereby establishing a generalized representation of synthetic artifacts. Our findings illuminate that the up-sampling operator can, beyond frequency-based artifacts, produce generalized forgery artifacts. In particular, the local interdependence among image pixels caused by upsampling operators is significantly demonstrated in synthetic images generated by GAN or diffusion. Building upon this observation, we introduce the concept of Neighboring Pixel Relationships(NPR) as a means to capture and characterize the generalized structural artifacts stemming from up-sampling operations. A comprehensive analysis is conducted on an open-world dataset, comprising samples generated by \\tft{28 distinct generative models}. This analysis culminates in the establishment of a novel state-of-the-art performance, showcasing a remarkable \\tft{11.6\\%} improvement over existing methods. The code is available at https://github.com/chuangchuangtan/NPR-DeepfakeDetection.",
        "page": "http://arxiv.org/abs/2312.10461",
        "pdf": "http://arxiv.org/pdf/2312.10461.pdf"
    },
    {
        "title": "CDFormer: When Degradation Prediction Embraces Diffusion Model for Blind Image Super-Resolution",
        "author": "Qingguo Liu, Chenyi Zhuang, Pan Gao, Jie Qin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents",
        "author": "Jieming Cui, Tengyu Liu, Nian Liu, Yaodong Yang, Yixin Zhu, Siyuan Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "$M^3$-UDA: A New Benchmark for Unsupervised Domain Adaptive Fetal Cardiac Structure Detection",
        "author": "Bin Pu, Liwen Wang, Jiewen Yang, He Guannan, Xingbo Dong, Shengli Li, Ying Tan, Ming Chen, Zhe Jin, Kenli Li, Xiaomeng Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)",
        "author": "Tsu-Ching Hsiao, Hao-Wei Chen, Hsuan-Kung Yang, Chun-Yi Lee",
        "abstract": "Addressing pose ambiguity in 6D object pose estimation from single RGB images presents a significant challenge, particularly due to object symmetries or occlusions. In response, we introduce a novel score-based diffusion method applied to the $SE(3)$ group, marking the first application of diffusion models to $SE(3)$ within the image domain, specifically tailored for pose estimation tasks. Extensive evaluations demonstrate the method's efficacy in handling pose ambiguity, mitigating perspective-induced ambiguity, and showcasing the robustness of our surrogate Stein score formulation on $SE(3)$. This formulation not only improves the convergence of denoising process but also enhances computational efficiency. Thus, we pioneer a promising strategy for 6D object pose estimation.",
        "page": "http://arxiv.org/abs/2305.15873",
        "pdf": "http://arxiv.org/pdf/2305.15873.pdf"
    },
    {
        "title": "QUADify: Extracting Meshes with Pixel-level Details and Materials from Images",
        "author": "Maximilian Fr\u00fchauf, Hayko Riemenschneider, Markus Gross, Christopher Schroers",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Unsupervised Hierarchical Representation with Reinforcement Learning",
        "author": "Ruyi An, Yewen Li, Xu He, Pengjie Gu, Mengchen Zhao, Dong Li, Jianye Hao, Bo An, Chaojie Wang, Mingyuan Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering",
        "author": "Tao Hu, Fangzhou Hong, Ziwei Liu",
        "abstract": "Dynamic human rendering from video sequences has achieved remarkable progress by formulating the rendering as a mapping from static poses to human images. However, existing methods focus on the human appearance reconstruction of every single frame while the temporal motion relations are not fully explored. In this paper, we propose a new 4D motion modeling paradigm, SurMo, that jointly models the temporal dynamics and human appearances in a unified framework with three key designs: 1) Surface-based motion encoding that models 4D human motions with an efficient compact surface-based triplane. It encodes both spatial and temporal motion relations on the dense surface manifold of a statistical body template, which inherits body topology priors for generalizable novel view synthesis with sparse training observations. 2) Physical motion decoding that is designed to encourage physical motion learning by decoding the motion triplane features at timestep t to predict both spatial derivatives and temporal derivatives at the next timestep t+1 in the training stage. 3) 4D appearance decoding that renders the motion triplanes into images by an efficient volumetric surface-conditioned renderer that focuses on the rendering of body surfaces with motion learning conditioning. Extensive experiments validate the state-of-the-art performance of our new paradigm and illustrate the expressiveness of surface-based motion triplanes for rendering high-fidelity view-consistent humans with fast motions and even motion-dependent shadows. Our project page is at: https://taohuumd.github.io/projects/SurMo/",
        "page": "http://arxiv.org/abs/2404.01225",
        "pdf": "http://arxiv.org/pdf/2404.01225.pdf"
    },
    {
        "title": "LASO: Language-guided Affordance Segmentation on 3D Object",
        "author": "Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, Tat-seng Chua",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DualAD: Disentangling the Dynamic and Static World for End-to-End Driving",
        "author": "Simon Doll, Niklas Hanselmann, Lukas Schneider, Richard Schulz, Marius Cordts, Markus Enzweiler, Hendrik Lensch",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models",
        "author": "Takami Sato, Justin Yue, Nanze Chen, Ningfei Wang, Alfred Chen",
        "abstract": "Denoising probabilistic diffusion models have shown breakthrough performance to generate more photo-realistic images or human-level illustrations than the prior models such as GANs. This high image-generation capability has stimulated the creation of many downstream applications in various areas. However, we find that this technology is actually a double-edged sword: We identify a new type of attack, called the Natural Denoising Diffusion (NDD) attack based on the finding that state-of-the-art deep neural network (DNN) models still hold their prediction even if we intentionally remove their robust features, which are essential to the human visual system (HVS), through text prompts. The NDD attack shows a significantly high capability to generate low-cost, model-agnostic, and transferable adversarial attacks by exploiting the natural attack capability in diffusion models. To systematically evaluate the risk of the NDD attack, we perform a large-scale empirical study with our newly created dataset, the Natural Denoising Diffusion Attack (NDDA) dataset. We evaluate the natural attack capability by answering 6 research questions. Through a user study, we find that it can achieve an 88% detection rate while being stealthy to 93% of human subjects; we also find that the non-robust features embedded by diffusion models contribute to the natural attack capability. To confirm the model-agnostic and transferable attack capability, we perform the NDD attack against the Tesla Model 3 and find that 73% of the physically printed attacks can be detected as stop signs. Our hope is that the study and dataset can help our community be aware of the risks in diffusion models and facilitate further research toward robust DNN models.",
        "page": "http://arxiv.org/abs/2308.15692",
        "pdf": "http://arxiv.org/pdf/2308.15692.pdf"
    },
    {
        "title": "FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation",
        "author": "Chris Rockwell, Nilesh Kulkarni, Linyi Jin, Jeong Joon Park, Justin Johnson, David Fouhey",
        "abstract": "Estimating relative camera poses between images has been a central problem in computer vision. Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision. We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales. At the heart of our model lies a Transformer that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization.",
        "page": "http://arxiv.org/abs/2403.03221",
        "pdf": "http://arxiv.org/pdf/2403.03221.pdf"
    },
    {
        "title": "ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object",
        "author": "Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao",
        "abstract": "We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\\%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at https://github.com/chenshuang-zhang/imagenet_d.",
        "page": "http://arxiv.org/abs/2403.18775",
        "pdf": "http://arxiv.org/pdf/2403.18775.pdf"
    },
    {
        "title": "Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos",
        "author": "Kumaranage Ravindu Nagasinghe, Honglu Zhou, Malitha Gunawardhana, Martin Renqiang Min, Daniel Harari, Muhammad Haris Khan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Time-, Memory- and Parameter-Efficient Visual Adaptation",
        "author": "Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab",
        "abstract": "As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly. We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism. Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time.",
        "page": "http://arxiv.org/abs/2402.02887",
        "pdf": "http://arxiv.org/pdf/2402.02887.pdf"
    },
    {
        "title": "Adaptive Slot Attention: Object Discovery with Dynamic Slot Number",
        "author": "Ke Fan, Zechen Bai, Tianjun Xiao, Tong He, Max Horn, Yanwei Fu, Francesco Locatello, Zheng Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Perceptual-Oriented Video Frame Interpolation Via Asymmetric Synergistic Blending",
        "author": "Guangyang Wu, Xin Tao, Changlin Li, Wenyi Wang, Xiaohong Liu, Qingqing Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "iToF-flow-based High Frame Rate Depth Imaging",
        "author": "Yu Meng, Zhou Xue, Xu Chang, Xuemei Hu, Tao Yue",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Revisiting Counterfactual Problems in Referring Expression Comprehension",
        "author": "Zhihan Yu, Ruifan Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Generalized Zero-Shot Learning by Exploring the Diverse Semantics from  External Class Names",
        "author": "Yapeng Li, Yong Luo, Zengmao Wang, Bo Du",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Continual Motion Prediction Learning Framework via Meta-Representation Learning and Optimal Memory Buffer Retention Strategy",
        "author": "Dae Jun Kang, Dongsuk Kum, Sanmin Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Detector-Free Structure from Motion",
        "author": "Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, Xiaowei Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiVAS: Video and Audio Synchronization with Dynamic Frame Rates",
        "author": "Clara Maria Fernandez Labrador, Mertcan Akcay, Eitan Abecassis, Joan Massich, Christopher Schroers",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Material Palette: Extraction of Materials from a Single Image",
        "author": "Ivan Lopes, Fabio Pizzati, Raoul de Charette",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos",
        "author": "Chen Liu, Peike Li, Qingtao Yu, Hongwei Sheng, Dadong Wang, Lincheng Li, Xin Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "C$^\\text{2}$RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction",
        "author": "Yiqun Lin, Jiewen Yang, hualiang wang, Xinpeng Ding, Wei Zhao, Xiaomeng Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Language Model Guided Interpretable Video Action Reasoning",
        "author": "Ning Wang, Guangming Zhu, Hongsheng Li, Liang Zhang, Syed Afaq Ali Shah, Mohammed Bennamoun",
        "abstract": "While neural networks have excelled in video action recognition tasks, their black-box nature often obscures the understanding of their decision-making processes. Recent approaches used inherently interpretable models to analyze video actions in a manner akin to human reasoning. These models, however, usually fall short in performance compared to their black-box counterparts. In this work, we present a new framework named Language-guided Interpretable Action Recognition framework (LaIAR). LaIAR leverages knowledge from language models to enhance both the recognition capabilities and the interpretability of video models. In essence, we redefine the problem of understanding video model decisions as a task of aligning video and language models. Using the logical reasoning captured by the language model, we steer the training of the video model. This integrated approach not only improves the video model's adaptability to different domains but also boosts its overall performance. Extensive experiments on two complex video action datasets, Charades & CAD-120, validates the improved performance and interpretability of our LaIAR framework. The code of LaIAR is available at https://github.com/NingWang2049/LaIAR.",
        "page": "http://arxiv.org/abs/2404.01591",
        "pdf": "http://arxiv.org/pdf/2404.01591.pdf"
    },
    {
        "title": "Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields",
        "author": "Leili Goli, Cody Reading, Silvia Sell\u00e1n, Alec Jacobson, Andrea Tagliasacchi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP",
        "author": "Yunshi HUANG, Fereshteh Shakeri, Jose Dolz, Malik Boudiaf, Houda Bahig, Ismail Ben Ayed",
        "abstract": "In a recent, strongly emergent literature on few-shot CLIP adaptation, Linear Probe (LP) has been often reported as a weak baseline. This has motivated intensive research building convoluted prompt learning or feature adaptation strategies. In this work, we propose and examine from convex-optimization perspectives a generalization of the standard LP baseline, in which the linear classifier weights are learnable functions of the text embedding, with class-wise multipliers blending image and text knowledge. As our objective function depends on two types of variables, i.e., the class visual prototypes and the learnable blending parameters, we propose a computationally efficient block coordinate Majorize-Minimize (MM) descent algorithm. In our full-batch MM optimizer, which we coin LP++, step sizes are implicit, unlike standard gradient descent practices where learning rates are intensively searched over validation sets. By examining the mathematical properties of our loss (e.g., Lipschitz gradient continuity), we build majorizing functions yielding data-driven learning rates and derive approximations of the loss's minima, which provide data-informed initialization of the variables. Our image-language objective function, along with these non-trivial optimization insights and ingredients, yields, surprisingly, highly competitive few-shot CLIP performances. Furthermore, LP++ operates in black-box, relaxes intensive validation searches for the optimization hyper-parameters, and runs orders-of-magnitudes faster than state-of-the-art few-shot CLIP adaptation methods. Our code is available at: \\url{https://github.com/FereshteShakeri/FewShot-CLIP-Strong-Baseline.git}.",
        "page": "http://arxiv.org/abs/2404.02285",
        "pdf": "http://arxiv.org/pdf/2404.02285.pdf"
    },
    {
        "title": "IBD-SLAM: Learning Image-Based Depth Fusion for Generalizable SLAM",
        "author": "Minghao Yin, Shangzhe Wu, Kai Han",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation",
        "author": "Jun Wang, Yuzhe Qin, Kaiming Kuang, Yigit Korkmaz, Akhilan Gurumoorthy, Hao Su, Xiaolong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Plasticity in Online Continual Learning via Collaborative Learning",
        "author": "Maorong Wang, Nicolas Michel, Ling Xiao, Toshihiko Yamasaki",
        "abstract": "Online Continual Learning (CL) solves the problem of learning the ever-emerging new classification tasks from a continuous data stream. Unlike its offline counterpart, in online CL, the training data can only be seen once. Most existing online CL research regards catastrophic forgetting (i.e., model stability) as almost the only challenge. In this paper, we argue that the model's capability to acquire new knowledge (i.e., model plasticity) is another challenge in online CL. While replay-based strategies have been shown to be effective in alleviating catastrophic forgetting, there is a notable gap in research attention toward improving model plasticity. To this end, we propose Collaborative Continual Learning (CCL), a collaborative learning based strategy to improve the model's capability in acquiring new concepts. Additionally, we introduce Distillation Chain (DC), a collaborative learning scheme to boost the training of the models. We adapt CCL-DC to existing representative online CL works. Extensive experiments demonstrate that even if the learners are well-trained with state-of-the-art online CL methods, our strategy can still improve model plasticity dramatically, and thereby improve the overall performance by a large margin. The source code of our work is available at https://github.com/maorong-wang/CCL-DC.",
        "page": "http://arxiv.org/abs/2312.00600",
        "pdf": "http://arxiv.org/pdf/2312.00600.pdf"
    },
    {
        "title": "VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation",
        "author": "Xudong Wang, Ishan Misra, Ziyun Zeng, Rohit Girdhar, Trevor Darrell",
        "abstract": "Existing approaches to unsupervised video instance segmentation typically rely on motion estimates and experience difficulties tracking small or divergent motions. We present VideoCutLER, a simple method for unsupervised multi-instance video segmentation without using motion-based learning signals like optical flow or training on natural videos. Our key insight is that using high-quality pseudo masks and a simple video synthesis method for model training is surprisingly sufficient to enable the resulting video model to effectively segment and track multiple instances across video frames. We show the first competitive unsupervised learning results on the challenging YouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previous state-of-the-art by a large margin. VideoCutLER can also serve as a strong pretrained model for supervised video instance segmentation tasks, exceeding DINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.",
        "page": "http://arxiv.org/abs/2308.14710",
        "pdf": "http://arxiv.org/pdf/2308.14710.pdf"
    },
    {
        "title": "Investigating and Mitigating the Side Effects of Noisy Views for Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios",
        "author": "Jie Xu, Yazhou Ren, Xiaolong Wang, Lei Feng, Zheng Zhang, Gang Niu, Xiaofeng Zhu",
        "abstract": "Multi-view clustering (MVC) aims at exploring category structures among multi-view data in self-supervised manners. Multiple views provide more information than single views and thus existing MVC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical multi-view scenarios. In this paper, we formally investigate the drawback of noisy views and then propose a theoretically grounded deep MVC method (namely MVCAN) to address this issue. Specifically, we propose a novel MVC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a two-level multi-view iterative optimization is designed to generate robust learning targets for refining individual views' representation learning. Theoretical analysis reveals that MVCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on extensive public datasets demonstrate that MVCAN outperforms state-of-the-art methods and is robust against the existence of noisy views.",
        "page": "http://arxiv.org/abs/2303.17245",
        "pdf": "http://arxiv.org/pdf/2303.17245.pdf"
    },
    {
        "title": "Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture",
        "author": "Fei Wang, Dan Guo, Kun Li, Zhun Zhong, Meng Wang",
        "abstract": "Video Motion Magnification (VMM) aims to reveal subtle and imperceptible motion information of objects in the macroscopic world. Prior methods directly model the motion field from the Eulerian perspective by Representation Learning that separates shape and texture or Multi-domain Learning from phase fluctuations. Inspired by the frequency spectrum, we observe that the low-frequency components with stable energy always possess spatial structure and less noise, making them suitable for modeling the subtle motion field. To this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion Magnification with a Multi-level Isomorphic Architecture to capture multi-level high-frequency details and a stable low-frequency structure (motion field) in video space. Since high-frequency details and subtle motions are susceptible to information degradation due to their inherent subtlety and unavoidable external interference from noise, we carefully design Sparse High/Low-pass Filters to enhance the integrity of details and motion structures, and a Sparse Frequency Mixer to promote seamless recoupling. Besides, we innovatively design a contrastive regularization for this task to strengthen the model's ability to discriminate irrelevant features, reducing undesired motion magnification. Extensive experiments on both Real-world and Synthetic Datasets show that our FD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\\times$ and boosts inference speed by 1.68$\\times$ than the latest method. Our code is available at https://github.com/Jiafei127/FD4MM.",
        "page": "http://arxiv.org/abs/2403.07347",
        "pdf": "http://arxiv.org/pdf/2403.07347.pdf"
    },
    {
        "title": "What Moves Together Belongs Together",
        "author": "Jenny Seidenschwarz, Aljo\u0161a O\u0161ep, Francesco Ferroni, Simon Lucey, Laura Leal-Taixe",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images",
        "author": "Yufei Han, Heng Guo, Koki Fukai, Hiroaki Santo, Boxin Shi, Fumio Okura, Zhanyu Ma, Yunpeng Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NAYER: Noisy Layer Data Generation for Efficient and Effective Data-free Knowledge Distillation",
        "author": "Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Quan Tran, Dinh Phung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning",
        "author": "Rongjie Li, Yu Wu, Xuming He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SODA: Bottleneck Diffusion Models for Representation Learning",
        "author": "Drew Hudson, Daniel Zoran, Mateusz Malinowski, Andrew Lampinen, Andrew Jaegle, James McClelland, Loic Matthey, Felix Hill, Alexander Lerchner",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MarkovGen: Structured Prediction for Efficient Text-to-Image Generation",
        "author": "Sadeep Jayasumana, Daniel Glasner, Srikumar Ramalingam, Andreas Veit, Ayan Chakrabarti, Sanjiv Kumar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Data-Free Quantization via Pseudo-label Filtering",
        "author": "Chunxiao Fan, Ziqi Wang, Dan Guo, Meng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fitting Flats to Flats",
        "author": "Gabriel Dogadov, Ugo Finnendahl, Marc Alexa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Bayesian Diffusion Models for 3D Shape Reconstruction",
        "author": "Haiyang Xu, Yu lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu",
        "abstract": "We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction.",
        "page": "http://arxiv.org/abs/2403.06973",
        "pdf": "http://arxiv.org/pdf/2403.06973.pdf"
    },
    {
        "title": "HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild",
        "author": "Supreeth Narasimhaswamy, Huy Anh Nguyen, Lihan Huang, Minh Hoai",
        "abstract": "We address the challenging task of identifying, segmenting, and tracking hand-held objects, which is crucial for applications such as human action segmentation and performance evaluation. This task is particularly challenging due to heavy occlusion, rapid motion, and the transitory nature of objects being hand-held, where an object may be held, released, and subsequently picked up again. To tackle these challenges, we have developed a novel transformer-based architecture called HOIST-Former. HOIST-Former is adept at spatially and temporally segmenting hands and objects by iteratively pooling features from each other, ensuring that the processes of identification, segmentation, and tracking of hand-held objects depend on the hands' positions and their contextual appearance. We further refine HOIST-Former with a contact loss that focuses on areas where hands are in contact with objects. Moreover, we also contribute an in-the-wild video dataset called HOIST, which comprises 4,125 videos complete with bounding boxes, segmentation masks, and tracking IDs for hand-held objects. Through experiments on the HOIST dataset and two additional public datasets, we demonstrate the efficacy of HOIST-Former in segmenting and tracking hand-held objects.",
        "page": "http://arxiv.org/abs/2404.13819",
        "pdf": "http://arxiv.org/pdf/2404.13819.pdf"
    },
    {
        "title": "Towards General Robustness Verification of MaxPool-based Convolutional Neural Networks via Tightening Linear Approximation",
        "author": "Yuan Xiao, Shiqing Ma, Juan Zhai, Chunrong Fang, Jinyuan Jia, Zhenyu Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "WaveMo: Learning Wavefront Modulations to See Through Scattering",
        "author": "Mingyang Xie, Haiyun Guo, Brandon Y. Feng, Lingbo Jin, Ashok Veeraraghavan, Christopher Metzler",
        "abstract": "Imaging through scattering media is a fundamental and pervasive challenge in fields ranging from medical diagnostics to astronomy. A promising strategy to overcome this challenge is wavefront modulation, which induces measurement diversity during image acquisition. Despite its importance, designing optimal wavefront modulations to image through scattering remains under-explored. This paper introduces a novel learning-based framework to address the gap. Our approach jointly optimizes wavefront modulations and a computationally lightweight feedforward \"proxy\" reconstruction network. This network is trained to recover scenes obscured by scattering, using measurements that are modified by these modulations. The learned modulations produced by our framework generalize effectively to unseen scattering scenarios and exhibit remarkable versatility. During deployment, the learned modulations can be decoupled from the proxy network to augment other more computationally expensive restoration algorithms. Through extensive experiments, we demonstrate our approach significantly advances the state of the art in imaging through scattering media. Our project webpage is at https://wavemo-2024.github.io/.",
        "page": "http://arxiv.org/abs/2404.07985",
        "pdf": "http://arxiv.org/pdf/2404.07985.pdf"
    },
    {
        "title": "DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations",
        "author": "Maximilian Augustin, Yannic Neuhaus, Matthias Hein",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Finsler-Laplace-Beltrami Operators with Application to Shape Analysis",
        "author": "Simon Weber, Thomas Dag\u00e8s, Maolin Gao, Daniel Cremers",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting",
        "author": "Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, Sanghyun Woo",
        "abstract": "We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework. Drawing on the Cook-Torrance reflectance model, we have meticulously configured the architecture design to precisely simulate light-surface interactions. Furthermore, to overcome the limitation of scarce high-quality lightstage data, we have developed a self-supervised pre-training strategy. This novel combination of accurate physical modeling and expanded training dataset establishes a new benchmark in relighting realism.",
        "page": "http://arxiv.org/abs/2402.18848",
        "pdf": "http://arxiv.org/pdf/2402.18848.pdf"
    },
    {
        "title": "Sparse Semi-Detr: Sparse Learnable Queries for Semi-Supervised Object Detection",
        "author": "Tahira Shehzadi, Khurram Azeem Hashmi, Didier Stricker, Muhammad Zeshan Afzal",
        "abstract": "In this paper, we address the limitations of the DETR-based semi-supervised object detection (SSOD) framework, particularly focusing on the challenges posed by the quality of object queries. In DETR-based SSOD, the one-to-one assignment strategy provides inaccurate pseudo-labels, while the one-to-many assignments strategy leads to overlapping predictions. These issues compromise training efficiency and degrade model performance, especially in detecting small or occluded objects. We introduce Sparse Semi-DETR, a novel transformer-based, end-to-end semi-supervised object detection solution to overcome these challenges. Sparse Semi-DETR incorporates a Query Refinement Module to enhance the quality of object queries, significantly improving detection capabilities for small and partially obscured objects. Additionally, we integrate a Reliable Pseudo-Label Filtering Module that selectively filters high-quality pseudo-labels, thereby enhancing detection accuracy and consistency. On the MS-COCO and Pascal VOC object detection benchmarks, Sparse Semi-DETR achieves a significant improvement over current state-of-the-art methods that highlight Sparse Semi-DETR's effectiveness in semi-supervised object detection, particularly in challenging scenarios involving small or partially obscured objects.",
        "page": "http://arxiv.org/abs/2404.01819",
        "pdf": "http://arxiv.org/pdf/2404.01819.pdf"
    },
    {
        "title": "EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models",
        "author": "Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, Yang Liu",
        "abstract": "Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to \"think\" from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics.",
        "page": "http://arxiv.org/abs/2311.15596",
        "pdf": "http://arxiv.org/pdf/2311.15596.pdf"
    },
    {
        "title": "Revisiting the Domain Shift and Sample Uncertainty in Multi-source Active Domain Transfer",
        "author": "Wenqiao Zhang, Zheqi Lv",
        "abstract": "Active Domain Adaptation (ADA) aims to maximally boost model adaptation in a new target domain by actively selecting a limited number of target data to annotate.This setting neglects the more practical scenario where training data are collected from multiple sources. This motivates us to target a new and challenging setting of knowledge transfer that extends ADA from a single source domain to multiple source domains, termed Multi-source Active Domain Adaptation (MADA). Not surprisingly, we find that most traditional ADA methods cannot work directly in such a setting, mainly due to the excessive domain gap introduced by all the source domains and thus their uncertainty-aware sample selection can easily become miscalibrated under the multi-domain shifts. Considering this, we propose a Dynamic integrated uncertainty valuation framework(Detective) that comprehensively consider the domain shift between multi-source domains and target domain to detect the informative target samples. Specifically, the leverages a dynamic Domain Adaptation(DA) model that learns how to adapt the model's parameters to fit the union of multi-source domains. This enables an approximate single-source domain modeling by the dynamic model. We then comprehensively measure both domain uncertainty and predictive uncertainty in the target domain to detect informative target samples using evidential deep learning, thereby mitigating uncertainty miscalibration. Furthermore, we introduce a contextual diversity-aware calculator to enhance the diversity of the selected samples. Experiments demonstrate that our solution outperforms existing methods by a considerable margin on three domain adaptation benchmarks.",
        "page": "http://arxiv.org/abs/2311.12905",
        "pdf": "http://arxiv.org/pdf/2311.12905.pdf"
    },
    {
        "title": "Weak-to-Strong 3D Object Detection with X-Ray Distillation",
        "author": "Alexander Gambashidze, Aleksandr Dadukin, Maksim Golyadkin, Maria Razzhivina, Ilya Makarov",
        "abstract": "This paper addresses the critical challenges of sparsity and occlusion in LiDAR-based 3D object detection. Current methods often rely on supplementary modules or specific architectural designs, potentially limiting their applicability to new and evolving architectures. To our knowledge, we are the first to propose a versatile technique that seamlessly integrates into any existing framework for 3D Object Detection, marking the first instance of Weak-to-Strong generalization in 3D computer vision. We introduce a novel framework, X-Ray Distillation with Object-Complete Frames, suitable for both supervised and semi-supervised settings, that leverages the temporal aspect of point cloud sequences. This method extracts crucial information from both previous and subsequent LiDAR frames, creating Object-Complete frames that represent objects from multiple viewpoints, thus addressing occlusion and sparsity. Given the limitation of not being able to generate Object-Complete frames during online inference, we utilize Knowledge Distillation within a Teacher-Student framework. This technique encourages the strong Student model to emulate the behavior of the weaker Teacher, which processes simple and informative Object-Complete frames, effectively offering a comprehensive view of objects as if seen through X-ray vision. Our proposed methods surpass state-of-the-art in semi-supervised learning by 1-1.5 mAP and enhance the performance of five established supervised models by 1-2 mAP on standard autonomous driving datasets, even with default hyperparameters. Code for Object-Complete frames is available here: https://github.com/sakharok13/X-Ray-Teacher-Patching-Tools.",
        "page": "http://arxiv.org/abs/2404.00679",
        "pdf": "http://arxiv.org/pdf/2404.00679.pdf"
    },
    {
        "title": "Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications",
        "author": "Karren Yang, Anurag Ranjan, Jen-Hao Rick Chang, Raviteja Vemulapalli, Oncel Tuzel",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection",
        "author": "Alon Zolfi, Guy AmiT, Amit Baras, Satoru Koda, Ikuya Morikawa, Yuval Elovici, Asaf Shabtai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model",
        "author": "song yiran, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "From Feature to Gaze: A Generalizable Replacement of Linear Layer for Gaze Estimation",
        "author": "Yiwei Bao, Feng Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NC-SDF: Enhancing Indoor Scene Reconstruction Using Neural SDFs with View-Dependent Normal Compensation",
        "author": "Ziyi Chen, Xiaolong Wu, Yu Zhang",
        "abstract": "State-of-the-art neural implicit surface representations have achieved impressive results in indoor scene reconstruction by incorporating monocular geometric priors as additional supervision. However, we have observed that multi-view inconsistency between such priors poses a challenge for high-quality reconstructions. In response, we present NC-SDF, a neural signed distance field (SDF) 3D reconstruction framework with view-dependent normal compensation (NC). Specifically, we integrate view-dependent biases in monocular normal priors into the neural implicit representation of the scene. By adaptively learning and correcting the biases, our NC-SDF effectively mitigates the adverse impact of inconsistent supervision, enhancing both the global consistency and local details in the reconstructions. To further refine the details, we introduce an informative pixel sampling strategy to pay more attention to intricate geometry with higher information content. Additionally, we design a hybrid geometry modeling approach to improve the neural implicit representation. Experiments on synthetic and real-world datasets demonstrate that NC-SDF outperforms existing approaches in terms of reconstruction quality.",
        "page": "http://arxiv.org/abs/2405.00340",
        "pdf": "http://arxiv.org/pdf/2405.00340.pdf"
    },
    {
        "title": "FedSOL: Stabilized Orthogonal Learning with Proximal Restrictions in Federated Learning",
        "author": "Gihun Lee, Minchan Jeong, SangMook Kim, Jaehoon Oh, Se-Young Yun",
        "abstract": "Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when clients have heterogeneous data distributions. This data heterogeneity causes the model to forget the global knowledge acquired from previously sampled clients after being trained on local datasets. Although the introduction of proximal objectives in local updates helps to preserve global knowledge, it can also hinder local learning by interfering with local objectives. To address this problem, we propose a novel method, Federated Stabilized Orthogonal Learning (FedSOL), which adopts an orthogonal learning strategy to balance the two conflicting objectives. FedSOL is designed to identify gradients of local objectives that are inherently orthogonal to directions affecting the proximal objective. Specifically, FedSOL targets parameter regions where learning on the local objective is minimally influenced by proximal weight perturbations. Our experiments demonstrate that FedSOL consistently achieves state-of-the-art performance across various scenarios.",
        "page": "http://arxiv.org/abs/2308.12532",
        "pdf": "http://arxiv.org/pdf/2308.12532.pdf"
    },
    {
        "title": "Learning Multi-dimensional Human Preference for Text-to-Image Generation",
        "author": "Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di ZHANG, Zhongyuan Wang",
        "abstract": "Current metrics for text-to-image models typically rely on statistical metrics which inadequately represent the real preference of humans. Although recent work attempts to learn these preferences via human annotated images, they reduce the rich tapestry of human preference to a single overall score. However, the preference results vary when humans evaluate images with different aspects. Therefore, to learn the multi-dimensional human preferences, we propose the Multi-dimensional Preference Score (MPS), the first multi-dimensional preference scoring model for the evaluation of text-to-image models. The MPS introduces the preference condition module upon CLIP model to learn these diverse preferences. It is trained based on our Multi-dimensional Human Preference (MHP) Dataset, which comprises 918,315 human preference choices across four dimensions (i.e., aesthetics, semantic alignment, detail quality and overall assessment) on 607,541 images. The images are generated by a wide range of latest text-to-image models. The MPS outperforms existing scoring methods across 3 datasets in 4 dimensions, enabling it a promising metric for evaluating and improving text-to-image generation.",
        "page": "http://arxiv.org/abs/2405.14705",
        "pdf": "http://arxiv.org/pdf/2405.14705.pdf"
    },
    {
        "title": "Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguation",
        "author": "Xiaoyang Chen, Hao Zheng, Yuemeng LI, Yuncong Ma, Liang Ma, Hongming Li, Yong Fan",
        "abstract": "A versatile medical image segmentation model applicable to images acquired with diverse equipment and protocols can facilitate model deployment and maintenance. However, building such a model typically demands a large, diverse, and fully annotated dataset, which is challenging to obtain due to the labor-intensive nature of data curation. To address this challenge, we propose a cost-effective alternative that harnesses multi-source data with only partial or sparse segmentation labels for training, substantially reducing the cost of developing a versatile model. We devise strategies for model self-disambiguation, prior knowledge incorporation, and imbalance mitigation to tackle challenges associated with inconsistently labeled multi-source data, including label ambiguity and modality, dataset, and class imbalances. Experimental results on a multi-modal dataset compiled from eight different sources for abdominal structure segmentation have demonstrated the effectiveness and superior performance of our method compared to state-of-the-art alternative approaches. We anticipate that its cost-saving features, which optimize the utilization of existing annotated data and reduce annotation efforts for new data, will have a significant impact in the field.",
        "page": "http://arxiv.org/abs/2311.10696",
        "pdf": "http://arxiv.org/pdf/2311.10696.pdf"
    },
    {
        "title": "Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft",
        "author": "Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai",
        "abstract": "Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax and semantic errors. Further, the Trajectory Analyzer summarizes possible failure causes and provides refinement suggestions according to collected trajectories. In the next round, Reward Designer will further refine and iterate the dense reward function based on feedback. Experiments demonstrate a significant improvement in the success rate and learning efficiency of our agents in complex tasks in Minecraft, such as obtaining diamond with the efficient ability to avoid lava, and efficiently explore trees and animals that are sparse in the plains biome.",
        "page": "http://arxiv.org/abs/2312.09238",
        "pdf": "http://arxiv.org/pdf/2312.09238.pdf"
    },
    {
        "title": "Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation",
        "author": "Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, Jia Deng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",
        "author": "Zhuowan Li, Bhavan Jasani, Peng Tang, Shabnam Ghadar",
        "abstract": "Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics. Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions. In this work, we address the lack of reasoning ability by data augmentation. We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images. The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results highlight the significance of the proposed step-by-step generation. By training with the LLM-augmented data (LAMENDA), we significantly enhance the chart VQA models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets. In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written questions in the ChartQA dataset, which needs strong reasoning. We hope our work underscores the potential of synthetic data and encourages further exploration of data augmentation using LLMs for reasoning-heavy tasks.",
        "page": "http://arxiv.org/abs/2403.16385",
        "pdf": "http://arxiv.org/pdf/2403.16385.pdf"
    },
    {
        "title": "Reconstructing Hands in 3D with Transformers",
        "author": "Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, Jitendra Malik",
        "abstract": "We present an approach that can reconstruct hands in 3D from monocular input. Our approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based architecture and can analyze hands with significantly increased accuracy and robustness compared to previous work. The key to HaMeR's success lies in scaling up both the data used for training and the capacity of the deep network for hand reconstruction. For training data, we combine multiple datasets that contain 2D or 3D hand annotations. For the deep model, we use a large scale Vision Transformer architecture. Our final model consistently outperforms the previous baselines on popular 3D hand pose benchmarks. To further evaluate the effect of our design in non-controlled settings, we annotate existing in-the-wild datasets with 2D hand keypoint annotations. On this newly collected dataset of annotations, HInt, we demonstrate significant improvements over existing baselines. We make our code, data and models available on the project website: https://geopavlakos.github.io/hamer/.",
        "page": "http://arxiv.org/abs/2312.05251",
        "pdf": "http://arxiv.org/pdf/2312.05251.pdf"
    },
    {
        "title": "Systematic comparison of semi-supervised and self-supervised learning for medical image classification",
        "author": "Zhe Huang, Ruijie Jiang, Shuchin Aeron, Michael C. Hughes",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Hierarchical Correlation Clustering and Tree Preserving Embedding",
        "author": "Morteza Haghir Chehreghani, Mostafa Haghir Chehreghani",
        "abstract": "We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose, we first investigate embedding the respective hierarchy to be used for tree-preserving embedding and feature extraction. Thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. Finally, we demonstrate the performance of our methods on several datasets.",
        "page": "http://arxiv.org/abs/2002.07756",
        "pdf": "http://arxiv.org/pdf/2002.07756.pdf"
    },
    {
        "title": "Distilling Vision-Language Models on Millions of Videos",
        "author": "Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, Philipp Kr\u00e4henb\u00fchl, Liangzhe Yuan",
        "abstract": "The recent advance in vision-language models is largely attributed to the abundance of image-text data. We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available. We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data. The resulting video model by video-instruction-tuning (VIIT) is then used to auto-label millions of videos to generate high-quality captions. We show the adapted video-language model performs well on a wide range of video-language benchmarks. For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods. Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models. Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%. As a side product, we generate the largest video caption dataset to date.",
        "page": "http://arxiv.org/abs/2401.06129",
        "pdf": "http://arxiv.org/pdf/2401.06129.pdf"
    },
    {
        "title": "Spatial-Aware Regression for Keypoint Localization",
        "author": "Dongkai Wang, Shiliang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Shadow-Enlightened Image Outpainting",
        "author": "Hang Yu, Ruilin Li, Shaorong Xie, Jiayan Qiu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation",
        "author": "Jiapeng Su, Qi Fan, Wenjie Pei, Guangming Lu, Fanglin Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Few-Shot Object Detection with Foundation Models",
        "author": "Guangxing Han, Ser-Nam Lim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LiDAR-Net: A Real-scanned 3D Point Cloud Dataset for Indoor Scenes",
        "author": "Yanwen Guo, Yuanqi Li, Dayong Ren, Xiaohong Zhang, Jiawei Li, Liang Pu, Changfeng Ma, xiaoyu zhan, Jie Guo, Mingqiang Wei, Yan Zhang, Piaopiao Yu, Shuangyu Yang, Donghao Ji, Huisheng Ye, Hao Sun, Yansong Liu, Yinuo Chen, Jiaqi Zhu, Hongyu Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PREGO: online mistake detection in PRocedural EGOcentric videos",
        "author": "Alessandro Flaborea, Guido M. D&amp;#x27;Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso",
        "abstract": "Promptly identifying procedural errors from egocentric videos in an online setting is highly challenging and valuable for detecting mistakes as soon as they happen. This capability has a wide range of applications across various fields, such as manufacturing and healthcare. The nature of procedural mistakes is open-set since novel types of failures might occur, which calls for one-class classifiers trained on correctly executed procedures. However, no technique can currently detect open-set procedural mistakes online. We propose PREGO, the first online one-class classification model for mistake detection in PRocedural EGOcentric videos. PREGO is based on an online action recognition component to model the current action, and a symbolic reasoning module to predict the next actions. Mistake detection is performed by comparing the recognized current action with the expected future one. We evaluate PREGO on two procedural egocentric video datasets, Assembly101 and Epic-tent, which we adapt for online benchmarking of procedural mistake detection to establish suitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets, respectively.",
        "page": "http://arxiv.org/abs/2404.01933",
        "pdf": "http://arxiv.org/pdf/2404.01933.pdf"
    },
    {
        "title": "M&M VTO: Multi-Garment Virtual Try-On and Editing",
        "author": "Luyang Zhu, Yingwei Li, Nan Liu, Hao Peng, Dawei Yang, Ira Kemelmacher-Shlizerman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Cross-view and Cross-pose Completion for 3D Human Understanding",
        "author": "Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas, Vincent Leroy, Romain BR\u00c9GIER, Philippe Weinzaepfel, Gr\u00e9gory Rogez",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Task-Aware Encoder Control for Deep Video Compression",
        "author": "Xingtong Ge, Jixiang Luo, XINJIE ZHANG, Tongda Xu, Guo Lu, Dailan He, Jing Geng, Yan Wang, Jun Zhang, Hongwei Qin",
        "abstract": "Prior research on deep video compression (DVC) for machine tasks typically necessitates training a unique codec for each specific task, mandating a dedicated decoder per task. In contrast, traditional video codecs employ a flexible encoder controller, enabling the adaptation of a single codec to different tasks through mechanisms like mode prediction. Drawing inspiration from this, we introduce an innovative encoder controller for deep video compression for machines. This controller features a mode prediction and a Group of Pictures (GoP) selection module. Our approach centralizes control at the encoding stage, allowing for adaptable encoder adjustments across different tasks, such as detection and tracking, while maintaining compatibility with a standard pre-trained DVC decoder. Empirical evidence demonstrates that our method is applicable across multiple tasks with various existing pre-trained DVCs. Moreover, extensive experiments demonstrate that our method outperforms previous DVC by about 25% bitrate for different tasks, with only one pre-trained decoder.",
        "page": "http://arxiv.org/abs/2404.04848",
        "pdf": "http://arxiv.org/pdf/2404.04848.pdf"
    },
    {
        "title": "DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception",
        "author": "Yibo Wang, Ruiyuan Gao, Kai Chen, Kaiqiang Zhou, Yingjie CAI, Lanqing Hong, Zhenguo Li, Lihui Jiang, Dit-Yan Yeung, Qiang Xu, Kai Zhang",
        "abstract": "Current perceptive models heavily depend on resource-intensive datasets, prompting the need for innovative solutions. Leveraging recent advances in diffusion models, synthetic data, by constructing image inputs from various annotations, proves beneficial for downstream tasks. While prior methods have separately addressed generative and perceptive models, DetDiffusion, for the first time, harmonizes both, tackling the challenges in generating effective data for perceptive models. To enhance image generation with perceptive models, we introduce perception-aware loss (P.A. loss) through segmentation, improving both quality and controllability. To boost the performance of specific perceptive models, our method customizes data augmentation by extracting and utilizing perception-aware attribute (P.A. Attr) during generation. Experimental results from the object detection task highlight DetDiffusion's superior performance, establishing a new state-of-the-art in layout-guided generation. Furthermore, image syntheses from DetDiffusion can effectively augment training data, significantly enhancing downstream detection performance.",
        "page": "http://arxiv.org/abs/2403.13304",
        "pdf": "http://arxiv.org/pdf/2403.13304.pdf"
    },
    {
        "title": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation",
        "author": "Li Hu",
        "abstract": "Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results.",
        "page": "http://arxiv.org/abs/2311.17117",
        "pdf": "http://arxiv.org/pdf/2311.17117.pdf"
    },
    {
        "title": "Effective Video Mirror Detection with Inconsistent Motion Cues",
        "author": "Alex Warren, Ke Xu, Jiaying Lin, Gary Tam, Rynson W.H. Lau",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance",
        "author": "Kelvin C.K. Chan, Yang Zhao, Xuhui Jia, Ming-Hsuan Yang, Huisheng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Spatio-Temporal Turbulence Mitigation: A Translational Perspective",
        "author": "Xingguang Zhang, Nicholas M Chimitt, Yiheng Chi, Zhiyuan Mao, Stanley H. Chan",
        "abstract": "Recovering images distorted by atmospheric turbulence is a challenging inverse problem due to the stochastic nature of turbulence. Although numerous turbulence mitigation (TM) algorithms have been proposed, their efficiency and generalization to real-world dynamic scenarios remain severely limited. Building upon the intuitions of classical TM algorithms, we present the Deep Atmospheric TUrbulence Mitigation network (DATUM). DATUM aims to overcome major challenges when transitioning from classical to deep learning approaches. By carefully integrating the merits of classical multi-frame TM methods into a deep network structure, we demonstrate that DATUM can efficiently perform long-range temporal aggregation using a recurrent fashion, while deformable attention and temporal-channel attention seamlessly facilitate pixel registration and lucky imaging. With additional supervision, tilt and blur degradation can be jointly mitigated. These inductive biases empower DATUM to significantly outperform existing methods while delivering a tenfold increase in processing speed. A large-scale training dataset, ATSyn, is presented as a co-invention to enable generalization in real turbulence. Our code and datasets are available at https://xg416.github.io/DATUM.",
        "page": "http://arxiv.org/abs/2401.04244",
        "pdf": "http://arxiv.org/pdf/2401.04244.pdf"
    },
    {
        "title": "Composing Object Relations and Attributes for Image-Text Matching",
        "author": "Khoi Pham, Chuong Huynh, Ser-Nam Lim, Abhinav Shrivastava",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Looking 3D: Anomaly Detection with 2D-3D Alignment",
        "author": "Ankan Kumar Bhunia, Changjian Li, Hakan Bilen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "On Train-Test Class Overlap and Detection for Image Retrieval",
        "author": "Chull Hwan Song, Jooyoung Yoon, Taebaek Hwang, Shunghyun Choi, Yeong Hyeon Gu, Yannis Avrithis",
        "abstract": "How important is it for training and evaluation sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with Revisited Oxford and Paris [34], the most popular evaluation set. By comparing the original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are striking. Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.What does it take to focus on objects or interest and ignore background clutter when indexing? Do we need to train an object detector and the representation separately? Do we need location supervision? We introduce Single-stage Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect objects of interest and extract a global image representation. We outperform previous state-of-the-art on both existing training sets and the new RGLDv2-clean. Our dataset is available at https://github.com/dealicious-inc/RGLDv2-clean.",
        "page": "http://arxiv.org/abs/2404.01524",
        "pdf": "http://arxiv.org/pdf/2404.01524.pdf"
    },
    {
        "title": "VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection",
        "author": "Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi",
        "abstract": "Monocular 3D object detection poses a significant challenge in 3D scene understanding due to its inherently ill-posed nature in monocular depth estimation. Existing methods heavily rely on supervised learning using abundant 3D labels, typically obtained through expensive and labor-intensive annotation on LiDAR point clouds. To tackle this problem, we propose a novel weakly supervised 3D object detection framework named VSRD (Volumetric Silhouette Rendering for Detection) to train 3D object detectors without any 3D supervision but only weak 2D supervision. VSRD consists of multi-view 3D auto-labeling and subsequent training of monocular 3D object detectors using the pseudo labels generated in the auto-labeling stage. In the auto-labeling stage, we represent the surface of each instance as a signed distance field (SDF) and render its silhouette as an instance mask through our proposed instance-aware volumetric silhouette rendering. To directly optimize the 3D bounding boxes through rendering, we decompose the SDF of each instance into the SDF of a cuboid and the residual distance field (RDF) that represents the residual from the cuboid. This mechanism enables us to optimize the 3D bounding boxes in an end-to-end manner by comparing the rendered instance masks with the ground truth instance masks. The optimized 3D bounding boxes serve as effective training data for 3D object detection. We conduct extensive experiments on the KITTI-360 dataset, demonstrating that our method outperforms the existing weakly supervised 3D object detection methods. The code is available at https://github.com/skmhrk1209/VSRD.",
        "page": "http://arxiv.org/abs/2404.00149",
        "pdf": "http://arxiv.org/pdf/2404.00149.pdf"
    },
    {
        "title": "Beyond Seen Primitive Concepts and Attribute-Object Compositional Learning",
        "author": "Nirat Saini, Khoi Pham, Abhinav Shrivastava",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Transductive Zero-Shot $\\&$ Few-Shot CLIP",
        "author": "S\u00e9gol\u00e8ne Martin, Yunshi HUANG, Fereshteh Shakeri, Jean-Christophe Pesquet, Ismail Ben Ayed",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SLICE: Stabilized LIME for Consistent Explanations for Image Classification",
        "author": "Revoti Prasad Bora, Kiran Raja, Philipp Terh\u00f6rst, Raymond Veldhuis, Raghavendra Ramachandra",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Contrastive Learning for DeepFake Classification and Localization via Multi-Label Ranking",
        "author": "Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models",
        "author": "Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, Ying Shan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds",
        "author": "Minghao Chen, Junyu Xie, Iro Laina, Andrea Vedaldi",
        "abstract": "We propose a novel feed-forward 3D editing framework called Shap-Editor. Prior research on editing 3D objects primarily concentrated on editing individual objects by leveraging off-the-shelf 2D image editing networks. This is achieved via a process called distillation, which transfers knowledge from the 2D network to 3D assets. Distillation necessitates at least tens of minutes per asset to attain satisfactory editing results, and is thus not very practical. In contrast, we ask whether 3D editing can be carried out directly by a feed-forward network, eschewing test-time optimisation. In particular, we hypothesise that editing can be greatly simplified by first encoding 3D objects in a suitable latent space. We validate this hypothesis by building upon the latent space of Shap-E. We demonstrate that direct 3D editing in this space is possible and efficient by building a feed-forward editor network that only requires approximately one second per edit. Our experiments show that Shap-Editor generalises well to both in-distribution and out-of-distribution 3D assets with different prompts, exhibiting comparable performance with methods that carry out test-time optimisation for each edited instance.",
        "page": "http://arxiv.org/abs/2312.09246",
        "pdf": "http://arxiv.org/pdf/2312.09246.pdf"
    },
    {
        "title": "Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions",
        "author": "Namitha Padmanabhan, Matthew A Gwilliam, Pulkit Kumar, Shishira R Maiya, Max Ehrlich, Abhinav Shrivastava",
        "abstract": "The many variations of Implicit Neural Representations (INRs), where a neural network is trained as a continuous representation of a signal, have tremendous practical utility for downstream tasks including novel view synthesis, video compression, and image superresolution. Unfortunately, the inner workings of these networks are seriously under-studied. Our work, eXplaining the Implicit Neural Canvas (XINC), is a unified framework for explaining properties of INRs by examining the strength of each neuron's contribution to each output pixel. We call the aggregate of these contribution maps the Implicit Neural Canvas and we use this concept to demonstrate that the INRs which we study learn to ''see'' the frames they represent in surprising ways. For example, INRs tend to have highly distributed representations. While lacking high-level object semantics, they have a significant bias for color and edges, and are almost entirely space-agnostic. We arrive at our conclusions by examining how objects are represented across time in video INRs, using clustering to visualize similar neurons across layers and architectures, and show that this is dominated by motion. These insights demonstrate the general usefulness of our analysis framework. Our project page is available at https://namithap10.github.io/xinc.",
        "page": "http://arxiv.org/abs/2401.10217",
        "pdf": "http://arxiv.org/pdf/2401.10217.pdf"
    },
    {
        "title": "Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding",
        "author": "Zhiheng Cheng, Qingyue Wei, Hongru Zhu, Yan Wang, Liangqiong Qu, Wei Shao, Yuyin Zhou",
        "abstract": "The Segment Anything Model (SAM) has garnered significant attention for its versatile segmentation abilities and intuitive prompt-based interface. However, its application in medical imaging presents challenges, requiring either substantial training costs and extensive medical datasets for full model fine-tuning or high-quality prompts for optimal performance. This paper introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient fine-tuning of medical images via a two-stage hierarchical decoding procedure. In the initial stage, H-SAM employs SAM's original decoder to generate a prior probabilistic mask, guiding a more intricate decoding process in the second stage. Specifically, we propose two key designs: 1) A class-balanced, mask-guided self-attention mechanism addressing the unbalanced label distribution, enhancing image embedding; 2) A learnable mask cross-attention mechanism spatially modulating the interplay among different image regions based on the prior mask. Moreover, the inclusion of a hierarchical pixel decoder in H-SAM enhances its proficiency in capturing fine-grained and localized details. This approach enables SAM to effectively integrate learned medical priors, facilitating enhanced adaptation for medical image segmentation with limited samples. Our H-SAM demonstrates a 4.78% improvement in average Dice compared to existing prompt-free SAM variants for multi-organ segmentation using only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM even outperforms state-of-the-art semi-supervised models relying on extensive unlabeled training data across various medical datasets. Our code is available at https://github.com/Cccccczh404/H-SAM.",
        "page": "http://arxiv.org/abs/2403.18271",
        "pdf": "http://arxiv.org/pdf/2403.18271.pdf"
    },
    {
        "title": "DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars",
        "author": "Tobias Kirschstein, Simon Giebenhain, Matthias Nie\u00dfner",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation",
        "author": "Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, Zhijian Song",
        "abstract": "Weakly supervised semantic segmentation (WSSS) with image-level labels aims to achieve segmentation tasks without dense annotations. However, attributed to the frequent coupling of co-occurring objects and the limited supervision from image-level labels, the challenging co-occurrence problem is widely present and leads to false activation of objects in WSSS. In this work, we devise a 'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of image space and feature space. In the image space, we propose to 'separate' the co-occurring objects with image decomposition by subdividing images into patches. Importantly, we assign each patch a category tag from Class Activation Maps (CAMs), which spatially helps remove the co-context bias and guide the subsequent representation. In the feature space, we propose to 'conquer' the false activation by enhancing semantic representation with multi-granularity knowledge contrast. To this end, a dual-teacher-single-student architecture is designed and tag-guided contrast is conducted, which guarantee the correctness of knowledge and further facilitate the discrepancy among co-contexts. We streamline the multi-staged WSSS pipeline end-to-end and tackle this issue without external supervision. Extensive experiments are conducted, validating the efficiency of our method and the superiority over previous single-staged and even multi-staged competitors on PASCAL VOC and MS COCO. Code is available at https://github.com/zwyang6/SeCo.git.",
        "page": "http://arxiv.org/abs/2402.18467",
        "pdf": "http://arxiv.org/pdf/2402.18467.pdf"
    },
    {
        "title": "LocLLM: Exploiting Generalizable Human Keypoint Localization via Large Language Model",
        "author": "Dongkai Wang, shiyu xuan, Shiliang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GARField: Group Anything with Radiance Fields",
        "author": "Chung Min Kim, Mingxuan Wu, Justin Kerr, Ken Goldberg, Matthew Tancik, Angjoo Kanazawa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Grounding Everything: Emerging Localization Properties in Vision-Language Transformers",
        "author": "Walid Bousselham, Felix Petersen, Vittorio Ferrari, Hilde Kuehne",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning",
        "author": "Rui Li, Tobias Fischer, Mattia Segu, Marc Pollefeys, Luc Van Gool, Federico Tombari",
        "abstract": "Recovering the 3D scene geometry from a single view is a fundamental yet ill-posed problem in computer vision. While classical depth estimation methods infer only a 2.5D scene representation limited to the image plane, recent approaches based on radiance fields reconstruct a full 3D representation. However, these methods still struggle with occluded regions since inferring geometry without visual observation requires (i) semantic knowledge of the surroundings, and (ii) reasoning about spatial context. We propose KYN, a novel method for single-view scene reconstruction that reasons about semantic and spatial context to predict each point's density. We introduce a vision-language modulation module to enrich point features with fine-grained semantic information. We aggregate point representations across the scene through a language-guided spatial attention mechanism to yield per-point density predictions aware of the 3D semantic context. We show that KYN improves 3D shape recovery compared to predicting density for each 3D point in isolation. We achieve state-of-the-art results in scene and object reconstruction on KITTI-360, and show improved zero-shot generalization compared to prior work. Project page: https://ruili3.github.io/kyn.",
        "page": "http://arxiv.org/abs/2404.03658",
        "pdf": "http://arxiv.org/pdf/2404.03658.pdf"
    },
    {
        "title": "Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer",
        "author": "Yuwen Tan, Qinhao Zhou, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li",
        "abstract": "Class-incremental learning (CIL) aims to enable models to continuously learn new classes while overcoming catastrophic forgetting. The introduction of pre-trained models has brought new tuning paradigms to CIL. In this paper, we revisit different parameter-efficient tuning (PET) methods within the context of continual learning. We observe that adapter tuning demonstrates superiority over prompt-based methods, even without parameter expansion in each learning session. Motivated by this, we propose incrementally tuning the shared adapter without imposing parameter update constraints, enhancing the learning capacity of the backbone. Additionally, we employ feature sampling from stored prototypes to retrain a unified classifier, further improving its performance. We estimate the semantic shift of old prototypes without access to past samples and update stored prototypes session by session. Our proposed method eliminates model expansion and avoids retaining any image samples. It surpasses previous pre-trained model-based CIL methods and demonstrates remarkable continual learning capabilities. Experimental results on five CIL benchmarks validate the effectiveness of our approach, achieving state-of-the-art (SOTA) performance.",
        "page": "http://arxiv.org/abs/2403.19979",
        "pdf": "http://arxiv.org/pdf/2403.19979.pdf"
    },
    {
        "title": "Error Detection in Egocentric Procedural Task Videos",
        "author": "Shih-Po Lee, Zijia Lu, Zekun Zhang, Minh Hoai, Ehsan Elhamifar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Regressor-Segmenter Mutual Prompt Learning for Crowd Counting",
        "author": "Mingyue Guo, Li Yuan, Zhaoyi Yan, Binghui Chen, Yaowei Wang, Qixiang Ye",
        "abstract": "Crowd counting has achieved significant progress by training regressors to predict instance positions. In heavily crowded scenarios, however, regressors are challenged by uncontrollable annotation variance, which causes density map bias and context information inaccuracy. In this study, we propose mutual prompt learning (mPrompt), which leverages a regressor and a segmenter as guidance for each other, solving bias and inaccuracy caused by annotation variance while distinguishing foreground from background. In specific, mPrompt leverages point annotations to tune the segmenter and predict pseudo head masks in a way of point prompt learning. It then uses the predicted segmentation masks, which serve as spatial constraint, to rectify biased point annotations as context prompt learning. mPrompt defines a way of mutual information maximization from prompt learning, mitigating the impact of annotation variance while improving model accuracy. Experiments show that mPrompt significantly reduces the Mean Average Error (MAE), demonstrating the potential to be general framework for down-stream vision tasks.",
        "page": "http://arxiv.org/abs/2312.01711",
        "pdf": "http://arxiv.org/pdf/2312.01711.pdf"
    },
    {
        "title": "Efficient Detection of Long Consistent Cycles and its Application to Distributed Synchronization",
        "author": "Shaohan Li, Yunpeng Shi, Gilad Lerman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Efficient Privacy-Preserving Visual Localization Using 3D Ray Clouds",
        "author": "Heejoon Moon, Chunghwan Lee, Je Hyeong Hong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior",
        "author": "Chen Cheng, Xiaofeng Yang, Fan Yang, Chengzeng Feng, ZHOUJIE FU, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI",
        "author": "Sean I. Young, Ya\u00ebl Balbastre, Bruce Fischl, Polina Golland, Juan Iglesias",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LoS: Local Structure Guided Stereo Matching",
        "author": "Kunhong Li, Longguang Wang, Ye Zhang, Kaiwen Xue, Shunbo Zhou, Yulan Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error",
        "author": "Jonas Ricker, Denis Lukovnikov, Asja Fischer",
        "abstract": "With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs, including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions. We release our code and data at https://github.com/jonasricker/aeroblade .",
        "page": "http://arxiv.org/abs/2401.17879",
        "pdf": "http://arxiv.org/pdf/2401.17879.pdf"
    },
    {
        "title": "Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training",
        "author": "Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, Hengshuang Zhao",
        "abstract": "The rapid advancement of deep learning models often attributes to their ability to leverage massive training data. In contrast, such privilege has not yet fully benefited 3D deep learning, mainly due to the limited availability of large-scale 3D datasets. Merging multiple available data sources and letting them collaboratively train a single model is a potential solution. However, due to the large domain gap between 3D point cloud datasets, such mixed supervision could adversely affect the model's performance and lead to degenerated performance (i.e., negative transfer) compared to single-dataset training. In view of this challenge, we introduce Point Prompt Training (PPT), a novel framework for multi-dataset synergistic learning in the context of 3D representation learning that supports multiple pre-training paradigms. Based on this framework, we propose Prompt-driven Normalization, which adapts the model to different datasets with domain-specific prompts and Language-guided Categorical Alignment that decently unifies the multiple-dataset label spaces by leveraging the relationship between label text. Extensive experiments verify that PPT can overcome the negative transfer associated with synergistic learning and produce generalizable representations. Notably, it achieves state-of-the-art performance on each dataset using a single weight-shared model with supervised multi-dataset training. Moreover, when served as a pre-training framework, it outperforms other pre-training approaches regarding representation quality and attains remarkable state-of-the-art performance across over ten diverse downstream tasks spanning both indoor and outdoor 3D scenarios.",
        "page": "http://arxiv.org/abs/2308.09718",
        "pdf": "http://arxiv.org/pdf/2308.09718.pdf"
    },
    {
        "title": "Meta-Point Learning and Refining for Category-Agnostic Pose Estimation",
        "author": "Junjie Chen, Jiebin Yan, Yuming Fang, Li Niu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation",
        "author": "Ce Zhang, Simon Stepputtis, Joseph Campbell, Katia Sycara, Yaqi Xie",
        "abstract": "Being able to understand visual scenes is a precursor for many downstream tasks, including autonomous driving, robotics, and other vision-based approaches. A common approach enabling the ability to reason over visual data is Scene Graph Generation (SGG); however, many existing approaches assume undisturbed vision, i.e., the absence of real-world corruptions such as fog, snow, smoke, as well as non-uniform perturbations like sun glare or water drops. In this work, we propose a novel SGG benchmark containing procedurally generated weather corruptions and other transformations over the Visual Genome dataset. Further, we introduce a corresponding approach, Hierarchical Knowledge Enhanced Robust Scene Graph Generation (HiKER-SGG), providing a strong baseline for scene graph generation under such challenging setting. At its core, HiKER-SGG utilizes a hierarchical knowledge graph in order to refine its predictions from coarse initial estimates to detailed predictions. In our extensive experiments, we show that HiKER-SGG does not only demonstrate superior performance on corrupted images in a zero-shot manner, but also outperforms current state-of-the-art methods on uncorrupted SGG tasks. Code is available at https://github.com/zhangce01/HiKER-SGG.",
        "page": "http://arxiv.org/abs/2403.12033",
        "pdf": "http://arxiv.org/pdf/2403.12033.pdf"
    },
    {
        "title": "GenTron: Diffusion Transformers for Image and Video Generation",
        "author": "Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, Juan-Manuel P\u00e9rez-R\u00faa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos",
        "author": "Changan Chen, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman",
        "abstract": "We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when any one pair does not. We show our approach can successfully discover how the long tail of human actions sound from egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks.",
        "page": "http://arxiv.org/abs/2404.05206",
        "pdf": "http://arxiv.org/pdf/2404.05206.pdf"
    },
    {
        "title": "SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples",
        "author": "Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan-Moreno, Anahita Bhiwandiwalla, Vasudev Lal",
        "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intersectional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender). Through our over-generate-then-filter methodology, we produce SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics. We conduct extensive experiments to demonstrate the usefulness of our generated dataset for probing and mitigating intersectional social biases in state-of-the-art VLMs.",
        "page": "http://arxiv.org/abs/2312.00825",
        "pdf": "http://arxiv.org/pdf/2312.00825.pdf"
    },
    {
        "title": "JDEC: JPEG Decoding via Enhanced Continuous Cosine Coefficients",
        "author": "Woo Kyoung Han, Sunghoon Im, Jaedeok Kim, Kyong Hwan Jin",
        "abstract": "We propose a practical approach to JPEG image decoding, utilizing a local implicit neural representation with continuous cosine formulation. The JPEG algorithm significantly quantizes discrete cosine transform (DCT) spectra to achieve a high compression rate, inevitably resulting in quality degradation while encoding an image. We have designed a continuous cosine spectrum estimator to address the quality degradation issue that restores the distorted spectrum. By leveraging local DCT formulations, our network has the privilege to exploit dequantization and upsampling simultaneously. Our proposed model enables decoding compressed images directly across different quality factors using a single pre-trained model without relying on a conventional JPEG decoder. As a result, our proposed network achieves state-of-the-art performance in flexible color image JPEG artifact removal tasks. Our source code is available at https://github.com/WooKyoungHan/JDEC.",
        "page": "http://arxiv.org/abs/2404.05558",
        "pdf": "http://arxiv.org/pdf/2404.05558.pdf"
    },
    {
        "title": "Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation",
        "author": "Renshuai Liu, Bowen Ma, Wei Zhang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Xuan Cheng",
        "abstract": "In human-centric content generation, the pre-trained text-to-image models struggle to produce user-wanted portrait images, which retain the identity of individuals while exhibiting diverse expressions. This paper introduces our efforts towards personalized face generation. To this end, we propose a novel multi-modal face generation framework, capable of simultaneous identity-expression control and more fine-grained expression synthesis. Our expression control is so sophisticated that it can be specialized by the fine-grained emotional vocabulary. We devise a novel diffusion model that can undertake the task of simultaneously face swapping and reenactment. Due to the entanglement of identity and expression, it's nontrivial to separately and precisely control them in one framework, thus has not been explored yet. To overcome this, we propose several innovative designs in the conditional diffusion model, including balancing identity and expression encoder, improved midpoint sampling, and explicitly background conditioning. Extensive experiments have demonstrated the controllability and scalability of the proposed framework, in comparison with state-of-the-art text-to-image, face swapping, and face reenactment methods.",
        "page": "http://arxiv.org/abs/2401.01207",
        "pdf": "http://arxiv.org/pdf/2401.01207.pdf"
    },
    {
        "title": "Geometrically-informed aggregation for zero-shot point cloud understanding",
        "author": "Guofeng Mei, Luigi Riz, Yiming Wang, Fabio Poiesi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding",
        "author": "Yonglu Li, Xiaoqian Wu, Xinpeng Liu, Zehao Wang, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Xudong LU, Jingru Tan, Cewu Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsupervised 3D Structure Inference from Category-Specific Image Collections",
        "author": "Weikang Wang, Dongliang Cao, Florian Bernard",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in Neural Radiance Fields",
        "author": "Joshua Ahn, Haochen Wang, Raymond A. Yeh, Greg Shakhnarovich",
        "abstract": "Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance fields, i.e., the densities double when scene size is halved, and vice versa. We call this property alpha invariance. For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance. We revisit a few popular radiance field models and find that these systems use various heuristics to deal with issues arising from scene scaling. We test their behaviors and show our recipe to be more robust.",
        "page": "http://arxiv.org/abs/2404.02155",
        "pdf": "http://arxiv.org/pdf/2404.02155.pdf"
    },
    {
        "title": "Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation",
        "author": "Qiyuan Dai, Sibei Yang",
        "abstract": "Referring image segmentation (RIS) aims to precisely segment referents in images through corresponding natural language expressions, yet relying on cost-intensive mask annotations. Weakly supervised RIS thus learns from image-text pairs to pixel-level semantics, which is challenging for segmenting fine-grained masks. A natural approach to enhancing segmentation precision is to empower weakly supervised RIS with the image segmentation foundation model SAM. Nevertheless, we observe that simply integrating SAM yields limited benefits and can even lead to performance regression due to the inevitable noise issues and challenges in excessive focus on object parts. In this paper, we present an innovative framework, Point PrompTing (PPT), incorporated with the proposed multi-source curriculum learning strategy to address these challenges. Specifically, the core of PPT is a point generator that not only harnesses CLIP's text-image alignment capability and SAM's powerful mask generation ability but also generates negative point prompts to address the noisy and excessive focus issues inherently and effectively. In addition, we introduce a curriculum learning strategy with object-centric images to help PPT gradually learn from simpler yet precise semantic alignment to more complex RIS. Experiments demonstrate that our PPT significantly and consistently outperforms prior weakly supervised techniques on mIoU by 11.34%, 14.14%, and 6.97% across RefCOCO, RefCOCO+, and G-Ref, respectively.",
        "page": "http://arxiv.org/abs/2404.11998",
        "pdf": "http://arxiv.org/pdf/2404.11998.pdf"
    },
    {
        "title": "RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation",
        "author": "Zeyuan Yang, LIU JIAGENG, Peihao Chen, Anoop Cherian, Tim Marks, Jonathan Le Roux, Chuang Gan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Sharingan: A Transformer Architecture for Multi-Person Gaze Following",
        "author": "Samy Tafasca, Anshul Gupta, Jean-marc Odobez",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Implicit Motion Function",
        "author": "Yue Gao, Jiahao Li, Lei Chu, Yan Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with Multi-Resolution Feature Perturbation",
        "author": "Sumanth Udupa, Prajwal Gurunath, Aniruddh Sikdar, Suresh Sundaram",
        "abstract": "Deep neural networks have shown exemplary performance on semantic scene understanding tasks on source domains, but due to the absence of style diversity during training, enhancing performance on unseen target domains using only single source domain data remains a challenging task. Generation of simulated data is a feasible alternative to retrieving large style-diverse real-world datasets as it is a cumbersome and budget-intensive process. However, the large domain-specfic inconsistencies between simulated and real-world data pose a significant generalization challenge in semantic segmentation. In this work, to alleviate this problem, we propose a novel MultiResolution Feature Perturbation (MRFP) technique to randomize domain-specific fine-grained features and perturb style of coarse features. Our experimental results on various urban-scene segmentation datasets clearly indicate that, along with the perturbation of style-information, perturbation of fine-feature components is paramount to learn domain invariant robust feature maps for semantic segmentation models. MRFP is a simple and computationally efficient, transferable module with no additional learnable parameters or objective functions, that helps state-of-the-art deep neural networks to learn robust domain invariant features for simulation-to-real semantic segmentation.",
        "page": "http://arxiv.org/abs/2311.18331",
        "pdf": "http://arxiv.org/pdf/2311.18331.pdf"
    },
    {
        "title": "MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World",
        "author": "Yining Hong, Zishuo Zheng, Peihao Chen, Yian Wang, Junyan Li, Chuang Gan",
        "abstract": "Human beings possess the capability to multiply a melange of multisensory cues while actively exploring and interacting with the 3D world. Current multi-modal large language models, however, passively absorb sensory data as inputs, lacking the capacity to actively interact with the objects in the 3D environment and dynamically collect their multisensory information. To usher in the study of this area, we propose MultiPLY, a multisensory embodied large language model that could incorporate multisensory interactive data, including visual, audio, tactile, and thermal information into large language models, thereby establishing the correlation among words, actions, and percepts. To this end, we first collect Multisensory Universe, a large-scale multisensory interaction dataset comprising 500k data by deploying an LLM-powered embodied agent to engage with the 3D environment. To perform instruction tuning with pre-trained LLM on such generated data, we first encode the 3D scene as abstracted object-centric representations and then introduce action tokens denoting that the embodied agent takes certain actions within the environment, as well as state tokens that represent the multisensory state observations of the agent at each time step. In the inference time, MultiPLY could generate action tokens, instructing the agent to take the action in the environment and obtain the next multisensory state observation. The observation is then appended back to the LLM via state tokens to generate subsequent text or action tokens. We demonstrate that MultiPLY outperforms baselines by a large margin through a diverse set of embodied tasks involving object retrieval, tool use, multisensory captioning, and task decomposition.",
        "page": "http://arxiv.org/abs/2401.08577",
        "pdf": "http://arxiv.org/pdf/2401.08577.pdf"
    },
    {
        "title": "ChAda-ViT : Channel Adaptive Attention for Joint Representation Learning of Heterogeneous Microscopy Images",
        "author": "Nicolas Bourriez, Ihab Bendidi, Cohen Ethan, Gabriel Watkinson, Maxime Sanchez, Guillaume Bollot, Auguste Genovesio",
        "abstract": "Unlike color photography images, which are consistently encoded into RGB channels, biological images encompass various modalities, where the type of microscopy and the meaning of each channel varies with each experiment. Importantly, the number of channels can range from one to a dozen and their correlation is often comparatively much lower than RGB, as each of them brings specific information content. This aspect is largely overlooked by methods designed out of the bioimage field, and current solutions mostly focus on intra-channel spatial attention, often ignoring the relationship between channels, yet crucial in most biological applications. Importantly, the variable channel type and count prevent the projection of several experiments to a unified representation for large scale pre-training. In this study, we propose ChAda-ViT, a novel Channel Adaptive Vision Transformer architecture employing an Inter-Channel Attention mechanism on images with an arbitrary number, order and type of channels. We also introduce IDRCell100k, a bioimage dataset with a rich set of 79 experiments covering 7 microscope modalities, with a multitude of channel types, and channel counts varying from 1 to 10 per experiment. Our proposed architecture, trained in a self-supervised manner, outperforms existing approaches in several biologically relevant downstream tasks. Additionally, it can be used to bridge the gap for the first time between assays with different microscopes, channel numbers or types by embedding various image and experimental modalities into a unified biological image representation. The latter should facilitate interdisciplinary studies and pave the way for better adoption of deep learning in biological image-based analyses. Code and Data to be released soon.",
        "page": "http://arxiv.org/abs/2311.15264",
        "pdf": "http://arxiv.org/pdf/2311.15264.pdf"
    },
    {
        "title": "Depth-Aware Concealed Crop Detection in Dense Agricultural Scenes",
        "author": "Liqiong Wang, Jinyu Yang, Yanfu Zhang, Fangyi Wang, Feng Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diffusion Models Without Attention",
        "author": "Jing Nathan Yan, Jiatao Gu, Alexander Rush",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffuseMix: Label-Preserving Data Augmentation with Diffusion Models",
        "author": "Khawar Islam, Muhammad Zaigham Zaheer, Arif Mahmood, Karthik Nandakumar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields with Diffusion Models",
        "author": "Meng-Li Shih, Wei-Chiu Ma, Lorenzo Boyice, Aleksander Holynski, Forrester Cole, Brian Curless, Janne Kontkanen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Real-Time Simulated Avatar from Head-Mounted Sensors",
        "author": "Zhengyi Luo, Jinkun Cao, Rawal Khirodkar, Alexander Winkler, Jing Huang, Kris Kitani, Weipeng Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Mining Supervision for Dynamic Regions in Self-Supervised Monocular Depth Estimation",
        "author": "Hoang Chuong Nguyen, Tianyu Wang, Jose M. Alvarez, Miaomiao Liu",
        "abstract": "This paper focuses on self-supervised monocular depth estimation in dynamic scenes trained on monocular videos. Existing methods jointly estimate pixel-wise depth and motion, relying mainly on an image reconstruction loss. Dynamic regions1 remain a critical challenge for these methods due to the inherent ambiguity in depth and motion estimation, resulting in inaccurate depth estimation. This paper proposes a self-supervised training framework exploiting pseudo depth labels for dynamic regions from training data. The key contribution of our framework is to decouple depth estimation for static and dynamic regions of images in the training data. We start with an unsupervised depth estimation approach, which provides reliable depth estimates for static regions and motion cues for dynamic regions and allows us to extract moving object information at the instance level. In the next stage, we use an object network to estimate the depth of those moving objects assuming rigid motions. Then, we propose a new scale alignment module to address the scale ambiguity between estimated depths for static and dynamic regions. We can then use the depth labels generated to train an end-to-end depth estimation network and improve its performance. Extensive experiments on the Cityscapes and KITTI datasets show that our self-training strategy consistently outperforms existing self/unsupervised depth estimation methods.",
        "page": "http://arxiv.org/abs/2404.14908",
        "pdf": "http://arxiv.org/pdf/2404.14908.pdf"
    },
    {
        "title": "AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation",
        "author": "Haonan Wang, Qixiang ZHANG, Yi Li, Xiaomeng Li",
        "abstract": "Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly complicated training pipeline designs. It can also be regarded as a flexible bottleneck module that can be seamlessly integrated into a general transformer-based segmentation model. The proposed AllSpark outperforms existing methods across all evaluation protocols on Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and model weights are available at: https://github.com/xmed-lab/AllSpark.",
        "page": "http://arxiv.org/abs/2403.01818",
        "pdf": "http://arxiv.org/pdf/2403.01818.pdf"
    },
    {
        "title": "Weakly-Supervised Audio-Visual Video Parsing with Prototype-based Pseudo-Labeling",
        "author": "Kranthi Kumar Rachavarapu, Kalyan Ramakrishnan, A. N. Rajagopalan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "State Space Models for Event Cameras",
        "author": "Nikola Zubic, Mathias Gehrig, Davide Scaramuzza",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TIM: A Time Interval Machine for Audio-Visual Action Recognition",
        "author": "Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen",
        "abstract": "Diverse actions give rise to rich audio-visual signals in long videos. Recent works showcase that the two modalities of audio and video exhibit different temporal extents of events and distinct labels. We address the interplay between the two modalities in long videos by explicitly modelling the temporal extents of audio and visual events. We propose the Time Interval Machine (TIM) where a modality-specific time interval poses as a query to a transformer encoder that ingests a long video input. The encoder then attends to the specified interval, as well as the surrounding context in both modalities, in order to recognise the ongoing action. We test TIM on three long audio-visual video datasets: EPIC-KITCHENS, Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. On EPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantly larger pre-training by 2.9% top-1 action recognition accuracy. Additionally, we show that TIM can be adapted for action detection, using dense multi-scale interval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, and showing strong performance on the Perception Test. Our ablations show the critical role of integrating the two modalities and modelling their time intervals in achieving this performance. Code and models at: https://github.com/JacobChalk/TIM",
        "page": "http://arxiv.org/abs/2404.05559",
        "pdf": "http://arxiv.org/pdf/2404.05559.pdf"
    },
    {
        "title": "READ: Retrieval-Enhanced Asymmetric Diffusion for Motion Planning",
        "author": "Takeru Oba, Matthew Walter, Norimichi Ukita",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models",
        "author": "Rongjie Li, Songyang Zhang, Dahua Lin, Kai Chen, Xuming He",
        "abstract": "Scene graph generation (SGG) aims to parse a visual scene into an intermediate graph representation for downstream reasoning tasks. Despite recent advancements, existing methods struggle to generate scene graphs with novel visual relation concepts. To address this challenge, we introduce a new open-vocabulary SGG framework based on sequence generation. Our framework leverages vision-language pre-trained models (VLM) by incorporating an image-to-graph generation paradigm. Specifically, we generate scene graph sequences via image-to-text generation with VLM and then construct scene graphs from these sequences. By doing so, we harness the strong capabilities of VLM for open-vocabulary SGG and seamlessly integrate explicit relational modeling for enhancing the VL tasks. Experimental results demonstrate that our design not only achieves superior performance with an open vocabulary but also enhances downstream vision-language task performance through explicit relation modeling knowledge.",
        "page": "http://arxiv.org/abs/2404.00906",
        "pdf": "http://arxiv.org/pdf/2404.00906.pdf"
    },
    {
        "title": "Spectrum AUC Difference (SAUCD): Human Aligned 3D Shape Evaluation",
        "author": "Tianyu Luan, Zhong Li, Lele Chen, Xuan Gong, Lichang Chen, Yi Xu, Junsong Yuan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model",
        "author": "Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models",
        "author": "Sanjoy Chowdhury, Sayan Nag, Joseph K J, Balaji Vasan Srinivasan, Dinesh Manocha",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-Space Alignments Towards Universal LiDAR Segmentation",
        "author": "Youquan Liu, Lingdong Kong, Xiaoyang Wu, Runnan Chen, Xin Li, Liang Pan, Ziwei Liu, Yuexin Ma",
        "abstract": "A unified and versatile LiDAR segmentation model with strong robustness and generalizability is desirable for safe autonomous driving perception. This work presents M3Net, a one-of-a-kind framework for fulfilling multi-task, multi-dataset, multi-modality LiDAR segmentation in a universal manner using just a single set of parameters. To better exploit data volume and diversity, we first combine large-scale driving datasets acquired by different types of sensors from diverse scenes and then conduct alignments in three spaces, namely data, feature, and label spaces, during the training. As a result, M3Net is capable of taming heterogeneous data for training state-of-the-art LiDAR segmentation models. Extensive experiments on twelve LiDAR segmentation datasets verify our effectiveness. Notably, using a shared set of parameters, M3Net achieves 75.1%, 83.1%, and 72.4% mIoU scores, respectively, on the official benchmarks of SemanticKITTI, nuScenes, and Waymo Open.",
        "page": "http://arxiv.org/abs/2405.01538",
        "pdf": "http://arxiv.org/pdf/2405.01538.pdf"
    },
    {
        "title": "DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions",
        "author": "Yunxiao Shi, Manish Singh, Hong Cai, Fatih Porikli",
        "abstract": "In this paper, we introduce a novel approach that harnesses both 2D and 3D attentions to enable highly accurate depth completion without requiring iterative spatial propagations. Specifically, we first enhance a baseline convolutional depth completion model by applying attention to 2D features in the bottleneck and skip connections. This effectively improves the performance of this simple network and sets it on par with the latest, complex transformer-based models. Leveraging the initial depths and features from this network, we uplift the 2D features to form a 3D point cloud and construct a 3D point transformer to process it, allowing the model to explicitly learn and exploit 3D geometric features. In addition, we propose normalization techniques to process the point cloud, which improves learning and leads to better accuracy than directly using point transformers off the shelf. Furthermore, we incorporate global attention on downsampled point cloud features, which enables long-range context while still being computationally feasible. We evaluate our method, DeCoTR, on established depth completion benchmarks, including NYU Depth V2 and KITTI, showcasing that it sets new state-of-the-art performance. We further conduct zero-shot evaluations on ScanNet and DDAD benchmarks and demonstrate that DeCoTR has superior generalizability compared to existing approaches.",
        "page": "http://arxiv.org/abs/2403.12202",
        "pdf": "http://arxiv.org/pdf/2403.12202.pdf"
    },
    {
        "title": "Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning",
        "author": "Dipam Goswami, Albin Soutif, Yuyang Liu, Sandesh Kamath, Bart\u0142omiej Twardowski, Joost van de Weijer",
        "abstract": "Continual learning methods are known to suffer from catastrophic forgetting, a phenomenon that is particularly hard to counter for methods that do not store exemplars of previous tasks. Therefore, to reduce potential drift in the feature extractor, existing exemplar-free methods are typically evaluated in settings where the first task is significantly larger than subsequent tasks. Their performance drops drastically in more challenging settings starting with a smaller first task. To address this problem of feature drift estimation for exemplar-free methods, we propose to adversarially perturb the current samples such that their embeddings are close to the old class prototypes in the old model embedding space. We then estimate the drift in the embedding space from the old to the new model using the perturbed images and compensate the prototypes accordingly. We exploit the fact that adversarial samples are transferable from the old to the new feature space in a continual learning setting. The generation of these images is simple and computationally cheap. We demonstrate in our experiments that the proposed approach better tracks the movement of prototypes in embedding space and outperforms existing methods on several standard continual learning benchmarks as well as on fine-grained datasets. Code is available at https://github.com/dipamgoswami/ADC.",
        "page": "http://arxiv.org/abs/2405.19074",
        "pdf": "http://arxiv.org/pdf/2405.19074.pdf"
    },
    {
        "title": "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers",
        "author": "Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, Sergey Tulyakov",
        "abstract": "The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video. Next, we finetune a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation. In this way, we get 70M videos paired with high-quality text captions. We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation. The models trained on the proposed data score substantially better on the majority of metrics across all the tasks.",
        "page": "http://arxiv.org/abs/2402.19479",
        "pdf": "http://arxiv.org/pdf/2402.19479.pdf"
    },
    {
        "title": "HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models",
        "author": "Mengcheng Li, Hongwen Zhang, Yuxiang Zhang, Ruizhi Shao, Tao Yu, Yebin Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FSC: Few-point Shape Completion",
        "author": "Xianzu Wu, Xianfeng Wu, Tianyu Luan, Yajing Bai, Zhongyuan Lai, Junsong Yuan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AV-RIR: Audio-Visual Room Impulse Response Estimation",
        "author": "Anton Ratnarajah, Sreyan Ghosh, Sonal Kumar, Purva Chiniya, Dinesh Manocha",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image Translation",
        "author": "Sidi Wu, Yizi Chen, Loic Landrieu, Nicolas Gonthier, Samuel Mermet, Lorenz Hurni, Konrad Schindler",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GLiDR: Topologically Regularized Graph Generative Network for Sparse LiDAR Point Clouds",
        "author": "Prashant Kumar, Kshitij Madhav Bhat, Vedang Bhupesh Shenvi Nadkarni, Prem Kalra",
        "abstract": "Sparse LiDAR point clouds cause severe loss of detail of static structures and reduce the density of static points available for navigation. Reduced density can be detrimental to navigation under several scenarios. We observe that despite high sparsity, in most cases, the global topology of LiDAR outlining the static structures can be inferred. We utilize this property to obtain a backbone skeleton of a LiDAR scan in the form of a single connected component that is a proxy to its global topology. We utilize the backbone to augment new points along static structures to overcome sparsity. Newly introduced points could correspond to existing static structures or to static points that were earlier obstructed by dynamic objects. To the best of our knowledge, we are the first to use such a strategy for sparse LiDAR point clouds. Existing solutions close to our approach fail to identify and preserve the global static LiDAR topology and generate sub-optimal points. We propose GLiDR, a Graph Generative network that is topologically regularized using 0-dimensional Persistent Homology ($\\mathcal{PH}$) constraints. This enables GLiDR to introduce newer static points along a topologically consistent global static LiDAR backbone. GLiDR generates precise static points using $32\\times$ sparser dynamic scans and performs better than the baselines across three datasets. GLiDR generates a valuable byproduct - an accurate binary segmentation mask of static and dynamic objects that are helpful for navigation planning and safety in constrained environments. The newly introduced static points allow GLiDR to outperform LiDAR-based navigation using SLAM in several settings. Source code is available at https://kshitijbhat.github.io/glidr",
        "page": "http://arxiv.org/abs/2312.00068",
        "pdf": "http://arxiv.org/pdf/2312.00068.pdf"
    },
    {
        "title": "ACT-Diffusion: Efficient Adversarial Consistency Training for One-step Diffusion Models",
        "author": "Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Xu, Heng Tao Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Accurate Training Data for Occupancy Map Prediction in Automated Driving using Evidence Theory",
        "author": "Jonas K\u00e4lble, Sascha Wirges, Maxim Tatarchenko, Eddy Ilg",
        "abstract": "Automated driving fundamentally requires knowledge about the surrounding geometry of the scene. Modern approaches use only captured images to predict occupancy maps that represent the geometry. Training these approaches requires accurate data that may be acquired with the help of LiDAR scanners. We show that the techniques used for current benchmarks and training datasets to convert LiDAR scans into occupancy grid maps yield very low quality, and subsequently present a novel approach using evidence theory that yields more accurate reconstructions. We demonstrate that these are superior by a large margin, both qualitatively and quantitatively, and that we additionally obtain meaningful uncertainty estimates. When converting the occupancy maps back to depth estimates and comparing them with the raw LiDAR measurements, our method yields a MAE improvement of 30% to 52% on nuScenes and 53% on Waymo over other occupancy ground-truth data. Finally, we use the improved occupancy maps to train a state-of-the-art occupancy prediction method and demonstrate that it improves the MAE by 25% on nuScenes.",
        "page": "http://arxiv.org/abs/2405.10575",
        "pdf": "http://arxiv.org/pdf/2405.10575.pdf"
    },
    {
        "title": "FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio",
        "author": "Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun",
        "abstract": "In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at https://github.com/modelscope/facechain.",
        "page": "http://arxiv.org/abs/2403.01901",
        "pdf": "http://arxiv.org/pdf/2403.01901.pdf"
    },
    {
        "title": "Backdoor Defense via Test-Time Detecting and Repairing",
        "author": "Jiyang Guan, Jian Liang, Ran He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Clustering for Protein Representation Learning",
        "author": "Ruijie Quan, Wenguan Wang, Fan Ma, Hehe Fan, Yi Yang",
        "abstract": "Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of the protein. We evaluate on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance.",
        "page": "http://arxiv.org/abs/2404.00254",
        "pdf": "http://arxiv.org/pdf/2404.00254.pdf"
    },
    {
        "title": "RoDLA: Benchmarking the Robustness of Document Layout Analysis Models",
        "author": "Yufan Chen, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ruiping Liu, Philip H.S. Torr, Rainer Stiefelhagen",
        "abstract": "Before developing a Document Layout Analysis (DLA) model in real-world applications, conducting comprehensive robustness testing is essential. However, the robustness of DLA models remains underexplored in the literature. To address this, we are the first to introduce a robustness benchmark for DLA models, which includes 450K document images of three datasets. To cover realistic corruptions, we propose a perturbation taxonomy with 36 common document perturbations inspired by real-world document processing. Additionally, to better understand document perturbation impacts, we propose two metrics, Mean Perturbation Effect (mPE) for perturbation assessment and Mean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we introduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA), which improves attention mechanisms to boost extraction of robust features. Experiments on the proposed benchmarks (PubLayNet-P, DocLayNet-P, and M$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of 115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA achieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.",
        "page": "http://arxiv.org/abs/2403.14442",
        "pdf": "http://arxiv.org/pdf/2403.14442.pdf"
    },
    {
        "title": "X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model",
        "author": "Lingmin Ran, Xiaodong Cun, Jia-Wei Liu, Rui Zhao, Song Zijie, Xintao Wang, Jussi Keppo, Mike Zheng Shou",
        "abstract": "We introduce X-Adapter, a universal upgrader to enable the pretrained plug-and-play modules (e.g., ControlNet, LoRA) to work directly with the upgraded text-to-image diffusion model (e.g., SDXL) without further retraining. We achieve this goal by training an additional network to control the frozen upgraded model with the new text-image data pairs. In detail, X-Adapter keeps a frozen copy of the old model to preserve the connectors of different plugins. Additionally, X-Adapter adds trainable mapping layers that bridge the decoders from models of different versions for feature remapping. The remapped features will be used as guidance for the upgraded model. To enhance the guidance ability of X-Adapter, we employ a null-text training strategy for the upgraded model. After training, we also introduce a two-stage denoising strategy to align the initial latents of X-Adapter and the upgraded model. Thanks to our strategies, X-Adapter demonstrates universal compatibility with various plugins and also enables plugins of different versions to work together, thereby expanding the functionalities of diffusion community. To verify the effectiveness of the proposed method, we conduct extensive experiments and the results show that X-Adapter may facilitate wider application in the upgraded foundational diffusion model.",
        "page": "http://arxiv.org/abs/2312.02238",
        "pdf": "http://arxiv.org/pdf/2312.02238.pdf"
    },
    {
        "title": "Label Propagation for Zero-shot Classification with Vision-Language Models",
        "author": "Vladan Stojni\u0107, Yannis Kalantidis, Giorgos Tolias",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation",
        "author": "Zhipeng Du, Miaojing Shi, Jiankang Deng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CommonCanvas: Open Diffusion Models Trained on Creative-Commons Images",
        "author": "Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, Volodymyr Kuleshov",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies",
        "author": "Lingdong Kong, Youquan Liu, Lai Xing Ng, Benoit Cottereau, Wei Tsang Ooi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "$\\mathcal{Z}^*$: Zero-shot $\\underline{S}$tyle $\\underline{T}$ransfer via $\\underline{A}$ttention $\\underline{R}$eweighting",
        "author": "Yingying Deng, Xiangyu He, Fan Tang, Weiming Dong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Cross-spectral Gated-RGB Stereo Depth Estimation",
        "author": "Samuel Brucker, Stefanie Walz, Mario Bijelic, Felix Heide",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ConCon-Chi: Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks",
        "author": "Andrea Rosasco, Stefano Berti, Giulia Pasquale, Damiano Malafronte, Shogo Sato, Hiroyuki Segawa, Tetsugo Inada, Lorenzo Natale",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous and Instruction-guided Driving",
        "author": "Brian Yang, Huangyuan Su, Nikolaos Gkanatsios, Tsung-Wei Ke, Ayush Jain, Jeff Schneider, Katerina Fragkiadaki",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Flow-Guided Online Stereo Rectification for Wide Baseline Stereo",
        "author": "Anush Kumar, Fahim Mannan, Omid Hosseini Jafari, Shile Li, Felix Heide",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning",
        "author": "Zhe Li, Laurence Yang, Bocheng Ren, Xin Nie, Zhangyang Gao, Cheng Tan, Stan Z. Li",
        "abstract": "The scarcity of annotated data has sparked significant interest in unsupervised pre-training methods that leverage medical reports as auxiliary signals for medical visual representation learning. However, existing research overlooks the multi-granularity nature of medical visual representation and lacks suitable contrastive learning techniques to improve the models' generalizability across different granularities, leading to the underutilization of image-text information. To address this, we propose MLIP, a novel framework leveraging domain-specific medical knowledge as guiding signals to integrate language information into the visual domain through image-text contrastive learning. Our model includes global contrastive learning with our designed divergence encoder, local token-knowledge-patch alignment contrastive learning, and knowledge-guided category-level contrastive learning with expert knowledge. Experimental evaluations reveal the efficacy of our model in enhancing transfer performance for tasks such as image classification, object detection, and semantic segmentation. Notably, MLIP surpasses state-of-the-art methods even with limited annotated data, highlighting the potential of multimodal pre-training in advancing medical representation learning.",
        "page": "http://arxiv.org/abs/2402.02045",
        "pdf": "http://arxiv.org/pdf/2402.02045.pdf"
    },
    {
        "title": "Mosaic-SDF for 3D Generative Models",
        "author": "Lior Yariv, Omri Puny, Oran Gafni, Yaron Lipman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models",
        "author": "Matthew Kowal, Richard P. Wildes, Kosta Derpanis",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation",
        "author": "Linglin Jing, Yiming Ding, Yunpeng Gao, Zhigang Wang, Xu Yan, Dong Wang, Gerald Schaefer, Hui Fang, Bin Zhao, Xuelong Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral Pedestrian Detection",
        "author": "Taeheon Kim, Sebin Shin, Youngjoon Yu, Hak Gu Kim, Yong Man Ro",
        "abstract": "RGBT multispectral pedestrian detection has emerged as a promising solution for safety-critical applications that require day/night operations. However, the modality bias problem remains unsolved as multispectral pedestrian detectors learn the statistical bias in datasets. Specifically, datasets in multispectral pedestrian detection mainly distribute between ROTO (day) and RXTO (night) data; the majority of the pedestrian labels statistically co-occur with their thermal features. As a result, multispectral pedestrian detectors show poor generalization ability on examples beyond this statistical correlation, such as ROTX data. To address this problem, we propose a novel Causal Mode Multiplexer (CMM) framework that effectively learns the causalities between multispectral inputs and predictions. Moreover, we construct a new dataset (ROTX-MP) to evaluate modality bias in multispectral pedestrian detection. ROTX-MP mainly includes ROTX examples not presented in previous datasets. Extensive experiments demonstrate that our proposed CMM framework generalizes well on existing datasets (KAIST, CVC-14, FLIR) and the new ROTX-MP. We will release our new dataset to the public for future research.",
        "page": "http://arxiv.org/abs/2403.01300",
        "pdf": "http://arxiv.org/pdf/2403.01300.pdf"
    },
    {
        "title": "Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects",
        "author": "Yijia Weng, Bowen Wen, Jonathan Tremblay, Valts Blukis, Dieter Fox, Leonidas Guibas, Stan Birchfield",
        "abstract": "We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors. Project page: https://github.com/NVlabs/DigitalTwinArt",
        "page": "http://arxiv.org/abs/2404.01440",
        "pdf": "http://arxiv.org/pdf/2404.01440.pdf"
    },
    {
        "title": "Sculpting Holistic 3D Representation in Contrastive Language-Image-3D Pre-training",
        "author": "Yipeng Gao, Zeyu Wang, Wei-Shi Zheng, Cihang Xie, Yuyin Zhou",
        "abstract": "Contrastive learning has emerged as a promising paradigm for 3D open-world understanding, i.e., aligning point cloud representation to image and text embedding space individually. In this paper, we introduce MixCon3D, a simple yet effective method aiming to sculpt holistic 3D representation in contrastive language-image-3D pre-training. In contrast to point cloud only, we develop the 3D object-level representation from complementary perspectives, e.g., multi-view rendered images with the point cloud. Then, MixCon3D performs language-3D contrastive learning, comprehensively depicting real-world 3D objects and bolstering text alignment. Additionally, we pioneer the first thorough investigation of various training recipes for the 3D contrastive learning paradigm, building a solid baseline with improved performance. Extensive experiments conducted on three representative benchmarks reveal that our method significantly improves over the baseline, surpassing the previous state-of-the-art performance on the challenging 1,156-category Objaverse-LVIS dataset by 5.7%. The versatility of MixCon3D is showcased in applications such as text-to-3D retrieval and point cloud captioning, further evidencing its efficacy in diverse scenarios. The code is available at https://github.com/UCSC-VLAA/MixCon3D.",
        "page": "http://arxiv.org/abs/2311.01734",
        "pdf": "http://arxiv.org/pdf/2311.01734.pdf"
    },
    {
        "title": "Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition",
        "author": "Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware Spatio-Temporal Sampling",
        "author": "Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang, Pedro Miraldo, Suhas Lohit, Moitreya Chatterjee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VLP: Vision Language Planning for Autonomous Driving",
        "author": "Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup Mallik, Alessandro G Allievi, Senem Velipasalar, Liu Ren",
        "abstract": "Autonomous driving is a complex and challenging task that aims at safe motion planning through scene understanding and reasoning. While vision-only autonomous driving methods have recently achieved notable performance, through enhanced scene understanding, several key issues, including lack of reasoning, low generalization performance and long-tail scenarios, still need to be addressed. In this paper, we present VLP, a novel Vision-Language-Planning framework that exploits language models to bridge the gap between linguistic understanding and autonomous driving. VLP enhances autonomous driving systems by strengthening both the source memory foundation and the self-driving car's contextual understanding. VLP achieves state-of-the-art end-to-end planning performance on the challenging NuScenes dataset by achieving 35.9\\% and 60.5\\% reduction in terms of average L2 error and collision rates, respectively, compared to the previous best method. Moreover, VLP shows improved performance in challenging long-tail scenarios and strong generalization capabilities when faced with new urban environments.",
        "page": "http://arxiv.org/abs/2401.05577",
        "pdf": "http://arxiv.org/pdf/2401.05577.pdf"
    },
    {
        "title": "Adversarial Text to Continuous Image Generation",
        "author": "Kilichbek Haydarov, Aashiq Muhamed, Xiaoqian Shen, Jovana Lazarevic, Ivan Skorokhodov, Chamuditha Jayanga Galappaththige, Mohamed Elhoseiny",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM",
        "author": "Linyu Tang, Lei Zhang",
        "abstract": "Numerous studies have demonstrated the susceptibility of deep neural networks (DNNs) to subtle adversarial perturbations, prompting the development of many advanced adversarial defense methods aimed at mitigating adversarial attacks. Current defense strategies usually train DNNs for a specific adversarial attack method and can achieve good robustness in defense against this type of adversarial attack. Nevertheless, when subjected to evaluations involving unfamiliar attack modalities, empirical evidence reveals a pronounced deterioration in the robustness of DNNs. Meanwhile, there is a trade-off between the classification accuracy of clean examples and adversarial examples. Most defense methods often sacrifice the accuracy of clean examples in order to improve the adversarial robustness of DNNs. To alleviate these problems and enhance the overall robust generalization of DNNs, we propose the Test-Time Pixel-Level Adversarial Purification (TPAP) method. This approach is based on the robust overfitting characteristic of DNNs to the fast gradient sign method (FGSM) on training and test datasets. It utilizes FGSM for adversarial purification, to process images for purifying unknown adversarial perturbations from pixels at testing time in a \"counter changes with changelessness\" manner, thereby enhancing the defense capability of DNNs against various unknown adversarial attacks. Extensive experimental results show that our method can effectively improve both overall robust generalization of DNNs, notably over previous methods.",
        "page": "http://arxiv.org/abs/2403.11448",
        "pdf": "http://arxiv.org/pdf/2403.11448.pdf"
    },
    {
        "title": "Uncertainty-aware Action Decoupling Transformer for Action Anticipation",
        "author": "Hongji Guo, Nakul Agarwal, Shao-Yuan Lo, Kwonjoon Lee, Qiang Ji",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation",
        "author": "Chen Sichen, Yingyi Zhang, Siming Huang, Ran Yi, Ke Fan, Ruixin Zhang, Peixian Chen, Jun Wang, Shouhong Ding, Lizhuang Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection",
        "author": "Kuan-Chih Huang, Weijie Lyu, Ming-Hsuan Yang, Yi-Hsuan Tsai",
        "abstract": "Recent temporal LiDAR-based 3D object detectors achieve promising performance based on the two-stage proposal-based approach. They generate 3D box candidates from the first-stage dense detector, followed by different temporal aggregation methods. However, these approaches require per-frame objects or whole point clouds, posing challenges related to memory bank utilization. Moreover, point clouds and trajectory features are combined solely based on concatenation, which may neglect effective interactions between them. In this paper, we propose a point-trajectory transformer with long short-term memory for efficient temporal 3D object detection. To this end, we only utilize point clouds of current-frame objects and their historical trajectories as input to minimize the memory bank storage requirement. Furthermore, we introduce modules to encode trajectory features, focusing on long short-term and future-aware perspectives, and then effectively aggregate them with point cloud features. We conduct extensive experiments on the large-scale Waymo dataset to demonstrate that our approach performs well against state-of-the-art methods. Code and models will be made publicly available at https://github.com/kuanchihhuang/PTT.",
        "page": "http://arxiv.org/abs/2312.08371",
        "pdf": "http://arxiv.org/pdf/2312.08371.pdf"
    },
    {
        "title": "Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation",
        "author": "ZHIXIANG WEI, Lin Chen, Xiaoxiao Ma, Huaian Chen, Tianle Liu, Pengyang Ling, Jinjin Zheng, Ben Wang, Yi Jin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OpenStreetView-5M: The Many Roads to Global Visual Geolocation",
        "author": "Guillaume Astruc, Nicolas Dufour, Ioannis Siglidis, Constantin Aronssohn, Nacim Bouia, Stephanie Fu, Romain Loiseau, Van Nguyen Nguyen, Charles Raude, Elliot Vincent, Lintao XU, Hongyu Zhou, Loic Landrieu",
        "abstract": "Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly relevant for evaluating computer vision algorithms. Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential. To address this issue, we introduce OpenStreetView-5M, a large-scale, open-access dataset comprising over 5.1 million geo-referenced street view images, covering 225 countries and territories. In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization. To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-the-art image encoders, spatial representations, and training strategies. All associated codes and models can be found at https://github.com/gastruc/osv5m.",
        "page": "http://arxiv.org/abs/2404.18873",
        "pdf": "http://arxiv.org/pdf/2404.18873.pdf"
    },
    {
        "title": "An Asymmetric Augmented Self-Supervised Learning Method for Unsupervised Fine-Grained Image Hashing",
        "author": "Feiran Hu, Chenlin Zhang, Jiangliang GUO, Xiu-Shen Wei, Lin Zhao, Anqi Xu, Lingyan Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Action Scene Graphs for Long-Form Understanding of Egocentric Videos",
        "author": "Ivan Rodin, Antonino Furnari, Kyle Min, Subarna Tripathi, Giovanni Maria Farinella",
        "abstract": "We present Egocentric Action Scene Graphs (EASGs), a new representation for long-form understanding of egocentric videos. EASGs extend standard manually-annotated representations of egocentric videos, such as verb-noun action labels, by providing a temporally evolving graph-based description of the actions performed by the camera wearer, including interacted objects, their relationships, and how actions unfold in time. Through a novel annotation procedure, we extend the Ego4D dataset by adding manually labeled Egocentric Action Scene Graphs offering a rich set of annotations designed for long-from egocentric video understanding. We hence define the EASG generation task and provide a baseline approach, establishing preliminary benchmarks. Experiments on two downstream tasks, egocentric action anticipation and egocentric activity summarization, highlight the effectiveness of EASGs for long-form egocentric video understanding. We will release the dataset and the code to replicate experiments and annotations.",
        "page": "http://arxiv.org/abs/2312.03391",
        "pdf": "http://arxiv.org/pdf/2312.03391.pdf"
    },
    {
        "title": "Multi-Session SLAM using Wide-Baseline Optical Flow",
        "author": "Lahav Lipson, Jia Deng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models",
        "author": "Jingyuan Yang, Jiawei Feng, Hui Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Image",
        "author": "Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, Jiajun Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Noisy-Correspondence Learning for Text-to-Image Person Re-identification",
        "author": "Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, Joey Tianyi Zhou, Peng Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LEMON: Learning 3D Human-Object Interaction Relation from 2D Images",
        "author": "Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Zheng-Jun Zha",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild",
        "author": "weining ren, Zihan Zhu, Boyang Sun, Jiaqi Chen, Marc Pollefeys, Songyou Peng",
        "abstract": "Neural Radiance Fields (NeRFs) have shown remarkable success in synthesizing photorealistic views from multi-view images of static scenes, but face challenges in dynamic, real-world environments with distractors like moving objects, shadows, and lighting changes. Existing methods manage controlled environments and low occlusion ratios but fall short in render quality, especially under high occlusion scenarios. In this paper, we introduce NeRF On-the-go, a simple yet effective approach that enables the robust synthesis of novel views in complex, in-the-wild scenes from only casually captured image sequences. Delving into uncertainty, our method not only efficiently eliminates distractors, even when they are predominant in captures, but also achieves a notably faster convergence speed. Through comprehensive experiments on various scenes, our method demonstrates a significant improvement over state-of-the-art techniques. This advancement opens new avenues for NeRF in diverse and dynamic real-world applications.",
        "page": "http://arxiv.org/abs/2405.18715",
        "pdf": "http://arxiv.org/pdf/2405.18715.pdf"
    },
    {
        "title": "Step differences in instructional video",
        "author": "Tushar Nagarajan, Lorenzo Torresani",
        "abstract": "Comparing a user video to a reference how-to video is a key requirement for AR/VR technology delivering personalized assistance tailored to the user's progress. However, current approaches for language-based assistance can only answer questions about a single video. We propose an approach that first automatically generates large amounts of visual instruction tuning data involving pairs of videos from HowTo100M by leveraging existing step annotations and accompanying narrations, and then trains a video-conditioned language model to jointly reason across multiple raw videos. Our model achieves state-of-the-art performance at identifying differences between video pairs and ranking videos based on the severity of these differences, and shows promising ability to perform general reasoning over multiple videos.",
        "page": "http://arxiv.org/abs/2404.16222",
        "pdf": "http://arxiv.org/pdf/2404.16222.pdf"
    },
    {
        "title": "OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition",
        "author": "Tongjia Chen, Hongshan Yu, Zhengeng Yang, Zechuan Li, Wei Sun, Chen Chen",
        "abstract": "Due to the resource-intensive nature of training vision-language models on expansive video data, a majority of studies have centered on adapting pre-trained image-language models to the video domain. Dominant pipelines propose to tackle the visual discrepancies with additional temporal learners while overlooking the substantial discrepancy for web-scaled descriptive narratives and concise action category names, leading to less distinct semantic space and potential performance limitations. In this work, we prioritize the refinement of text knowledge to facilitate generalizable video recognition. To address the limitations of the less distinct semantic space of category names, we prompt a large language model (LLM) to augment action class names into Spatio-Temporal Descriptors thus bridging the textual discrepancy and serving as a knowledge base for general recognition. Moreover, to assign the best descriptors with different video instances, we propose Optimal Descriptor Solver, forming the video recognition problem as solving the optimal matching flow across frame-level representations and descriptors. Comprehensive evaluations in zero-shot, few-shot, and fully supervised video recognition highlight the effectiveness of our approach. Our best model achieves a state-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.",
        "page": "http://arxiv.org/abs/2312.00096",
        "pdf": "http://arxiv.org/pdf/2312.00096.pdf"
    },
    {
        "title": "SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining",
        "author": "Chull Hwan Song, Taebaek Hwang, Jooyoung Yoon, Shunghyun Choi, Yeong Hyeon Gu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Total Selfie: Generating Full-Body Selfies",
        "author": "Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, Steve Seitz",
        "abstract": "We present a method to generate full-body selfies from photographs originally taken at arms length. Because self-captured photos are typically taken close up, they have limited field of view and exaggerated perspective that distorts facial shapes. We instead seek to generate the photo some one else would take of you from a few feet away. Our approach takes as input four selfies of your face and body, a background image, and generates a full-body selfie in a desired target pose. We introduce a novel diffusion-based approach to combine all of this information into high-quality, well-composed photos of you with the desired pose and background.",
        "page": "http://arxiv.org/abs/2308.14740",
        "pdf": "http://arxiv.org/pdf/2308.14740.pdf"
    },
    {
        "title": "LayoutFormer: Hierarchical Text Detection Towards Scene Text Understanding",
        "author": "Min Liang, Jia-Wei Ma, Xiaobin Zhu, Jingyan Qin, Xu-Cheng Yin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Accelerating Neural Field Training via Soft Mining",
        "author": "Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi",
        "abstract": "We present an approach to accelerate Neural Field training by efficiently selecting sampling locations. While Neural Fields have recently become popular, it is often trained by uniformly sampling the training domain, or through handcrafted heuristics. We show that improved convergence and final training quality can be achieved by a soft mining technique based on importance sampling: rather than either considering or ignoring a pixel completely, we weigh the corresponding loss by a scalar. To implement our idea we use Langevin Monte-Carlo sampling. We show that by doing so, regions with higher error are being selected more frequently, leading to more than 2x improvement in convergence speed. The code and related resources for this study are publicly available at https://ubc-vision.github.io/nf-soft-mining/.",
        "page": "http://arxiv.org/abs/2312.00075",
        "pdf": "http://arxiv.org/pdf/2312.00075.pdf"
    },
    {
        "title": "PEGASUS: Personalized Generative 3D Avatars with Composable Attributes",
        "author": "Hyunsoo Cha, Byungjun Kim, Hanbyul Joo",
        "abstract": "We present PEGASUS, a method for constructing a personalized generative 3D face avatar from monocular video sources. Our generative 3D avatar enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) while preserving the identity. Our approach consists of two stages: synthetic database generation and constructing a personalized generative avatar. We generate a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing the attributes from monocular videos of diverse identities. Then, we build a person-specific generative 3D avatar that can modify its attributes continuously while preserving its identity. Through extensive experiments, we demonstrate that our method of generating a synthetic database and creating a 3D generative avatar is the most effective in preserving identity while achieving high realism. Subsequently, we introduce a zero-shot approach to achieve the same goal of generative modeling more efficiently by leveraging a previously constructed personalized generative model.",
        "page": "http://arxiv.org/abs/2402.10636",
        "pdf": "http://arxiv.org/pdf/2402.10636.pdf"
    },
    {
        "title": "Depth Prompting for Sensor-Agnostic Depth Estimation",
        "author": "Jin-Hwi Park, Chanhwi Jeong, Junoh Lee, Hae-Gon Jeon",
        "abstract": "Dense depth maps have been used as a key element of visual perception tasks. There have been tremendous efforts to enhance the depth quality, ranging from optimization-based to learning-based methods. Despite the remarkable progress for a long time, their applicability in the real world is limited due to systematic measurement biases such as density, sensing pattern, and scan range. It is well-known that the biases make it difficult for these methods to achieve their generalization. We observe that learning a joint representation for input modalities (e.g., images and depth), which most recent methods adopt, is sensitive to the biases. In this work, we disentangle those modalities to mitigate the biases with prompt engineering. For this, we design a novel depth prompt module to allow the desirable feature representation according to new depth distributions from either sensor types or scene configurations. Our depth prompt can be embedded into foundation models for monocular depth estimation. Through this embedding process, our method helps the pretrained model to be free from restraint of depth scan range and to provide absolute scale depth maps. We demonstrate the effectiveness of our method through extensive evaluations. Source code is publicly available at https://github.com/JinhwiPark/DepthPrompting .",
        "page": "http://arxiv.org/abs/2405.11867",
        "pdf": "http://arxiv.org/pdf/2405.11867.pdf"
    },
    {
        "title": "BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning",
        "author": "Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang",
        "abstract": "Studying backdoor attacks is valuable for model copyright protection and enhancing defenses. While existing backdoor attacks have successfully infected multimodal contrastive learning models such as CLIP, they can be easily countered by specialized backdoor defenses for MCL models. This paper reveals the threats in this practical scenario that backdoor attacks can remain effective even after defenses and introduces the \\emph{\\toolns} attack, which is resistant to backdoor detection and model fine-tuning defenses. To achieve this, we draw motivations from the perspective of the Bayesian rule and propose a dual-embedding guided framework for backdoor attacks. Specifically, we ensure that visual trigger patterns approximate the textual target semantics in the embedding space, making it challenging to detect the subtle parameter variations induced by backdoor learning on such natural trigger patterns. Additionally, we optimize the visual trigger patterns to align the poisoned samples with target vision features in order to hinder the backdoor unlearning through clean fine-tuning. Extensive experiments demonstrate that our attack significantly outperforms state-of-the-art baselines (+45.3% ASR) in the presence of SoTA backdoor defenses, rendering these mitigation and detection strategies virtually ineffective. Furthermore, our approach effectively attacks some more rigorous scenarios like downstream tasks. We believe that this paper raises awareness regarding the potential threats associated with the practical application of multimodal contrastive learning and encourages the development of more robust defense mechanisms.",
        "page": "http://arxiv.org/abs/2311.12075",
        "pdf": "http://arxiv.org/pdf/2311.12075.pdf"
    },
    {
        "title": "Weakly Supervised Video Individual Counting",
        "author": "Xinyan Liu, Guorong Li, Yuankai Qi, Ziheng Yan, Zhenjun Han, Anton van den Hengel, Ming-Hsuan Yang, Qingming Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SHINOBI: SHape and Illumination using Neural Object decomposition via BRDF optimization and Inverse rendering from unconstrained Image collections",
        "author": "Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Ricardo Martin-Brualla, Jonathan T. Barron, Deqing Sun, Hendrik Lensch, Varun Jampani",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
        "author": "Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, Lidong Bing",
        "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.",
        "page": "http://arxiv.org/abs/2311.16922",
        "pdf": "http://arxiv.org/pdf/2311.16922.pdf"
    },
    {
        "title": "AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation",
        "author": "Taeckyung Lee, Sorn Chottananurak, Taesik Gong, Sung-Ju Lee",
        "abstract": "Test-time adaptation (TTA) has emerged as a viable solution to adapt pre-trained models to domain shifts using unlabeled test data. However, TTA faces challenges of adaptation failures due to its reliance on blind adaptation to unknown test samples in dynamic scenarios. Traditional methods for out-of-distribution performance estimation are limited by unrealistic assumptions in the TTA context, such as requiring labeled data or re-training models. To address this issue, we propose AETTA, a label-free accuracy estimation algorithm for TTA. We propose the prediction disagreement as the accuracy estimate, calculated by comparing the target model prediction with dropout inferences. We then improve the prediction disagreement to extend the applicability of AETTA under adaptation failures. Our extensive evaluation with four baselines and six TTA methods demonstrates that AETTA shows an average of 19.8%p more accurate estimation compared with the baselines. We further demonstrate the effectiveness of accuracy estimation with a model recovery case study, showcasing the practicality of our model recovery based on accuracy estimation. The source code is available at https://github.com/taeckyung/AETTA.",
        "page": "http://arxiv.org/abs/2404.01351",
        "pdf": "http://arxiv.org/pdf/2404.01351.pdf"
    },
    {
        "title": "Multi-Object Tracking in the Dark",
        "author": "Xinzhe Wang, Kang Ma, Qiankun Liu, Yunhao Zou, Ying Fu",
        "abstract": "Low-light scenes are prevalent in real-world applications (e.g. autonomous driving and surveillance at night). Recently, multi-object tracking in various practical use cases have received much attention, but multi-object tracking in dark scenes is rarely considered. In this paper, we focus on multi-object tracking in dark scenes. To address the lack of datasets, we first build a Low-light Multi-Object Tracking (LMOT) dataset. LMOT provides well-aligned low-light video pairs captured by our dual-camera system, and high-quality multi-object tracking annotations for all videos. Then, we propose a low-light multi-object tracking method, termed as LTrack. We introduce the adaptive low-pass downsample module to enhance low-frequency components of images outside the sensor noises. The degradation suppression learning strategy enables the model to learn invariant information under noise disturbance and image quality degradation. These components improve the robustness of multi-object tracking in dark scenes. We conducted a comprehensive analysis of our LMOT dataset and proposed LTrack. Experimental results demonstrate the superiority of the proposed method and its competitiveness in real night low-light scenes. Dataset and Code: https: //github.com/ying-fu/LMOT",
        "page": "http://arxiv.org/abs/2405.06600",
        "pdf": "http://arxiv.org/pdf/2405.06600.pdf"
    },
    {
        "title": "RoMa: Robust Dense Feature Matching",
        "author": "Johan Edstedt, Qiyu Sun, Georg B\u00f6kman, M\u00e5rten Wadenb\u00e4ck, Michael Felsberg",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsupervised Keypoints from Pretrained Diffusion Models",
        "author": "Eric Hedlin, Gopal Sharma, Shweta Mahajan, Xingzhe He, Hossam Isack, Abhishek Kar, Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi",
        "abstract": "Unsupervised learning of keypoints and landmarks has seen significant progress with the help of modern neural network architectures, but performance is yet to match the supervised counterpart, making their practicability questionable. We leverage the emergent knowledge within text-to-image diffusion models, towards more robust unsupervised keypoints. Our core idea is to find text embeddings that would cause the generative model to consistently attend to compact regions in images (i.e. keypoints). To do so, we simply optimize the text embedding such that the cross-attention maps within the denoising network are localized as Gaussians with small standard deviations. We validate our performance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD, DeepFashion, and Human3.6m datasets. We achieve significantly improved accuracy, sometimes even outperforming supervised ones, particularly for data that is non-aligned and less curated. Our code is publicly available and can be found through our project page: https://ubc-vision.github.io/StableKeypoints/",
        "page": "http://arxiv.org/abs/2312.00065",
        "pdf": "http://arxiv.org/pdf/2312.00065.pdf"
    },
    {
        "title": "Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving",
        "author": "JINLONG LI, Baolu Li, Zhengzhong Tu, XINYU LIU, Qing Guo, Felix Juefei Xu, Runsheng Xu, Hongkai Yu",
        "abstract": "Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection model's knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving.",
        "page": "http://arxiv.org/abs/2404.04804",
        "pdf": "http://arxiv.org/pdf/2404.04804.pdf"
    },
    {
        "title": "DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation",
        "author": "Haonan Lin",
        "abstract": "While large-scale pre-trained text-to-image models can synthesize diverse and high-quality human-centered images, novel challenges arise with a nuanced task of \"identity fine editing\": precisely modifying specific features of a subject while maintaining its inherent identity and context. Existing personalization methods either require time-consuming optimization or learning additional encoders, adept in \"identity re-contextualization\". However, they often struggle with detailed and sensitive tasks like human face editing. To address these challenges, we introduce DreamSalon, a noise-guided, staged-editing framework, uniquely focusing on detailed image manipulations and identity-context preservation. By discerning editing and boosting stages via the frequency and gradient of predicted noises, DreamSalon first performs detailed manipulations on specific features in the editing stage, guided by high-frequency information, and then employs stochastic denoising in the boosting stage to improve image quality. For more precise editing, DreamSalon semantically mixes source and target textual prompts, guided by differences in their embedding covariances, to direct the model's focus on specific manipulation areas. Our experiments demonstrate DreamSalon's ability to efficiently and faithfully edit fine details on human faces, outperforming existing methods both qualitatively and quantitatively.",
        "page": "http://arxiv.org/abs/2403.19235",
        "pdf": "http://arxiv.org/pdf/2403.19235.pdf"
    },
    {
        "title": "Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer",
        "author": "Yuang Ai, Xiaoqiang Zhou, Huaibo Huang, Lei Zhang, Ran He",
        "abstract": "Unsupervised Domain Adaptation (UDA) can effectively address domain gap issues in real-world image Super-Resolution (SR) by accessing both the source and target data. Considering privacy policies or transmission restrictions of source data in practical scenarios, we propose a SOurce-free Domain Adaptation framework for image SR (SODA-SR) to address this issue, i.e., adapt a source-trained model to a target domain with only unlabeled target data. SODA-SR leverages the source-trained model to generate refined pseudo-labels for teacher-student learning. To better utilize pseudo-labels, we propose a novel wavelet-based augmentation method, named Wavelet Augmentation Transformer (WAT), which can be flexibly incorporated with existing networks, to implicitly produce useful augmented data. WAT learns low-frequency information of varying levels across diverse samples, which is aggregated efficiently via deformable attention. Furthermore, an uncertainty-aware self-training mechanism is proposed to improve the accuracy of pseudo-labels, with inaccurate predictions being rectified by uncertainty estimation. To acquire better SR results and avoid overfitting pseudo-labels, several regularization losses are proposed to constrain target LR and SR images in the frequency domain. Experiments show that without accessing source data, SODA-SR outperforms state-of-the-art UDA methods in both synthetic$\\rightarrow$real and real$\\rightarrow$real adaptation settings, and is not constrained by specific network architectures.",
        "page": "http://arxiv.org/abs/2303.17783",
        "pdf": "http://arxiv.org/pdf/2303.17783.pdf"
    },
    {
        "title": "Intensity-Robust Autofocus for Spike Camera",
        "author": "Changqing Su, Zhiyuan Ye, Yongsheng Xiao, You Zhou, Zhen Cheng, Bo Xiong, Zhaofei Yu, Tiejun Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LASA: Instance Reconstruction from Real Scans using A Large-scale Aligned Shape Annotation Dataset",
        "author": "Haolin Liu, Chongjie Ye, Yinyu Nie, Yingfan He, Xiaoguang Han",
        "abstract": "Instance shape reconstruction from a 3D scene involves recovering the full geometries of multiple objects at the semantic instance level. Many methods leverage data-driven learning due to the intricacies of scene complexity and significant indoor occlusions. Training these methods often requires a large-scale, high-quality dataset with aligned and paired shape annotations with real-world scans. Existing datasets are either synthetic or misaligned, restricting the performance of data-driven methods on real data. To this end, we introduce LASA, a Large-scale Aligned Shape Annotation Dataset comprising 10,412 high-quality CAD annotations aligned with 920 real-world scene scans from ArkitScenes, created manually by professional artists. On this top, we propose a novel Diffusion-based Cross-Modal Shape Reconstruction (DisCo) method. It is empowered by a hybrid feature aggregation design to fuse multi-modal inputs and recover high-fidelity object geometries. Besides, we present an Occupancy-Guided 3D Object Detection (OccGOD) method and demonstrate that our shape annotations provide scene occupancy clues that can further improve 3D object detection. Supported by LASA, extensive experiments show that our methods achieve state-of-the-art performance in both instance-level scene reconstruction and 3D object detection tasks.",
        "page": "http://arxiv.org/abs/2312.12418",
        "pdf": "http://arxiv.org/pdf/2312.12418.pdf"
    },
    {
        "title": "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks",
        "author": "Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, Dong Chen, Baining Guo",
        "abstract": "We present InstructDiffusion, a unifying and generic framework for aligning computer vision tasks with human instructions. Unlike existing approaches that integrate prior knowledge and pre-define the output space (e.g., categories and coordinates) for each vision task, we cast diverse vision tasks into a human-intuitive image-manipulating process whose output space is a flexible and interactive pixel space. Concretely, the model is built upon the diffusion process and is trained to predict pixels according to user instructions, such as encircling the man's left shoulder in red or applying a blue mask to the left car. InstructDiffusion could handle a variety of vision tasks, including understanding tasks (such as segmentation and keypoint detection) and generative tasks (such as editing and enhancement). It even exhibits the ability to handle unseen tasks and outperforms prior methods on novel datasets. This represents a significant step towards a generalist modeling interface for vision tasks, advancing artificial general intelligence in the field of computer vision.",
        "page": "http://arxiv.org/abs/2309.03895",
        "pdf": "http://arxiv.org/pdf/2309.03895.pdf"
    },
    {
        "title": "GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement",
        "author": "Linfang Zheng, Tze Ho Elden Tse, Chen Wang, Yinghan Sun, Hua Chen, Ale\u0161 Leonardis, Wei Zhang, Hyung Jin Chang",
        "abstract": "Object pose refinement is essential for robust object pose estimation. Previous work has made significant progress towards instance-level object pose refinement. Yet, category-level pose refinement is a more challenging problem due to large shape variations within a category and the discrepancies between the target object and the shape prior. To address these challenges, we introduce a novel architecture for category-level object pose refinement. Our approach integrates an HS-layer and learnable affine transformations, which aims to enhance the extraction and alignment of geometric information. Additionally, we introduce a cross-cloud transformation mechanism that efficiently merges diverse data sources. Finally, we push the limits of our model by incorporating the shape prior information for translation and size error prediction. We conducted extensive experiments to demonstrate the effectiveness of the proposed framework. Through extensive quantitative experiments, we demonstrate significant improvement over the baseline method by a large margin across all metrics.",
        "page": "http://arxiv.org/abs/2404.11139",
        "pdf": "http://arxiv.org/pdf/2404.11139.pdf"
    },
    {
        "title": "When StyleGAN Meets Stable Diffusion: a ${\\mathcal{W}_+}$ Adapter for Personalized Image Generation",
        "author": "Xiaoming Li, Xinyu Hou, Chen Change Loy",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Prompt-enhanced Multiple Instance Learning for Weakly Supervised Anomaly Detection",
        "author": "Junxi Chen, Liang Li, Li Su, Zheng-Jun Zha, Qingming Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers",
        "author": "Han Liang, Jiacheng Bao, Ruichi Zhang, Sihan Ren, Yuecheng Xu, Sibei Yang, Xin Chen, Jingyi Yu, Lan Xu",
        "abstract": "We have recently seen tremendous progress in realistic text-to-motion generation. Yet, the existing methods often fail or produce implausible motions with unseen text inputs, which limits the applications. In this paper, we present OMG, a novel framework, which enables compelling motion generation from zero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the pretrain-then-finetune paradigm into the text-to-motion generation. At the pre-training stage, our model improves the generation ability by learning the rich out-of-domain inherent motion traits. To this end, we scale up a large unconditional diffusion model up to 1B parameters, so as to utilize the massive unlabeled motion data up to over 20M motion instances. At the subsequent fine-tuning stage, we introduce motion ControlNet, which incorporates text prompts as conditioning information, through a trainable copy of the pre-trained model and the proposed novel Mixture-of-Controllers (MoC) block. MoC block adaptively recognizes various ranges of the sub-motions with a cross-attention mechanism and processes them separately with the text-token-specific experts. Such a design effectively aligns the CLIP token embeddings of text prompts to various ranges of compact and expressive motion features. Extensive experiments demonstrate that our OMG achieves significant improvements over the state-of-the-art methods on zero-shot text-to-motion generation. Project page: https://tr3e.github.io/omg-page.",
        "page": "http://arxiv.org/abs/2312.08985",
        "pdf": "http://arxiv.org/pdf/2312.08985.pdf"
    },
    {
        "title": "Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange",
        "author": "Yanhao Wu, Tong Zhang, Wei Ke, Congpei Qiu, Sabine S\u00fcsstrunk, Mathieu Salzmann",
        "abstract": "In the realm of point cloud scene understanding, particularly in indoor scenes, objects are arranged following human habits, resulting in objects of certain semantics being closely positioned and displaying notable inter-object correlations. This can create a tendency for neural networks to exploit these strong dependencies, bypassing the individual object patterns. To address this challenge, we introduce a novel self-supervised learning (SSL) strategy. Our approach leverages both object patterns and contextual cues to produce robust features. It begins with the formulation of an object-exchanging strategy, where pairs of objects with comparable sizes are exchanged across different scenes, effectively disentangling the strong contextual dependencies. Subsequently, we introduce a context-aware feature learning strategy, which encodes object patterns without relying on their specific context by aggregating object features across various scenes. Our extensive experiments demonstrate the superiority of our method over existing SSL techniques, further showing its better robustness to environmental changes. Moreover, we showcase the applicability of our approach by transferring pre-trained models to diverse point cloud datasets.",
        "page": "http://arxiv.org/abs/2404.07504",
        "pdf": "http://arxiv.org/pdf/2404.07504.pdf"
    },
    {
        "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
        "author": "Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue",
        "abstract": "We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities. The code and models are available at https://github.com/AILab-CVC/M2PT.",
        "page": "http://arxiv.org/abs/2401.14405",
        "pdf": "http://arxiv.org/pdf/2401.14405.pdf"
    },
    {
        "title": "Rethinking Multi-view Representation Learning via Distilled Disentangling",
        "author": "Guanzhou Ke, Bo Wang, Xiao-Li Wang, Shengfeng He",
        "abstract": "Multi-view representation learning aims to derive robust representations that are both view-consistent and view-specific from diverse data sources. This paper presents an in-depth analysis of existing approaches in this domain, highlighting a commonly overlooked aspect: the redundancy between view-consistent and view-specific representations. To this end, we propose an innovative framework for multi-view representation learning, which incorporates a technique we term 'distilled disentangling'. Our method introduces the concept of masked cross-view prediction, enabling the extraction of compact, high-quality view-consistent representations from various sources without incurring extra computational overhead. Additionally, we develop a distilled disentangling module that efficiently filters out consistency-related information from multi-view representations, resulting in purer view-specific representations. This approach significantly reduces redundancy between view-consistent and view-specific representations, enhancing the overall efficiency of the learning process. Our empirical evaluations reveal that higher mask ratios substantially improve the quality of view-consistent representations. Moreover, we find that reducing the dimensionality of view-consistent representations relative to that of view-specific representations further refines the quality of the combined representations. Our code is accessible at: https://github.com/Guanzhou-Ke/MRDD.",
        "page": "http://arxiv.org/abs/2403.10897",
        "pdf": "http://arxiv.org/pdf/2403.10897.pdf"
    },
    {
        "title": "FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation",
        "author": "Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Condition-Aware Neural Network for Controlled Image Generation",
        "author": "Han Cai, Muyang Li, Qinsheng Zhang, Ming-Yu Liu, Song Han",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "On The Vulnerability of Efficient Vision Transformers to Adversarial Computation Attacks",
        "author": "Navaneet K L, Soroush Abbasi Koohpayegani, Essam Sleiman, Hamed Pirsiavash",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Sparse views, Near light: A practical paradigm for uncalibrated point-light photometric stereo",
        "author": "Mohammed Brahimi, Bjoern Haefner, Zhenzhang Ye, Bastian Goldluecke, Daniel Cremers",
        "abstract": "Neural approaches have shown a significant progress on camera-based reconstruction. But they require either a fairly dense sampling of the viewing sphere, or pre-training on an existing dataset, thereby limiting their generalizability. In contrast, photometric stereo (PS) approaches have shown great potential for achieving high-quality reconstruction under sparse viewpoints. Yet, they are impractical because they typically require tedious laboratory conditions, are restricted to dark rooms, and often multi-staged, making them subject to accumulated errors. To address these shortcomings, we propose an end-to-end uncalibrated multi-view PS framework for reconstructing high-resolution shapes acquired from sparse viewpoints in a real-world environment. We relax the dark room assumption, and allow a combination of static ambient lighting and dynamic near LED lighting, thereby enabling easy data capture outside the lab. Experimental validation confirms that it outperforms existing baseline approaches in the regime of sparse viewpoints by a large margin. This allows to bring high-accuracy 3D reconstruction from the dark room to the real world, while maintaining a reasonable data capture complexity.",
        "page": "http://arxiv.org/abs/2404.00098",
        "pdf": "http://arxiv.org/pdf/2404.00098.pdf"
    },
    {
        "title": "PanoContext-Former: Panoramic Total Scene Understanding with a Transformer",
        "author": "Yuan Dong, Chuan Fang, Liefeng Bo, Zilong Dong, Ping Tan",
        "abstract": "Panoramic image enables deeper understanding and more holistic perception of $360^\\circ$ surrounding environment, which can naturally encode enriched scene context information compared to standard perspective image. Previous work has made lots of effort to solve the scene understanding task in a bottom-up form, thus each sub-task is processed separately and few correlations are explored in this procedure. In this paper, we propose a novel method using depth prior for holistic indoor scene understanding which recovers the objects' shapes, oriented bounding boxes and the 3D room layout simultaneously from a single panorama. In order to fully utilize the rich context information, we design a transformer-based context module to predict the representation and relationship among each component of the scene. In addition, we introduce a real-world dataset for scene understanding, including photo-realistic panoramas, high-fidelity depth images, accurately annotated room layouts, and oriented object bounding boxes and shapes. Experiments on the synthetic and real-world datasets demonstrate that our method outperforms previous panoramic scene understanding methods in terms of both layout estimation and 3D object detection.",
        "page": "http://arxiv.org/abs/2305.12497",
        "pdf": "http://arxiv.org/pdf/2305.12497.pdf"
    },
    {
        "title": "FreeKD: Knowledge Distillation via Semantic Frequency Prompt",
        "author": "Yuan Zhang, Tao Huang, Jiaming Liu, Tao Jiang, Kuan Cheng, Shanghang Zhang",
        "abstract": "Knowledge distillation (KD) has been applied to various tasks successfully, and mainstream methods typically boost the student model via spatial imitation losses. However, the consecutive downsamplings induced in the spatial domain of teacher model is a type of corruption, hindering the student from analyzing what specific information needs to be imitated, which results in accuracy degradation. To better understand the underlying pattern of corrupted feature maps, we shift our attention to the frequency domain. During frequency distillation, we encounter a new challenge: the low-frequency bands convey general but minimal context, while the high are more informative but also introduce noise. Not each pixel within the frequency bands contributes equally to the performance. To address the above problem: (1) We propose the Frequency Prompt plugged into the teacher model, absorbing the semantic frequency context during finetuning. (2) During the distillation period, a pixel-wise frequency mask is generated via Frequency Prompt, to localize those pixel of interests (PoIs) in various frequency bands. Additionally, we employ a position-aware relational frequency loss for dense prediction tasks, delivering a high-order spatial enhancement to the student model. We dub our Frequency Knowledge Distillation method as FreeKD, which determines the optimal localization and extent for the frequency distillation. Extensive experiments demonstrate that FreeKD not only outperforms spatial-based distillation methods consistently on dense prediction tasks (e.g., FreeKD brings 3.8 AP gains for RepPoints-R50 on COCO2017 and 4.55 mIoU gains for PSPNet-R18 on Cityscapes), but also conveys more robustness to the student. Notably, we also validate the generalization of our approach on large-scale vision models (e.g., DINO and SAM).",
        "page": "http://arxiv.org/abs/2311.12079",
        "pdf": "http://arxiv.org/pdf/2311.12079.pdf"
    },
    {
        "title": "Data-Efficient Multimodal Fusion on a Single GPU",
        "author": "No\u00ebl Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims Volkovs",
        "abstract": "The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\\sim \\! 600\\times$ fewer GPU days and $\\sim \\! 80\\times$ fewer image-text pairs. Additionally, we show how our method can be applied to convert pre-trained text-to-image generative models into audio-to-image ones. Code is available at: https://github.com/layer6ai-labs/fusemix.",
        "page": "http://arxiv.org/abs/2312.10144",
        "pdf": "http://arxiv.org/pdf/2312.10144.pdf"
    },
    {
        "title": "Solving Masked Jigsaw Puzzles with Diffusion Transformers",
        "author": "Jinyang Liu, Wondmgezahu Teshome, Sandesh Ghimire, Mario Sznaier, Octavia Camps",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
        "author": "Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households",
        "author": "Zhihao Cao, ZiDong Wang, Siwen Xie, Anji Liu, Lifeng Fan",
        "abstract": "Despite the significant demand for assistive technology among vulnerable groups (e.g., the elderly, children, and the disabled) in daily tasks, research into advanced AI-driven assistive solutions that genuinely accommodate their diverse needs remains sparse. Traditional human-machine interaction tasks often require machines to simply help without nuanced consideration of human abilities and feelings, such as their opportunity for practice and learning, sense of self-improvement, and self-esteem. Addressing this gap, we define a pivotal and novel challenge Smart Help, which aims to provide proactive yet adaptive support to human agents with diverse disabilities and dynamic goals in various tasks and environments. To establish this challenge, we leverage AI2-THOR to build a new interactive 3D realistic household environment for the Smart Help task. We introduce an innovative opponent modeling module that provides a nuanced understanding of the main agent's capabilities and goals, in order to optimize the assisting agent's helping policy. Rigorous experiments validate the efficacy of our model components and show the superiority of our holistic approach against established baselines. Our findings illustrate the potential of AI-imbued assistive robots in improving the well-being of vulnerable groups.",
        "page": "http://arxiv.org/abs/2404.09001",
        "pdf": "http://arxiv.org/pdf/2404.09001.pdf"
    },
    {
        "title": "HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
        "author": "Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou",
        "abstract": "We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only highlights the observed failure modes, including language hallucination and visual illusion, but also deepens an understanding of these pitfalls. Our comprehensive case studies within HallusionBench shed light on the challenges of hallucination and illusion in LVLMs. Based on these insights, we suggest potential pathways for their future improvement. The benchmark and codebase can be accessed at https://github.com/tianyi-lab/HallusionBench.",
        "page": "http://arxiv.org/abs/2310.14566",
        "pdf": "http://arxiv.org/pdf/2310.14566.pdf"
    },
    {
        "title": "HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models",
        "author": "Li Pang, Xiangyu Rui, Long Cui, Hongzhong Wang, Deyu Meng, Xiangyong Cao",
        "abstract": "Hyperspectral image (HSI) restoration aims at recovering clean images from degraded observations and plays a vital role in downstream tasks. Existing model-based methods have limitations in accurately modeling the complex image characteristics with handcraft priors, and deep learning-based methods suffer from poor generalization ability. To alleviate these issues, this paper proposes an unsupervised HSI restoration framework with pre-trained diffusion model (HIR-Diff), which restores the clean HSIs from the product of two low-rank components, i.e., the reduced image and the coefficient matrix. Specifically, the reduced image, which has a low spectral dimension, lies in the image field and can be inferred from our improved diffusion model where a new guidance function with total variation (TV) prior is designed to ensure that the reduced image can be well sampled. The coefficient matrix can be effectively pre-estimated based on singular value decomposition (SVD) and rank-revealing QR (RRQR) factorization. Furthermore, a novel exponential noise schedule is proposed to accelerate the restoration process (about 5$\\times$ acceleration for denoising) with little performance decrease. Extensive experimental results validate the superiority of our method in both performance and speed on a variety of HSI restoration tasks, including HSI denoising, noisy HSI super-resolution, and noisy HSI inpainting. The code is available at https://github.com/LiPang/HIRDiff.",
        "page": "http://arxiv.org/abs/2402.15865",
        "pdf": "http://arxiv.org/pdf/2402.15865.pdf"
    },
    {
        "title": "Mitigating Noisy Correspondence by Geometrical Structure Consistency Learning",
        "author": "Zihua Zhao, Mengxi Chen, Tianjie Dai, Jiangchao Yao, Bo Han, Ya Zhang, Yanfeng Wang",
        "abstract": "Noisy correspondence that refers to mismatches in cross-modal data pairs, is prevalent on human-annotated or web-crawled datasets. Prior approaches to leverage such data mainly consider the application of uni-modal noisy label learning without amending the impact on both cross-modal and intra-modal geometrical structures in multimodal learning. Actually, we find that both structures are effective to discriminate noisy correspondence through structural differences when being well-established. Inspired by this observation, we introduce a Geometrical Structure Consistency (GSC) method to infer the true correspondence. Specifically, GSC ensures the preservation of geometrical structures within and between modalities, allowing for the accurate discrimination of noisy samples based on structural differences. Utilizing these inferred true correspondence labels, GSC refines the learning of geometrical structures by filtering out the noisy samples. Experiments across four cross-modal datasets confirm that GSC effectively identifies noisy samples and significantly outperforms the current leading methods.",
        "page": "http://arxiv.org/abs/2405.16996",
        "pdf": "http://arxiv.org/pdf/2405.16996.pdf"
    },
    {
        "title": "Towards Automatic Power Battery Detection:  New Challenge, Benchmark Dataset and Baseline",
        "author": "Xiaoqi Zhao, Youwei Pang, Zhenyu Chen, Qian Yu, Lihe Zhang, Hanqi Liu, Jiaming Zuo, Huchuan Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Cinematic Behavior Transfer via NeRF-based Differentiable Filming",
        "author": "Xuekun Jiang, Anyi Rao, Jingbo Wang, Dahua Lin, Bo Dai",
        "abstract": "In the evolving landscape of digital media and video production, the precise manipulation and reproduction of visual elements like camera movements and character actions are highly desired. Existing SLAM methods face limitations in dynamic scenes and human pose estimation often focuses on 2D projections, neglecting 3D statuses. To address these issues, we first introduce a reverse filming behavior estimation technique. It optimizes camera trajectories by leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then introduce a cinematic transfer pipeline that is able to transfer various shot types to a new 2D video or a 3D virtual environment. The incorporation of 3D engine workflow enables superior rendering and control abilities, which also achieves a higher rating in the user study.",
        "page": "http://arxiv.org/abs/2311.17754",
        "pdf": "http://arxiv.org/pdf/2311.17754.pdf"
    },
    {
        "title": "$360+x$: A Panoptic Multi-modal Scene Understanding Dataset",
        "author": "Hao Chen, Yuqi Hou, Chenyuan Qu, Irene Testini, Xiaohan Hong, Jianbo Jiao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from Aerial Imagery",
        "author": "Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui",
        "abstract": "We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D. This is a challenging problem due to two primary reasons. Firstly, objects in urban aerial images exhibit substantial variations in size, including buildings, cars, and roads, which pose a significant challenge for accurate 2D segmentation. Secondly, the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem, especially in the case of aerial images, where each image captures only a small portion of the entire scene. To overcome these limitations, we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels. Furthermore, we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field, resulting in enhanced segmentation results. Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods, highlighting its effectiveness.",
        "page": "http://arxiv.org/abs/2403.11812",
        "pdf": "http://arxiv.org/pdf/2403.11812.pdf"
    },
    {
        "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback",
        "author": "Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran",
        "abstract": "We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs.",
        "page": "http://arxiv.org/abs/2311.10081",
        "pdf": "http://arxiv.org/pdf/2311.10081.pdf"
    },
    {
        "title": "Readout Guidance: Learning Control from Diffusion Features",
        "author": "Grace Luo, Trevor Darrell, Oliver Wang, Dan B Goldman, Aleksander Holynski",
        "abstract": "We present Readout Guidance, a method for controlling text-to-image diffusion models with learned signals. Readout Guidance uses readout heads, lightweight networks trained to extract signals from the features of a pre-trained, frozen diffusion model at every timestep. These readouts can encode single-image properties, such as pose, depth, and edges; or higher-order properties that relate multiple images, such as correspondence and appearance similarity. Furthermore, by comparing the readout estimates to a user-defined target, and back-propagating the gradient through the readout head, these estimates can be used to guide the sampling process. Compared to prior methods for conditional generation, Readout Guidance requires significantly fewer added parameters and training samples, and offers a convenient and simple recipe for reproducing different forms of conditional control under a single framework, with a single architecture and sampling procedure. We showcase these benefits in the applications of drag-based manipulation, identity-consistent generation, and spatially aligned control. Project page: https://readout-guidance.github.io.",
        "page": "http://arxiv.org/abs/2312.02150",
        "pdf": "http://arxiv.org/pdf/2312.02150.pdf"
    },
    {
        "title": "Neural Lineage",
        "author": "Runpeng Yu, Xinchao Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Structure-from-Motion from Pixel-wise Correspondences",
        "author": "Philipp Lindenberger, Paul-Edouard Sarlin, Marc Pollefeys",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "RNb-NeuS: Reflectance and Normal-based Multi-View 3D Reconstruction",
        "author": "Baptiste Brument, Robin Bruneau, Yvain Queau, Jean M\u00e9lou, Francois Lauze, Jean-Denis Durou, Lilian Calvet",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis",
        "author": "Zhicheng Lu, xiang guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, feng zhu, Yuchao Dai",
        "abstract": "In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance. The project is available at https://npucvr.github.io/GaGS/",
        "page": "http://arxiv.org/abs/2404.06270",
        "pdf": "http://arxiv.org/pdf/2404.06270.pdf"
    },
    {
        "title": "DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing",
        "author": "Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adversarially Robust Few-shot Learning via Parameter Co-distillation of Similarity and Class Concept Learners",
        "author": "Junhao Dong, Piotr Koniusz, Junxi Chen, Xiaohua Xie, Yew-Soon Ong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Uncovering What, Why and How:  A Comprehensive Benchmark for Causation Understanding of Video Anomaly",
        "author": "Hang Du, Sicheng Zhang, Binzhu Xie, Guoshun Nan, Jiayang Zhang, Junrui Xu, Hangyu Liu, Sicong Leng, Jiangming Liu, Hehe Fan, Dajiu Huang, Jing Feng, Linli Chen, Can Zhang, Xuhuan Li, Hao Zhang, Jianhang Chen, Qimei Cui, Xiaofeng Tao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Bird\u2019s Eye View Semantic Segmentation by Task Decomposition",
        "author": "Tianhao Zhao, Yongcan Chen, Yu Wu, Tianyang Liu, Bo Du, Peilun Xiao, shi qiu, Hongda Yang, Guozhen Li, yi yang, Yutian Lin, Yutian Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "KD-DETR: Knowledge Distillation for Detection Transformer with Consistent Distillation Points Sampling",
        "author": "Yu Wang, Xin Li, Shengzhao Wen, gang zhang, Haixiao Yue, Haocheng Feng, Junyu Han, Errui Ding",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Efficient Test-Time Adaptation of Vision-Language Models",
        "author": "Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, Eric P. Xing",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving",
        "author": "Mozhgan Pourkeshavarz, Mohammad Sabokrou, Amir Rasouli",
        "abstract": "In autonomous driving, behavior prediction is fundamental for safe motion planning, hence the security and robustness of prediction models against adversarial attacks are of paramount importance. We propose a novel adversarial backdoor attack against trajectory prediction models as a means of studying their potential vulnerabilities. Our attack affects the victim at training time via naturalistic, hence stealthy, poisoned samples crafted using a novel two-step approach. First, the triggers are crafted by perturbing the trajectory of attacking vehicle and then disguised by transforming the scene using a bi-level optimization technique. The proposed attack does not depend on a particular model architecture and operates in a black-box manner, thus can be effective without any knowledge of the victim model. We conduct extensive empirical studies using state-of-the-art prediction models on two benchmark datasets using metrics customized for trajectory prediction. We show that the proposed attack is highly effective, as it can significantly hinder the performance of prediction models, unnoticeable by the victims, and efficient as it forces the victim to generate malicious behavior even under constrained conditions. Via ablative studies, we analyze the impact of different attack design choices followed by an evaluation of existing defence mechanisms against the proposed attack.",
        "page": "http://arxiv.org/abs/2306.15755",
        "pdf": "http://arxiv.org/pdf/2306.15755.pdf"
    },
    {
        "title": "Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach",
        "author": "Mir Hossain Hossain, Mennatullah Siam, Leonid Sigal, Jim Little",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Object Pose Estimation via the Aggregation of Diffusion Features",
        "author": "Tianfu Wang, Guosheng Hu, Hongguang Wang",
        "abstract": "Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at https://github.com/Tianfu18/diff-feats-pose.",
        "page": "http://arxiv.org/abs/2403.18791",
        "pdf": "http://arxiv.org/pdf/2403.18791.pdf"
    },
    {
        "title": "Edge-Aware 3D Instance Segmentation Network with Intelligent Semantic Prior",
        "author": "Wonseok Roh, Hwanhee Jung, Giljoo Nam, Jinseop Yeom, Hyunje Park, Sang Ho Yoon, Sangpil Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Binding Touch to Everything: Learning Unified Multimodal Tactile Representations",
        "author": "Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, xien chen, Suchisrit Gangopadhyay, Andrew Owens, Alex Wong",
        "abstract": "The ability to associate touch with other modalities has huge implications for humans and computational systems. However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs. We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound. We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities. We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time. UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering. To the best of our knowledge, UniTouch is the first to demonstrate such capabilities. Project page: https://cfeng16.github.io/UniTouch/",
        "page": "http://arxiv.org/abs/2401.18084",
        "pdf": "http://arxiv.org/pdf/2401.18084.pdf"
    },
    {
        "title": "EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars",
        "author": "Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation",
        "author": "Qi Yang, Xing Nie, Tong Li, Gaopengfei, Ying Guo, Cheng Zhen, Pengfei Yan, Shiming Xiang",
        "abstract": "Recently, an audio-visual segmentation (AVS) task has been introduced, aiming to group pixels with sounding objects within a given video. This task necessitates a first-ever audio-driven pixel-level understanding of the scene, posing significant challenges. In this paper, we propose an innovative audio-visual transformer framework, termed COMBO, an acronym for COoperation of Multi-order Bilateral relatiOns. For the first time, our framework explores three types of bilateral entanglements within AVS: pixel entanglement, modality entanglement, and temporal entanglement. Regarding pixel entanglement, we employ a Siam-Encoder Module (SEM) that leverages prior knowledge to generate more precise visual features from the foundational model. For modality entanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO to align corresponding visual and auditory signals bi-directionally. As for temporal entanglement, we introduce an innovative adaptive inter-frame consistency loss according to the inherent rules of temporal. Comprehensive experiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIou on MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate that COMBO surpasses previous state-of-the-art methods. Code and more results will be publicly available at https://yannqi.github.io/AVS-COMBO/.",
        "page": "http://arxiv.org/abs/2312.06462",
        "pdf": "http://arxiv.org/pdf/2312.06462.pdf"
    },
    {
        "title": "Single-View Refractive Index Tomography with Neural Fields",
        "author": "Brandon Zhao, Aviad Levis, Liam Connor, Pratul P. Srinivasan, Katherine Bouman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MTLoRA: Low-Rank Adaptation Approach for Efficient Multi-Task Learning",
        "author": "Ahmed Agiza, Marina Neseem, Sherief Reda",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion",
        "author": "Zuoyue Li, Zhenqiang Li, Zhaopeng Cui, Marc Pollefeys, Martin R. Oswald",
        "abstract": "Directly generating scenes from satellite imagery offers exciting possibilities for integration into applications like games and map services. However, challenges arise from significant view changes and scene scale. Previous efforts mainly focused on image or video generation, lacking exploration into the adaptability of scene generation for arbitrary views. Existing 3D generation works either operate at the object level or are difficult to utilize the geometry obtained from satellite imagery. To overcome these limitations, we propose a novel architecture for direct 3D scene generation by introducing diffusion models into 3D sparse representations and combining them with neural rendering techniques. Specifically, our approach generates texture colors at the point level for a given geometry using a 3D diffusion model first, which is then transformed into a scene representation in a feed-forward manner. The representation can be utilized to render arbitrary views which would excel in both single-frame quality and inter-frame consistency. Experiments in two city-scale datasets show that our model demonstrates proficiency in generating photo-realistic street-view image sequences and cross-view urban scenes from satellite imagery.",
        "page": "http://arxiv.org/abs/2401.10786",
        "pdf": "http://arxiv.org/pdf/2401.10786.pdf"
    },
    {
        "title": "See, Say, and Segment: Correcting False Premises with LMMs",
        "author": "Tsung-Han Wu, Giscard Biamby, David Chan, Lisa Dunlap, Ritwik Gupta, Xudong Wang, Trevor Darrell, Joseph Gonzalez",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Discriminative Dynamics with Label Corruption for Noisy Label Detection",
        "author": "Suyeon Kim, Dongha Lee, SeongKu Kang, Sukang Chae, Sanghwan Jang, Hwanjo Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Efficient Replay in Federated Incremental Learning",
        "author": "Yichen Li, Qunwei Li, Haozhao Wang, Ruixuan Li, Wenliang Zhong, Guannan Zhang",
        "abstract": "In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we empirically show that Re-Fed achieves competitive performance compared to state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2403.05890",
        "pdf": "http://arxiv.org/pdf/2403.05890.pdf"
    },
    {
        "title": "ControlRoom3D: Room Generation using Semantic Controls",
        "author": "Jonas Schult, Sam Tsai, Lukas H\u00f6llein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, Peizhao Zhang, Bastian Leibe, Peter Vajda, Ji Hou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diff-BGM: A Diffusion Model for Video Background Music Generation",
        "author": "Sizhe Li, Yiming Qin, Minghang Zheng, Xin Jin, Yang Liu",
        "abstract": "When editing a video, a piece of attractive background music is indispensable. However, video background music generation tasks face several challenges, for example, the lack of suitable training datasets, and the difficulties in flexibly controlling the music generation process and sequentially aligning the video and music. In this work, we first propose a high-quality music-video dataset BGM909 with detailed annotation and shot detection to provide multi-modal information about the video and music. We then present evaluation metrics to assess music quality, including music diversity and alignment between music and video with retrieval precision metrics. Finally, we propose the Diff-BGM framework to automatically generate the background music for a given video, which uses different signals to control different aspects of the music during the generation process, i.e., uses dynamic video features to control music rhythm and semantic features to control the melody and atmosphere. We propose to align the video and music sequentially by introducing a segment-aware cross-attention layer. Experiments verify the effectiveness of our proposed method. The code and models are available at https://github.com/sizhelee/Diff-BGM.",
        "page": "http://arxiv.org/abs/2405.11913",
        "pdf": "http://arxiv.org/pdf/2405.11913.pdf"
    },
    {
        "title": "RMem: Restricted Memory Banks Improve Video Object Segmentation",
        "author": "Junbao Zhou, Ziqi Pang, Yu-Xiong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiaLoc: An Iterative Approach to Embodied Dialog Localization",
        "author": "Chao Zhang, Mohan Li, Ignas Budvytis, Stephan Liwicki",
        "abstract": "Multimodal learning has advanced the performance for many vision-language tasks. However, most existing works in embodied dialog research focus on navigation and leave the localization task understudied. The few existing dialog-based localization approaches assume the availability of entire dialog prior to localizaiton, which is impractical for deployed dialog-based localization. In this paper, we propose DiaLoc, a new dialog-based localization framework which aligns with a real human operator behavior. Specifically, we produce an iterative refinement of location predictions which can visualize current pose believes after each dialog turn. DiaLoc effectively utilizes the multimodal data for multi-shot localization, where a fusion encoder fuses vision and dialog information iteratively. We achieve state-of-the-art results on embodied dialog-based localization task, in single-shot (+7.08% in Acc5@valUnseen) and multi-shot settings (+10.85% in Acc5@valUnseen). DiaLoc narrows the gap between simulation and real-world applications, opening doors for future research on collaborative localization and navigation.",
        "page": "http://arxiv.org/abs/2403.06846",
        "pdf": "http://arxiv.org/pdf/2403.06846.pdf"
    },
    {
        "title": "Artist-Friendly Relightable and Animatable Neural Heads",
        "author": "Yingyan Xu, Prashanth Chandran, Sebastian Weiss, Markus Gross, Gaspard Zoss, Derek Bradley",
        "abstract": "An increasingly common approach for creating photo-realistic digital avatars is through the use of volumetric neural fields. The original neural radiance field (NeRF) allowed for impressive novel view synthesis of static heads when trained on a set of multi-view images, and follow up methods showed that these neural representations can be extended to dynamic avatars. Recently, new variants also surpassed the usual drawback of baked-in illumination in neural representations, showing that static neural avatars can be relit in any environment. In this work we simultaneously tackle both the motion and illumination problem, proposing a new method for relightable and animatable neural heads. Our method builds on a proven dynamic avatar approach based on a mixture of volumetric primitives, combined with a recently-proposed lightweight hardware setup for relightable neural fields, and includes a novel architecture that allows relighting dynamic neural avatars performing unseen expressions in any environment, even with nearfield illumination and viewpoints.",
        "page": "http://arxiv.org/abs/2312.03420",
        "pdf": "http://arxiv.org/pdf/2312.03420.pdf"
    },
    {
        "title": "Circuit Design and Efficient Simulation of Quantum Inner Product and Empirical Studies of Its Effect on Near-Term Hybrid Quantum-Classic Machine Learning",
        "author": "Hao Xiong, Yehui Tang, Xinyu Ye, Junchi Yan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neural Implicit Morphing of Face Images",
        "author": "Guilherme Schardong, Tiago Novello, Hallison Paz, Iurii Medvedev, Vin\u00edcius Silva, Luiz Velho, Nuno Gon\u00e7alves",
        "abstract": "Face morphing is a problem in computer graphics with numerous artistic and forensic applications. It is challenging due to variations in pose, lighting, gender, and ethnicity. This task consists of a warping for feature alignment and a blending for a seamless transition between the warped images. We propose to leverage coord-based neural networks to represent such warpings and blendings of face images. During training, we exploit the smoothness and flexibility of such networks by combining energy functionals employed in classical approaches without discretizations. Additionally, our method is time-dependent, allowing a continuous warping/blending of the images. During morphing inference, we need both direct and inverse transformations of the time-dependent warping. The first (second) is responsible for warping the target (source) image into the source (target) image. Our neural warping stores those maps in a single network dismissing the need for inverting them. The results of our experiments indicate that our method is competitive with both classical and generative models under the lens of image quality and face-morphing detectors. Aesthetically, the resulting images present a seamless blending of diverse faces not yet usual in the literature.",
        "page": "http://arxiv.org/abs/2308.13888",
        "pdf": "http://arxiv.org/pdf/2308.13888.pdf"
    },
    {
        "title": "GDA: Generalized Diffusion for Robust Test-time Adaptation",
        "author": "Yun-Yun Tsai, Fu-Chen Chen, Albert Chen, Junfeng Yang, Che-Chun Su, Min Sun, Cheng-Hao Kuo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SubT-MRS Datasets: Pushing SLAM Towards All-weather Environments",
        "author": "Shibo Zhao, Yuanjun Gao, Tianhao Wu, Damanpreet Singh, Rushan Jiang, Haoxiang Sun, Mansi Sarawata, Warren Whittaker, Ian Higgins, Shaoshu Su, Yi Du, Can Xu, John Keller, Jay Karhade, Lucas Nogueira, Sourojit Saha, Yuheng Qiu, Ji Zhang, Wenshan Wang, Chen Wang, Sebastian Scherer",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DemoCaricature: Democratising Caricature Generation with a Rough Sketch",
        "author": "Dar-Yen Chen, Ayan Kumar Bhunia, Subhadeep Koley, Aneeshan Sain, Pinaki Nath Chowdhury, Yi-Zhe Song",
        "abstract": "In this paper, we democratise caricature generation, empowering individuals to effortlessly craft personalised caricatures with just a photo and a conceptual sketch. Our objective is to strike a delicate balance between abstraction and identity, while preserving the creativity and subjectivity inherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing alongside single-image personalisation, selectively applying nuanced edits to cross-attention layers for a seamless merge of identity and style. Additionally, we propose Random Mask Reconstruction to enhance robustness, directing the model to focus on distinctive identity and style features. Crucially, our aim is not to replace artists but to eliminate accessibility barriers, allowing enthusiasts to engage in the artistry.",
        "page": "http://arxiv.org/abs/2312.04364",
        "pdf": "http://arxiv.org/pdf/2312.04364.pdf"
    },
    {
        "title": "SPIDeRS: Structured Polarization for Invisible Depth and Reflectance Sensing",
        "author": "Tomoki Ichikawa, Shohei Nobuhara, Ko Nishino",
        "abstract": "Can we capture shape and reflectance in stealth? Such capability would be valuable for many application domains in vision, xR, robotics, and HCI. We introduce structured polarization for invisible depth and reflectance sensing (SPIDeRS), the first depth and reflectance sensing method using patterns of polarized light. The key idea is to modulate the angle of linear polarization (AoLP) of projected light at each pixel. The use of polarization makes it invisible and lets us recover not only depth but also directly surface normals and even reflectance. We implement SPIDeRS with a liquid crystal spatial light modulator (SLM) and a polarimetric camera. We derive a novel method for robustly extracting the projected structured polarization pattern from the polarimetric object appearance. We evaluate the effectiveness of SPIDeRS by applying it to a number of real-world objects. The results show that our method successfully reconstructs object shapes of various materials and is robust to diffuse reflection and ambient light. We also demonstrate relighting using recovered surface normals and reflectance. We believe SPIDeRS opens a new avenue of polarization use in visual sensing.",
        "page": "http://arxiv.org/abs/2312.04553",
        "pdf": "http://arxiv.org/pdf/2312.04553.pdf"
    },
    {
        "title": "UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion",
        "author": "Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han",
        "abstract": "Diffusion models have shown remarkable results for image generation, editing and inpainting. Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function. However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces. In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally. Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation. Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks. Page: https://weiqi-zhang.github.io/UDiFF.",
        "page": "http://arxiv.org/abs/2404.06851",
        "pdf": "http://arxiv.org/pdf/2404.06851.pdf"
    },
    {
        "title": "VTimeLLM: Empower LLM to Grasp Video Moments",
        "author": "Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu",
        "abstract": "Large language models (LLMs) have shown remarkable text understanding capabilities, which have been extended as Video LLMs to handle video data for comprehending visual details. However, existing Video LLMs can only provide a coarse description of the entire video, failing to capture the precise start and end time boundary of specific events. In this paper, we solve this issue via proposing VTimeLLM, a novel Video LLM designed for fine-grained video moment understanding and reasoning with respect to time boundary. Specifically, our VTimeLLM adopts a boundary-aware three-stage training strategy, which respectively utilizes image-text pairs for feature alignment, multiple-event videos to increase temporal-boundary awareness, and high-quality video-instruction tuning to further improve temporal understanding ability as well as align with human intents. Extensive experiments demonstrate that in fine-grained time-related comprehension tasks for videos such as Temporal Video Grounding and Dense Video Captioning, VTimeLLM significantly outperforms existing Video LLMs. Besides, benefits from the fine-grained temporal understanding of the videos further enable VTimeLLM to beat existing Video LLMs in video dialogue benchmark, showing its superior cross-modal understanding and reasoning abilities.",
        "page": "http://arxiv.org/abs/2311.18445",
        "pdf": "http://arxiv.org/pdf/2311.18445.pdf"
    },
    {
        "title": "HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning for RGB-D 6DoF Object Pose Estimation",
        "author": "Yongliang Lin, Yongzhi Su, Praveen Nathan, Sandeep Inuganti, Yan Di, Martin Sundermeyer, Fabian Manhardt, Didier Stricker, Jason Rambach, Yu Zhang",
        "abstract": "In this work, we present a novel dense-correspondence method for 6DoF object pose estimation from a single RGB-D image. While many existing data-driven methods achieve impressive performance, they tend to be time-consuming due to their reliance on rendering-based refinement approaches. To circumvent this limitation, we present HiPose, which establishes 3D-3D correspondences in a coarse-to-fine manner with a hierarchical binary surface encoding. Unlike previous dense-correspondence methods, we estimate the correspondence surface by employing point-to-surface matching and iteratively constricting the surface until it becomes a correspondence point while gradually removing outliers. Extensive experiments on public benchmarks LM-O, YCB-V, and T-Less demonstrate that our method surpasses all refinement-free methods and is even on par with expensive refinement-based approaches. Crucially, our approach is computationally efficient and enables real-time critical applications with high accuracy requirements.",
        "page": "http://arxiv.org/abs/2311.12588",
        "pdf": "http://arxiv.org/pdf/2311.12588.pdf"
    },
    {
        "title": "AnyScene: Customized Image Synthesis with Composited Foreground",
        "author": "Ruidong Chen, Lanjun Wang, Weizhi Nie, Yongdong Zhang, An-An Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions",
        "author": "Saeed Khorram, Mingqi Jiang, Mohamad Shahbazi, Mohamad Hosein Danesh, Li Fuxin",
        "abstract": "Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on several long-tail benchmarks and GAN architectures demonstrate a significant improvement over existing methods in both the diversity and fidelity of the generated images. The code is available at https://github.com/khorrams/utlo.",
        "page": "http://arxiv.org/abs/2402.17065",
        "pdf": "http://arxiv.org/pdf/2402.17065.pdf"
    },
    {
        "title": "TRINS: Towards Multimodal Language Models That Can Read",
        "author": "Ruiyi Zhang, Yanzhe Zhang, Jian Chen, Yufan Zhou, Jiuxiang Gu, Changyou Chen, Tong Sun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Unified and Interpretable Emotion Representation and Expression Generation",
        "author": "Reni Paskaleva, Mykyta Holubakha, Andela Ilic, Saman Motamed, Luc Van Gool, Danda Paudel",
        "abstract": "Canonical emotions, such as happy, sad, and fearful, are easy to understand and annotate. However, emotions are often compound, e.g. happily surprised, and can be mapped to the action units (AUs) used for expressing emotions, and trivially to the canonical ones. Intuitively, emotions are continuous as represented by the arousal-valence (AV) model. An interpretable unification of these four modalities - namely, Canonical, Compound, AUs, and AV - is highly desirable, for a better representation and understanding of emotions. However, such unification remains to be unknown in the current literature. In this work, we propose an interpretable and unified emotion model, referred as C2A2. We also develop a method that leverages labels of the non-unified models to annotate the novel unified one. Finally, we modify the text-conditional diffusion models to understand continuous numbers, which are then used to generate continuous expressions using our unified emotion model. Through quantitative and qualitative experiments, we show that our generated images are rich and capture subtle expressions. Our work allows a fine-grained generation of expressions in conjunction with other textual inputs and offers a new label space for emotions at the same time.",
        "page": "http://arxiv.org/abs/2404.01243",
        "pdf": "http://arxiv.org/pdf/2404.01243.pdf"
    },
    {
        "title": "ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models",
        "author": "Xinyu Tian, Shu Zou, Zhaoyuan Yang, Jing Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "6-DoF Pose Estimation with MultiScale Residual Correlation",
        "author": "Yuelong Li, Yafei Mao, Raja Bala, Sunil Hadap",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PI3D: Efficient Text-to-3D Generation with Pseudo-Image Diffusion",
        "author": "Ying-Tian Liu, Yuan-Chen Guo, Guan Luo, Heyi Sun, Wei Yin, Song-Hai Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes",
        "author": "Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang",
        "abstract": "We present DrivingGaussian, an efficient and effective framework for surrounding dynamic autonomous driving scenes. For complex scenes with moving objects, we first sequentially and progressively model the static background of the entire scene with incremental static 3D Gaussians. We then leverage a composite dynamic Gaussian graph to handle multiple moving objects, individually reconstructing each object and restoring their accurate positions and occlusion relationships within the scene. We further use a LiDAR prior for Gaussian Splatting to reconstruct scenes with greater details and maintain panoramic consistency. DrivingGaussian outperforms existing methods in dynamic driving scene reconstruction and enables photorealistic surround-view synthesis with high-fidelity and multi-camera consistency. Our project page is at: https://github.com/VDIGPKU/DrivingGaussian.",
        "page": "http://arxiv.org/abs/2312.07920",
        "pdf": "http://arxiv.org/pdf/2312.07920.pdf"
    },
    {
        "title": "Interactive3D: Create What You Want by Interactive 3D Generation",
        "author": "Shaocong Dong, Lihe Ding, Zhanpeng Huang, Zibin Wang, Tianfan Xue, Dan Xu",
        "abstract": "3D object generation has undergone significant advancements, yielding high-quality results. However, fall short of achieving precise user control, often yielding results that do not align with user expectations, thus limiting their applicability. User-envisioning 3D object generation faces significant challenges in realizing its concepts using current generative models due to limited interaction capabilities. Existing methods mainly offer two approaches: (i) interpreting textual instructions with constrained controllability, or (ii) reconstructing 3D objects from 2D images. Both of them limit customization to the confines of the 2D reference and potentially introduce undesirable artifacts during the 3D lifting process, restricting the scope for direct and versatile 3D modifications. In this work, we introduce Interactive3D, an innovative framework for interactive 3D generation that grants users precise control over the generative process through extensive 3D interaction capabilities. Interactive3D is constructed in two cascading stages, utilizing distinct 3D representations. The first stage employs Gaussian Splatting for direct user interaction, allowing modifications and guidance of the generative direction at any intermediate step through (i) Adding and Removing components, (ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv) Semantic Editing. Subsequently, the Gaussian splats are transformed into InstantNGP. We introduce a novel (v) Interactive Hash Refinement module to further add details and extract the geometry in the second stage. Our experiments demonstrate that Interactive3D markedly improves the controllability and quality of 3D generation. Our project webpage is available at \\url{https://interactive-3d.github.io/}.",
        "page": "http://arxiv.org/abs/2404.16510",
        "pdf": "http://arxiv.org/pdf/2404.16510.pdf"
    },
    {
        "title": "Finding Lottery Tickets in Vision Models via Data-driven Spectral Foresight Pruning",
        "author": "Leonardo Iurada, Marco Ciccone, Tatiana Tommasi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Gaussian Splatting SLAM",
        "author": "Hidenobu Matsuki, Riku Murai, Paul Kelly, Andrew J. Davison",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Simple Baseline for Efficient Hand Mesh Reconstruction",
        "author": "zhishan zhou, shihao zhou, Zhi Lv, minqiang zou, Yao Tang, Jiajun Liang",
        "abstract": "3D hand pose estimation has found broad application in areas such as gesture recognition and human-machine interaction tasks. As performance improves, the complexity of the systems also increases, which can limit the comparative analysis and practical implementation of these methods. In this paper, we propose a simple yet effective baseline that not only surpasses state-of-the-art (SOTA) methods but also demonstrates computational efficiency. To establish this baseline, we abstract existing work into two components: a token generator and a mesh regressor, and then examine their core structures. A core structure, in this context, is one that fulfills intrinsic functions, brings about significant improvements, and achieves excellent performance without unnecessary complexities. Our proposed approach is decoupled from any modifications to the backbone, making it adaptable to any modern models. Our method outperforms existing solutions, achieving state-of-the-art (SOTA) results across multiple datasets. On the FreiHAND dataset, our approach produced a PA-MPJPE of 5.7mm and a PA-MPVPE of 6.0mm. Similarly, on the Dexycb dataset, we observed a PA-MPJPE of 5.5mm and a PA-MPVPE of 5.0mm. As for performance speed, our method reached up to 33 frames per second (fps) when using HRNet and up to 70 fps when employing FastViT-MA36",
        "page": "http://arxiv.org/abs/2403.01813",
        "pdf": "http://arxiv.org/pdf/2403.01813.pdf"
    },
    {
        "title": "EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting",
        "author": "Zitao Wang, Qiguang Miao, Yue Xi, Peipei Zhao",
        "abstract": "The portrait matting task aims to extract an alpha matte with complete semantics and finely-detailed contours. In comparison to CNN-based approaches, transformers with self-attention module have a better capacity to capture long-range dependencies and low-frequency semantic information of a portrait. However, the recent research shows that self-attention mechanism struggles with modeling high-frequency contour information and capturing fine contour details, which can lead to bias while predicting the portrait's contours. To deal with this issue, we propose EFormer to enhance the model's attention towards both of the low-frequency semantic and high-frequency contour features. For the high-frequency contours, our research demonstrates that cross-attention module between different resolutions can guide our model to allocate attention appropriately to these contour regions. Supported on this, we can successfully extract the high-frequency detail information around the portrait's contours, which are previously ignored by self-attention. Based on cross-attention module, we further build a semantic and contour detector (SCD) to accurately capture both of the low-frequency semantic and high-frequency contour features. And we design contour-edge extraction branch and semantic extraction branch to extract refined high-frequency contour features and complete low-frequency semantic information, respectively. Finally, we fuse the two kinds of features and leverage segmentation head to generate a predicted portrait matte. Experiments on VideoMatte240K (JPEG SD Format) and Adobe Image Matting (AIM) datasets demonstrate that EFormer outperforms previous portrait matte methods.",
        "page": "http://arxiv.org/abs/2308.12831",
        "pdf": "http://arxiv.org/pdf/2308.12831.pdf"
    },
    {
        "title": "BilevelPruning: Unified Dynamic and Static Channel Pruning for Convolutional Neural Networks",
        "author": "Shangqian Gao, Yanfu Zhang, Feihu Huang, Heng Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ReCoRe: Regularized Contrastive Representation Learning of World Model",
        "author": "Rudra P,K. Poudel, Harit Pandya, Stephan Liwicki, Roberto Cipolla",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Utility-Fairness Trade-Offs and How to Find Them",
        "author": "Sepehr Dehdashtian, Bashir Sadeghi, Vishnu Naresh Boddeti",
        "abstract": "When building classification systems with demographic fairness considerations, there are two objectives to satisfy: 1) maximizing utility for the specific task and 2) ensuring fairness w.r.t. a known demographic attribute. These objectives often compete, so optimizing both can lead to a trade-off between utility and fairness. While existing works acknowledge the trade-offs and study their limits, two questions remain unanswered: 1) What are the optimal trade-offs between utility and fairness? and 2) How can we numerically quantify these trade-offs from data for a desired prediction task and demographic attribute of interest? This paper addresses these questions. We introduce two utility-fairness trade-offs: the Data-Space and Label-Space Trade-off. The trade-offs reveal three regions within the utility-fairness plane, delineating what is fully and partially possible and impossible. We propose U-FaTE, a method to numerically quantify the trade-offs for a given prediction task and group fairness definition from data samples. Based on the trade-offs, we introduce a new scheme for evaluating representations. An extensive evaluation of fair representation learning methods and representations from over 1000 pre-trained models revealed that most current approaches are far from the estimated and achievable fairness-utility trade-offs across multiple datasets and prediction tasks.",
        "page": "http://arxiv.org/abs/2404.09454",
        "pdf": "http://arxiv.org/pdf/2404.09454.pdf"
    },
    {
        "title": "Learning Continuous 3D Words for Text-to-Image Generation",
        "author": "Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, Niki Trigoni",
        "abstract": "Current controls over diffusion models (e.g., through text or ControlNet) for image generation fall short in recognizing abstract, continuous attributes like illumination direction or non-rigid shape change. In this paper, we present an approach for allowing users of text-to-image models to have fine-grained control of several attributes in an image. We do this by engineering special sets of input tokens that can be transformed in a continuous manner -- we call them Continuous 3D Words. These attributes can, for example, be represented as sliders and applied jointly with text prompts for fine-grained control over image generation. Given only a single mesh and a rendering engine, we show that our approach can be adopted to provide continuous user control over several 3D-aware attributes, including time-of-day illumination, bird wing orientation, dollyzoom effect, and object poses. Our method is capable of conditioning image creation with multiple Continuous 3D Words and text descriptions simultaneously while adding no overhead to the generative process. Project Page: https://ttchengab.github.io/continuous_3d_words",
        "page": "http://arxiv.org/abs/2402.08654",
        "pdf": "http://arxiv.org/pdf/2402.08654.pdf"
    },
    {
        "title": "Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions",
        "author": "Weizhen He, Yiheng Deng, SHIXIANG TANG, Qihao CHEN, Qingsong Xie, Yizhou Wang, Lei Bai, Feng Zhu, Rui Zhao, Wanli Ouyang, Donglian Qi, Yunfeng Yan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Inter-X: Towards Versatile Human-Human Interaction Analysis",
        "author": "Liang Xu, Xintao Lv, Yichao Yan, Xin Jin, Wu Shuwen, Congsheng Xu, Yifan Liu, Yizhou Zhou, Fengyun Rao, Xingdong Sheng, Yunhui LIU, Wenjun Zeng, Xiaokang Yang",
        "abstract": "The analysis of the ubiquitous human-human interactions is pivotal for understanding humans as social beings. Existing human-human interaction datasets typically suffer from inaccurate body motions, lack of hand gestures and fine-grained textual descriptions. To better perceive and generate human-human interactions, we propose Inter-X, a currently largest human-human interaction dataset with accurate body movements and diverse interaction patterns, together with detailed hand gestures. The dataset includes ~11K interaction sequences and more than 8.1M frames. We also equip Inter-X with versatile annotations of more than 34K fine-grained human part-level textual descriptions, semantic interaction categories, interaction order, and the relationship and personality of the subjects. Based on the elaborate annotations, we propose a unified benchmark composed of 4 categories of downstream tasks from both the perceptual and generative directions. Extensive experiments and comprehensive analysis show that Inter-X serves as a testbed for promoting the development of versatile human-human interaction analysis. Our dataset and benchmark will be publicly available for research purposes.",
        "page": "http://arxiv.org/abs/2312.16051",
        "pdf": "http://arxiv.org/pdf/2312.16051.pdf"
    },
    {
        "title": "Tactile-Augmented Radiance Fields",
        "author": "Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, Andrew Owens",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MedBN: Robust Test-Time Adaptation against Malicious Test Samples",
        "author": "Hyejin Park, Jeongyeon Hwang, Sunung Mun, Sangdon Park, Jungseul Ok",
        "abstract": "Test-time adaptation (TTA) has emerged as a promising solution to address performance decay due to unforeseen distribution shifts between training and test data. While recent TTA methods excel in adapting to test data variations, such adaptability exposes a model to vulnerability against malicious examples, an aspect that has received limited attention. Previous studies have uncovered security vulnerabilities within TTA even when a small proportion of the test batch is maliciously manipulated. In response to the emerging threat, we propose median batch normalization (MedBN), leveraging the robustness of the median for statistics estimation within the batch normalization layer during test-time inference. Our method is algorithm-agnostic, thus allowing seamless integration with existing TTA frameworks. Our experimental results on benchmark datasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently demonstrate that MedBN outperforms existing approaches in maintaining robust performance across different attack scenarios, encompassing both instant and cumulative attacks. Through extensive experiments, we show that our approach sustains the performance even in the absence of attacks, achieving a practical balance between robustness and performance.",
        "page": "http://arxiv.org/abs/2403.19326",
        "pdf": "http://arxiv.org/pdf/2403.19326.pdf"
    },
    {
        "title": "PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation",
        "author": "Ardian Umam, Cheng-Kun Yang, Min-Hung Chen, Jen-Hui Chuang, Yen-Yu Lin",
        "abstract": "This paper proposes a cross-modal distillation framework, PartDistill, which transfers 2D knowledge from vision-language models (VLMs) to facilitate 3D shape part segmentation. PartDistill addresses three major challenges in this task: the lack of 3D segmentation in invisible or undetected regions in the 2D projections, inconsistent 2D predictions by VLMs, and the lack of knowledge accumulation across different 3D shapes. PartDistill consists of a teacher network that uses a VLM to make 2D predictions and a student network that learns from the 2D predictions while extracting geometrical features from multiple 3D shapes to carry out 3D part segmentation. A bi-directional distillation, including forward and backward distillations, is carried out within the framework, where the former forward distills the 2D predictions to the student network, and the latter improves the quality of the 2D predictions, which subsequently enhances the final 3D segmentation. Moreover, PartDistill can exploit generative models that facilitate effortless 3D shape creation for generating knowledge sources to be distilled. Through extensive experiments, PartDistill boosts the existing methods with substantial margins on widely used ShapeNetPart and PartNetE datasets, by more than 15% and 12% higher mIoU scores, respectively. The code for this work is available at https://github.com/ardianumam/PartDistill.",
        "page": "http://arxiv.org/abs/2312.04016",
        "pdf": "http://arxiv.org/pdf/2312.04016.pdf"
    },
    {
        "title": "Improving Training Efficiency of Diffusion Models via Multi-Stage Framework and Tailored Multi-Decoder Architectures",
        "author": "Huijie Zhang, Yifu Lu, Ismail Alkhouri, Saiprasad Ravishankar, Dogyoon Song, Qing Qu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VecFusion: Vector Font Generation with Diffusion",
        "author": "Vikas Thamizharasan, Difan Liu, Shantanu Agarwal, Matthew Fisher, Micha\u00ebl Gharbi, Oliver Wang, Alec Jacobson, Evangelos Kalogerakis",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising",
        "author": "Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, Yun Fu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
        "author": "Boran Han, Shuai Zhang, Xingjian Shi, Markus Reichstein",
        "abstract": "In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.",
        "page": "http://arxiv.org/abs/2404.01260",
        "pdf": "http://arxiv.org/pdf/2404.01260.pdf"
    },
    {
        "title": "RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D",
        "author": "Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, Xiaoguang Han",
        "abstract": "Lifting 2D diffusion for 3D generation is a challenging problem due to the lack of geometric prior and the complex entanglement of materials and lighting in natural images. Existing methods have shown promise by first creating the geometry through score-distillation sampling (SDS) applied to rendered surface normals, followed by appearance modeling. However, relying on a 2D RGB diffusion model to optimize surface normals is suboptimal due to the distribution discrepancy between natural images and normals maps, leading to instability in optimization. In this paper, recognizing that the normal and depth information effectively describe scene geometry and be automatically estimated from images, we propose to learn a generalizable Normal-Depth diffusion model for 3D generation. We achieve this by training on the large-scale LAION dataset together with the generalizable image-to-depth and normal prior models. In an attempt to alleviate the mixed illumination effects in the generated materials, we introduce an albedo diffusion model to impose data-driven constraints on the albedo component. Our experiments show that when integrated into existing text-to-3D pipelines, our models significantly enhance the detail richness, achieving state-of-the-art results. Our project page is https://aigc3d.github.io/richdreamer/.",
        "page": "http://arxiv.org/abs/2311.16918",
        "pdf": "http://arxiv.org/pdf/2311.16918.pdf"
    },
    {
        "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
        "author": "Atefeh Khoshkhahtinat, Ali Zafari, Piyush Mehta, Nasser Nasrabadi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ModaVerse: Efficiently Transforming Modalities with LLMs",
        "author": "Xinyu Wang, Bohan Zhuang, Qi Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "3D Face Tracking from 2D Video through Iterative Dense UV to Image Flow",
        "author": "Felix Taubner, Prashant Raina, Mathieu Tuli, Eu Wern Teh, Chul Lee, Jinmiao Huang",
        "abstract": "When working with 3D facial data, improving fidelity and avoiding the uncanny valley effect is critically dependent on accurate 3D facial performance capture. Because such methods are expensive and due to the widespread availability of 2D videos, recent methods have focused on how to perform monocular 3D face tracking. However, these methods often fall short in capturing precise facial movements due to limitations in their network architecture, training, and evaluation processes. Addressing these challenges, we propose a novel face tracker, FlowFace, that introduces an innovative 2D alignment network for dense per-vertex alignment. Unlike prior work, FlowFace is trained on high-quality 3D scan annotations rather than weak supervision or synthetic data. Our 3D model fitting module jointly fits a 3D face model from one or many observations, integrating existing neutral shape priors for enhanced identity and expression disentanglement and per-vertex deformations for detailed facial feature reconstruction. Additionally, we propose a novel metric and benchmark for assessing tracking accuracy. Our method exhibits superior performance on both custom and publicly available benchmarks. We further validate the effectiveness of our tracker by generating high-quality 3D data from 2D videos, which leads to performance gains on downstream tasks.",
        "page": "http://arxiv.org/abs/2404.09819",
        "pdf": "http://arxiv.org/pdf/2404.09819.pdf"
    },
    {
        "title": "Traffic Scene Parsing through the TSP6K Dataset",
        "author": "Peng-Tao Jiang, Yuqi Yang, Yang Cao, Qibin Hou, Ming-Ming Cheng, Chunhua Shen",
        "abstract": "Traffic scene perception in computer vision is a critically important task to achieve intelligent cities. To date, most existing datasets focus on autonomous driving scenes. We observe that the models trained on those driving datasets often yield unsatisfactory results on traffic monitoring scenes. However, little effort has been put into improving the traffic monitoring scene understanding, mainly due to the lack of specific datasets. To fill this gap, we introduce a specialized traffic monitoring dataset, termed TSP6K, containing images from the traffic monitoring scenario, with high-quality pixel-level and instance-level annotations. The TSP6K dataset captures more crowded traffic scenes with several times more traffic participants than the existing driving scenes. We perform a detailed analysis of the dataset and comprehensively evaluate previous popular scene parsing methods, instance segmentation methods and unsupervised domain adaption methods. Furthermore, considering the vast difference in instance sizes, we propose a detail refining decoder for scene parsing, which recovers the details of different semantic regions in traffic scenes owing to the proposed TSP6K dataset. Experiments show its effectiveness in parsing the traffic monitoring scenes. Code and dataset are available at https://github.com/PengtaoJiang/TSP6K.",
        "page": "http://arxiv.org/abs/2303.02835",
        "pdf": "http://arxiv.org/pdf/2303.02835.pdf"
    },
    {
        "title": "KPConvX: Modernizing Kernel Point Convolution with Kernel Attention",
        "author": "Hugues Thomas, Yao-Hung Hubert Tsai, Timothy Barfoot, Jian Zhang",
        "abstract": "In the field of deep point cloud understanding, KPConv is a unique architecture that uses kernel points to locate convolutional weights in space, instead of relying on Multi-Layer Perceptron (MLP) encodings. While it initially achieved success, it has since been surpassed by recent MLP networks that employ updated designs and training strategies. Building upon the kernel point principle, we present two novel designs: KPConvD (depthwise KPConv), a lighter design that enables the use of deeper architectures, and KPConvX, an innovative design that scales the depthwise convolutional weights of KPConvD with kernel attention values. Using KPConvX with a modern architecture and training strategy, we are able to outperform current state-of-the-art approaches on the ScanObjectNN, Scannetv2, and S3DIS datasets. We validate our design choices through ablation studies and release our code and models.",
        "page": "http://arxiv.org/abs/2405.13194",
        "pdf": "http://arxiv.org/pdf/2405.13194.pdf"
    },
    {
        "title": "DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models",
        "author": "Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, Song Han",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking FID: Towards a Better Evaluation Metric for Image Generation",
        "author": "Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, Sanjiv Kumar",
        "abstract": "As with many machine learning problems, the progress of image generation methods hinges on good evaluation metrics. One of the most popular is the Frechet Inception Distance (FID). FID estimates the distance between a distribution of Inception-v3 features of real images, and those of images generated by the algorithm. We highlight important drawbacks of FID: Inception's poor representation of the rich and varied content generated by modern text-to-image models, incorrect normality assumptions, and poor sample complexity. We call for a reevaluation of FID's use as the primary quality metric for generated images. We empirically demonstrate that FID contradicts human raters, it does not reflect gradual improvement of iterative text-to-image models, it does not capture distortion levels, and that it produces inconsistent results when varying the sample size. We also propose an alternative new metric, CMMD, based on richer CLIP embeddings and the maximum mean discrepancy distance with the Gaussian RBF kernel. It is an unbiased estimator that does not make any assumptions on the probability distribution of the embeddings and is sample efficient. Through extensive experiments and analysis, we demonstrate that FID-based evaluations of text-to-image models may be unreliable, and that CMMD offers a more robust and reliable assessment of image quality.",
        "page": "http://arxiv.org/abs/2401.09603",
        "pdf": "http://arxiv.org/pdf/2401.09603.pdf"
    },
    {
        "title": "CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition",
        "author": "Feng Lu, Xiangyuan Lan, Lijun Zhang, Dongmei Jiang, Yaowei Wang, Chun Yuan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor",
        "author": "Jaewon Jung, Hongsun Jang, Jaeyong Song, Jinho Lee",
        "abstract": "Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust against the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which target the parameters of the student network in the adversarial distillation process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneously in order to specialize the peer network for defending the student network. We observe that such peer networks surpass the robustness of the pretrained robust teacher model against adversarial examples aimed at the student network. With this peer network and adversarial distillation, PeerAiD achieves significantly higher robustness of the student network with AutoAttack (AA) accuracy by up to 1.66%p and improves the natural accuracy of the student network by up to 4.72%p with ResNet-18 on TinyImageNet dataset. Code is available at https://github.com/jaewonalive/PeerAiD.",
        "page": "http://arxiv.org/abs/2403.06668",
        "pdf": "http://arxiv.org/pdf/2403.06668.pdf"
    },
    {
        "title": "VBench: Comprehensive Benchmark Suite for Video Generative Models",
        "author": "Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Jin Qingyang, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, Ziwei Liu",
        "abstract": "Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects \"video generation quality\" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has three appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). The evaluation metrics with fine-grained levels reveal individual models' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models' ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. We will open-source VBench, including all prompts, evaluation methods, generated videos, and human preference annotations, and also include more video generation models in VBench to drive forward the field of video generation.",
        "page": "http://arxiv.org/abs/2311.17982",
        "pdf": "http://arxiv.org/pdf/2311.17982.pdf"
    },
    {
        "title": "Generalized Predictive Model for Autonomous Driving",
        "author": "Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, Hongyang Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations",
        "author": "Chenyu You, Yifei Min, Weicheng Dai, Jasjeet Sekhon, Lawrence Staib, James Duncan",
        "abstract": "Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any group annotation. To this end, we systematically study the existence of spurious correlation on CLIP and CILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR), verify that last-layer retraining can greatly improve group robustness on pretrained CLIP. In view of them, we advocate a lightweight representation calibration method for fine-tuning CLIP, by first generating a calibration set using the pretrained CLIP, and then calibrating representations of samples within this set through contrastive learning, all without the need for group labels. Extensive experiments and in-depth visualizations on several benchmarks validate the effectiveness of our proposals, largely reducing reliance and significantly boosting the model generalization.",
        "page": "http://arxiv.org/abs/2403.07241",
        "pdf": "http://arxiv.org/pdf/2403.07241.pdf"
    },
    {
        "title": "Neural Sign Actors: A diffusion model for 3D sign language production from text",
        "author": "Vasileios Baltatzis, Rolandos Alexandros Potamias, Evangelos Ververas, Guanxiong Sun, Jiankang Deng, Stefanos Zafeiriou",
        "abstract": "Sign Languages (SL) serve as the primary mode of communication for the Deaf and Hard of Hearing communities. Deep learning methods for SL recognition and translation have achieved promising results. However, Sign Language Production (SLP) poses a challenge as the generated motions must be realistic and have precise semantic meaning. Most SLP methods rely on 2D data, which hinders their realism. In this work, a diffusion-based SLP model is trained on a curated large-scale dataset of 4D signing avatars and their corresponding text transcripts. The proposed method can generate dynamic sequences of 3D avatars from an unconstrained domain of discourse using a diffusion process formed on a novel and anatomically informed graph neural network defined on the SMPL-X body skeleton. Through quantitative and qualitative experiments, we show that the proposed method considerably outperforms previous methods of SLP. This work makes an important step towards realistic neural sign avatars, bridging the communication gap between Deaf and hearing communities.",
        "page": "http://arxiv.org/abs/2312.02702",
        "pdf": "http://arxiv.org/pdf/2312.02702.pdf"
    },
    {
        "title": "HumMUSS: Human Motion Understanding using State Space Models",
        "author": "Arnab Mondal, Stefano Alletto, Denis Tome",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "APISR: Anime Production Inspired Real-World Anime Super-Resolution",
        "author": "Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, Hanbin Zhao",
        "abstract": "While real-world anime super-resolution (SR) has gained increasing attention in the SR community, existing methods still adopt techniques from the photorealistic domain. In this paper, we analyze the anime production workflow and rethink how to use characteristics of it for the sake of the real-world anime SR. First, we argue that video networks and datasets are not necessary for anime SR due to the repetition use of hand-drawing frames. Instead, we propose an anime image collection pipeline by choosing the least compressed and the most informative frames from the video sources. Based on this pipeline, we introduce the Anime Production-oriented Image (API) dataset. In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts. We address the first issue by introducing a prediction-oriented compression module in the image degradation model and a pseudo-ground truth preparation with enhanced hand-drawn lines. In addition, we introduce the balanced twin perceptual loss combining both anime and photorealistic high-level features to mitigate unwanted color artifacts and increase visual clarity. We evaluate our method through extensive experiments on the public benchmark, showing our method outperforms state-of-the-art anime dataset-trained approaches.",
        "page": "http://arxiv.org/abs/2403.01598",
        "pdf": "http://arxiv.org/pdf/2403.01598.pdf"
    },
    {
        "title": "ShapeWalk: Compositional Shape Editing through Language-Guided Chains",
        "author": "Habib Slim, Mohamed Elhoseiny",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion",
        "author": "Pancheng Zhao, Peng Xu, Pengda Qin, Deng-Ping Fan, Zhicheng Zhang, Guoli Jia, Bowen Zhou, Jufeng Yang",
        "abstract": "Camouflaged vision perception is an important vision task with numerous practical applications. Due to the expensive collection and labeling costs, this community struggles with a major bottleneck that the species category of its datasets is limited to a small number of object species. However, the existing camouflaged generation methods require specifying the background manually, thus failing to extend the camouflaged sample diversity in a low-cost manner. In this paper, we propose a Latent Background Knowledge Retrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. To our knowledge, our contributions mainly include: (1) For the first time, we propose a camouflaged generation paradigm that does not need to receive any background inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmented method with interpretability for camouflaged generation, in which we propose an idea that knowledge retrieval and reasoning enhancement are separated explicitly, to alleviate the task-specific challenges. Moreover, our method is not restricted to specific foreground targets or backgrounds, offering a potential for extending camouflaged vision perception to more diverse domains. (3) Experimental results demonstrate that our method outperforms the existing approaches, generating more realistic camouflage images.",
        "page": "http://arxiv.org/abs/2404.00292",
        "pdf": "http://arxiv.org/pdf/2404.00292.pdf"
    },
    {
        "title": "G-FARS: Gradient-Field-based Auto-Regressive Sampling for 3D Part Grouping",
        "author": "Junfeng Cheng, Tania Stathaki",
        "abstract": "This paper proposes a novel task named \"3D part grouping\". Suppose there is a mixed set containing scattered parts from various shapes. This task requires algorithms to find out every possible combination among all the parts. To address this challenge, we propose the so called Gradient Field-based Auto-Regressive Sampling framework (G-FARS) tailored specifically for the 3D part grouping task. In our framework, we design a gradient-field-based selection graph neural network (GNN) to learn the gradients of a log conditional probability density in terms of part selection, where the condition is the given mixed part set. This innovative approach, implemented through the gradient-field-based selection GNN, effectively captures complex relationships among all the parts in the input. Upon completion of the training process, our framework becomes capable of autonomously grouping 3D parts by iteratively selecting them from the mixed part set, leveraging the knowledge acquired by the trained gradient-field-based selection GNN. Our code is available at: https://github.com/J-F-Cheng/G-FARS-3DPartGrouping.",
        "page": "http://arxiv.org/abs/2405.06828",
        "pdf": "http://arxiv.org/pdf/2405.06828.pdf"
    },
    {
        "title": "Dual-Enhanced Coreset Selection with Class-wise Collaboration for Online Blurry Class Incremental Learning",
        "author": "Yutian Luo, Shiqi Zhao, Haoran Wu, Zhiwu Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TULIP: Multi-camera 3D Precision Assessment of Parkinson's Disease",
        "author": "Kyungdo Kim, Sihan Lyu, Sneha Mantri, Timothy DUNN",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BANF: Band-limited Neural Fields for Levels of Detail Reconstruction",
        "author": "Ahan Shabanov, Shrisudhan Govindarajan, Cody Reading, Leili Goli, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi",
        "abstract": "Largely due to their implicit nature, neural fields lack a direct mechanism for filtering, as Fourier analysis from discrete signal processing is not directly applicable to these representations. Effective filtering of neural fields is critical to enable level-of-detail processing in downstream applications, and support operations that involve sampling the field on regular grids (e.g. marching cubes). Existing methods that attempt to decompose neural fields in the frequency domain either resort to heuristics or require extensive modifications to the neural field architecture. We show that via a simple modification, one can obtain neural fields that are low-pass filtered, and in turn show how this can be exploited to obtain a frequency decomposition of the entire signal. We demonstrate the validity of our technique by investigating level-of-detail reconstruction, and showing how coarser representations can be computed effectively.",
        "page": "http://arxiv.org/abs/2404.13024",
        "pdf": "http://arxiv.org/pdf/2404.13024.pdf"
    },
    {
        "title": "Weakly Supervised Point Cloud Semantic Segmentation via Artificial Oracle",
        "author": "Hyeokjun Kweon, Jihun Kim, Kuk-Jin Yoon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "StraightPCF: Straight Point Cloud Filtering",
        "author": "Dasith de Silva Edirimuni, Xuequan Lu, Gang Li, Lei Wei, Antonio Robles-Kelly, Hongdong Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving",
        "author": "Yiming Xie, Henglu Wei, Zhenyi Liu, Xiaoyu Wang, Xiangyang Ji",
        "abstract": "To advance research in learning-based defogging algorithms, various synthetic fog datasets have been developed. However, existing datasets created using the Atmospheric Scattering Model (ASM) or real-time rendering engines often struggle to produce photo-realistic foggy images that accurately mimic the actual imaging process. This limitation hinders the effective generalization of models from synthetic to real data. In this paper, we introduce an end-to-end simulation pipeline designed to generate photo-realistic foggy images. This pipeline comprehensively considers the entire physically-based foggy scene imaging process, closely aligning with real-world image capture methods. Based on this pipeline, we present a new synthetic fog dataset named SynFog, which features both sky light and active lighting conditions, as well as three levels of fog density. Experimental results demonstrate that models trained on SynFog exhibit superior performance in visual perception and detection accuracy compared to others when applied to real-world foggy images.",
        "page": "http://arxiv.org/abs/2403.17094",
        "pdf": "http://arxiv.org/pdf/2403.17094.pdf"
    },
    {
        "title": "LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction",
        "author": "Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao",
        "abstract": "Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-following datasets. Furthermore, we unify the modeling of multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a powerful visual instruction follower without the need for complex multi-modal alignment. Our proposed approach is evaluated in language-only and multi-modal tuning experimental scenarios. Notably, LLaMA-Excitor is the only method that maintains basic capabilities while achieving a significant improvement (+6%) on the MMLU benchmark. In the visual instruction tuning, we achieve a new state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a comparable performance (88.39%) on ScienceQA to cutting-edge models with more parameters and extensive vision-language pertaining.",
        "page": "http://arxiv.org/abs/2404.00913",
        "pdf": "http://arxiv.org/pdf/2404.00913.pdf"
    },
    {
        "title": "UnSAMFlow: Unsupervised Optical Flow Guided by Segment Anything Model",
        "author": "Shuai Yuan, Lei Luo, Zhuo Hui, Can Pu, Xiaoyu Xiang, Rakesh Ranjan, Denis Demandolx",
        "abstract": "Traditional unsupervised optical flow methods are vulnerable to occlusions and motion boundaries due to lack of object-level information. Therefore, we propose UnSAMFlow, an unsupervised flow network that also leverages object information from the latest foundation model Segment Anything Model (SAM). We first include a self-supervised semantic augmentation module tailored to SAM masks. We also analyze the poor gradient landscapes of traditional smoothness losses and propose a new smoothness definition based on homography instead. A simple yet effective mask feature module has also been added to further aggregate features on the object level. With all these adaptations, our method produces clear optical flow estimation with sharp boundaries around objects, which outperforms state-of-the-art methods on both KITTI and Sintel datasets. Our method also generalizes well across domains and runs very efficiently.",
        "page": "http://arxiv.org/abs/2405.02608",
        "pdf": "http://arxiv.org/pdf/2405.02608.pdf"
    },
    {
        "title": "Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting",
        "author": "Taeho Kang, Youngki Lee",
        "abstract": "We present EgoTAP, a heatmap-to-3D pose lifting method for highly accurate stereo egocentric 3D pose estimation. Severe self-occlusion and out-of-view limbs in egocentric camera views make accurate pose estimation a challenging problem. To address the challenge, prior methods employ joint heatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3D pose conversion still remains an inaccurate process. We propose a novel heatmap-to-3D lifting method composed of the Grid ViT Encoder and the Propagation Network. The Grid ViT Encoder summarizes joint heatmaps into effective feature embedding using self-attention. Then, the Propagation Network estimates the 3D pose by utilizing skeletal information to better estimate the position of obscure joints. Our method significantly outperforms the previous state-of-the-art qualitatively and quantitatively demonstrated by a 23.9\\% reduction of error in an MPJPE metric. Our source code is available in GitHub.",
        "page": "http://arxiv.org/abs/2402.18330",
        "pdf": "http://arxiv.org/pdf/2402.18330.pdf"
    },
    {
        "title": "Progress-Aware Online Action Segmentation for Egocentric Procedural Task Videos",
        "author": "Yuhan Shen, Ehsan Elhamifar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "REACTO: Reconstructing Articulated Objects from a Single Video",
        "author": "Chaoyue Song, Jiacheng Wei, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu",
        "abstract": "In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video. Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models. To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints. Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation. Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets. Project page: https://chaoyuesong.github.io/REACTO.",
        "page": "http://arxiv.org/abs/2404.11151",
        "pdf": "http://arxiv.org/pdf/2404.11151.pdf"
    },
    {
        "title": "Structured Gradient-based Interpretations via Norm-Regularized Adversarial Training",
        "author": "Shizhan Gong, Qi Dou, Farzan Farnia",
        "abstract": "Gradient-based saliency maps have been widely used to explain the decisions of deep neural network classifiers. However, standard gradient-based interpretation maps, including the simple gradient and integrated gradient algorithms, often lack desired structures such as sparsity and connectedness in their application to real-world computer vision models. A frequently used approach to inducing sparsity structures into gradient-based saliency maps is to alter the simple gradient scheme using sparsification or norm-based regularization. A drawback with such post-processing methods is their frequently-observed significant loss in fidelity to the original simple gradient map. In this work, we propose to apply adversarial training as an in-processing scheme to train neural networks with structured simple gradient maps. We show a duality relation between the regularized norms of the adversarial perturbations and gradient-based maps, based on which we design adversarial training loss functions promoting sparsity and group-sparsity properties in simple gradient maps. We present several numerical results to show the influence of our proposed norm-based adversarial training methods on the standard gradient-based maps of standard neural network architectures on benchmark image datasets.",
        "page": "http://arxiv.org/abs/2404.04647",
        "pdf": "http://arxiv.org/pdf/2404.04647.pdf"
    },
    {
        "title": "A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning",
        "author": "Siddharth Srivastava, Gaurav Sharma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Transfer CLIP for Generalizable Image Denoising",
        "author": "Jun Cheng, Dong Liang, Shan Tan",
        "abstract": "Image denoising is a fundamental task in computer vision. While prevailing deep learning-based supervised and self-supervised methods have excelled in eliminating in-distribution noise, their susceptibility to out-of-distribution (OOD) noise remains a significant challenge. The recent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation. Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored. This paper uncovers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising. Leveraging these properties, we devise an asymmetrical encoder-decoder denoising network, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising. The progressive feature augmentation strategy is further proposed to mitigate feature overfitting and improve the robustness of the learnable decoder. Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method.",
        "page": "http://arxiv.org/abs/2403.15132",
        "pdf": "http://arxiv.org/pdf/2403.15132.pdf"
    },
    {
        "title": "LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images",
        "author": "Jing Zhang, Irving Fang, Hao Wu, Akshat Kaushik, Alice Rodriguez, Hanwen Zhao, Juexiao Zhang, Zhuo Zheng, Radu Iovita, Chen Feng",
        "abstract": "Lithic Use-Wear Analysis (LUWA) using microscopic images is an underexplored vision-for-science research area. It seeks to distinguish the worked material, which is critical for understanding archaeological artifacts, material interactions, tool functionalities, and dental records. However, this challenging task goes beyond the well-studied image classification problem for common objects. It is affected by many confounders owing to the complex wear mechanism and microscopic imaging, which makes it difficult even for human experts to identify the worked material successfully. In this paper, we investigate the following three questions on this unique vision task for the first time:(i) How well can state-of-the-art pre-trained models (like DINOv2) generalize to the rarely seen domain? (ii) How can few-shot learning be exploited for scarce microscopic images? (iii) How do the ambiguous magnification and sensing modality influence the classification accuracy? To study these, we collaborated with archaeologists and built the first open-source and the largest LUWA dataset containing 23,130 microscopic images with different magnifications and sensing modalities. Extensive experiments show that existing pre-trained models notably outperform human experts but still leave a large gap for improvements. Most importantly, the LUWA dataset provides an underexplored opportunity for vision and learning communities and complements existing image classification problems on common objects.",
        "page": "http://arxiv.org/abs/2403.13171",
        "pdf": "http://arxiv.org/pdf/2403.13171.pdf"
    },
    {
        "title": "Correlation-aware Coarse-to-fine MLPs for Deformable Medical Image Registration",
        "author": "Mingyuan Meng, Dagan Feng, Lei Bi, Jinman Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PixelLM: Pixel Reasoning with Large Multimodal Model",
        "author": "Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, Xiaojie Jin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neural Fields as Distributions: Signal Processing Beyond Euclidean Space",
        "author": "Daniel Rebain, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OmniMotionGPT: Animal Motion Generation with Limited Data",
        "author": "Zhangsihao Yang, Mingyuan Zhou, Mengyi Shan, Bingbing Wen, Ziwei Xuan, Mitch Hill, Junjie Bai, Guo-Jun Qi, Yalin Wang",
        "abstract": "Our paper aims to generate diverse and realistic animal motion sequences from textual descriptions, without a large-scale animal text-motion dataset. While the task of text-driven human motion synthesis is already extensively studied and benchmarked, it remains challenging to transfer this success to other skeleton structures with limited data. In this work, we design a model architecture that imitates Generative Pretraining Transformer (GPT), utilizing prior knowledge learned from human data to the animal domain. We jointly train motion autoencoders for both animal and human motions and at the same time optimize through the similarity scores among human motion encoding, animal motion encoding, and text CLIP embedding. Presenting the first solution to this problem, we are able to generate animal motions with high diversity and fidelity, quantitatively and qualitatively outperforming the results of training human motion generation baselines on animal data. Additionally, we introduce AnimalML3D, the first text-animal motion dataset with 1240 animation sequences spanning 36 different animal identities. We hope this dataset would mediate the data scarcity problem in text-driven animal motion generation, providing a new playground for the research community.",
        "page": "http://arxiv.org/abs/2311.18303",
        "pdf": "http://arxiv.org/pdf/2311.18303.pdf"
    },
    {
        "title": "Learning SO(3)-Invariant Semantic Correspondence via Local Shape Transform",
        "author": "Chunghyun Park, Seungwook Kim, Jaesik Park, Minsu Cho",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning",
        "author": "Rishub Tamirisa, Chulin Xie, Wenxuan Bao, Andy Zhou, Ron Arel, Aviv Shamsian",
        "abstract": "Standard federated learning approaches suffer when client data distributions have sufficient heterogeneity. Recent methods addressed the client data heterogeneity issue via personalized federated learning (PFL) - a class of FL algorithms aiming to personalize learned global knowledge to better suit the clients' local data distributions. Existing PFL methods usually decouple global updates in deep neural networks by performing personalization on particular layers (i.e. classifier heads) and global aggregation for the rest of the network. However, preselecting network layers for personalization may result in suboptimal storage of global knowledge. In this work, we propose FedSelect, a novel PFL algorithm inspired by the iterative subnetwork discovery procedure used for the Lottery Ticket Hypothesis. FedSelect incrementally expands subnetworks to personalize client parameters, concurrently conducting global aggregations on the remaining parameters. This approach enables the personalization of both client parameters and subnetwork structure during the training process. Finally, we show that FedSelect outperforms recent state-of-the-art PFL algorithms under challenging client data heterogeneity settings and demonstrates robustness to various real-world distributional shifts. Our code is available at https://github.com/lapisrocks/fedselect.",
        "page": "http://arxiv.org/abs/2404.02478",
        "pdf": "http://arxiv.org/pdf/2404.02478.pdf"
    },
    {
        "title": "Test-Time Adaptation for Depth Completion",
        "author": "Hyoungseob Park, Anjali W Gupta, Alex Wong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dispel Darkness for Better Fusion: A Controllable Visual Enhancer based on Cross-modal Conditional Adversarial Learning",
        "author": "HAO ZHANG, Linfeng Tang, Xinyu Xiang, Xuhui Zuo, Jiayi Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GenN2N: Generative NeRF2NeRF Translation",
        "author": "Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan, Li Yi",
        "abstract": "We present GenN2N, a unified NeRF-to-NeRF translation framework for various NeRF translation tasks such as text-driven NeRF editing, colorization, super-resolution, inpainting, etc. Unlike previous methods designed for individual translation tasks with task-specific schemes, GenN2N achieves all these NeRF editing tasks by employing a plug-and-play image-to-image translator to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF space. Since the 3D consistency of 2D edits may not be assured, we propose to model the distribution of the underlying 3D edits through a generative model that can cover all possible edited NeRFs. To model the distribution of 3D edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes images while decoding NeRFs. The latent space is trained to align with a Gaussian distribution and the NeRFs are supervised through an adversarial loss on its renderings. To ensure the latent code does not depend on 2D viewpoints but truly reflects the 3D edits, we also regularize the latent code through a contrastive learning scheme. Extensive experiments on various editing tasks show GenN2N, as a universal framework, performs as well or better than task-specific specialists while possessing flexible generative power. More results on our project page: https://xiangyueliu.github.io/GenN2N/",
        "page": "http://arxiv.org/abs/2404.02788",
        "pdf": "http://arxiv.org/pdf/2404.02788.pdf"
    },
    {
        "title": "Parameter Efficient Self-Supervised Geospatial Domain Adaptation",
        "author": "Linus Scheibenreif, Michael Mommert, Damian Borth",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multimodal Representation Learning by Alternating Unimodal Adaptation",
        "author": "Xiaohui Zhang, Xiaohui Zhang, Jaehong Yoon, Mohit Bansal, Huaxiu Yao",
        "abstract": "Multimodal learning, which integrates data from diverse sensory modes, plays a pivotal role in artificial intelligence. However, existing multimodal learning methods often struggle with challenges where some modalities appear more dominant than others during multimodal learning, resulting in suboptimal performance. To address this challenge, we propose MLA (Multimodal Learning with Alternating Unimodal Adaptation). MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities. Simultaneously, it captures cross-modal interactions through a shared head, which undergoes continuous optimization across different modalities. This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information. During the inference phase, MLA utilizes a test-time uncertainty-based model fusion mechanism to integrate multimodal information. Extensive experiments are conducted on five diverse datasets, encompassing scenarios with complete modalities and scenarios with missing modalities. These experiments demonstrate the superiority of MLA over competing prior approaches. Our code is available at https://github.com/Cecile-hi/Multimodal-Learning-with-Alternating-Unimodal-Adaptation.",
        "page": "http://arxiv.org/abs/2311.10707",
        "pdf": "http://arxiv.org/pdf/2311.10707.pdf"
    },
    {
        "title": "SNED: Superposition Network Architecture Search for Efficient Video Diffusion Model",
        "author": "Zhengang Li, Yan Kang, Yuchen Liu, Difan Liu, Tobias Hinz, Feng Liu, Yanzhi Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Compositional Video Understanding with Spatiotemporal Structure-based Transformers",
        "author": "Hoyeoung Yun, Jinwoo Ahn, Minseo Kim, Eun-Sol Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CoDi-2: Interleaved and In-Context Any-to-Any Generation",
        "author": "Zineng Tang, Ziyi Yang, MAHMOUD KHADEMI, Yang Liu, Chenguang Zhu, Mohit Bansal",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation",
        "author": "Yamei Chen, Yan Di, Guangyao Zhai, Fabian Manhardt, Chenyangguang Zhang, Ruida Zhang, Federico Tombari, Nassir Navab, Benjamin Busam",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation",
        "author": "Dar-Yen Chen, Hamish Tennent, Ching-Wen Hsu",
        "abstract": "This work introduces ArtAdapter, a transformative text-to-image (T2I) style transfer framework that transcends traditional limitations of color, brushstrokes, and object shape, capturing high-level style elements such as composition and distinctive artistic expression. The integration of a multi-level style encoder with our proposed explicit adaptation mechanism enables ArtAdapter to achieve unprecedented fidelity in style transfer, ensuring close alignment with textual descriptions. Additionally, the incorporation of an Auxiliary Content Adapter (ACA) effectively separates content from style, alleviating the borrowing of content from style references. Moreover, our novel fast finetuning approach could further enhance zero-shot style representation while mitigating the risk of overfitting. Comprehensive evaluations confirm that ArtAdapter surpasses current state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2312.02109",
        "pdf": "http://arxiv.org/pdf/2312.02109.pdf"
    },
    {
        "title": "On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation",
        "author": "Agneet Chatterjee, Tejas Gokhale, Chitta Baral, 'YZ' Yezhou Yang",
        "abstract": "Recent advances in monocular depth estimation have been made by incorporating natural language as additional guidance. Although yielding impressive results, the impact of the language prior, particularly in terms of generalization and robustness, remains unexplored. In this paper, we address this gap by quantifying the impact of this prior and introduce methods to benchmark its effectiveness across various settings. We generate \"low-level\" sentences that convey object-centric, three-dimensional spatial relationships, incorporate them as additional language priors and evaluate their downstream impact on depth estimation. Our key finding is that current language-guided depth estimators perform optimally only with scene-level descriptions and counter-intuitively fare worse with low level descriptions. Despite leveraging additional data, these methods are not robust to directed adversarial attacks and decline in performance with an increase in distribution shift. Finally, to provide a foundation for future research, we identify points of failures and offer insights to better understand these shortcomings. With an increasing number of methods using language for depth estimation, our findings highlight the opportunities and pitfalls that require careful consideration for effective deployment in real-world settings",
        "page": "http://arxiv.org/abs/2404.08540",
        "pdf": "http://arxiv.org/pdf/2404.08540.pdf"
    },
    {
        "title": "SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model",
        "author": "Inhwan Bae, Young-Jae Park, Hae-Gon Jeon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TexVocab: Texture Vocabulary-conditioned Human Avatars",
        "author": "Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Hyper-MD: Mesh Denoising with Customized Parameters Aware of Noise Intensity and Geometric Characteristics",
        "author": "Xingtao Wang, Hongliang Wei, Xiaopeng Fan, Debin Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsupervised Template-assisted  Point Cloud Shape Correspondence Network",
        "author": "Jiacheng Deng, Jiahao Lu, Tianzhu Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VideoCon: Robust Video-Language Alignment via Contrast Captions",
        "author": "Hritik Bansal, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang, Aditya Grover",
        "abstract": "Despite being (pre)trained on a massive amount of data, state-of-the-art video-language alignment models are not robust to semantically-plausible contrastive changes in the video captions. Our work addresses this by identifying a broad spectrum of contrast misalignments, such as replacing entities, actions, and flipping event order, which alignment models should be robust against. To this end, we introduce the VideoCon, a video-language alignment dataset constructed by a large language model that generates plausible contrast video captions and explanations for differences between original and contrast video captions. Then, a generative video-language model is finetuned with VideoCon to assess video-language entailment and generate explanations. Our VideoCon-based alignment model significantly outperforms current models. It exhibits a 12-point increase in AUC for the video-language alignment task on human-generated contrast captions. Finally, our model sets new state of the art zero-shot performance in temporally-extensive video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video question answering (ATP-Hard). Moreover, our model shows superior performance on novel videos and human-crafted captions and explanations. Our code and data are available at https://github.com/Hritikbansal/videocon.",
        "page": "http://arxiv.org/abs/2311.10111",
        "pdf": "http://arxiv.org/pdf/2311.10111.pdf"
    },
    {
        "title": "UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All",
        "author": "Yuanhuiyi Lyu, Xu Zheng, Jiazhou Zhou, Lin Wang",
        "abstract": "We present UniBind, a flexible and efficient approach that learns a unified representation space for seven diverse modalities -- images, text, audio, point cloud, thermal, video, and event data. Existing works, eg., ImageBind, treat the image as the central modality and build an image-centered representation space; however, the space may be sub-optimal as it leads to an unbalanced representation space among all modalities. Moreover, the category names are directly used to extract text embeddings for the downstream tasks, making it hardly possible to represent the semantics of multi-modal data. The 'out-of-the-box' insight of our UniBind is to make the alignment center modality-agnostic and further learn a unified and balanced representation space, empowered by the large language models (LLMs). UniBind is superior in its flexible application to all CLIP-style models and delivers remarkable performance boosts. To make this possible, we 1) construct a knowledge base of text embeddings with the help of LLMs and multi-modal LLMs; 2) adaptively build LLM-augmented class-wise embedding center on top of the knowledge base and encoded visual embeddings; 3) align all the embeddings to the LLM-augmented embedding center via contrastive learning to achieve a unified and balanced representation space. UniBind shows strong zero-shot recognition performance gains over prior arts by an average of 6.36%. Finally, we achieve new state-of-the-art performance, eg., a 6.75% gain on ImageNet, on the multi-modal fine-tuning setting while reducing 90% of the learnable parameters.",
        "page": "http://arxiv.org/abs/2403.12532",
        "pdf": "http://arxiv.org/pdf/2403.12532.pdf"
    },
    {
        "title": "HEAL-SWIN: A Vision Transformer On The Sphere",
        "author": "Oscar Carlsson, Jan E. Gerken, Hampus Linander, Heiner Spiess, Fredrik Ohlsson, Christoffer Petersson, Daniel Persson",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VCoder: Versatile Vision Encoders for Multimodal Large Language Models",
        "author": "Jitesh Jain, Jianwei Yang, Humphrey Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "QN-Mixer: A Quasi-Newton MLP-Mixer Model for Sparse-View CT Reconstruction",
        "author": "Ishak Ayad, Nicolas Larue, Mai K. Nguyen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CrossKD: Cross-Head Knowledge Distillation for Dense Object Detection",
        "author": "JiaBao Wang, yuming chen, Zhaohui Zheng, Xiang Li, Ming-Ming Cheng, Qibin Hou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Leveraging Camera Triplets for Efficient and Accurate Structure-from-Motion",
        "author": "Lalit Manam, Venu Madhav Govindu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Equivariant Multi-Modality Image Fusion",
        "author": "Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Kai Zhang, Shuang Xu, Dongdong Chen, Radu Timofte, Luc Van Gool",
        "abstract": "Multi-modality image fusion is a technique that combines information from different sensors or modalities, enabling the fused image to retain complementary features from each modality, such as functional highlights and texture details. However, effective training of such fusion models is challenging due to the scarcity of ground truth fusion data. To tackle this issue, we propose the Equivariant Multi-Modality imAge fusion (EMMA) paradigm for end-to-end self-supervised learning. Our approach is rooted in the prior knowledge that natural imaging responses are equivariant to certain transformations. Consequently, we introduce a novel training paradigm that encompasses a fusion module, a pseudo-sensing module, and an equivariant fusion module. These components enable the net training to follow the principles of the natural sensing-imaging process while satisfying the equivariant imaging prior. Extensive experiments confirm that EMMA yields high-quality fusion results for infrared-visible and medical images, concurrently facilitating downstream multi-modal segmentation and detection tasks. The code is available at https://github.com/Zhaozixiang1228/MMIF-EMMA.",
        "page": "http://arxiv.org/abs/2305.11443",
        "pdf": "http://arxiv.org/pdf/2305.11443.pdf"
    },
    {
        "title": "CCEdit: Creative and Controllable Video Editing via Diffusion Models",
        "author": "Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, Baining Guo",
        "abstract": "In this paper, we present CCEdit, a versatile generative video editing framework based on diffusion models. Our approach employs a novel trident network structure that separates structure and appearance control, ensuring precise and creative editing capabilities. Utilizing the foundational ControlNet architecture, we maintain the structural integrity of the video during editing. The incorporation of an additional appearance branch enables users to exert fine-grained control over the edited key frame. These two side branches seamlessly integrate into the main branch, which is constructed upon existing text-to-image (T2I) generation models, through learnable temporal layers. The versatility of our framework is demonstrated through a diverse range of choices in both structure representations and personalized T2I models, as well as the option to provide the edited key frame. To facilitate comprehensive evaluation, we introduce the BalanceCC benchmark dataset, comprising 100 videos and 4 target prompts for each video. Our extensive user studies compare CCEdit with eight state-of-the-art video editing methods. The outcomes demonstrate CCEdit's substantial superiority over all other methods.",
        "page": "http://arxiv.org/abs/2309.16496",
        "pdf": "http://arxiv.org/pdf/2309.16496.pdf"
    },
    {
        "title": "MoDE: CLIP Data Experts via Clustering",
        "author": "Jiawei Ma, Po-Yao Huang, Saining Xie, Shang-Wen Li, Luke Zettlemoyer, Shih-Fu Chang, Wen-tau Yih, Hu Xu",
        "abstract": "The success of contrastive language-image pretraining (CLIP) relies on the supervision from the pairing between images and captions, which tends to be noisy in web-crawled data. We present Mixture of Data Experts (MoDE) and learn a system of CLIP data experts via clustering. Each data expert is trained on one data cluster, being less sensitive to false negative noises in other clusters. At inference time, we ensemble their outputs by applying weights determined through the correlation between task metadata and cluster conditions. To estimate the correlation precisely, the samples in one cluster should be semantically similar, but the number of data experts should still be reasonable for training and inference. As such, we consider the ontology in human language and propose to use fine-grained cluster centers to represent each data expert at a coarse-grained level. Experimental studies show that four CLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and OpenCLIP on zero-shot image classification but with less ($<$35\\%) training cost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly include new data experts. The code is available at https://github.com/facebookresearch/MetaCLIP/tree/main/mode.",
        "page": "http://arxiv.org/abs/2404.16030",
        "pdf": "http://arxiv.org/pdf/2404.16030.pdf"
    },
    {
        "title": "SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation",
        "author": "Aysim Toker, Marvin Eisenberger, Daniel Cremers, Laura Leal-Taixe",
        "abstract": "In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery. Yet, a prevalent limitation of supervised learning techniques remains the need for extensive manual annotations by experts. In this work, we explore the potential of generative image diffusion to address the scarcity of annotated data in earth observation tasks. The main idea is to learn the joint data manifold of images and labels, leveraging recent advancements in denoising diffusion probabilistic models. To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation. We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity. Both aspects are crucial for earth observation data, where semantic classes can vary severely in scale and occurrence frequency. We employ the novel data instances for downstream segmentation, as a form of data augmentation. In our experiments, we provide comparisons to prior works based on discriminative diffusion models or GANs. We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation -- both compared to baselines and when training only on the original data.",
        "page": "http://arxiv.org/abs/2403.16605",
        "pdf": "http://arxiv.org/pdf/2403.16605.pdf"
    },
    {
        "title": "Dual-consistency Model Inversion for Non-exemplar Class Incremental Learning",
        "author": "Zihuan Qiu, Yi Xu, Fanman Meng, Hongliang Li, Linfeng Xu, Qingbo Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Class Tokens Infusion for Weakly Supervised Semantic Segmentation",
        "author": "Sung-Hoon Yoon, Hoyong Kwon, Hyeonseong Kim, Kuk-Jin Yoon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "JoAPR: Cleaning the Lens of Prompt Learning for Vision-Language Models",
        "author": "YUNCHENG GUO, Xiaodong Gu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Category Agnostic Model for Visual Rearrangement",
        "author": "Yuyi Liu, Xinhang Song, Weijie Li, Xiaohan Wang, Shuqiang Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
        "author": "Ziyao Zeng, Hyoungseob Park, Fengyu Yang, Daniel Wang, Stefano Soatto, Dong Lao, Alex Wong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
        "author": "Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj Jha, Yuchen Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Forecasting of 3D Whole-body Human Poses with Grasping Objects",
        "author": "yan haitao, Qiongjie Cui, Jiexin Xie, Shijie Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
        "author": "Xiang Li, Qianli Shen, Kenji Kawaguchi",
        "abstract": "The booming use of text-to-image generative models has raised concerns about their high risk of producing copyright-infringing content. While probabilistic copyright protection methods provide a probabilistic guarantee against such infringement, in this paper, we introduce Virtually Assured Amplification Attack (VA3), a novel online attack framework that exposes the vulnerabilities of these protection mechanisms. The proposed framework significantly amplifies the probability of generating infringing content on the sustained interactions with generative models and a non-trivial lower-bound on the success probability of each engagement. Our theoretical and experimental results demonstrate the effectiveness of our approach under various scenarios. These findings highlight the potential risk of implementing probabilistic copyright protection in practical applications of text-to-image generative models. Code is available at https://github.com/South7X/VA3.",
        "page": "http://arxiv.org/abs/2312.00057",
        "pdf": "http://arxiv.org/pdf/2312.00057.pdf"
    },
    {
        "title": "Towards Co-Evaluation of Cameras, HDR, and Algorithms for Industrial-Grade 6DoF Pose Estimation",
        "author": "Agastya Kalra, Guy Stoppi, Dmitrii Marin, Vage Taamazyan, Aarrushi Shandilya, Rishav Agarwal, Anton Boykov, Aaron Chong, Michael Stark",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Correcting Diffusion Generation through Resampling",
        "author": "Yujian Liu, Yang Zhang, Tommi Jaakkola, Shiyu Chang",
        "abstract": "Despite diffusion models' superior capabilities in modeling complex distributions, there are still non-trivial distributional discrepancies between generated and ground-truth images, which has resulted in several notable problems in image generation, including missing object errors in text-to-image generation and low image quality. Existing methods that attempt to address these problems mostly do not tend to address the fundamental cause behind these problems, which is the distributional discrepancies, and hence achieve sub-optimal results. In this paper, we propose a particle filtering framework that can effectively address both problems by explicitly reducing the distributional discrepancies. Specifically, our method relies on a set of external guidance, including a small set of real images and a pre-trained object detector, to gauge the distribution gap, and then design the resampling weight accordingly to correct the gap. Experiments show that our methods can effectively correct missing object errors and improve image quality in various image generation tasks. Notably, our method outperforms the existing strongest baseline by 5% in object occurrence and 1.0 in FID on MS-COCO. Our code is publicly available at https://github.com/UCSB-NLP-Chang/diffusion_resampling.git.",
        "page": "http://arxiv.org/abs/2312.06038",
        "pdf": "http://arxiv.org/pdf/2312.06038.pdf"
    },
    {
        "title": "Partial-to-Partial Shape Matching with Geometric Consistency",
        "author": "Viktoria Ehm, Maolin Gao, Paul Roetzer, Marvin Eisenberger, Daniel Cremers, Florian Bernard",
        "abstract": "Finding correspondences between 3D shapes is an important and long-standing problem in computer vision, graphics and beyond. A prominent challenge are partial-to-partial shape matching settings, which occur when the shapes to match are only observed incompletely (e.g. from 3D scanning). Although partial-to-partial matching is a highly relevant setting in practice, it is rarely explored. Our work bridges the gap between existing (rather artificial) 3D full shape matching and partial-to-partial real-world settings by exploiting geometric consistency as a strong constraint. We demonstrate that it is indeed possible to solve this challenging problem in a variety of settings. For the first time, we achieve geometric consistency for partial-to-partial matching, which is realized by a novel integer non-linear program formalism building on triangle product spaces, along with a new pruning algorithm based on linear integer programming. Further, we generate a new inter-class dataset for partial-to-partial shape-matching. We show that our method outperforms current SOTA methods on both an established intra-class dataset and our novel inter-class dataset.",
        "page": "http://arxiv.org/abs/2404.12209",
        "pdf": "http://arxiv.org/pdf/2404.12209.pdf"
    },
    {
        "title": "Text-guided Explorable Image Super-resolution",
        "author": "Kanchana Vaishnavi Gandikota, Paramanand Chandramouli",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised Clustering Analysis",
        "author": "Mingyang Zhao, Jiang Jingen, Lei Ma, Shiqing Xin, Gaofeng Meng, Dong-Ming Yan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation",
        "author": "Philipp Schr\u00f6ppel, Christopher Wewer, Jan Lenssen, Eddy Ilg, Thomas Brox",
        "abstract": "Controllable generation of 3D assets is important for many practical applications like content creation in movies, games and engineering, as well as in AR/VR. Recently, diffusion models have shown remarkable results in generation quality of 3D objects. However, none of the existing models enable disentangled generation to control the shape and appearance separately. For the first time, we present a suitable representation for 3D diffusion models to enable such disentanglement by introducing a hybrid point cloud and neural radiance field approach. We model a diffusion process over point positions jointly with a high-dimensional feature space for a local density and radiance decoder. While the point positions represent the coarse shape of the object, the point features allow modeling the geometry and appearance details. This disentanglement enables us to sample both independently and therefore to control both separately. Our approach sets a new state of the art in generation compared to previous disentanglement-capable methods by reduced FID scores of 30-90% and is on-par with other non disentanglement-capable state-of-the art methods.",
        "page": "http://arxiv.org/abs/2312.14124",
        "pdf": "http://arxiv.org/pdf/2312.14124.pdf"
    },
    {
        "title": "Exploiting Style Latent Flows for Generalizing Video Deepfake Detection",
        "author": "Jongwook Choi, Taehoon Kim, Yonghyun Jeong, Seungryul Baek, Jongwon Choi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge",
        "author": "Haoxiang Ma, Modi Shi, Boyang GAO, Di Huang",
        "abstract": "We focus on the generalization ability of the 6-DoF grasp detection method in this paper. While learning-based grasp detection methods can predict grasp poses for unseen objects using the grasp distribution learned from the training set, they often exhibit a significant performance drop when encountering objects with diverse shapes and structures. To enhance the grasp detection methods' generalization ability, we incorporate domain prior knowledge of robotic grasping, enabling better adaptation to objects with significant shape and structure differences. More specifically, we employ the physical constraint regularization during the training phase to guide the model towards predicting grasps that comply with the physical rule on grasping. For the unstable grasp poses predicted on novel objects, we design a contact-score joint optimization using the projection contact map to refine these poses in cluttered scenarios. Extensive experiments conducted on the GraspNet-1billion benchmark demonstrate a substantial performance gain on the novel object set and the real-world grasping experiments also demonstrate the effectiveness of our generalizing 6-DoF grasp detection method.",
        "page": "http://arxiv.org/abs/2404.01727",
        "pdf": "http://arxiv.org/pdf/2404.01727.pdf"
    },
    {
        "title": "Grid Diffusion Models for Text-to-Video Generation",
        "author": "Taegyeong Lee, Soyeong Kwon, Taehwan Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Personalized Residuals for Concept-Driven Text-to-Image Generation",
        "author": "Cusuh Ham, Matthew Fisher, James Hays, Nicholas Kolkin, Yuchen Liu, Richard Zhang, Tobias Hinz",
        "abstract": "We present personalized residuals and localized attention-guided sampling for efficient concept-driven generation using text-to-image diffusion models. Our method first represents concepts by freezing the weights of a pretrained text-conditioned diffusion model and learning low-rank residuals for a small subset of the model's layers. The residual-based approach then directly enables application of our proposed sampling technique, which applies the learned residuals only in areas where the concept is localized via cross-attention and applies the original diffusion weights in all other regions. Localized sampling therefore combines the learned identity of the concept with the existing generative prior of the underlying diffusion model. We show that personalized residuals effectively capture the identity of a concept in ~3 minutes on a single GPU without the use of regularization images and with fewer parameters than previous models, and localized sampling allows using the original model as strong prior for large parts of the image.",
        "page": "http://arxiv.org/abs/2405.12978",
        "pdf": "http://arxiv.org/pdf/2405.12978.pdf"
    },
    {
        "title": "Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration",
        "author": "Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, Ran He",
        "abstract": "Despite substantial progress, all-in-one image restoration (IR) grapples with persistent challenges in handling intricate real-world degradations. This paper introduces MPerceiver: a novel multimodal prompt learning approach that harnesses Stable Diffusion (SD) priors to enhance adaptiveness, generalizability and fidelity for all-in-one image restoration. Specifically, we develop a dual-branch module to master two types of SD prompts: textual for holistic representation and visual for multiscale detail representation. Both prompts are dynamically adjusted by degradation predictions from the CLIP image encoder, enabling adaptive responses to diverse unknown degradations. Moreover, a plug-in detail refinement module improves restoration fidelity via direct encoder-to-decoder information transformation. To assess our method, MPerceiver is trained on 9 tasks for all-in-one IR and outperforms state-of-the-art task-specific methods across most tasks. Post multitask pre-training, MPerceiver attains a generalized representation in low-level vision, exhibiting remarkable zero-shot and few-shot capabilities in unseen tasks. Extensive experiments on 16 IR tasks underscore the superiority of MPerceiver in terms of adaptiveness, generalizability and fidelity.",
        "page": "http://arxiv.org/abs/2312.02918",
        "pdf": "http://arxiv.org/pdf/2312.02918.pdf"
    },
    {
        "title": "Balancing Act: Distribution-Guided Debiasing in Diffusion Models",
        "author": "Rishubh Parihar, Abhijnya Bhat, Abhipsa Basu, Saswat Mallick, Jogendra Kundu Kundu, R. Venkatesh Babu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning to Count without Annotations",
        "author": "Lukas Knobel, Tengda Han, Yuki Asano",
        "abstract": "While recent supervised methods for reference-based object counting continue to improve the performance on benchmark datasets, they have to rely on small datasets due to the cost associated with manually annotating dozens of objects in images. We propose UnCounTR, a model that can learn this task without requiring any manual annotations. To this end, we construct \"Self-Collages\", images with various pasted objects as training samples, that provide a rich learning signal covering arbitrary object types and counts. Our method builds on existing unsupervised representations and segmentation techniques to successfully demonstrate for the first time the ability of reference-based counting without manual supervision. Our experiments show that our method not only outperforms simple baselines and generic models such as FasterRCNN and DETR, but also matches the performance of supervised counting models in some domains.",
        "page": "http://arxiv.org/abs/2307.08727",
        "pdf": "http://arxiv.org/pdf/2307.08727.pdf"
    },
    {
        "title": "Towards Backward-Compatible Continual Learning of Image Compression",
        "author": "Zhihao Duan, Ming Lu, Justin Yang, Jiangpeng He, Zhan Ma, Fengqing Zhu",
        "abstract": "This paper explores the possibility of extending the capability of pre-trained neural image compressors (e.g., adapting to new data or target bitrates) without breaking backward compatibility, the ability to decode bitstreams encoded by the original model. We refer to this problem as continual learning of image compression. Our initial findings show that baseline solutions, such as end-to-end fine-tuning, do not preserve the desired backward compatibility. To tackle this, we propose a knowledge replay training strategy that effectively addresses this issue. We also design a new model architecture that enables more effective continual learning than existing baselines. Experiments are conducted for two scenarios: data-incremental learning and rate-incremental learning. The main conclusion of this paper is that neural image compressors can be fine-tuned to achieve better performance (compared to their pre-trained version) on new data and rates without compromising backward compatibility. Our code is available at https://gitlab.com/viper-purdue/continual-compression",
        "page": "http://arxiv.org/abs/2402.18862",
        "pdf": "http://arxiv.org/pdf/2402.18862.pdf"
    },
    {
        "title": "Spike-guided Motion Deblurring with Unknown Modal Spatiotemporal Alignment",
        "author": "Jiyuan Zhang, Shiyan Chen, Yajing Zheng, Zhaofei Yu, Tiejun Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-Task Dense Prediction via Mixture of Low-Rank Experts",
        "author": "Yuqi Yang, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Bo Li",
        "abstract": "Previous multi-task dense prediction methods based on the Mixture of Experts (MoE) have received great performance but they neglect the importance of explicitly modeling the global relations among all tasks. In this paper, we present a novel decoder-focused method for multi-task dense prediction, called Mixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships, MLoRE adds a generic convolution path to the original MoE structure, where each task feature can go through this path for explicit parameter sharing. Furthermore, to control the parameters and computational cost brought by the increase in the number of experts, we take inspiration from LoRA and propose to leverage the low-rank format of a vanilla convolution in the expert network. Since the low-rank experts have fewer parameters and can be dynamically parameterized into the generic convolution, the parameters and computational cost do not change much with the increase of experts. Benefiting from this design, we increase the number of experts and its reception field to enlarge the representation capacity, facilitating multiple dense tasks learning in a unified network. Extensive experiments on the PASCAL-Context and NYUD-v2 benchmarks show that our MLoRE achieves superior performance compared to previous state-of-the-art methods on all metrics. Our code is available at https://github.com/YuqiYang213/MLoRE.",
        "page": "http://arxiv.org/abs/2403.17749",
        "pdf": "http://arxiv.org/pdf/2403.17749.pdf"
    },
    {
        "title": "Making Visual Sense of Oracle Bones for You and Me",
        "author": "Runqi Qiao, LAN YANG, Kaiyue Pang, Honggang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Omni-Q: Omni-Directional Scene Understanding for Unsupervised Visual Grounding",
        "author": "Sai Wang, Yutian Lin, Yutian Lin, Yu Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Communication-Efficient Federated Learning with Accelerated Client Gradient",
        "author": "Geeho Kim, Jinkyu Kim, Bohyung Han",
        "abstract": "Federated learning often suffers from slow and unstable convergence due to the heterogeneous characteristics of participating client datasets. Such a tendency is aggravated when the client participation ratio is low since the information collected from the clients has large variations. To address this challenge, we propose a simple but effective federated learning framework, which improves the consistency across clients and facilitates the convergence of the server model. This is achieved by making the server broadcast a global model with a lookahead gradient. This strategy enables the proposed approach to convey the projected global update information to participants effectively without additional client memory and extra communication costs. We also regularize local updates by aligning each client with the overshot global model to reduce bias and improve the stability of our algorithm. We provide the theoretical convergence rate of our algorithm and demonstrate remarkable performance gains in terms of accuracy and communication efficiency compared to the state-of-the-art methods, especially with low client participation rates. The source code is available at our project page.",
        "page": "http://arxiv.org/abs/2201.03172",
        "pdf": "http://arxiv.org/pdf/2201.03172.pdf"
    },
    {
        "title": "NeISF: Neural Incident Stokes Field for Geometry and Material Estimation",
        "author": "Chenhao Li, Taishi Ono, Takeshi Uemori, Hajime Mihara, Alexander Gatto, Hajime Nagahara, Yusuke Moriuchi",
        "abstract": "Multi-view inverse rendering is the problem of estimating the scene parameters such as shapes, materials, or illuminations from a sequence of images captured under different viewpoints. Many approaches, however, assume single light bounce and thus fail to recover challenging scenarios like inter-reflections. On the other hand, simply extending those methods to consider multi-bounced light requires more assumptions to alleviate the ambiguity. To address this problem, we propose Neural Incident Stokes Fields (NeISF), a multi-view inverse rendering framework that reduces ambiguities using polarization cues. The primary motivation for using polarization cues is that it is the accumulation of multi-bounced light, providing rich information about geometry and material. Based on this knowledge, the proposed incident Stokes field efficiently models the accumulated polarization effect with the aid of an original physically-based differentiable polarimetric renderer. Lastly, experimental results show that our method outperforms the existing works in synthetic and real scenarios.",
        "page": "http://arxiv.org/abs/2311.13187",
        "pdf": "http://arxiv.org/pdf/2311.13187.pdf"
    },
    {
        "title": "MaskPLAN: Masked Generative Layout Planning from Partial Input",
        "author": "Hang Zhang, Anton Savov, Benjamin Dillenburger",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rapid 3D Model Generation with Intuitive 3D Input",
        "author": "Tianrun Chen, Chaotao Ding, Shangzhan Zhang, Chunan Yu, Ying Zang, Zejian Li, Sida Peng, Lingyun Sun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception",
        "author": "Thien-Minh Nguyen, Shenghai Yuan, Thien Nguyen, Pengyu Yin, Haozhi Cao, Lihua Xie, Maciej Wozniak, Patric Jensfelt, Marko Thiel, Justin Ziegenbein, Noel Blunder",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Splatter Image: Ultra-Fast Single-View 3D Reconstruction",
        "author": "Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi",
        "abstract": "We introduce the \\method, an ultra-efficient approach for monocular 3D object reconstruction. Splatter Image is based on Gaussian Splatting, which allows fast and high-quality reconstruction of 3D scenes from multiple images. We apply Gaussian Splatting to monocular reconstruction by learning a neural network that, at test time, performs reconstruction in a feed-forward manner, at 38 FPS. Our main innovation is the surprisingly straightforward design of this network, which, using 2D operators, maps the input image to one 3D Gaussian per pixel. The resulting set of Gaussians thus has the form an image, the Splatter Image. We further extend the method take several images as input via cross-view attention. Owning to the speed of the renderer (588 FPS), we use a single GPU for training while generating entire images at each iteration to optimize perceptual metrics like LPIPS. On several synthetic, real, multi-category and large-scale benchmark datasets, we achieve better results in terms of PSNR, LPIPS, and other metrics while training and evaluating much faster than prior works. Code, models, demo and more results are available at https://szymanowiczs.github.io/splatter-image.",
        "page": "http://arxiv.org/abs/2312.13150",
        "pdf": "http://arxiv.org/pdf/2312.13150.pdf"
    },
    {
        "title": "$CrowdDiff$: Multi-hypothesis Crowd Density Estimation using Diffusion Models",
        "author": "Yasiru Ranasinghe, Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel",
        "abstract": "Crowd counting is a fundamental problem in crowd analysis which is typically accomplished by estimating a crowd density map and summing over the density values. However, this approach suffers from background noise accumulation and loss of density due to the use of broad Gaussian kernels to create the ground truth density maps. This issue can be overcome by narrowing the Gaussian kernel. However, existing approaches perform poorly when trained with ground truth density maps with broad kernels. To deal with this limitation, we propose using conditional diffusion models to predict density maps, as diffusion models show high fidelity to training data during generation. With that, we present $CrowdDiff$ that generates the crowd density map as a reverse diffusion process. Furthermore, as the intermediate time steps of the diffusion process are noisy, we incorporate a regression branch for direct crowd estimation only during training to improve the feature learning. In addition, owing to the stochastic nature of the diffusion model, we introduce producing multiple density maps to improve the counting performance contrary to the existing crowd counting pipelines. We conduct extensive experiments on publicly available datasets to validate the effectiveness of our method. $CrowdDiff$ outperforms existing state-of-the-art crowd counting methods on several public crowd analysis benchmarks with significant improvements.",
        "page": "http://arxiv.org/abs/2303.12790",
        "pdf": "http://arxiv.org/pdf/2303.12790.pdf"
    },
    {
        "title": "Text-Enhanced Data-free Approach for Federated Class-Incremental Learning",
        "author": "Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Dinh Phung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsupervised Salient Instance Detection",
        "author": "Xin Tian, Ke Xu, Rynson W.H. Lau",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering",
        "author": "Kim Youwang, Tae-Hyun Oh, Gerard Pons-Moll",
        "abstract": "We present Paint-it, a text-driven high-fidelity texture map synthesis method for 3D meshes via neural re-parameterized texture optimization. Paint-it synthesizes texture maps from a text description by synthesis-through-optimization, exploiting the Score-Distillation Sampling (SDS). We observe that directly applying SDS yields undesirable texture quality due to its noisy gradients. We reveal the importance of texture parameterization when using SDS. Specifically, we propose Deep Convolutional Physically-Based Rendering (DC-PBR) parameterization, which re-parameterizes the physically-based rendering (PBR) texture maps with randomly initialized convolution-based neural kernels, instead of a standard pixel-based parameterization. We show that DC-PBR inherently schedules the optimization curriculum according to texture frequency and naturally filters out the noisy signals from SDS. In experiments, Paint-it obtains remarkable quality PBR texture maps within 15 min., given only a text description. We demonstrate the generalizability and practicality of Paint-it by synthesizing high-quality texture maps for large-scale mesh datasets and showing test-time applications such as relighting and material control using a popular graphics engine. Project page: https://kim-youwang.github.io/paint-it",
        "page": "http://arxiv.org/abs/2312.11360",
        "pdf": "http://arxiv.org/pdf/2312.11360.pdf"
    },
    {
        "title": "L-MAGIC: Language Model Assisted Generation of Images with Consistency",
        "author": "zhipeng cai, Matthias Mueller, Reiner Birkl, Diana Wofk, Shao-Yen Tseng, JunDa Cheng, Gabriela Ben Melech Stan, Vasudev Lal, Michael Paulitsch",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GLACE: Global Local Accelerated Coordinate Encoding",
        "author": "Fangjinhua Wang, Xudong Jiang, Silvano Galliani, Christoph Vogel, Marc Pollefeys",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HouseCat6D - A Large-Scale Multi-Modal Category Level 6D Object Perception Dataset with Household Objects in Realistic Scenarios",
        "author": "HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Guangyao Zhai, Hannah Schieber, Giulia Rizzoli, Pengyuan Wang, Hongcheng Zhao, Lorenzo Garattoni, Sven Meier, Daniel Roth, Nassir Navab, Benjamin Busam",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Customization Assistant for Text-to-image Generation",
        "author": "Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Tong Sun",
        "abstract": "Customizing pre-trained text-to-image generation model has attracted massive research interest recently, due to its huge potential in real-world applications. Although existing methods are able to generate creative content for a novel concept contained in single user-input image, their capability are still far from perfection. Specifically, most existing methods require fine-tuning the generative model on testing images. Some existing methods do not require fine-tuning, while their performance are unsatisfactory. Furthermore, the interaction between users and models are still limited to directive and descriptive prompts such as instructions and captions. In this work, we build a customization assistant based on pre-trained large language model and diffusion model, which can not only perform customized generation in a tuning-free manner, but also enable more user-friendly interactions: users can chat with the assistant and input either ambiguous text or clear instruction. Specifically, we propose a new framework consists of a new model design and a novel training strategy. The resulting assistant can perform customized generation in 2-5 seconds without any test time fine-tuning. Extensive experiments are conducted, competitive results have been obtained across different domains, illustrating the effectiveness of the proposed method.",
        "page": "http://arxiv.org/abs/2312.03045",
        "pdf": "http://arxiv.org/pdf/2312.03045.pdf"
    },
    {
        "title": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures",
        "author": "Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guo-Jun Qi",
        "abstract": "Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments.",
        "page": "http://arxiv.org/abs/2401.11078",
        "pdf": "http://arxiv.org/pdf/2401.11078.pdf"
    },
    {
        "title": "Event-based Structure-from-Orbit",
        "author": "Ethan Elms, Yasir Latif, Tae Ha Park, Tat-Jun Chin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "From-Ground-To-Objects: Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior",
        "author": "Jaeho Moon, Juan Luis Gonzalez Bello, Byeongjun Kwon, Munchurl Kim",
        "abstract": "Self-supervised monocular depth estimation (DE) is an approach to learning depth without costly depth ground truths. However, it often struggles with moving objects that violate the static scene assumption during training. To address this issue, we introduce a coarse-to-fine training strategy leveraging the ground contacting prior based on the observation that most moving objects in outdoor scenes contact the ground. In the coarse training stage, we exclude the objects in dynamic classes from the reprojection loss calculation to avoid inaccurate depth learning. To provide precise supervision on the depth of the objects, we present a novel Ground-contacting-prior Disparity Smoothness Loss (GDS-Loss) that encourages a DE network to align the depth of the objects with their ground-contacting points. Subsequently, in the fine training stage, we refine the DE network to learn the detailed depth of the objects from the reprojection loss, while ensuring accurate DE on the moving object regions by employing our regularization loss with a cost-volume-based weighting factor. Our overall coarse-to-fine training strategy can easily be integrated with existing DE methods without any modifications, significantly enhancing DE performance on challenging Cityscapes and KITTI datasets, especially in the moving object regions.",
        "page": "http://arxiv.org/abs/2312.10118",
        "pdf": "http://arxiv.org/pdf/2312.10118.pdf"
    },
    {
        "title": "SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields",
        "author": "Quentin HERAU, Nathan Piasco, Moussab Bennehar, Luis Guiller,o Roldao Jimenez, Dzmitry Tsishkou, MigniotCyrille, Mod\u00e9lisation Information Syst\u00e8mes, Cedric Demonceaux",
        "abstract": "In rapidly-evolving domains such as autonomous driving, the use of multiple sensors with different modalities is crucial to ensure high operational precision and stability. To correctly exploit the provided information by each sensor in a single common frame, it is essential for these sensors to be accurately calibrated. In this paper, we leverage the ability of Neural Radiance Fields (NeRF) to represent different sensors modalities in a common volumetric representation to achieve robust and accurate spatio-temporal sensor calibration. By designing a partitioning approach based on the visible part of the scene for each sensor, we formulate the calibration problem using only the overlapping areas. This strategy results in a more robust and accurate calibration that is less prone to failure. We demonstrate that our approach works on outdoor urban scenes by validating it on multiple established driving datasets. Results show that our method is able to get better accuracy and robustness compared to existing methods.",
        "page": "http://arxiv.org/abs/2311.15803",
        "pdf": "http://arxiv.org/pdf/2311.15803.pdf"
    },
    {
        "title": "Label-Efficient Group Robustness via Out-of-Distribution Concept Curation",
        "author": "Yiwei Yang, Anthony Liu, Robert Wolfe, Aylin Caliskan, Bill Howe",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "StyLitGAN: Image-based Relighting via Latent Control",
        "author": "Anand Bhattad, James Soole, David Forsyth",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Anomaly Score: Evaluating Generative Models and Individual Generated Images based on Complexity and Vulnerability",
        "author": "Jaehui Hwang, Junghyuk Lee, Jong-Seok Lee",
        "abstract": "With the advancement of generative models, the assessment of generated images becomes more and more important. Previous methods measure distances between features of reference and generated images from trained vision models. In this paper, we conduct an extensive investigation into the relationship between the representation space and input space around generated images. We first propose two measures related to the presence of unnatural elements within images: complexity, which indicates how non-linear the representation space is, and vulnerability, which is related to how easily the extracted feature changes by adversarial input changes. Based on these, we introduce a new metric to evaluating image-generative models called anomaly score (AS). Moreover, we propose AS-i (anomaly score for individual images) that can effectively evaluate generated images individually. Experimental results demonstrate the validity of the proposed approach.",
        "page": "http://arxiv.org/abs/2312.10634",
        "pdf": "http://arxiv.org/pdf/2312.10634.pdf"
    },
    {
        "title": "ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization",
        "author": "Weiyao Wang, Pierre Gleize, Hao Tang, Xingyu Chen, Kevin Liang, Matt Feiszli",
        "abstract": "Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces ``confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.",
        "page": "http://arxiv.org/abs/2401.08937",
        "pdf": "http://arxiv.org/pdf/2401.08937.pdf"
    },
    {
        "title": "FreeDrag: Feature Dragging for Reliable Point-based Image Editing",
        "author": "Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin, Jinjin Zheng",
        "abstract": "To serve the intricate and varied demands of image editing, precise and flexible manipulation in image content is indispensable. Recently, Drag-based editing methods have gained impressive performance. However, these methods predominantly center on point dragging, resulting in two noteworthy drawbacks, namely \"miss tracking\", where difficulties arise in accurately tracking the predetermined handle points, and \"ambiguous tracking\", where tracked points are potentially positioned in wrong regions that closely resemble the handle points. To address the above issues, we propose FreeDrag, a feature dragging methodology designed to free the burden on point tracking. The FreeDrag incorporates two key designs, i.e., template feature via adaptive updating and line search with backtracking, the former improves the stability against drastic content change by elaborately controls feature updating scale after each dragging, while the latter alleviates the misguidance from similar points by actively restricting the search area in a line. These two technologies together contribute to a more stable semantic dragging with higher efficiency. Comprehensive experimental results substantiate that our approach significantly outperforms pre-existing methodologies, offering reliable point-based editing even in various complex scenarios.",
        "page": "http://arxiv.org/abs/2307.04684",
        "pdf": "http://arxiv.org/pdf/2307.04684.pdf"
    },
    {
        "title": "Instance-Aware Group Quantization for Vision Transformers",
        "author": "Jaehyeon Moon, Dohyung Kim, Jun Yong Cheon, Bumsub Ham",
        "abstract": "Post-training quantization (PTQ) is an efficient model compression technique that quantizes a pretrained full-precision model using only a small calibration set of unlabeled samples without retraining. PTQ methods for convolutional neural networks (CNNs) provide quantization results comparable to full-precision counterparts. Directly applying them to vision transformers (ViTs), however, incurs severe performance degradation, mainly due to the differences in architectures between CNNs and ViTs. In particular, the distribution of activations for each channel vary drastically according to input instances, making PTQ methods for CNNs inappropriate for ViTs. To address this, we introduce instance-aware group quantization for ViTs (IGQ-ViT). To this end, we propose to split the channels of activation maps into multiple groups dynamically for each input instance, such that activations within each group share similar statistical properties. We also extend our scheme to quantize softmax attentions across tokens. In addition, the number of groups for each layer is adjusted to minimize the discrepancies between predictions from quantized and full-precision models, under a bit-operation (BOP) constraint. We show extensive experimental results on image classification, object detection, and instance segmentation, with various transformer architectures, demonstrating the effectiveness of our approach.",
        "page": "http://arxiv.org/abs/2404.00928",
        "pdf": "http://arxiv.org/pdf/2404.00928.pdf"
    },
    {
        "title": "Viewpoint-Aware Visual Grounding in 3D Scenes",
        "author": "Xiangxi Shi, Zhonghua Wu, Stefan Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VTQA: Visual Text Question Answering via Entity Alignment and Cross-Media Reasoning",
        "author": "Kang chenkang, Xiangqian Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Batch Normalization Alleviates the Spectral Bias in Coordinate Networks",
        "author": "Zhicheng Cai, Hao Zhu, Qiu Shen, Xinran Wang, Xun Cao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OMG-Seg: Is One Model Good Enough For All Segmentation?",
        "author": "Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Pose Adapted Shape Learning for Large-Pose Face Reenactment",
        "author": "Gee-Sern Hsu, Jie-Ying Zhang, Yu-Hsiang Huang, Wei-Jie Hong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Make Me a BNN: A Simple Strategy for Estimating Bayesian Uncertainty from Pre-trained Models",
        "author": "Gianni Franchi, Olivier Laurent, Maxence Legu\u00e9ry, Andrei Bursuc, Andrea Pilzer, Angela Yao",
        "abstract": "Deep Neural Networks (DNNs) are powerful tools for various computer vision tasks, yet they often struggle with reliable uncertainty quantification - a critical requirement for real-world applications. Bayesian Neural Networks (BNN) are equipped for uncertainty estimation but cannot scale to large DNNs that are highly unstable to train. To address this challenge, we introduce the Adaptable Bayesian Neural Network (ABNN), a simple and scalable strategy to seamlessly transform DNNs into BNNs in a post-hoc manner with minimal computational and training overheads. ABNN preserves the main predictive properties of DNNs while enhancing their uncertainty quantification abilities through simple BNN adaptation layers (attached to normalization layers) and a few fine-tuning steps on pre-trained models. We conduct extensive experiments across multiple datasets for image classification and semantic segmentation tasks, and our results demonstrate that ABNN achieves state-of-the-art performance without the computational budget typically associated with ensemble methods.",
        "page": "http://arxiv.org/abs/2312.15297",
        "pdf": "http://arxiv.org/pdf/2312.15297.pdf"
    },
    {
        "title": "Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation",
        "author": "Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara",
        "abstract": "Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training.",
        "page": "http://arxiv.org/abs/2404.06542",
        "pdf": "http://arxiv.org/pdf/2404.06542.pdf"
    },
    {
        "title": "EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling",
        "author": "Haiyang Liu, Zihao Zhu, Giorgio Becherini, YICHEN PENG, Mingyang Su, YOU ZHOU, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, Michael J. Black",
        "abstract": "We propose EMAGE, a framework to generate full-body human gestures from audio and masked gestures, encompassing facial, local body, hands, and global movements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new mesh-level holistic co-speech dataset. BEAT2 combines a MoShed SMPL-X body with FLAME head parameters and further refines the modeling of head, neck, and finger movements, offering a community-standardized, high-quality 3D motion captured dataset. EMAGE leverages masked body gesture priors during training to boost inference performance. It involves a Masked Audio Gesture Transformer, facilitating joint training on audio-to-gesture generation and masked gesture reconstruction to effectively encode audio and body gesture hints. Encoded body hints from masked gestures are then separately employed to generate facial and body movements. Moreover, EMAGE adaptively merges speech features from the audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance the results' fidelity and diversity. Experiments demonstrate that EMAGE generates holistic gestures with state-of-the-art performance and is flexible in accepting predefined spatial-temporal gesture inputs, generating complete, audio-synchronized results. Our code and dataset are available https://pantomatrix.github.io/EMAGE/",
        "page": "http://arxiv.org/abs/2401.00374",
        "pdf": "http://arxiv.org/pdf/2401.00374.pdf"
    },
    {
        "title": "NB-GTR: Narrow-Band Guided Turbulence Removal",
        "author": "Yifei Xia, Chu Zhou, Chengxuan Zhu, Minggui Teng, Chao Xu, Boxin Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LaneCPP: Continuous 3D Lane Detection using Physical Priors",
        "author": "Maximilian Pittner, Joel Janai, Alexandru Paul Condurache",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Large Language Models are Good Prompt Learners for Low-Shot Image Classification",
        "author": "Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, Ram Nevatia",
        "abstract": "Low-shot image classification, where training images are limited or inaccessible, has benefited from recent progress on pre-trained vision-language (VL) models with strong generalizability, e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs), with their vast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we discuss the integration of LLMs to enhance pre-trained VL models, specifically on low-shot classification. However, the domain gap between language and vision blocks the direct application of LLMs. Thus, we propose LLaMP, Large Language Models as Prompt learners, that produces adaptive prompts for the CLIP text encoder, establishing it as the connecting bridge. Experiments show that, compared with other state-of-the-art prompt learning methods, LLaMP yields better performance on both zero-shot generalization and few-shot image classification, over a spectrum of 11 datasets. Code will be made available at: https://github.com/zhaohengz/LLaMP.",
        "page": "http://arxiv.org/abs/2312.04076",
        "pdf": "http://arxiv.org/pdf/2312.04076.pdf"
    },
    {
        "title": "FineSports: A Multi-person Hierarchical Sports Video Dataset for Fine-grained Action Understanding",
        "author": "Jinglin Xu, Guohao Zhao, Sibo Yin, Wenhao Zhou, Yuxin Peng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "An Aggregation-Free Federated Learning for Tackling Data Heterogeneity",
        "author": "Yuan Wang, Huazhu Fu, Renuga Kanagavelu, Qingsong Wei, Yong Liu, Rick Goh",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Infrared Adversarial Car Stickers",
        "author": "Xiaopei Zhu, Yuqiu Liu, Zhanhao Hu, Jianmin Li, Xiaolin Hu",
        "abstract": "Infrared physical adversarial examples are of great significance for studying the security of infrared AI systems that are widely used in our lives such as autonomous driving. Previous infrared physical attacks mainly focused on 2D infrared pedestrian detection which may not fully manifest its destructiveness to AI systems. In this work, we propose a physical attack method against infrared detectors based on 3D modeling, which is applied to a real car. The goal is to design a set of infrared adversarial stickers to make cars invisible to infrared detectors at various viewing angles, distances, and scenes. We build a 3D infrared car model with real infrared characteristics and propose an infrared adversarial pattern generation method based on 3D mesh shadow. We propose a 3D control points-based mesh smoothing algorithm and use a set of smoothness loss functions to enhance the smoothness of adversarial meshes and facilitate the sticker implementation. Besides, We designed the aluminum stickers and conducted physical experiments on two real Mercedes-Benz A200L cars. Our adversarial stickers hid the cars from Faster RCNN, an object detector, at various viewing angles, distances, and scenes. The attack success rate (ASR) was 91.49% for real cars. In comparison, the ASRs of random stickers and no sticker were only 6.21% and 0.66%, respectively. In addition, the ASRs of the designed stickers against six unseen object detectors such as YOLOv3 and Deformable DETR were between 73.35%-95.80%, showing good transferability of the attack performance across detectors.",
        "page": "http://arxiv.org/abs/2405.09924",
        "pdf": "http://arxiv.org/pdf/2405.09924.pdf"
    },
    {
        "title": "MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning",
        "author": "Matteo Farina, Massimiliano Mancini, Elia Cunegatti, Gaowen Liu, Giovanni Iacca, Elisa Ricci",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction",
        "author": "Cheng Sun, Wei-En Tai, Yu-Lin Shih, Kuan-Wei Chen, Yong-Jing Syu, Kent Selwyn The, Yu-Chiang Frank Wang, Hwann-Tzong Chen",
        "abstract": "State-of-the-art single-view 360-degree room layout reconstruction methods formulate the problem as a high-level 1D (per-column) regression task. On the other hand, traditional low-level 2D layout segmentation is simpler to learn and can represent occluded regions, but it requires complex post-processing for the targeting layout polygon and sacrifices accuracy. We present Seg2Reg to render 1D layout depth regression from the 2D segmentation map in a differentiable and occlusion-aware way, marrying the merits of both sides. Specifically, our model predicts floor-plan density for the input equirectangular 360-degree image. Formulating the 2D layout representation as a density field enables us to employ `flattened' volume rendering to form 1D layout depth regression. In addition, we propose a novel 3D warping augmentation on layout to improve generalization. Finally, we re-implement recent room layout reconstruction methods into our codebase for benchmarking and explore modern backbones and training techniques to serve as the strong baseline. Our model significantly outperforms previous arts. The code will be made available upon publication.",
        "page": "http://arxiv.org/abs/2311.18695",
        "pdf": "http://arxiv.org/pdf/2311.18695.pdf"
    },
    {
        "title": "Boosting Adversarial Transferability by Block Shuffle and Rotation",
        "author": "Kunyu Wang, he xuanran, Wenxuan Wang, Xiaosen Wang",
        "abstract": "Adversarial examples mislead deep neural networks with imperceptible perturbations and have brought significant threats to deep learning. An important aspect is their transferability, which refers to their ability to deceive other models, thus enabling attacks in the black-box setting. Though various methods have been proposed to boost transferability, the performance still falls short compared with white-box attacks. In this work, we observe that existing input transformation based attacks, one of the mainstream transfer-based attacks, result in different attention heatmaps on various models, which might limit the transferability. We also find that breaking the intrinsic relation of the image can disrupt the attention heatmap of the original image. Based on this finding, we propose a novel input transformation based attack called block shuffle and rotation (BSR). Specifically, BSR splits the input image into several blocks, then randomly shuffles and rotates these blocks to construct a set of new images for gradient calculation. Empirical evaluations on the ImageNet dataset demonstrate that BSR could achieve significantly better transferability than the existing input transformation based methods under single-model and ensemble-model settings. Combining BSR with the current input transformation method can further improve the transferability, which significantly outperforms the state-of-the-art methods. Code is available at https://github.com/Trustworthy-AI-Group/BSR",
        "page": "http://arxiv.org/abs/2308.10299",
        "pdf": "http://arxiv.org/pdf/2308.10299.pdf"
    },
    {
        "title": "GALA: Generating Animatable Layered Assets from a Single Scan",
        "author": "Taeksoo Kim, Byungjun Kim, Shunsuke Saito, Hanbyul Joo",
        "abstract": "We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed human avatars with any pose. Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications. Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions. Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses. To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets. We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses. Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions.",
        "page": "http://arxiv.org/abs/2401.12979",
        "pdf": "http://arxiv.org/pdf/2401.12979.pdf"
    },
    {
        "title": "Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling",
        "author": "Shentong Mo, Pedro Morgado",
        "abstract": "Humans possess a remarkable ability to integrate auditory and visual information, enabling a deeper understanding of the surrounding environment. This early fusion of audio and visual cues, demonstrated through cognitive psychology and neuroscience research, offers promising potential for developing multimodal perception models. However, training early fusion architectures poses significant challenges, as the increased model expressivity requires robust learning frameworks to harness their enhanced capabilities. In this paper, we address this challenge by leveraging the masked reconstruction framework, previously successful in unimodal settings, to train audio-visual encoders with early fusion. Additionally, we propose an attention-based fusion module that captures interactions between local audio and visual representations, enhancing the model's ability to capture fine-grained interactions. While effective, this procedure can become computationally intractable, as the number of local representations increases. Thus, to address the computational complexity, we propose an alternative procedure that factorizes the local representations before representing audio-visual interactions. Extensive evaluations on a variety of datasets demonstrate the superiority of our approach in audio-event classification, visual sound localization, sound separation, and audio-visual segmentation. These contributions enable the efficient training of deeply integrated audio-visual models and significantly advance the usefulness of early fusion architectures.",
        "page": "http://arxiv.org/abs/2312.01017",
        "pdf": "http://arxiv.org/pdf/2312.01017.pdf"
    },
    {
        "title": "Move Anything with Layered Scene Diffusion",
        "author": "Jiawei Ren, Mengmeng Xu, Jui-Chieh Wu, Ziwei Liu, Tao Xiang, Antoine Toisoul",
        "abstract": "Diffusion models generate images with an unprecedented level of quality, but how can we freely rearrange image layouts? Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to diffusion models due to their fixed forward process. In this work, we propose SceneDiffusion to optimize a layered scene representation during the diffusion sampling process. Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts. Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing. Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images. Notably, this approach is training-free, compatible with general text-to-image diffusion models, and responsive in less than a second.",
        "page": "http://arxiv.org/abs/2404.07178",
        "pdf": "http://arxiv.org/pdf/2404.07178.pdf"
    },
    {
        "title": "Learning Diffusion Texture Priors for Image Restoration",
        "author": "Tian Ye, Sixiang Chen, Wenhao Chai, Zhaohu Xing, Jing Qin, Ge lin, Lei Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions",
        "author": "Runhao Zeng, Xiaoyong Chen, Jiaming Liang, Huisi Wu, Guang-Zhong Cao, Yong Guo",
        "abstract": "Temporal action detection (TAD) aims to locate action positions and recognize action categories in long-term untrimmed videos. Although many methods have achieved promising results, their robustness has not been thoroughly studied. In practice, we observe that temporal information in videos can be occasionally corrupted, such as missing or blurred frames. Interestingly, existing methods often incur a significant performance drop even if only one frame is affected. To formally evaluate the robustness, we establish two temporal corruption robustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper, we extensively analyze the robustness of seven leading TAD methods and obtain some interesting findings: 1) Existing methods are particularly vulnerable to temporal corruptions, and end-to-end methods are often more susceptible than those with a pre-trained feature extractor; 2) Vulnerability mainly comes from localization error rather than classification error; 3) When corruptions occur in the middle of an action instance, TAD models tend to yield the largest performance drop. Besides building a benchmark, we further develop a simple but effective robust training method to defend against temporal corruptions, through the FrameDrop augmentation and Temporal-Robust Consistency loss. Remarkably, our approach not only improves robustness but also yields promising improvements on clean data. We believe that this study will serve as a benchmark for future research in robust video analysis. Source code and models are available at https://github.com/Alvin-Zeng/temporal-robustness-benchmark.",
        "page": "http://arxiv.org/abs/2403.20254",
        "pdf": "http://arxiv.org/pdf/2403.20254.pdf"
    },
    {
        "title": "Implicit Event-RGBD Neural SLAM",
        "author": "Delin Qu, Chi Yan, Dong Wang, Jie Yin, Qizhi Chen, Dan Xu, Yiting Zhang, Bin Zhao, Xuelong Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Scene-adaptive and Region-aware Multi-modal Prompt for Open Vocabulary Object Detection",
        "author": "Xiaowei Zhao, Xianglong Liu, Duorui Wang, Yajun Gao, Zhide Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "RepKPU: Point Cloud Upsampling with Kernel Point Representation and Deformation",
        "author": "Yi Rong, Haoran Zhou, Kang Xia, Cheng Mei, Jiahao Wang, Tong Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "One-step Diffusion with Distribution Matching Distillation",
        "author": "Tianwei Yin, Micha\u00ebl Gharbi, Micha\u00ebl Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, Taesung Park",
        "abstract": "Diffusion models generate high-quality images but require dozens of forward passes. We introduce Distribution Matching Distillation (DMD), a procedure to transform a diffusion model into a one-step image generator with minimal impact on image quality. We enforce the one-step image generator match the diffusion model at distribution level, by minimizing an approximate KL divergence whose gradient can be expressed as the difference between 2 score functions, one of the target distribution and the other of the synthetic distribution being produced by our one-step generator. The score functions are parameterized as two diffusion models trained separately on each distribution. Combined with a simple regression loss matching the large-scale structure of the multi-step diffusion outputs, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.",
        "page": "http://arxiv.org/abs/2311.18828",
        "pdf": "http://arxiv.org/pdf/2311.18828.pdf"
    },
    {
        "title": "On Exact Inversion of DPM-Solvers",
        "author": "Seongmin Hong, Kyeonghyun Lee, Suh Yoon Jeon, Hyewon Bae, Se Young Chun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Privacy-Preserving Face Recognition Using Trainable Feature Subtraction",
        "author": "Yuxi Mi, Zhizhou Zhong, Yuge Huang, Jiazhen Ji, Jianqing Xu, Jun Wang, ShaoMing Wang, Shouhong Ding, Shuigeng Zhou",
        "abstract": "The widespread adoption of face recognition has led to increasing privacy concerns, as unauthorized access to face images can expose sensitive personal information. This paper explores face image protection against viewing and recovery attacks. Inspired by image compression, we propose creating a visually uninformative face image through feature subtraction between an original face and its model-produced regeneration. Recognizable identity features within the image are encouraged by co-training a recognition model on its high-dimensional feature representation. To enhance privacy, the high-dimensional representation is crafted through random channel shuffling, resulting in randomized recognizable images devoid of attacker-leverageable texture details. We distill our methodologies into a novel privacy-preserving face recognition method, MinusFace. Experiments demonstrate its high recognition accuracy and effective privacy protection. Its code is available at https://github.com/Tencent/TFace.",
        "page": "http://arxiv.org/abs/2403.12457",
        "pdf": "http://arxiv.org/pdf/2403.12457.pdf"
    },
    {
        "title": "DAVE -- A Detect-and-Verify Paradigm for Low-Shot Counting",
        "author": "Jer Pelhan, Alan Lukezic, Vitjan Zavrtanik, Matej Kristan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CONFORM: Contrast is All You Need for High-Fidelity Text-to-Image Diffusion Models",
        "author": "Tuna Han Salih Meral, Enis Simsar, Federico Tombari, Pinar Yanardag",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Scaling Up Video Summarization Pretraining with Large Language Models",
        "author": "Dawit Argaw Argaw, Seunghyun Yoon, Fabian Caba Heilbron, Hanieh Deilamsalehy, Trung Bui, Zhaowen Wang, Franck Dernoncourt, Joon Chung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Single-View Scene Point Cloud Human Grasp Generation",
        "author": "Yan-Kang Wang, Chengyi Xing, Yi-Lin Wei, Xiao-Ming Wu, Wei-Shi Zheng",
        "abstract": "In this work, we explore a novel task of generating human grasps based on single-view scene point clouds, which more accurately mirrors the typical real-world situation of observing objects from a single viewpoint. Due to the incompleteness of object point clouds and the presence of numerous scene points, the generated hand is prone to penetrating into the invisible parts of the object and the model is easily affected by scene points. Thus, we introduce S2HGrasp, a framework composed of two key modules: the Global Perception module that globally perceives partial object point clouds, and the DiffuGrasp module designed to generate high-quality human grasps based on complex inputs that include scene points. Additionally, we introduce S2HGD dataset, which comprises approximately 99,000 single-object single-view scene point clouds of 1,668 unique objects, each annotated with one human grasp. Our extensive experiments demonstrate that S2HGrasp can not only generate natural human grasps regardless of scene points, but also effectively prevent penetration between the hand and invisible parts of the object. Moreover, our model showcases strong generalization capability when applied to unseen objects. Our code and dataset are available at https://github.com/iSEE-Laboratory/S2HGrasp.",
        "page": "http://arxiv.org/abs/2404.15815",
        "pdf": "http://arxiv.org/pdf/2404.15815.pdf"
    },
    {
        "title": "UVEB: A Large-scale Benchmark and Baseline Towards Real-World Underwater Video Enhancement",
        "author": "yaofeng xie, Lingwei Kong, Kai Chen, Zheng Ziqiang, Xiao Yu, Zhibin Yu, Bing Zheng",
        "abstract": "Learning-based underwater image enhancement (UIE) methods have made great progress. However, the lack of large-scale and high-quality paired training samples has become the main bottleneck hindering the development of UIE. The inter-frame information in underwater videos can accelerate or optimize the UIE process. Thus, we constructed the first large-scale high-resolution underwater video enhancement benchmark (UVEB) to promote the development of underwater vision.It contains 1,308 pairs of video sequences and more than 453,000 high-resolution with 38\\% Ultra-High-Definition (UHD) 4K frame pairs. UVEB comes from multiple countries, containing various scenes and video degradation types to adapt to diverse and complex underwater environments. We also propose the first supervised underwater video enhancement method, UVE-Net. UVE-Net converts the current frame information into convolutional kernels and passes them to adjacent frames for efficient inter-frame information exchange. By fully utilizing the redundant degraded information of underwater videos, UVE-Net completes video enhancement better. Experiments show the effective network design and good performance of UVE-Net.",
        "page": "http://arxiv.org/abs/2404.14542",
        "pdf": "http://arxiv.org/pdf/2404.14542.pdf"
    },
    {
        "title": "GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects",
        "author": "Sungphill Moon, Hyeontae Son, Dongcheol Hur, Sangwook Kim",
        "abstract": "Despite the progress of learning-based methods for 6D object pose estimation, the trade-off between accuracy and scalability for novel objects still exists. Specifically, previous methods for novel objects do not make good use of the target object's 3D shape information since they focus on generalization by processing the shape indirectly, making them less effective. We present GenFlow, an approach that enables both accuracy and generalization to novel objects with the guidance of the target object's shape. Our method predicts optical flow between the rendered image and the observed image and refines the 6D pose iteratively. It boosts the performance by a constraint of the 3D shape and the generalizable geometric knowledge learned from an end-to-end differentiable system. We further improve our model by designing a cascade network architecture to exploit the multi-scale correlations and coarse-to-fine refinement. GenFlow ranked first on the unseen object pose estimation benchmarks in both the RGB and RGB-D cases. It also achieves performance competitive with existing state-of-the-art methods for the seen object pose estimation without any fine-tuning.",
        "page": "http://arxiv.org/abs/2403.11510",
        "pdf": "http://arxiv.org/pdf/2403.11510.pdf"
    },
    {
        "title": "DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling",
        "author": "Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations",
        "author": "Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, 'YZ' Yezhou Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Nearest Is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks",
        "author": "Boheng Li, Yishuo Cai, Haowei Li, Feng Xue, Zhifeng Li, Yiming Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neural Exposure Fusion for High-Dynamic Range Object Detection",
        "author": "Emmanuel Onzon, Maximilian B\u00f6mer, Fahim Mannan, Felix Heide",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Model Adaptation for Time Constrained Embodied Control",
        "author": "Jaehyun Song, Minjong Yoo, Honguk Woo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GigaTraj: Predicting Long-term Trajectories of Hundreds of Pedestrians in Gigapixel Complex Scenes",
        "author": "Haozhe Lin, Chunyu Wei, Li He, Yuchen Guo, Yuchy Zhao, Shanglong Li, Lu Fang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Outdoor Scene Extrapolation with Hierarchical Generative Cellular Automata",
        "author": "Dongsu Zhang, Francis Williams, \u017dan Goj\u010di\u010d, Karsten Kreis, Sanja Fidler, Young Min Kim, Amlan Kar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TTA-EVF: Test-Time Adaptation for Event-based Video Frame Interpolation via Reliable Pixel and Sample Estimation",
        "author": "Hoonhee Cho, Taewoo Kim, Yuhwan Jeong, Kuk-Jin Yoon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "One-Prompt to Segment All Medical Images",
        "author": "Wu, Min Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Quantifying Task Priority for Multi-Task Optimization",
        "author": "Wooseong Jeong, Kuk-Jin Yoon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence",
        "author": "Ruihai Wu, Haoran Lu, Yiyan Wang, Yubo Wang, Hao Dong",
        "abstract": "Garment manipulation (e.g., unfolding, folding and hanging clothes) is essential for future robots to accomplish home-assistant tasks, while highly challenging due to the diversity of garment configurations, geometries and deformations. Although able to manipulate similar shaped garments in a certain task, previous works mostly have to design different policies for different tasks, could not generalize to garments with diverse geometries, and often rely heavily on human-annotated data. In this paper, we leverage the property that, garments in a certain category have similar structures, and then learn the topological dense (point-level) visual correspondence among garments in the category level with different deformations in the self-supervised manner. The topological correspondence can be easily adapted to the functional correspondence to guide the manipulation policies for various downstream tasks, within only one or few-shot demonstrations. Experiments over garments in 3 different categories on 3 representative tasks in diverse scenarios, using one or two arms, taking one or more steps, inputting flat or messy garments, demonstrate the effectiveness of our proposed method. Project page: https://warshallrho.github.io/unigarmentmanip.",
        "page": "http://arxiv.org/abs/2405.06903",
        "pdf": "http://arxiv.org/pdf/2405.06903.pdf"
    },
    {
        "title": "Mirasol3B: A Multimodal Autoregressive Model for Time-Aligned and Contextual Modalities",
        "author": "AJ Piergiovanni, Isaac Noble, Dahun Kim, Michael Ryoo, Victor Gomes, Anelia Angelova",
        "abstract": "One of the main challenges of multimodal learning is the need to combine heterogeneous modalities (e.g., video, audio, text). For example, video and audio are obtained at much higher rates than text and are roughly aligned in time. They are often not synchronized with text, which comes as a global context, e.g., a title, or a description. Furthermore, video and audio inputs are of much larger volumes, and grow as the video length increases, which naturally requires more compute dedicated to these modalities and makes modeling of long-range dependencies harder. We here decouple the multimodal modeling, dividing it into separate, focused autoregressive models, processing the inputs according to the characteristics of the modalities. We propose a multimodal model, called Mirasol3B, consisting of an autoregressive component for the time-synchronized modalities (audio and video), and an autoregressive component for the context modalities which are not necessarily aligned in time but are still sequential. To address the long-sequences of the video-audio inputs, we propose to further partition the video and audio sequences in consecutive snippets and autoregressively process their representations. To that end, we propose a Combiner mechanism, which models the audio-video information jointly within a timeframe. The Combiner learns to extract audio and video features from raw spatio-temporal signals, and then learns to fuse these features producing compact but expressive representations per snippet. Our approach achieves the state-of-the-art on well established multimodal benchmarks, outperforming much larger models. It effectively addresses the high computational demand of media inputs by both learning compact representations, controlling the sequence length of the audio-video feature representations, and modeling their dependencies in time.",
        "page": "http://arxiv.org/abs/2311.05698",
        "pdf": "http://arxiv.org/pdf/2311.05698.pdf"
    },
    {
        "title": "Evaluating Transferability in Retrieval Tasks: An Approach Using MMD and Kernel Methods",
        "author": "Mengyu Dai, Amir Hossein Raffiee, Aashish Jain, Joshua Correa",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Instance Tracking in 3D Scenes from Egocentric Videos",
        "author": "Yunhan Zhao, Haoyu Ma, Shu Kong, Charless Fowlkes",
        "abstract": "Egocentric sensors such as AR/VR devices capture human-object interactions and offer the potential to provide task-assistance by recalling 3D locations of objects of interest in the surrounding environment. This capability requires instance tracking in real-world 3D scenes from egocentric videos (IT3DEgo). We explore this problem by first introducing a new benchmark dataset, consisting of RGB and depth videos, per-frame camera pose, and instance-level annotations in both 2D camera and 3D world coordinates. We present an evaluation protocol which evaluates tracking performance in 3D coordinates with two settings for enrolling instances to track: (1) single-view online enrollment where an instance is specified on-the-fly based on the human wearer's interactions. and (2) multi-view pre-enrollment where images of an instance to be tracked are stored in memory ahead of time. To address IT3DEgo, we first re-purpose methods from relevant areas, e.g., single object tracking (SOT) -- running SOT methods to track instances in 2D frames and lifting them to 3D using camera pose and depth. We also present a simple method that leverages pretrained segmentation and detection models to generate proposals from RGB frames and match proposals with enrolled instance images. Perhaps surprisingly, our extensive experiments show that our method (with no finetuning) significantly outperforms SOT-based approaches. We conclude by arguing that the problem of egocentric instance tracking is made easier by leveraging camera pose and using a 3D allocentric (world) coordinate representation.",
        "page": "http://arxiv.org/abs/2312.04117",
        "pdf": "http://arxiv.org/pdf/2312.04117.pdf"
    },
    {
        "title": "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos",
        "author": "Shoukang Hu, Tao Hu, Ziwei Liu",
        "abstract": "We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians.",
        "page": "http://arxiv.org/abs/2312.02973",
        "pdf": "http://arxiv.org/pdf/2312.02973.pdf"
    },
    {
        "title": "Cyclic Learning for Binaural Audio Generation and Localization",
        "author": "Zhaojian Li, Bin Zhao, Yuan Yuan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Frequency-aware Event-based Video Deblurring for Real-World Motion Blur",
        "author": "Taewoo Kim, Hoonhee Cho, Kuk-Jin Yoon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset",
        "author": "Yiming Li, Zhiheng Li, Nuo Chen, Moonjun Gong, Zonglin Lyu, Zehong Wang, Peili Jiang, Chen Feng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Video Interpolation with Diffusion Models",
        "author": "Siddhant Jain, Daniel Watson, Aleksander Holynski, Eric Tabellion, Ben Poole, Janne Kontkanen",
        "abstract": "We present VIDIM, a generative model for video interpolation, which creates short videos given a start and end frame. In order to achieve high fidelity and generate motions unseen in the input data, VIDIM uses cascaded diffusion models to first generate the target video at low resolution, and then generate the high-resolution video conditioned on the low-resolution generated video. We compare VIDIM to previous state-of-the-art methods on video interpolation, and demonstrate how such works fail in most settings where the underlying motion is complex, nonlinear, or ambiguous while VIDIM can easily handle such cases. We additionally demonstrate how classifier-free guidance on the start and end frame and conditioning the super-resolution model on the original high-resolution frames without additional parameters unlocks high-fidelity results. VIDIM is fast to sample from as it jointly denoises all the frames to be generated, requires less than a billion parameters per diffusion model to produce compelling results, and still enjoys scalability and improved quality at larger parameter counts.",
        "page": "http://arxiv.org/abs/2404.01203",
        "pdf": "http://arxiv.org/pdf/2404.01203.pdf"
    },
    {
        "title": "DeMatch: Deep Decomposition of Motion Field for Two-View Correspondence Learning",
        "author": "Shihua Zhang, Zizhuo Li, Yuan Gao, Jiayi Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VSCode: General Visual Salient and Camouflaged Object Detection with 2D Prompt Learning",
        "author": "Ziyang Luo, Nian Liu, Wangbo Zhao, Xuguang Yang, Dingwen Zhang, Deng-Ping Fan, Fahad Shahbaz Khan, Junwei Han",
        "abstract": "Salient object detection (SOD) and camouflaged object detection (COD) are related yet distinct binary mapping tasks. These tasks involve multiple modalities, sharing commonalities and unique cues. Existing research often employs intricate task-specific specialist models, potentially leading to redundancy and suboptimal results. We introduce VSCode, a generalist model with novel 2D prompt learning, to jointly address four SOD tasks and three COD tasks. We utilize VST as the foundation model and introduce 2D prompts within the encoder-decoder architecture to learn domain and task-specific knowledge on two separate dimensions. A prompt discrimination loss helps disentangle peculiarities to benefit model optimization. VSCode outperforms state-of-the-art methods across six tasks on 26 datasets and exhibits zero-shot generalization to unseen tasks by combining 2D prompts, such as RGB-D COD. Source code has been available at https://github.com/Sssssuperior/VSCode.",
        "page": "http://arxiv.org/abs/2311.15011",
        "pdf": "http://arxiv.org/pdf/2311.15011.pdf"
    },
    {
        "title": "Self-supervised debiasing using low rank regularization",
        "author": "Geon Yeong Park, Chanyong Jung, Sangmin Lee, Jong Chul Ye, Sang Wan Lee",
        "abstract": "Spurious correlations can cause strong biases in deep neural networks, impairing generalization ability. While most existing debiasing methods require full supervision on either spurious attributes or target labels, training a debiased model from a limited amount of both annotations is still an open question. To address this issue, we investigate an interesting phenomenon using the spectral analysis of latent representations: spuriously correlated attributes make neural networks inductively biased towards encoding lower effective rank representations. We also show that a rank regularization can amplify this bias in a way that encourages highly correlated features. Leveraging these findings, we propose a self-supervised debiasing framework potentially compatible with unlabeled samples. Specifically, we first pretrain a biased encoder in a self-supervised manner with the rank regularization, serving as a semantic bottleneck to enforce the encoder to learn the spuriously correlated attributes. This biased encoder is then used to discover and upweight bias-conflicting samples in a downstream task, serving as a boosting to effectively debias the main model. Remarkably, the proposed debiasing framework significantly improves the generalization performance of self-supervised learning baselines and, in some cases, even outperforms state-of-the-art supervised debiasing approaches.",
        "page": "http://arxiv.org/abs/2210.05248",
        "pdf": "http://arxiv.org/pdf/2210.05248.pdf"
    },
    {
        "title": "Neural Markov Random Field for Stereo Matching",
        "author": "Tongfan Guan, Chen Wang, Yun-Hui Liu",
        "abstract": "Stereo matching is a core task for many computer vision and robotics applications. Despite their dominance in traditional stereo methods, the hand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy compared to end-to-end deep models. While deep learning representations have greatly improved the unary terms of the MRF models, the overall accuracy is still severely limited by the hand-crafted pairwise terms and message passing. To address these issues, we propose a neural MRF model, where both potential functions and message passing are designed using data-driven neural networks. Our fully data-driven model is built on the foundation of variational inference theory, to prevent convergence issues and retain stereo MRF's graph inductive bias. To make the inference tractable and scale well to high-resolution images, we also propose a Disparity Proposal Network (DPN) to adaptively prune the search space of disparity. The proposed approach ranks $1^{st}$ on both KITTI 2012 and 2015 leaderboards among all published methods while running faster than 100 ms. This approach significantly outperforms prior global methods, e.g., lowering D1 metric by more than 50% on KITTI 2015. In addition, our method exhibits strong cross-domain generalization and can recover sharp edges. The codes at https://github.com/aeolusguan/NMRF",
        "page": "http://arxiv.org/abs/2403.11193",
        "pdf": "http://arxiv.org/pdf/2403.11193.pdf"
    },
    {
        "title": "Ungeneralizable Examples",
        "author": "Jingwen Ye, Xinchao Wang",
        "abstract": "The training of contemporary deep learning models heavily relies on publicly available data, posing a risk of unauthorized access to online data and raising concerns about data privacy. Current approaches to creating unlearnable data involve incorporating small, specially designed noises, but these methods strictly limit data usability, overlooking its potential usage in authorized scenarios. In this paper, we extend the concept of unlearnable data to conditional data learnability and introduce \\textbf{U}n\\textbf{G}eneralizable \\textbf{E}xamples (UGEs). UGEs exhibit learnability for authorized users while maintaining unlearnability for potential hackers. The protector defines the authorized network and optimizes UGEs to match the gradients of the original data and its ungeneralizable version, ensuring learnability. To prevent unauthorized learning, UGEs are trained by maximizing a designated distance loss in a common feature space. Additionally, to further safeguard the authorized side from potential attacks, we introduce additional undistillation optimization. Experimental results on multiple datasets and various networks demonstrate that the proposed UGEs framework preserves data usability while reducing training performance on hacker networks, even under different types of attacks.",
        "page": "http://arxiv.org/abs/2404.14016",
        "pdf": "http://arxiv.org/pdf/2404.14016.pdf"
    },
    {
        "title": "Unifying Correspondence, Pose and NeRF for Pose-Free Novel View Synthesis from Stereo Pairs",
        "author": "Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jiaolong Yang, Chong Luo, Seungryong Kim",
        "abstract": "This work delves into the task of pose-free novel view synthesis from stereo pairs, a challenging and pioneering task in 3D vision. Our innovative framework, unlike any before, seamlessly integrates 2D correspondence matching, camera pose estimation, and NeRF rendering, fostering a synergistic enhancement of these tasks. We achieve this through designing an architecture that utilizes a shared representation, which serves as a foundation for enhanced 3D geometry understanding. Capitalizing on the inherent interplay between the tasks, our unified framework is trained end-to-end with the proposed training strategy to improve overall model accuracy. Through extensive evaluations across diverse indoor and outdoor scenes from two real-world datasets, we demonstrate that our approach achieves substantial improvement over previous methodologies, especially in scenarios characterized by extreme viewpoint changes and the absence of accurate camera poses.",
        "page": "http://arxiv.org/abs/2312.07246",
        "pdf": "http://arxiv.org/pdf/2312.07246.pdf"
    },
    {
        "title": "Text-to-3D Generation with Bidirectional Diffusion using both 3D and 2D priors",
        "author": "Lihe Ding, Shaocong Dong, Zhanpeng Huang, Zibin Wang, Yiyuan Zhang, Kaixiong Gong, Dan Xu, Tianfan Xue",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Beyond Average: Individualized Visual Scanpath Prediction",
        "author": "Xianyu Chen, Ming Jiang, Qi Zhao",
        "abstract": "Understanding how attention varies across individuals has significant scientific and societal impacts. However, existing visual scanpath models treat attention uniformly, neglecting individual differences. To bridge this gap, this paper focuses on individualized scanpath prediction (ISP), a new attention modeling task that aims to accurately predict how different individuals shift their attention in diverse visual tasks. It proposes an ISP method featuring three novel technical components: (1) an observer encoder to characterize and integrate an observer's unique attention traits, (2) an observer-centric feature integration approach that holistically combines visual features, task guidance, and observer-specific characteristics, and (3) an adaptive fixation prioritization mechanism that refines scanpath predictions by dynamically prioritizing semantic feature maps based on individual observers' attention traits. These novel components allow scanpath models to effectively address the attention variations across different observers. Our method is generally applicable to different datasets, model architectures, and visual tasks, offering a comprehensive tool for transforming general scanpath models into individualized ones. Comprehensive evaluations using value-based and ranking-based metrics verify the method's effectiveness and generalizability.",
        "page": "http://arxiv.org/abs/2404.12235",
        "pdf": "http://arxiv.org/pdf/2404.12235.pdf"
    },
    {
        "title": "Style Blind Domain Generalized Semantic Segmentation via Covariance Alignment and Semantic Consistence Contrastive Learning",
        "author": "Woo-Jin Ahn, Geun-Yeong Yang, Hyunduck Choi, Myo-Taeg Lim",
        "abstract": "Deep learning models for semantic segmentation often experience performance degradation when deployed to unseen target domains unidentified during the training phase. This is mainly due to variations in image texture (\\ie style) from different data sources. To tackle this challenge, existing domain generalized semantic segmentation (DGSS) methods attempt to remove style variations from the feature. However, these approaches struggle with the entanglement of style and content, which may lead to the unintentional removal of crucial content information, causing performance degradation. This study addresses this limitation by proposing BlindNet, a novel DGSS approach that blinds the style without external modules or datasets. The main idea behind our proposed approach is to alleviate the effect of style in the encoder whilst facilitating robust segmentation in the decoder. To achieve this, BlindNet comprises two key components: covariance alignment and semantic consistency contrastive learning. Specifically, the covariance alignment trains the encoder to uniformly recognize various styles and preserve the content information of the feature, rather than removing the style-sensitive factor. Meanwhile, semantic consistency contrastive learning enables the decoder to construct discriminative class embedding space and disentangles features that are vulnerable to misclassification. Through extensive experiments, our approach outperforms existing DGSS methods, exhibiting robustness and superior performance for semantic segmentation on unseen target domains.",
        "page": "http://arxiv.org/abs/2403.06122",
        "pdf": "http://arxiv.org/pdf/2403.06122.pdf"
    },
    {
        "title": "DiffusionRegPose: Enhancing Multi-Person Pose Estimation using a Diffusion-Based End-to-End Regression Approach",
        "author": "Dayi Tan, Hansheng Chen, Wei Tian, Lu Xiong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Test-Time Domain Generalization for Face Anti-Spoofing",
        "author": "Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Xuequan Lu, Shouhong Ding, Lizhuang Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos",
        "author": "Leonhard Sommer, Artur Jesslen, Eddy Ilg, Adam Kortylewski",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fooling Polarization-based Vision using Locally Controllable Polarizing Projection",
        "author": "Zhuoxiao Li, Zhihang Zhong, Shohei Nobuhara, Ko Nishino, Yinqiang Zheng",
        "abstract": "Polarization is a fundamental property of light that encodes abundant information regarding surface shape, material, illumination and viewing geometry. The computer vision community has witnessed a blossom of polarization-based vision applications, such as reflection removal, shape-from-polarization, transparent object segmentation and color constancy, partially due to the emergence of single-chip mono/color polarization sensors that make polarization data acquisition easier than ever. However, is polarization-based vision vulnerable to adversarial attacks? If so, is that possible to realize these adversarial attacks in the physical world, without being perceived by human eyes? In this paper, we warn the community of the vulnerability of polarization-based vision, which can be more serious than RGB-based vision. By adapting a commercial LCD projector, we achieve locally controllable polarizing projection, which is successfully utilized to fool state-of-the-art polarization-based vision algorithms for glass segmentation and color constancy. Compared with existing physical attacks on RGB-based vision, which always suffer from the trade-off between attack efficacy and eye conceivability, the adversarial attackers based on polarizing projection are contact-free and visually imperceptible, since naked human eyes can rarely perceive the difference of viciously manipulated polarizing light and ordinary illumination. This poses unprecedented risks on polarization-based vision, both in the monochromatic and trichromatic domain, for which due attentions should be paid and counter measures be considered.",
        "page": "http://arxiv.org/abs/2303.17890",
        "pdf": "http://arxiv.org/pdf/2303.17890.pdf"
    },
    {
        "title": "Affine Equivariant Networks Based on Differential Invariants",
        "author": "Yikang Li, Yeqing Qiu, Yuxuan Chen, Lingshen He, Zhouchen Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "C3: High-performance and low-complexity neural compression from a single image or video",
        "author": "Hyunjik Kim, Matthias Bauer, Lucas Theis, Jonathan Richard Schwarz, Emilien Dupont",
        "abstract": "Most neural compression models are trained on large datasets of images or videos in order to generalize to unseen data. Such generalization typically requires large and expressive architectures with a high decoding complexity. Here we introduce C3, a neural compression method with strong rate-distortion (RD) performance that instead overfits a small model to each image or video separately. The resulting decoding complexity of C3 can be an order of magnitude lower than neural baselines with similar RD performance. C3 builds on COOL-CHIC (Ladune et al.) and makes several simple and effective improvements for images. We further develop new methodology to apply C3 to videos. On the CLIC2020 image benchmark, we match the RD performance of VTM, the reference implementation of the H.266 codec, with less than 3k MACs/pixel for decoding. On the UVG video benchmark, we match the RD performance of the Video Compression Transformer (Mentzer et al.), a well-established neural video codec, with less than 5k MACs/pixel for decoding.",
        "page": "http://arxiv.org/abs/2312.02753",
        "pdf": "http://arxiv.org/pdf/2312.02753.pdf"
    },
    {
        "title": "AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing",
        "author": "Fan Yang, Tianyi Chen, XIAOSHENG HE, Zhongang Cai, Lei Yang, Si Wu, Guosheng Lin",
        "abstract": "Editable 3D-aware generation, which supports user-interacted editing, has witnessed rapid development recently. However, existing editable 3D GANs either fail to achieve high-accuracy local editing or suffer from huge computational costs. We propose AttriHuman-3D, an editable 3D human generation model, which address the aforementioned problems with attribute decomposition and indexing. The core idea of the proposed model is to generate all attributes (e.g. human body, hair, clothes and so on) in an overall attribute space with six feature planes, which are then decomposed and manipulated with different attribute indexes. To precisely extract features of different attributes from the generated feature planes, we propose a novel attribute indexing method as well as an orthogonal projection regularization to enhance the disentanglement. We also introduce a hyper-latent training strategy and an attribute-specific sampling strategy to avoid style entanglement and misleading punishment from the discriminator. Our method allows users to interactively edit selected attributes in the generated 3D human avatars while keeping others fixed. Both qualitative and quantitative experiments demonstrate that our model provides a strong disentanglement between different attributes, allows fine-grained image editing and generates high-quality 3D human avatars.",
        "page": "http://arxiv.org/abs/2312.02209",
        "pdf": "http://arxiv.org/pdf/2312.02209.pdf"
    },
    {
        "title": "PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization",
        "author": "Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, Rongrong Ji",
        "abstract": "Recent advancements in personalized image generation using diffusion models have been noteworthy. However, existing methods suffer from inefficiencies due to the requirement for subject-specific fine-tuning. This computationally intensive process hinders efficient deployment, limiting practical usability. Moreover, these methods often grapple with identity distortion and limited expression diversity. In light of these challenges, we propose PortraitBooth, an innovative approach designed for high efficiency, robust identity preservation, and expression-editable text-to-image generation, without the need for fine-tuning. PortraitBooth leverages subject embeddings from a face recognition model for personalized image generation without fine-tuning. It eliminates computational overhead and mitigates identity distortion. The introduced dynamic identity preservation strategy further ensures close resemblance to the original image identity. Moreover, PortraitBooth incorporates emotion-aware cross-attention control for diverse facial expressions in generated images, supporting text-driven expression editing. Its scalability enables efficient and high-quality image creation, including multi-subject generation. Extensive results demonstrate superior performance over other state-of-the-art methods in both single and multiple image generation scenarios.",
        "page": "http://arxiv.org/abs/2312.06354",
        "pdf": "http://arxiv.org/pdf/2312.06354.pdf"
    },
    {
        "title": "HDQMF: Holographic Feature Decomposition Using Quantum Algorithms",
        "author": "Prathyush Poduval, Zhuowen Zou, Mohsen Imani",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fair-VPT: Fair Visual Prompt Tuning for Image Classification",
        "author": "Sungho Park, Hyeran Byun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning",
        "author": "Jaewoo Jeong, Daehee Park, Kuk-Jin Yoon",
        "abstract": "Human pose forecasting garners attention for its diverse applications. However, challenges in modeling the multi-modal nature of human motion and intricate interactions among agents persist, particularly with longer timescales and more agents. In this paper, we propose an interaction-aware trajectory-conditioned long-term multi-agent human pose forecasting model, utilizing a coarse-to-fine prediction approach: multi-modal global trajectories are initially forecasted, followed by respective local pose forecasts conditioned on each mode. In doing so, our Trajectory2Pose model introduces a graph-based agent-wise interaction module for a reciprocal forecast of local motion-conditioned global trajectory and trajectory-conditioned local pose. Our model effectively handles the multi-modality of human motion and the complexity of long-term multi-agent interactions, improving performance in complex environments. Furthermore, we address the lack of long-term (6s+) multi-agent (5+) datasets by constructing a new dataset from real-world images and 2D annotations, enabling a comprehensive evaluation of our proposed model. State-of-the-art prediction performance on both complex and simpler datasets confirms the generalized effectiveness of our method. The code is available at https://github.com/Jaewoo97/T2P.",
        "page": "http://arxiv.org/abs/2404.05218",
        "pdf": "http://arxiv.org/pdf/2404.05218.pdf"
    },
    {
        "title": "Revisiting Single Image Reflection Removal In the Wild",
        "author": "Yurui Zhu, Bo Li, Xueyang Fu, Peng-Tao Jiang, Hao Zhang, Qibin Sun, Zheng-Jun Zha, Jinwei Chen",
        "abstract": "This research focuses on the issue of single-image reflection removal (SIRR) in real-world conditions, examining it from two angles: the collection pipeline of real reflection pairs and the perception of real reflection locations. We devise an advanced reflection collection pipeline that is highly adaptable to a wide range of real-world reflection scenarios and incurs reduced costs in collecting large-scale aligned reflection pairs. In the process, we develop a large-scale, high-quality reflection dataset named Reflection Removal in the Wild (RRW). RRW contains over 14,950 high-resolution real-world reflection pairs, a dataset forty-five times larger than its predecessors. Regarding perception of reflection locations, we identify that numerous virtual reflection objects visible in reflection images are not present in the corresponding ground-truth images. This observation, drawn from the aligned pairs, leads us to conceive the Maximum Reflection Filter (MaxRF). The MaxRF could accurately and explicitly characterize reflection locations from pairs of images. Building upon this, we design a reflection location-aware cascaded framework, specifically tailored for SIRR. Powered by these innovative techniques, our solution achieves superior performance than current leading methods across multiple real-world benchmarks. Codes and datasets will be publicly available.",
        "page": "http://arxiv.org/abs/2311.17320",
        "pdf": "http://arxiv.org/pdf/2311.17320.pdf"
    },
    {
        "title": "Turb-Seg-Res: A Segment-then-Restore Pipeline for Dynamic Videos with Atmospheric Turbulence",
        "author": "Ripon Saha, Dehao Qin, Nianyi Li, Jinwei Ye, Suren Jayasuriya",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "IIRP-Net: Iterative Inference Residual Pyramid Network for Enhanced Image Registration",
        "author": "Tai Ma, zhangsuwei, Jiafeng Li, Ying Wen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SC-Tune: Unleashing Self-Consistent Referential Comprehension  in Large Vision Language Models",
        "author": "Tongtian Yue, Jie Cheng, Longteng Guo, Xingyuan Dai, Zijia Zhao, Xingjian He, Gang Xiong, Yisheng Lv, Jing Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation",
        "author": "Jiehong Lin, lihua liu, Dekun Lu, Kui Jia",
        "abstract": "Zero-shot 6D object pose estimation involves the detection of novel objects with their 6D poses in cluttered scenes, presenting significant challenges for model generalizability. Fortunately, the recent Segment Anything Model (SAM) has showcased remarkable zero-shot transfer performance, which provides a promising solution to tackle this task. Motivated by this, we introduce SAM-6D, a novel framework designed to realize the task through two steps, including instance segmentation and pose estimation. Given the target objects, SAM-6D employs two dedicated sub-networks, namely Instance Segmentation Model (ISM) and Pose Estimation Model (PEM), to perform these steps on cluttered RGB-D images. ISM takes SAM as an advanced starting point to generate all possible object proposals and selectively preserves valid ones through meticulously crafted object matching scores in terms of semantics, appearance and geometry. By treating pose estimation as a partial-to-partial point matching problem, PEM performs a two-stage point matching process featuring a novel design of background tokens to construct dense 3D-3D correspondence, ultimately yielding the pose estimates. Without bells and whistles, SAM-6D outperforms the existing methods on the seven core datasets of the BOP Benchmark for both instance segmentation and pose estimation of novel objects.",
        "page": "http://arxiv.org/abs/2311.15707",
        "pdf": "http://arxiv.org/pdf/2311.15707.pdf"
    },
    {
        "title": "MatSynth: A Modern PBR Materials Dataset",
        "author": "Giuseppe Vecchio, Valentin Deschaintre",
        "abstract": "We introduce MatSynth, a dataset of 4,000+ CC0 ultra-high resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation and acquisition. However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications. The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth.",
        "page": "http://arxiv.org/abs/2401.06056",
        "pdf": "http://arxiv.org/pdf/2401.06056.pdf"
    },
    {
        "title": "$MonoDiff$: Monocular 3D Object Detection and Pose Estimation with Diffusion Models",
        "author": "Yasiru Ranasinghe, Deepti Hegde, Vishal M. Patel",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BIVDiff: A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models",
        "author": "Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, Limin Wang",
        "abstract": "Diffusion models have made tremendous progress in text-driven image and video generation. Now text-to-image foundation models are widely applied to various downstream image synthesis tasks, such as controllable image generation and image editing, while downstream video synthesis tasks are less explored for several reasons. First, it requires huge memory and computation overhead to train a video generation foundation model. Even with video foundation models, additional costly training is still required for downstream video synthesis tasks. Second, although some works extend image diffusion models into videos in a training-free manner, temporal consistency cannot be well preserved. Finally, these adaption methods are specifically designed for one task and fail to generalize to different tasks. To mitigate these issues, we propose a training-free general-purpose video synthesis framework, coined as {\\bf BIVDiff}, via bridging specific image diffusion models and general text-to-video foundation diffusion models. Specifically, we first use a specific image diffusion model (e.g., ControlNet and Instruct Pix2Pix) for frame-wise video generation, then perform Mixed Inversion on the generated video, and finally input the inverted latents into the video diffusion models (e.g., VidRD and ZeroScope) for temporal smoothing. This decoupled framework enables flexible image model selection for different purposes with strong task generalization and high efficiency. To validate the effectiveness and general use of BIVDiff, we perform a wide range of video synthesis tasks, including controllable video generation, video editing, video inpainting, and outpainting.",
        "page": "http://arxiv.org/abs/2312.02813",
        "pdf": "http://arxiv.org/pdf/2312.02813.pdf"
    },
    {
        "title": "Bi-Causal: Group Activity Recognition via Bidirectional Causality",
        "author": "Youliang Zhang, Wenxuan Liu, danni xu, Zhuo Zhou, Zheng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PerAda: Parameter-Efficient Federated Learning Personalization with Generalization Guarantees",
        "author": "Chulin Xie, De-An Huang, Wenda Chu, Daguang Xu, Chaowei Xiao, Bo Li, Anima Anandkumar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "How to Train Neural Field Representations: A Comprehensive Study and Benchmark",
        "author": "Samuele Papa, Riccardo Valperga, David Knigge, Miltiadis Kofinas, Phillip Lippe, Jan-Jakob Sonke, Efstratios Gavves",
        "abstract": "Neural fields (NeFs) have recently emerged as a versatile method for modeling signals of various modalities, including images, shapes, and scenes. Subsequently, a number of works have explored the use of NeFs as representations for downstream tasks, e.g. classifying an image based on the parameters of a NeF that has been fit to it. However, the impact of the NeF hyperparameters on their quality as downstream representation is scarcely understood and remains largely unexplored. This is in part caused by the large amount of time required to fit datasets of neural fields. In this work, we propose $\\verb|fit-a-nef|$, a JAX-based library that leverages parallelization to enable fast optimization of large-scale NeF datasets, resulting in a significant speed-up. With this library, we perform a comprehensive study that investigates the effects of different hyperparameters -- including initialization, network architecture, and optimization strategies -- on fitting NeFs for downstream tasks. Our study provides valuable insights on how to train NeFs and offers guidance for optimizing their effectiveness in downstream applications. Finally, based on the proposed library and our analysis, we propose Neural Field Arena, a benchmark consisting of neural field variants of popular vision datasets, including MNIST, CIFAR, variants of ImageNet, and ShapeNetv2. Our library and the Neural Field Arena will be open-sourced to introduce standardized benchmarking and promote further research on neural fields.",
        "page": "http://arxiv.org/abs/2312.10531",
        "pdf": "http://arxiv.org/pdf/2312.10531.pdf"
    },
    {
        "title": "Semantic-Aware Multi-Label Adversarial Attacks",
        "author": "Hassan Mahmood, Ehsan Elhamifar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Motion-adaptive Separable Collaborative Filters for Blind Motion Deblurring",
        "author": "Chengxu Liu, Xuan Wang, Xiangyu Xu, Ruhao Tian, Shuai Li, Xueming Qian, Ming-Hsuan Yang",
        "abstract": "Eliminating image blur produced by various kinds of motion has been a challenging problem. Dominant approaches rely heavily on model capacity to remove blurring by reconstructing residual from blurry observation in feature space. These practices not only prevent the capture of spatially variable motion in the real world but also ignore the tailored handling of various motions in image space. In this paper, we propose a novel real-world deblurring filtering model called the Motion-adaptive Separable Collaborative (MISC) Filter. In particular, we use a motion estimation network to capture motion information from neighborhoods, thereby adaptively estimating spatially-variant motion flow, mask, kernels, weights, and offsets to obtain the MISC Filter. The MISC Filter first aligns the motion-induced blurring patterns to the motion middle along the predicted flow direction, and then collaboratively filters the aligned image through the predicted kernels, weights, and offsets to generate the output. This design can handle more generalized and complex motion in a spatially differentiated manner. Furthermore, we analyze the relationships between the motion estimation network and the residual reconstruction network. Extensive experiments on four widely used benchmarks demonstrate that our method provides an effective solution for real-world motion blur removal and achieves state-of-the-art performance. Code is available at https://github.com/ChengxuLiu/MISCFilter",
        "page": "http://arxiv.org/abs/2404.13153",
        "pdf": "http://arxiv.org/pdf/2404.13153.pdf"
    },
    {
        "title": "The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose Refinement",
        "author": "Gabriele Trivigno, Carlo Masone, Barbara Caputo, Torsten Sattler",
        "abstract": "Pose refinement is an interesting and practically relevant research direction. Pose refinement can be used to (1) obtain a more accurate pose estimate from an initial prior (e.g., from retrieval), (2) as pre-processing, i.e., to provide a better starting point to a more expensive pose estimator, (3) as post-processing of a more accurate localizer. Existing approaches focus on learning features / scene representations for the pose refinement task. This involves training an implicit scene representation or learning features while optimizing a camera pose-based loss. A natural question is whether training specific features / representations is truly necessary or whether similar results can be already achieved with more generic features. In this work, we present a simple approach that combines pre-trained features with a particle filter and a renderable representation of the scene. Despite its simplicity, it achieves state-of-the-art results, demonstrating that one can easily build a pose refiner without the need for specific training. The code is at https://github.com/ga1i13o/mcloc_poseref",
        "page": "http://arxiv.org/abs/2404.10438",
        "pdf": "http://arxiv.org/pdf/2404.10438.pdf"
    },
    {
        "title": "PointInfinity: Resolution-Invariant Point Diffusion Models",
        "author": "Zixuan Huang, Justin Johnson, Shoubhik Debnath, James Rehg, Chao-Yuan Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "F$^3$Loc: Fusion and Filtering for Floorplan Localization",
        "author": "Changan Chen, Rui Wang, Christoph Vogel, Marc Pollefeys",
        "abstract": "In this paper we propose an efficient data-driven solution to self-localization within a floorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal filtering module. Operating internally with an efficient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to benefit from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art by a significant margin.",
        "page": "http://arxiv.org/abs/2403.03370",
        "pdf": "http://arxiv.org/pdf/2403.03370.pdf"
    },
    {
        "title": "Construct to Associate: Cooperative Context Learning for Domain Adaptive Point Cloud Segmentation",
        "author": "Guangrui Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EarthLoc: Astronaut Photography Localization by Indexing Earth from Space",
        "author": "Gabriele Berton, Alex Stoken, Barbara Caputo, Carlo Masone",
        "abstract": "Astronaut photography, spanning six decades of human spaceflight, presents a unique Earth observations dataset with immense value for both scientific research and disaster response. Despite its significance, accurately localizing the geographical extent of these images, crucial for effective utilization, poses substantial challenges. Current manual localization efforts are time-consuming, motivating the need for automated solutions. We propose a novel approach - leveraging image retrieval - to address this challenge efficiently. We introduce innovative training techniques, including Year-Wise Data Augmentation and a Neutral-Aware Multi-Similarity Loss, which contribute to the development of a high-performance model, EarthLoc. We develop six evaluation datasets and perform a comprehensive benchmark comparing EarthLoc to existing methods, showcasing its superior efficiency and accuracy. Our approach marks a significant advancement in automating the localization of astronaut photography, which will help bridge a critical gap in Earth observations data. Code and datasets are available at https://github.com/gmberton/EarthLoc",
        "page": "http://arxiv.org/abs/2403.06758",
        "pdf": "http://arxiv.org/pdf/2403.06758.pdf"
    },
    {
        "title": "Accelerating Diffusion Sampling with Optimized Time Steps",
        "author": "Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, Zhenguo Li",
        "abstract": "Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds. Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps.",
        "page": "http://arxiv.org/abs/2402.17376",
        "pdf": "http://arxiv.org/pdf/2402.17376.pdf"
    },
    {
        "title": "Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression",
        "author": "Hancheng Ye, Chong Yu, Peng Ye, Renqiu Xia, Bo Zhang, Yansong Tang, Jiwen Lu, Tao Chen",
        "abstract": "Recent Vision Transformer Compression (VTC) works mainly follow a two-stage scheme, where the importance score of each model unit is first evaluated or preset in each submodule, followed by the sparsity score evaluation according to the target sparsity constraint. Such a separate evaluation process induces the gap between importance and sparsity score distributions, thus causing high search costs for VTC. In this work, for the first time, we investigate how to integrate the evaluations of importance and sparsity scores into a single stage, searching the optimal subnets in an efficient manner. Specifically, we present OFB, a cost-efficient approach that simultaneously evaluates both importance and sparsity scores, termed Once for Both (OFB), for VTC. First, a bi-mask scheme is developed by entangling the importance score and the differentiable sparsity score to jointly determine the pruning potential (prunability) of each unit. Such a bi-mask search strategy is further used together with a proposed adaptive one-hot loss to realize the progressive-and-efficient search for the most important subnet. Finally, Progressive Masked Image Modeling (PMIM) is proposed to regularize the feature space to be more representative during the search process, which may be degraded by the dimension reduction. Extensive experiments demonstrate that OFB can achieve superior compression performance over state-of-the-art searching-based and pruning-based methods under various Vision Transformer architectures, meanwhile promoting search efficiency significantly, e.g., costing one GPU search day for the compression of DeiT-S on ImageNet-1K.",
        "page": "http://arxiv.org/abs/2403.15835",
        "pdf": "http://arxiv.org/pdf/2403.15835.pdf"
    },
    {
        "title": "One-Shot Structure-Aware Stylized Image Synthesis",
        "author": "Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong",
        "abstract": "While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models.",
        "page": "http://arxiv.org/abs/2402.17275",
        "pdf": "http://arxiv.org/pdf/2402.17275.pdf"
    },
    {
        "title": "Grounded Text-to-Image Synthesis with Attention Refocusing",
        "author": "Quynh Phung, Songwei Ge, Jia-Bin Huang",
        "abstract": "Driven by the scalable diffusion models trained on large-scale datasets, text-to-image synthesis methods have shown compelling results. However, these models still fail to precisely follow the text prompt involving multiple objects, attributes, or spatial compositions. In this paper, we reveal the potential causes in the diffusion model's cross-attention and self-attention layers. We propose two novel losses to refocus attention maps according to a given spatial layout during sampling. Creating the layouts manually requires additional effort and can be tedious. Therefore, we explore using large language models (LLM) to produce these layouts for our method. We conduct extensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate our proposed method. We show that our proposed attention refocusing effectively improves the controllability of existing approaches.",
        "page": "http://arxiv.org/abs/2306.05427",
        "pdf": "http://arxiv.org/pdf/2306.05427.pdf"
    },
    {
        "title": "Tyche: Stochastic in Context Learning for Medical Image Segmentation",
        "author": "Marianne Rakic, Hallee Wong, Jose Javier Gonzalez Ortiz, Beth Cimini, John Guttag, Adrian V. Dalca",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "What, How, and When Should Object Detectors Update in Continually Changing Test Domains?",
        "author": "Jayeon Yoo, Dongkwan Lee, Inseop Chung, Donghyun Kim, Nojun Kwak",
        "abstract": "It is a well-known fact that the performance of deep learning models deteriorates when they encounter a distribution shift at test time. Test-time adaptation (TTA) algorithms have been proposed to adapt the model online while inferring test data. However, existing research predominantly focuses on classification tasks through the optimization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it challenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to update it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Furthermore, we present a practical and straightforward class-wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by determining when the model is sufficiently adapted or when additional adaptation is needed due to changes in the test distribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9\\%p and 7.9\\%p in mAP for COCO $\\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher.",
        "page": "http://arxiv.org/abs/2312.08875",
        "pdf": "http://arxiv.org/pdf/2312.08875.pdf"
    },
    {
        "title": "Learning Correlation Structures for Vision Transformers",
        "author": "Manjin Kim, Paul Hongsuck Seo, Cordelia Schmid, Minsu Cho",
        "abstract": "We introduce a new attention mechanism, dubbed structural self-attention (StructSA), that leverages rich correlation patterns naturally emerging in key-query interactions of attention. StructSA generates attention maps by recognizing space-time structures of key-query correlations via convolution and uses them to dynamically aggregate local contexts of value features. This effectively leverages rich structural patterns in images and videos such as scene layouts, object motion, and inter-object relations. Using StructSA as a main building block, we develop the structural vision transformer (StructViT) and evaluate its effectiveness on both image and video classification tasks, achieving state-of-the-art results on ImageNet-1K, Kinetics-400, Something-Something V1 & V2, Diving-48, and FineGym.",
        "page": "http://arxiv.org/abs/2404.03924",
        "pdf": "http://arxiv.org/pdf/2404.03924.pdf"
    },
    {
        "title": "Equivariant plug-and-play image reconstruction",
        "author": "Matthieu Terris, Thomas Moreau, Nelly Pustelnik, Juli\u00e1n Tachella",
        "abstract": "Plug-and-play algorithms constitute a popular framework for solving inverse imaging problems that rely on the implicit definition of an image prior via a denoiser. These algorithms can leverage powerful pre-trained denoisers to solve a wide range of imaging tasks, circumventing the necessity to train models on a per-task basis. Unfortunately, plug-and-play methods often show unstable behaviors, hampering their promise of versatility and leading to suboptimal quality of reconstructed images. In this work, we show that enforcing equivariance to certain groups of transformations (rotations, reflections, and/or translations) on the denoiser strongly improves the stability of the algorithm as well as its reconstruction quality. We provide a theoretical analysis that illustrates the role of equivariance on better performance and stability. We present a simple algorithm that enforces equivariance on any existing denoiser by simply applying a random transformation to the input of the denoiser and the inverse transformation to the output at each iteration of the algorithm. Experiments on multiple imaging modalities and denoising networks show that the equivariant plug-and-play algorithm improves both the reconstruction performance and the stability compared to their non-equivariant counterparts.",
        "page": "http://arxiv.org/abs/2312.01831",
        "pdf": "http://arxiv.org/pdf/2312.01831.pdf"
    },
    {
        "title": "Visual Objectification in Films: Towards a New AI Task for Video Interpretation",
        "author": "Julie Tores, Lucile Sassatelli, Hui-Yin Wu, Clement Bergman, L\u00e9a Andolfi, Victor Ecrement, Frederic Precioso, Thierry Devars, Magali GUARESI, Virginie Julliard, Sarah L\u00e9cossais",
        "abstract": "In film gender studies, the concept of 'male gaze' refers to the way the characters are portrayed on-screen as objects of desire rather than subjects. In this article, we introduce a novel video-interpretation task, to detect character objectification in films. The purpose is to reveal and quantify the usage of complex temporal patterns operated in cinema to produce the cognitive perception of objectification. We introduce the ObyGaze12 dataset, made of 1914 movie clips densely annotated by experts for objectification concepts identified in film studies and psychology. We evaluate recent vision models, show the feasibility of the task and where the challenges remain with concept bottleneck models. Our new dataset and code are made available to the community.",
        "page": "http://arxiv.org/abs/2401.13296",
        "pdf": "http://arxiv.org/pdf/2401.13296.pdf"
    },
    {
        "title": "Ink Dot-Oriented Differentiable Optimization for Neural Image Halftoning",
        "author": "Hao Jiang, Bingfeng Zhou, Yadong Mu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow",
        "author": "Hanyu Zhou, Yi Chang, Zhiwei Shi",
        "abstract": "Single RGB or LiDAR is the mainstream sensor for the challenging scene flow, which relies heavily on visual features to match motion features. Compared with single modality, existing methods adopt a fusion strategy to directly fuse the cross-modal complementary knowledge in motion space. However, these direct fusion methods may suffer the modality gap due to the visual intrinsic heterogeneous nature between RGB and LiDAR, thus deteriorating motion features. We discover that event has the homogeneous nature with RGB and LiDAR in both visual and motion spaces. In this work, we bring the event as a bridge between RGB and LiDAR, and propose a novel hierarchical visual-motion fusion framework for scene flow, which explores a homogeneous space to fuse the cross-modal complementary knowledge for physical interpretation. In visual fusion, we discover that event has a complementarity (relative v.s. absolute) in luminance space with RGB for high dynamic imaging, and has a complementarity (local boundary v.s. global shape) in scene structure space with LiDAR for structure integrity. In motion fusion, we figure out that RGB, event and LiDAR are complementary (spatial-dense, temporal-dense v.s. spatiotemporal-sparse) to each other in correlation space, which motivates us to fuse their motion correlations for motion continuity. The proposed hierarchical fusion can explicitly fuse the multimodal knowledge to progressively improve scene flow from visual space to motion space. Extensive experiments have been performed to verify the superiority of the proposed method.",
        "page": "http://arxiv.org/abs/2403.07432",
        "pdf": "http://arxiv.org/pdf/2403.07432.pdf"
    },
    {
        "title": "FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Action Segmentation",
        "author": "Zijia Lu, Ehsan Elhamifar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Patch2Self2: Self-supervised Denoising on Coresets via Matrix Sketching",
        "author": "Shreyas Fadnavis, Agniva Chowdhury, Joshua Batson, Petros Drineas, Eleftherios Garyfallidis",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Optimizing Diffusion Noise Can Serve As Universal Motion Priors",
        "author": "Korrawe Karunratanakul, Konpat Preechakul, Emre Aksan, Thabo Beeler, Supasorn Suwajanakorn, Siyu Tang",
        "abstract": "We propose Diffusion Noise Optimization (DNO), a new method that effectively leverages existing motion diffusion models as motion priors for a wide range of motion-related tasks. Instead of training a task-specific diffusion model for each new task, DNO operates by optimizing the diffusion latent noise of an existing pre-trained text-to-motion model. Given the corresponding latent noise of a human motion, it propagates the gradient from the target criteria defined on the motion space through the whole denoising process to update the diffusion latent noise. As a result, DNO supports any use cases where criteria can be defined as a function of motion. In particular, we show that, for motion editing and control, DNO outperforms existing methods in both achieving the objective and preserving the motion content. DNO accommodates a diverse range of editing modes, including changing trajectory, pose, joint locations, or avoiding newly added obstacles. In addition, DNO is effective in motion denoising and completion, producing smooth and realistic motion from noisy and partial inputs. DNO achieves these results at inference time without the need for model retraining, offering great versatility for any defined reward or loss function on the motion representation.",
        "page": "http://arxiv.org/abs/2312.11994",
        "pdf": "http://arxiv.org/pdf/2312.11994.pdf"
    },
    {
        "title": "Efficient Vision-Language Pre-training by Cluster Masking",
        "author": "Zihao Wei, Zixuan Pan, Andrew Owens",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generative Unlearning for Any Identity",
        "author": "Juwon Seo, Sung-Hoon Lee, Tae-Young Lee, SeungJun Moon, Gyeong-Moon Park",
        "abstract": "Recent advances in generative models trained on large-scale datasets have made it possible to synthesize high-quality samples across various domains. Moreover, the emergence of strong inversion networks enables not only a reconstruction of real-world images but also the modification of attributes through various editing methods. However, in certain domains related to privacy issues, e.g., human faces, advanced generative models along with strong inversion methods can lead to potential misuses. In this paper, we propose an essential yet under-explored task called generative identity unlearning, which steers the model not to generate an image of a specific identity. In the generative identity unlearning, we target the following objectives: (i) preventing the generation of images with a certain identity, and (ii) preserving the overall quality of the generative model. To satisfy these goals, we propose a novel framework, Generative Unlearning for Any Identity (GUIDE), which prevents the reconstruction of a specific identity by unlearning the generator with only a single image. GUIDE consists of two parts: (i) finding a target point for optimization that un-identifies the source latent code and (ii) novel loss functions that facilitate the unlearning procedure while less affecting the learned distribution. Our extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in the generative machine unlearning task. The code is available at https://github.com/KHU-AGI/GUIDE.",
        "page": "http://arxiv.org/abs/2405.09879",
        "pdf": "http://arxiv.org/pdf/2405.09879.pdf"
    },
    {
        "title": "Enhancing Multimodal Cooperation via Sample-level Modality Valuation",
        "author": "Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation",
        "author": "Xiongwei Wu, Sicheng Yu, Ee-Peng Lim, Chong Wah Ngo",
        "abstract": "In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs associated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting. These methods often fall short in effectively handling the ingredients, particularly new and diverse ones. In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context. By integrating vision-language models (VLMs), our approach enriches text embedding with image-specific information through two innovative modules, eg, an image-to-text learner FoodLearner and an Image-Informed Text Encoder. The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLearner and the subsequent learning phase for segmentation. The pre-training phase equips FoodLearner with the capability to align visual information with corresponding textual representations that are specifically related to food, while the second phase adapts both the FoodLearner and the Image-Informed Text Encoder for the segmentation task. By addressing the deficiencies of previous models, OVFoodSeg demonstrates a significant improvement, achieving an 4.9\\% increase in mean Intersection over Union (mIoU) on the FoodSeg103 dataset, setting a new milestone for food image segmentation.",
        "page": "http://arxiv.org/abs/2404.01409",
        "pdf": "http://arxiv.org/pdf/2404.01409.pdf"
    },
    {
        "title": "CaKDP: Category-aware Knowledge Distillation and Pruning Framework for Lightweight 3D Object Detection",
        "author": "Haonan Zhang, Longjun Liu, Yuqi Huang, YangZhao, Xinyu Lei, Bihan Wen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SfmCAD: Unsupervised CAD Reconstruction by Learning Sketch-based Feature Modeling Operations",
        "author": "Pu Li, Jianwei Guo, HUIBIN LI, Bedrich Benes, Dong-Ming Yan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection",
        "author": "Ting Lei, Shaofeng Yin, Yang Liu",
        "abstract": "Open-vocabulary human-object interaction (HOI) detection, which is concerned with the problem of detecting novel HOIs guided by natural language, is crucial for understanding human-centric scenes. However, prior zero-shot HOI detectors often employ the same levels of feature maps to model HOIs with varying distances, leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances. In addition, these detectors primarily rely on category names and overlook the rich contextual information that language can provide, which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone. In this paper, we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE), harnessing the potential of Visual-Language Models (VLMs). Specifically, we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process. Furthermore, by leveraging large language models (LLMs) such as GPT models, we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions. Then we integrate the generalizable and fine-grained semantics of human body parts to improve interaction recognition. Experimental results on two datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection. The code and models are available at https://github.com/ltttpku/CMD-SE-release.",
        "page": "http://arxiv.org/abs/2404.06194",
        "pdf": "http://arxiv.org/pdf/2404.06194.pdf"
    },
    {
        "title": "Adapt or Perish: Adaptive Sparse Transformer with Attentive Feature Refinement for Image Restoration",
        "author": "Shihao Zhou, Duosheng Chen, Jinshan Pan, Jinglei Shi, Jufeng Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation",
        "author": "Yifei Li, Hsiaoyu Chen, Egor Larionov, Nikolaos Sarafianos, Wojciech Matusik, Tuur Stuyck",
        "abstract": "The realism of digital avatars is crucial in enabling telepresence applications with self-expression and customization. While physical simulations can produce realistic motions for clothed humans, they require high-quality garment assets with associated physical parameters for cloth simulations. However, manually creating these assets and calibrating their parameters is labor-intensive and requires specialized expertise. Current methods focus on reconstructing geometry, but don't generate complete assets for physics-based applications. To address this gap, we propose \\papername,~a novel approach that performs body and garment co-optimization using differentiable simulation. By integrating physical simulation into the optimization loop and accounting for the complex nonlinear behavior of cloth and its intricate interaction with the body, our framework recovers body and garment geometry and extracts important material parameters in a physically plausible way. Our experiments demonstrate that our approach generates realistic clothing and body shape suitable for downstream applications. We provide additional insights and results on our webpage: https://people.csail.mit.edu/liyifei/publication/diffavatar/",
        "page": "http://arxiv.org/abs/2311.12194",
        "pdf": "http://arxiv.org/pdf/2311.12194.pdf"
    },
    {
        "title": "OED: Towards One-stage End-to-End Dynamic Scene Graph Generation",
        "author": "Guan Wang, Zhimin Li, Qingchao Chen, Yang Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "On the Estimation of Image-matching Uncertainty in Visual Place Recognition",
        "author": "Mubariz Zaffar, Liangliang Nan, Julian F. P. Kooij",
        "abstract": "In Visual Place Recognition (VPR) the pose of a query image is estimated by comparing the image to a map of reference images with known reference poses. As is typical for image retrieval problems, a feature extractor maps the query and reference images to a feature space, where a nearest neighbor search is then performed. However, till recently little attention has been given to quantifying the confidence that a retrieved reference image is a correct match. Highly certain but incorrect retrieval can lead to catastrophic failure of VPR-based localization pipelines. This work compares for the first time the main approaches for estimating the image-matching uncertainty, including the traditional retrieval-based uncertainty estimation, more recent data-driven aleatoric uncertainty estimation, and the compute-intensive geometric verification. We further formulate a simple baseline method, ``SUE'', which unlike the other methods considers the freely-available poses of the reference images in the map. Our experiments reveal that a simple L2-distance between the query and reference descriptors is already a better estimate of image-matching uncertainty than current data-driven approaches. SUE outperforms the other efficient uncertainty estimation methods, and its uncertainty estimates complement the computationally expensive geometric verification approach. Future works for uncertainty estimation in VPR should consider the baselines discussed in this work.",
        "page": "http://arxiv.org/abs/2404.00546",
        "pdf": "http://arxiv.org/pdf/2404.00546.pdf"
    },
    {
        "title": "3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surfaces",
        "author": "Linyi Jin, Nilesh Kulkarni, David Fouhey",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships",
        "author": "Rangel Daroya, Aaron Sun, Subhransu Maji",
        "abstract": "Modeling and visualizing relationships between tasks or datasets is an important step towards solving various meta-tasks such as dataset discovery, multi-tasking, and transfer learning. However, many relationships, such as containment and transferability, are naturally asymmetric and current approaches for representation and visualization (e.g., t-SNE) do not readily support this. We propose Task2Box, an approach to represent tasks using box embeddings -- axis-aligned hyperrectangles in low dimensional spaces -- that can capture asymmetric relationships between them through volumetric overlaps. We show that Task2Box accurately predicts unseen hierarchical relationships between nodes in ImageNet and iNaturalist datasets, as well as transferability between tasks in the Taskonomy benchmark. We also show that box embeddings estimated from task representations (e.g., CLIP, Task2Vec, or attribute based) can be used to predict relationships between unseen tasks more accurately than classifiers trained on the same representations, as well as handcrafted asymmetric distances (e.g., KL divergence). This suggests that low-dimensional box embeddings can effectively capture these task relationships and have the added advantage of being interpretable. We use the approach to visualize relationships among publicly available image classification datasets on popular dataset hosting platform called Hugging Face.",
        "page": "http://arxiv.org/abs/2403.17173",
        "pdf": "http://arxiv.org/pdf/2403.17173.pdf"
    },
    {
        "title": "RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception",
        "author": "Ruiyang Hao, Siqi Fan, Yingru Dai, Zhenlin Zhang, Chenxi Li, YuntianWang, Haibao Yu, Wenxian Yang, Jirui Yuan, Zaiqing Nie",
        "abstract": "The value of roadside perception, which could extend the boundaries of autonomous driving and traffic management, has gradually become more prominent and acknowledged in recent years. However, existing roadside perception approaches only focus on the single-infrastructure sensor system, which cannot realize a comprehensive understanding of a traffic area because of the limited sensing range and blind spots. Orienting high-quality roadside perception, we need Roadside Cooperative Perception (RCooper) to achieve practical area-coverage roadside perception for restricted traffic areas. Rcooper has its own domain-specific challenges, but further exploration is hindered due to the lack of datasets. We hence release the first real-world, large-scale RCooper dataset to bloom the research on practical roadside cooperative perception, including detection and tracking. The manually annotated dataset comprises 50k images and 30k point clouds, including two representative traffic scenes (i.e., intersection and corridor). The constructed benchmarks prove the effectiveness of roadside cooperation perception and demonstrate the direction of further research. Codes and dataset can be accessed at: https://github.com/AIR-THU/DAIR-RCooper.",
        "page": "http://arxiv.org/abs/2403.10145",
        "pdf": "http://arxiv.org/pdf/2403.10145.pdf"
    },
    {
        "title": "ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding",
        "author": "Le Xue, Ning Yu, Shu Zhang, Artemis Panagopoulou, Junnan Li, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese",
        "abstract": "Recent advancements in multimodal pre-training have shown promising efficacy in 3D representation learning by aligning multimodal features across 3D shapes, their 2D counterparts, and language descriptions. However, the methods used by existing frameworks to curate such multimodal data, in particular language descriptions for 3D shapes, are not scalable, and the collected language descriptions are not diverse. To address this, we introduce ULIP-2, a simple yet effective tri-modal pre-training framework that leverages large multimodal models to automatically generate holistic language descriptions for 3D shapes. It only needs 3D data as input, eliminating the need for any manual 3D annotations, and is therefore scalable to large datasets. ULIP-2 is also equipped with scaled-up backbones for better multimodal representation learning. We conduct experiments on two large-scale 3D datasets, Objaverse and ShapeNet, and augment them with tri-modal datasets of 3D point clouds, images, and language for training ULIP-2. Experiments show that ULIP-2 demonstrates substantial benefits in three downstream tasks: zero-shot 3D classification, standard 3D classification with fine-tuning, and 3D captioning (3D-to-language generation). It achieves a new SOTA of 50.6% (top-1) on Objaverse-LVIS and 84.7% (top-1) on ModelNet40 in zero-shot classification. In the ScanObjectNN benchmark for standard fine-tuning, ULIP-2 reaches an overall accuracy of 91.5% with a compact model of only 1.4 million parameters. ULIP-2 sheds light on a new paradigm for scalable multimodal 3D representation learning without human annotations and shows significant improvements over existing baselines. The code and datasets are released at https://github.com/salesforce/ULIP.",
        "page": "http://arxiv.org/abs/2305.08275",
        "pdf": "http://arxiv.org/pdf/2305.08275.pdf"
    },
    {
        "title": "CFAT: Unleashing Triangular Windows for Image Super-resolution",
        "author": "Abhisek Ray, Gaurav Kumar, Maheshkumar Kolekar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Relaxed Contrastive Learning for Federated Learning",
        "author": "Seonguk Seo, Jinkyu Kim, Geeho Kim, Bohyung Han",
        "abstract": "We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\\\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge margins on the standard benchmarks through extensive experimental results.",
        "page": "http://arxiv.org/abs/2401.04928",
        "pdf": "http://arxiv.org/pdf/2401.04928.pdf"
    },
    {
        "title": "Can\u2019t make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models",
        "author": "Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, Kwonjoon Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video",
        "author": "Hongchi Xia, Chih-Hao Lin, Wei-Chiu Ma, Shenlong Wang",
        "abstract": "Creating high-quality and interactive virtual environments, such as games and simulators, often involves complex and costly manual modeling processes. In this paper, we present Video2Game, a novel approach that automatically converts videos of real-world scenes into realistic and interactive game environments. At the heart of our system are three core components:(i) a neural radiance fields (NeRF) module that effectively captures the geometry and visual appearance of the scene; (ii) a mesh module that distills the knowledge from NeRF for faster rendering; and (iii) a physics module that models the interactions and physical dynamics among the objects. By following the carefully designed pipeline, one can construct an interactable and actionable digital replica of the real world. We benchmark our system on both indoor and large-scale outdoor scenes. We show that we can not only produce highly-realistic renderings in real-time, but also build interactive games on top.",
        "page": "http://arxiv.org/abs/2404.09833",
        "pdf": "http://arxiv.org/pdf/2404.09833.pdf"
    },
    {
        "title": "Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding",
        "author": "Le Zhang, Rabiul Awal, Aishwarya Agrawal",
        "abstract": "Vision-Language Models (VLMs), such as CLIP, exhibit strong image-text comprehension abilities, facilitating advances in several downstream tasks such as zero-shot image classification, image-text retrieval, and text-to-image generation. However, the compositional reasoning abilities of existing VLMs remains subpar. The root of this limitation lies in the inadequate alignment between the images and captions in the pretraining datasets. Additionally, the current contrastive learning objective fails to focus on fine-grained grounding components like relations, actions, and attributes, resulting in \"bag-of-words\" representations. We introduce a simple and effective method to improve compositional reasoning in VLMs. Our method better leverages available datasets by refining and expanding the standard image-text contrastive learning framework. Our approach does not require specific annotations and does not incur extra parameters. When integrated with CLIP, our technique yields notable improvement over state-of-the-art baselines across five vision-language compositional benchmarks. We open-source our code at https://github.com/lezhang7/Enhance-FineGrained.",
        "page": "http://arxiv.org/abs/2306.08832",
        "pdf": "http://arxiv.org/pdf/2306.08832.pdf"
    },
    {
        "title": "Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal",
        "author": "Yijun Yang, Hongtao Wu, Angelica I. Aviles-Rivero, Yulun Zhang, Jing Qin, Lei Zhu",
        "abstract": "Real-world vision tasks frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops. In the last decade, convolutional neural networks and vision transformers have yielded outstanding results in single-weather video removal. However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions. Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time. In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process. Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage. During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation. Experimental results, on benchmark datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions. Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos.",
        "page": "http://arxiv.org/abs/2403.07684",
        "pdf": "http://arxiv.org/pdf/2403.07684.pdf"
    },
    {
        "title": "FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding",
        "author": "Jun Xiang, Xuan Gao, Yudong Guo, Juyong Zhang",
        "abstract": "We propose FlashAvatar, a novel and lightweight 3D animatable avatar representation that could reconstruct a digital avatar from a short monocular video sequence in minutes and render high-fidelity photo-realistic images at 300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D Gaussian field embedded in the surface of a parametric face model and learn extra spatial offset to model non-surface regions and subtle facial details. While full use of geometric priors can capture high-frequency facial details and preserve exaggerated expressions, proper initialization can help reduce the number of Gaussians, thus enabling super-fast rendering speed. Extensive experimental results demonstrate that FlashAvatar outperforms existing works regarding visual quality and personalized details and is almost an order of magnitude faster in rendering speed. Project page: https://ustc3dv.github.io/FlashAvatar/",
        "page": "http://arxiv.org/abs/2312.02214",
        "pdf": "http://arxiv.org/pdf/2312.02214.pdf"
    },
    {
        "title": "DAP: A Dynamic Adversarial Patch for Evading Person Detectors",
        "author": "Amira Guesmi, Ruitian Ding, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique",
        "abstract": "Patch-based adversarial attacks were proven to compromise the robustness and reliability of computer vision systems. However, their conspicuous and easily detectable nature challenge their practicality in real-world setting. To address this, recent work has proposed using Generative Adversarial Networks (GANs) to generate naturalistic patches that may not attract human attention. However, such approaches suffer from a limited latent space making it challenging to produce a patch that is efficient, stealthy, and robust to multiple real-world transformations. This paper introduces a novel approach that produces a Dynamic Adversarial Patch (DAP) designed to overcome these limitations. DAP maintains a naturalistic appearance while optimizing attack efficiency and robustness to real-world transformations. The approach involves redefining the optimization problem and introducing a novel objective function that incorporates a similarity metric to guide the patch's creation. Unlike GAN-based techniques, the DAP directly modifies pixel values within the patch, providing increased flexibility and adaptability to multiple transformations. Furthermore, most clothing-based physical attacks assume static objects and ignore the possible transformations caused by non-rigid deformation due to changes in a person's pose. To address this limitation, a 'Creases Transformation' (CT) block is introduced, enhancing the patch's resilience to a variety of real-world distortions. Experimental results demonstrate that the proposed approach outperforms state-of-the-art attacks, achieving a success rate of up to 82.28% in the digital world when targeting the YOLOv7 detector and 65% in the physical world when targeting YOLOv3tiny detector deployed in edge-based smart cameras.",
        "page": "http://arxiv.org/abs/2305.11618",
        "pdf": "http://arxiv.org/pdf/2305.11618.pdf"
    },
    {
        "title": "Dynamic Support Information Mining for Category-Agnostic Pose Estimation",
        "author": "Pengfei Ren, Yuanyuan Gao, Haifeng Sun, Qi Qi, Jingyu Wang, Jianxin Liao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Orthogonal Adaptation for Modular Customization of Diffusion Models",
        "author": "Ryan Po, Guandao Yang, Kfir Aberman, Gordon Wetzstein",
        "abstract": "Customization techniques for text-to-image models have paved the way for a wide range of previously unattainable applications, enabling the generation of specific concepts across diverse contexts and styles. While existing methods facilitate high-fidelity customization for individual concepts or a limited, pre-defined set of them, they fall short of achieving scalability, where a single model can seamlessly render countless concepts. In this paper, we address a new problem called Modular Customization, with the goal of efficiently merging customized models that were fine-tuned independently for individual concepts. This allows the merged model to jointly synthesize concepts in one image without compromising fidelity or incurring any additional computational costs. To address this problem, we introduce Orthogonal Adaptation, a method designed to encourage the customized models, which do not have access to each other during fine-tuning, to have orthogonal residual weights. This ensures that during inference time, the customized models can be summed with minimal interference. Our proposed method is both simple and versatile, applicable to nearly all optimizable weights in the model architecture. Through an extensive set of quantitative and qualitative evaluations, our method consistently outperforms relevant baselines in terms of efficiency and identity preservation, demonstrating a significant leap toward scalable customization of diffusion models.",
        "page": "http://arxiv.org/abs/2312.02432",
        "pdf": "http://arxiv.org/pdf/2312.02432.pdf"
    },
    {
        "title": "MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation",
        "author": "Zhicheng Zhang, Pancheng Zhao, Eunil Park, Jufeng Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models",
        "author": "Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, Kwang Moo Yi",
        "abstract": "Generating novel views of an object from a single image is a challenging task. It requires an understanding of the underlying 3D structure of the object from an image and rendering high-quality, spatially consistent new views. While recent methods for view synthesis based on diffusion have shown great progress, achieving consistency among various view estimates and at the same time abiding by the desired camera pose remains a critical problem yet to be solved. In this work, we demonstrate a strikingly simple method, where we utilize a pre-trained video diffusion model to solve this problem. Our key idea is that synthesizing a novel view could be reformulated as synthesizing a video of a camera going around the object of interest -- a scanning video -- which then allows us to leverage the powerful priors that a video diffusion model would have learned. Thus, to perform novel-view synthesis, we create a smooth camera trajectory to the target view that we wish to render, and denoise using both a view-conditioned diffusion model and a video diffusion model. By doing so, we obtain a highly consistent novel view synthesis, outperforming the state of the art.",
        "page": "http://arxiv.org/abs/2312.01305",
        "pdf": "http://arxiv.org/pdf/2312.01305.pdf"
    },
    {
        "title": "Map-Relative Pose Regression for Visual Re-Localization",
        "author": "Shuai Chen, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Generalizable Tumor Synthesis",
        "author": "Qi Chen, Xiaoxi Chen, Haorui Song, Alan L. Yuille, Zhiwei Xiong, Chen Wei, Zongwei Zhou",
        "abstract": "Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that generative AI models, e.g., Diffusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities.",
        "page": "http://arxiv.org/abs/2402.19470",
        "pdf": "http://arxiv.org/pdf/2402.19470.pdf"
    },
    {
        "title": "VicTR: Video-conditioned Text Representations for Activity Recognition",
        "author": "Kumara Kahatapitiya, Anurag Arnab, Arsha Nagrani, Michael Ryoo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Bi-SSC: Geometric-Semantic Bidirectional Fusion for Camera-based 3D Semantic Scene Completion",
        "author": "Yujie Xue, Ruihui Li, F anWu, Zhuo Tang, Kenli Li, Duan Mingxing",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FC-GNN: Recovering Reliable and Accurate Correspondences from Interferences",
        "author": "Haobo Xu, Jun Zhou, Hua Yang, Renjie Pan, Cunyan Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Gradient-based Parameter Selection for Efficient Fine-Tuning",
        "author": "Zhi Zhang, Qizhe Zhang, Zijun Gao, Renrui Zhang, Ekaterina Shutova, Shiji Zhou, Shanghang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
        "author": "Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, Ying Shan",
        "abstract": "Large-kernel convolutional neural networks (ConvNets) have recently received extensive research attention, but two unresolved and critical issues demand further investigation. 1) The architectures of existing large-kernel ConvNets largely follow the design principles of conventional ConvNets or transformers, while the architectural design for large-kernel ConvNets remains under-addressed. 2) As transformers have dominated multiple modalities, it remains to be investigated whether ConvNets also have a strong universal perception ability in domains beyond vision. In this paper, we contribute from two aspects. 1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep. Following such guidelines, our proposed large-kernel ConvNet shows leading performance in image recognition (ImageNet accuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%), demonstrating better performance and higher speed than the recent powerful competitors. 2) We discover large kernels are the key to unlocking the exceptional performance of ConvNets in domains where they were originally not proficient. With certain modality-related preprocessing approaches, the proposed model achieves state-of-the-art performance on time-series forecasting and audio recognition tasks even without modality-specific customization to the architecture. All the code and models are publicly available on GitHub and Huggingface.",
        "page": "http://arxiv.org/abs/2311.15599",
        "pdf": "http://arxiv.org/pdf/2311.15599.pdf"
    },
    {
        "title": "A General and Efficient Training for Transformer via Token Expansion",
        "author": "Wenxuan Huang, Yunhang Shen, Jiao Xie, Baochang Zhang, Gaoqi He, Ke Li, Xing Sun, Shaohui Lin",
        "abstract": "The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an \"initialization-expansion-merging\" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e.g., DeiT and LV-ViT), but also effective for efficient training frameworks (e.g., EfficientTrain), without twisting the original training hyper-parameters, architecture, and introducing additional training strategies. Extensive experiments demonstrate that ToE achieves about 1.3x faster for the training of ViTs in a lossless manner, or even with performance gains over the full-token training baselines. Code is available at https://github.com/Osilly/TokenExpansion .",
        "page": "http://arxiv.org/abs/2404.00672",
        "pdf": "http://arxiv.org/pdf/2404.00672.pdf"
    },
    {
        "title": "Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models",
        "author": "Xin Li, Yunfei Wu, Xinghua Jiang, ZhiHao Guo, Mingming Gong, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun",
        "abstract": "Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks.",
        "page": "http://arxiv.org/abs/2402.19014",
        "pdf": "http://arxiv.org/pdf/2402.19014.pdf"
    },
    {
        "title": "Learning Vision from Models Rivals Learning Vision from Data",
        "author": "Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, Phillip Isola",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MindBridge: A Cross-Subject Brain Decoding Framework",
        "author": "Shizun Wang, Songhua Liu, Zhenxiong Tan, Xinchao Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Collaborating Foundation models for Domain Generalized Semantic Segmentation",
        "author": "Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, St\u00e9phane Lathuili\u00e8re",
        "abstract": "Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during inference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP backbone for its robust feature representation, (ii) generative models to diversify the content, thereby covering various modes of the possible target distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS excels in adapting from synthetic to real DGSS benchmarks and under varying weather conditions, notably outperforming prior methods by 5.6% and 6.7% on averaged miou, respectively. The code is available at : https://github.com/yasserben/CLOUDS",
        "page": "http://arxiv.org/abs/2312.09788",
        "pdf": "http://arxiv.org/pdf/2312.09788.pdf"
    },
    {
        "title": "3D Face Reconstruction with the Geometric Guidance of Facial Part Segmentation",
        "author": "Zidu Wang, Xiangyu Zhu, Tianshuo Zhang, baiqin wang, Zhen Lei",
        "abstract": "3D Morphable Models (3DMMs) provide promising 3D face reconstructions in various applications. However, existing methods struggle to reconstruct faces with extreme expressions due to deficiencies in supervisory signals, such as sparse or inaccurate landmarks. Segmentation information contains effective geometric contexts for face reconstruction. Certain attempts intuitively depend on differentiable renderers to compare the rendered silhouettes of reconstruction with segmentation, which is prone to issues like local optima and gradient instability. In this paper, we fully utilize the facial part segmentation geometry by introducing Part Re-projection Distance Loss (PRDL). Specifically, PRDL transforms facial part segmentation into 2D points and re-projects the reconstruction onto the image plane. Subsequently, by introducing grid anchors and computing different statistical distances from these anchors to the point sets, PRDL establishes geometry descriptors to optimize the distribution of the point sets for face reconstruction. PRDL exhibits a clear gradient compared to the renderer-based methods and presents state-of-the-art reconstruction performance in extensive quantitative and qualitative experiments. Our project is available at https://github.com/wang-zidu/3DDFA-V3 .",
        "page": "http://arxiv.org/abs/2312.00311",
        "pdf": "http://arxiv.org/pdf/2312.00311.pdf"
    },
    {
        "title": "HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances",
        "author": "Supreeth Narasimhaswamy, Uttaran Bhattacharya, Xiang Chen, Ishita Dasgupta, Saayan Mitra, Minh Hoai",
        "abstract": "Text-to-image generative models can generate high-quality humans, but realism is lost when generating hands. Common artifacts include irregular hand poses, shapes, incorrect numbers of fingers, and physically implausible finger orientations. To generate images with realistic hands, we propose a novel diffusion-based architecture called HanDiffuser that achieves realism by injecting hand embeddings in the generative process. HanDiffuser consists of two components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and MANO-Hand parameters from input text prompts, and a Text-Guided Hand-Params-to-Image diffusion model to synthesize images by conditioning on the prompts and hand parameters generated by the previous component. We incorporate multiple aspects of hand representation, including 3D shapes and joint-level finger positions, orientations and articulations, for robust learning and reliable performance during inference. We conduct extensive quantitative and qualitative experiments and perform user studies to demonstrate the efficacy of our method in generating images with high-quality hands.",
        "page": "http://arxiv.org/abs/2403.01693",
        "pdf": "http://arxiv.org/pdf/2403.01693.pdf"
    },
    {
        "title": "BoQ: A Place is Worth a Bag of Learnable Queries",
        "author": "Amar Ali-bey, Brahim Chaib-draa, Philippe Gigu\u00e8re",
        "abstract": "In visual place recognition, accurately identifying and matching images of locations under varying environmental conditions and viewpoints remains a significant challenge. In this paper, we introduce a new technique, called Bag-of-Queries (BoQ), which learns a set of global queries designed to capture universal place-specific attributes. Unlike existing methods that employ self-attention and generate the queries directly from the input features, BoQ employs distinct learnable global queries, which probe the input features via cross-attention, ensuring consistent information aggregation. In addition, our technique provides an interpretable attention mechanism and integrates with both CNN and Vision Transformer backbones. The performance of BoQ is demonstrated through extensive experiments on 14 large-scale benchmarks. It consistently outperforms current state-of-the-art techniques including NetVLAD, MixVPR and EigenPlaces. Moreover, as a global retrieval technique (one-stage), BoQ surpasses two-stage retrieval methods, such as Patch-NetVLAD, TransVPR and R2Former, all while being orders of magnitude faster and more efficient. The code and model weights are publicly available at https://github.com/amaralibey/Bag-of-Queries.",
        "page": "http://arxiv.org/abs/2405.07364",
        "pdf": "http://arxiv.org/pdf/2405.07364.pdf"
    },
    {
        "title": "Generalizable Face Landmarking Guided by Conditional Face Warping",
        "author": "Jiayi Liang, Haotian Liu, Hongteng Xu, Dixin Luo",
        "abstract": "As a significant step for human face modeling, editing, and generation, face landmarking aims at extracting facial keypoints from images. A generalizable face landmarker is required in practice because real-world facial images, e.g., the avatars in animations and games, are often stylized in various ways. However, achieving generalizable face landmarking is challenging due to the diversity of facial styles and the scarcity of labeled stylized faces. In this study, we propose a simple but effective paradigm to learn a generalizable face landmarker based on labeled real human faces and unlabeled stylized faces. Our method learns the face landmarker as the key module of a conditional face warper. Given a pair of real and stylized facial images, the conditional face warper predicts a warping field from the real face to the stylized one, in which the face landmarker predicts the ending points of the warping field and provides us with high-quality pseudo landmarks for the corresponding stylized facial images. Applying an alternating optimization strategy, we learn the face landmarker to minimize $i)$ the discrepancy between the stylized faces and the warped real ones and $ii)$ the prediction errors of both real and pseudo landmarks. Experiments on various datasets show that our method outperforms existing state-of-the-art domain adaptation methods in face landmarking tasks, leading to a face landmarker with better generalizability. Code is available at https://plustwo0.github.io/project-face-landmarker.",
        "page": "http://arxiv.org/abs/2404.12322",
        "pdf": "http://arxiv.org/pdf/2404.12322.pdf"
    },
    {
        "title": "Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses",
        "author": "Inhee Lee, Byungjun Kim, Hanbyul Joo",
        "abstract": "In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together. In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world. To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations. After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches.",
        "page": "http://arxiv.org/abs/2404.14410",
        "pdf": "http://arxiv.org/pdf/2404.14410.pdf"
    },
    {
        "title": "FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition",
        "author": "Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, Chunhua Shen",
        "abstract": "Benefiting from large-scale pre-trained text-to-image (T2I) generative models, impressive progress has been achieved in customized image generation, which aims to generate user-specified concepts. Existing approaches have extensively focused on single-concept customization and still encounter challenges when it comes to complex scenarios that involve combining multiple concepts. These approaches often require retraining/fine-tuning using a few images, leading to time-consuming training processes and impeding their swift implementation. Furthermore, the reliance on multiple images to represent a singular concept increases the difficulty of customization. To this end, we propose FreeCustom, a novel tuning-free method to generate customized images of multi-concept composition based on reference concepts, using only one image per concept as input. Specifically, we introduce a new multi-reference self-attention (MRSA) mechanism and a weighted mask strategy that enables the generated image to access and focus more on the reference concepts. In addition, MRSA leverages our key finding that input concepts are better preserved when providing images with context interactions. Experiments show that our method's produced images are consistent with the given concepts and better aligned with the input text. Our method outperforms or performs on par with other training-based methods in terms of multi-concept composition and single-concept customization, but is simpler. Codes can be found at https://github.com/aim-uofa/FreeCustom.",
        "page": "http://arxiv.org/abs/2405.13870",
        "pdf": "http://arxiv.org/pdf/2405.13870.pdf"
    },
    {
        "title": "ExtDM: Distribution Extrapolation Diffusion Model for Video Prediction",
        "author": "Zhicheng Zhang, Junyao Hu, Wentao Cheng, Danda Paudel, Jufeng Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TASeg: Temporal Aggregation Network for LiDAR Semantic Segmentation",
        "author": "Xiaopei Wu, Yuenan Hou, Xiaoshui Huang, Binbin Lin, Tong He, Xinge Zhu, Yuexin Ma, Boxi Wu, Haifeng Liu, Deng Cai, Wanli Ouyang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing",
        "author": "Jan-Nico Zaech, Martin Danelljan, Tolga Birdal, Luc Van Gool",
        "abstract": "Adiabatic quantum computing (AQC) is a promising approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic tasks and real visual data.",
        "page": "http://arxiv.org/abs/2310.12153",
        "pdf": "http://arxiv.org/pdf/2310.12153.pdf"
    },
    {
        "title": "Learning Occupancy for Monocular 3D Object Detection",
        "author": "Liang Peng, Junkai Xu, Haoran Cheng, Zheng Yang, Xiaopei Wu, Wei Qian, Wenxiao Wang, Boxi Wu, Deng Cai",
        "abstract": "Monocular 3D detection is a challenging task due to the lack of accurate 3D information. Existing approaches typically rely on geometry constraints and dense depth estimates to facilitate the learning, but often fail to fully exploit the benefits of three-dimensional feature extraction in frustum and 3D space. In this paper, we propose \\textbf{OccupancyM3D}, a method of learning occupancy for monocular 3D detection. It directly learns occupancy in frustum and 3D space, leading to more discriminative and informative 3D features and representations. Specifically, by using synchronized raw sparse LiDAR point clouds, we define the space status and generate voxel-based occupancy labels. We formulate occupancy prediction as a simple classification problem and design associated occupancy losses. Resulting occupancy estimates are employed to enhance original frustum/3D features. As a result, experiments on KITTI and Waymo open datasets demonstrate that the proposed method achieves a new state of the art and surpasses other methods by a significant margin. Codes and pre-trained models will be available at \\url{https://github.com/SPengLiang/OccupancyM3D}.",
        "page": "http://arxiv.org/abs/2305.15694",
        "pdf": "http://arxiv.org/pdf/2305.15694.pdf"
    },
    {
        "title": "SeeSR: Towards Semantics-Aware Real-World Image Super-Resolution",
        "author": "Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang ZHANG, Shuai Li, Lei Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling",
        "author": "Miguel Fainstein, Viviana Siless, Emmanuel Iarussi",
        "abstract": "In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction. However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces. In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions. This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields. Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance. Moreover, the unlocked field's differentiability allows the accurate computation of essential topological properties such as normal directions and curvatures, pervasive in downstream tasks such as rendering. Through extensive experiments, we validate our approach across various data sets and against competitive baselines. The results demonstrate enhanced accuracy and up to an order of magnitude increase in speed compared to previous methods.",
        "page": "http://arxiv.org/abs/2402.08876",
        "pdf": "http://arxiv.org/pdf/2402.08876.pdf"
    },
    {
        "title": "DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral Diffusion Model",
        "author": "Zhenghao Pan, Haijin Zeng, Jiezhang Cao, Kai Zhang, Yongyong Chen",
        "abstract": "This paper endeavors to advance the precision of snapshot compressive imaging (SCI) reconstruction for multispectral image (MSI). To achieve this, we integrate the advantageous attributes of established SCI techniques and an image generative model, propose a novel structured zero-shot diffusion model, dubbed DiffSCI. DiffSCI leverages the structural insights from the deep prior and optimization-based methodologies, complemented by the generative capabilities offered by the contemporary denoising diffusion model. Specifically, firstly, we employ a pre-trained diffusion model, which has been trained on a substantial corpus of RGB images, as the generative denoiser within the Plug-and-Play framework for the first time. This integration allows for the successful completion of SCI reconstruction, especially in the case that current methods struggle to address effectively. Secondly, we systematically account for spectral band correlations and introduce a robust methodology to mitigate wavelength mismatch, thus enabling seamless adaptation of the RGB diffusion model to MSIs. Thirdly, an accelerated algorithm is implemented to expedite the resolution of the data subproblem. This augmentation not only accelerates the convergence rate but also elevates the quality of the reconstruction process. We present extensive testing to show that DiffSCI exhibits discernible performance enhancements over prevailing self-supervised and zero-shot approaches, surpassing even supervised transformer counterparts across both simulated and real datasets. Our code will be available.",
        "page": "http://arxiv.org/abs/2311.11417",
        "pdf": "http://arxiv.org/pdf/2311.11417.pdf"
    },
    {
        "title": "WildlifeMapper:  Aerial Image Analysis for Multi-Species Detection and Identification",
        "author": "Satish Kumar, Bowen Zhang, Chandrakanth Gudavalli, Connor Levenson, Lacey Hughey, Jared Stabach, Irene Amoke, Gordon Ojwang, Joseph Mukeka, Howard Frederick, Stephen Mwiu, Joseph Ochieng Ogutu, B S Manjunath",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Pre-training Vision Models with Mandelbulb Variations",
        "author": "Benjamin N. Chiche, Yuto Horikawa, Ryo Fujita",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models",
        "author": "Luo Jiayun, Siddhesh Khandelwal, Leonid Sigal, Boyang Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion",
        "author": "Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao, Hanlin Xu, Zenan Lin, Kailin Li, Cewu Lu",
        "abstract": "We present OAKINK2, a dataset of bimanual object manipulation tasks for complex daily activities. In pursuit of constructing the complex tasks into a structured representation, OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 features on an object-centric perspective for decoding the complex tasks, treating them as a sequence of object affordance fulfillment. The first level, Affordance, outlines the functionalities that objects in the scene can afford, the second level, Primitive Task, describes the minimal interaction units that humans interact with the object to achieve its affordance, and the third level, Complex Task, illustrates how Primitive Tasks are composed and interdependent. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis. Based on the 3-level abstraction of OAKINK2, we explore a task-oriented framework for Complex Task Completion (CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task objectives. Within the CTC framework, we employ Large Language Models (LLMs) to decompose the complex task objectives into sequences of Primitive Tasks and have developed a Motion Fulfillment Model that generates bimanual hand motion for each Primitive Task. OAKINK2 datasets and models are available at https://oakink.net/v2.",
        "page": "http://arxiv.org/abs/2403.19417",
        "pdf": "http://arxiv.org/pdf/2403.19417.pdf"
    },
    {
        "title": "ReGenNet: Towards Human Action-Reaction Synthesis",
        "author": "Liang Xu, Yizhou Zhou, Yichao Yan, Xin Jin, Wenhan Zhu, Fengyun Rao, Xiaokang Yang, Wenjun Zeng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffForensics: Leveraging Diffusion Prior to Image Forgery Detection and Localization",
        "author": "Zeqin Yu, Jiangqun Ni, Yuzhen Lin, Haoyi Deng, Bin Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Initialization Matters for Adversarial Transfer Learning",
        "author": "Andong Hua, Jindong Gu, Zhiyu Xue, Nicholas Carlini, Eric Wong, Yao Qin",
        "abstract": "With the prevalence of the Pretraining-Finetuning paradigm in transfer learning, the robustness of downstream tasks has become a critical concern. In this work, we delve into adversarial robustness in transfer learning and reveal the critical role of initialization, including both the pretrained model and the linear head. First, we discover the necessity of an adversarially robust pretrained model. Specifically, we reveal that with a standard pretrained model, Parameter-Efficient Finetuning (PEFT) methods either fail to be adversarially robust or continue to exhibit significantly degraded adversarial robustness on downstream tasks, even with adversarial training during finetuning. Leveraging a robust pretrained model, surprisingly, we observe that a simple linear probing can outperform full finetuning and other PEFT methods with random initialization on certain datasets. We further identify that linear probing excels in preserving robustness from the robust pretraining. Based on this, we propose Robust Linear Initialization (RoLI) for adversarial finetuning, which initializes the linear head with the weights obtained by adversarial linear probing to maximally inherit the robustness from pretraining. Across five different image classification datasets, we demonstrate the effectiveness of RoLI and achieve new state-of-the-art results. Our code is available at \\url{https://github.com/DongXzz/RoLI}.",
        "page": "http://arxiv.org/abs/2312.05716",
        "pdf": "http://arxiv.org/pdf/2312.05716.pdf"
    },
    {
        "title": "Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation",
        "author": "Hang Li, Chengzhi Shen, Philip H.S. Torr, Volker Tresp, Jindong Gu",
        "abstract": "Diffusion-based models have gained significant popularity for text-to-image generation due to their exceptional image-generation capabilities. A risk with these models is the potential generation of inappropriate content, such as biased or harmful images. However, the underlying reasons for generating such undesired content from the perspective of the diffusion model's internal representation remain unclear. Previous work interprets vectors in an interpretable latent space of diffusion models as semantic concepts. However, existing approaches cannot discover directions for arbitrary concepts, such as those related to inappropriate concepts. In this work, we propose a novel self-supervised approach to find interpretable latent directions for a given concept. With the discovered vectors, we further propose a simple approach to mitigate inappropriate generation. Extensive experiments have been conducted to verify the effectiveness of our mitigation approach, namely, for fair generation, safe generation, and responsible text-enhancing generation. Project page: \\url{https://interpretdiffusion.github.io}.",
        "page": "http://arxiv.org/abs/2311.17216",
        "pdf": "http://arxiv.org/pdf/2311.17216.pdf"
    },
    {
        "title": "Universal Segmentation at Arbitrary Granularity with Language Instruction",
        "author": "Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang, Yujiu Yang, Yansong Tang",
        "abstract": "This paper aims to achieve universal segmentation of arbitrary semantic level. Despite significant progress in recent years, specialist segmentation approaches are limited to specific tasks and data distribution. Retraining a new model for adaptation to new scenarios or settings takes expensive computation and time cost, which raises the demand for versatile and universal segmentation model that can cater to various granularity. Although some attempts have been made for unifying different segmentation tasks or generalization to various scenarios, limitations in the definition of paradigms and input-output spaces make it difficult for them to achieve accurate understanding of content at arbitrary granularity. To this end, we present UniLSeg, a universal segmentation model that can perform segmentation at any semantic level with the guidance of language instructions. For training UniLSeg, we reorganize a group of tasks from original diverse distributions into a unified data format, where images with texts describing segmentation targets as input and corresponding masks are output. Combined with a automatic annotation engine for utilizing numerous unlabeled data, UniLSeg achieves excellent performance on various tasks and settings, surpassing both specialist and unified segmentation models.",
        "page": "http://arxiv.org/abs/2312.01623",
        "pdf": "http://arxiv.org/pdf/2312.01623.pdf"
    },
    {
        "title": "Dr. Bokeh: DiffeRentiable Occlusion-aware Bokeh Rendering",
        "author": "Yichen Sheng, Zixun Yu, Lu Ling, Zhiwen Cao, Xuaner Zhang, Xin Lu, Ke Xian, Haiting Lin, Bedrich Benes",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BrainWash: A Poisoning Attack to Forget in Continual Learning",
        "author": "Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri",
        "abstract": "Continual learning has gained substantial attention within the deep learning community, offering promising solutions to the challenging problem of sequential learning. Yet, a largely unexplored facet of this paradigm is its susceptibility to adversarial attacks, especially with the aim of inducing forgetting. In this paper, we introduce \"BrainWash,\" a novel data poisoning method tailored to impose forgetting on a continual learner. By adding the BrainWash noise to a variety of baselines, we demonstrate how a trained continual learner can be induced to forget its previously learned tasks catastrophically, even when using these continual learning baselines. An important feature of our approach is that the attacker requires no access to previous tasks' data and is armed merely with the model's current parameters and the data belonging to the most recent task. Our extensive experiments highlight the efficacy of BrainWash, showcasing degradation in performance across various regularization-based continual learning methods.",
        "page": "http://arxiv.org/abs/2311.11995",
        "pdf": "http://arxiv.org/pdf/2311.11995.pdf"
    },
    {
        "title": "Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos",
        "author": "Mehmet Saygin Seyfioglu, Wisdom Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, Linda Shapiro",
        "abstract": "Diagnosis in histopathology requires a global whole slide images (WSIs) analysis, requiring pathologists to compound evidence from different WSI patches. The gigapixel scale of WSIs poses a challenge for histopathology multi-modal models. Training multi-model models for histopathology requires instruction tuning datasets, which currently contain information for individual image patches, without a spatial grounding of the concepts within each patch and without a wider view of the WSI. Therefore, they lack sufficient diagnostic capacity for histopathology. To bridge this gap, we introduce Quilt-Instruct, a large-scale dataset of 107,131 histopathology-specific instruction question/answer pairs, grounded within diagnostically relevant image patches that make up the WSI. Our dataset is collected by leveraging educational histopathology videos from YouTube, which provides spatial localization of narrations by automatically extracting the narrators' cursor positions. Quilt-Instruct supports contextual reasoning by extracting diagnosis and supporting facts from the entire WSI. Using Quilt-Instruct, we train Quilt-LLaVA, which can reason beyond the given single image patch, enabling diagnostic reasoning across patches. To evaluate Quilt-LLaVA, we propose a comprehensive evaluation dataset created from 985 images and 1283 human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using public histopathology datasets, where Quilt-LLaVA significantly outperforms SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set VQA. Our code, data, and model are publicly accessible at quilt-llava.github.io.",
        "page": "http://arxiv.org/abs/2312.04746",
        "pdf": "http://arxiv.org/pdf/2312.04746.pdf"
    },
    {
        "title": "Selective nonlinearities removal from digital signals",
        "author": "Krzysztof Maliszewski, Magdalena Urbanska, Varvara Vetrova, Sylwia Kolenderska",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D",
        "author": "Mukund Varma T, Peihao Wang, Zhiwen Fan, Zhangyang Wang, Hao Su, Ravi Ramamoorthi",
        "abstract": "In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, style transfer or scene editing, enabled by large-scale 2D image datasets. At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images. However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging. Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization. In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions. We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as style transfer, super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method. In many cases, we even outperform state-of-the-art methods specialized for the task in question. Moreover, Lift3D is a zero-shot method, in the sense that it requires no task-specific training, nor scene-specific optimization.",
        "page": "http://arxiv.org/abs/2403.18922",
        "pdf": "http://arxiv.org/pdf/2403.18922.pdf"
    },
    {
        "title": "Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving",
        "author": "JunDa Cheng, Wei Yin, Kaixuan Wang, Xiaozhi Chen, Shijie Wang, Xin Yang",
        "abstract": "Multi-view depth estimation has achieved impressive performance over various benchmarks. However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings. To address this challenge, we propose a single-view and multi-view fused depth estimation system, which adaptively integrates high-confident multi-view and single-view results for both robust and accurate depth estimations. The adaptive fusion module performs fusion by dynamically selecting high-confidence regions between two branches based on a wrapping confidence map. Thus, the system tends to choose the more reliable branch when facing textureless scenes, inaccurate calibration, dynamic objects, and other degradation or challenging conditions. Our method outperforms state-of-the-art multi-view and fusion methods under robustness testing. Furthermore, we achieve state-of-the-art performance on challenging benchmarks (KITTI and DDAD) when given accurate pose estimations. Project website: https://github.com/Junda24/AFNet/.",
        "page": "http://arxiv.org/abs/2403.07535",
        "pdf": "http://arxiv.org/pdf/2403.07535.pdf"
    },
    {
        "title": "Self-correcting LLM-controlled Diffusion",
        "author": "Tsung-Han Wu, Long Lian, Joseph Gonzalez, Boyi Li, Trevor Darrell",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion",
        "author": "Fan Zhang, Shaodi You, Yu Li, Ying Fu",
        "abstract": "Monocular depth estimation has experienced significant progress on terrestrial images in recent years, largely due to deep learning advancements. However, it remains inadequate for underwater scenes, primarily because of data scarcity. Given the inherent challenges of light attenuation and backscattering in water, acquiring clear underwater images or precise depth information is notably difficult and costly. Consequently, learning-based approaches often rely on synthetic data or turn to unsupervised or self-supervised methods to mitigate this lack of data. Nonetheless, the performance of these methods is often constrained by the domain gap and looser constraints. In this paper, we propose a novel pipeline for generating photorealistic underwater images using accurate terrestrial depth data. This approach facilitates the training of supervised models for underwater depth estimation, effectively reducing the performance disparity between terrestrial and underwater environments. Contrary to prior synthetic datasets that merely apply style transfer to terrestrial images without altering the scene content, our approach uniquely creates vibrant, non-existent underwater scenes by leveraging terrestrial depth data through the innovative Stable Diffusion model. Specifically, we introduce a unique Depth2Underwater ControlNet, trained on specially prepared \\{Underwater, Depth, Text\\} data triplets, for this generation task. Our newly developed dataset enables terrestrial depth estimation models to achieve considerable improvements, both quantitatively and qualitatively, on unseen underwater images, surpassing their terrestrial pre-trained counterparts. Moreover, the enhanced depth accuracy for underwater scenes also aids underwater image restoration techniques that rely on depth maps, further demonstrating our dataset's utility. The dataset will be available at https://github.com/zkawfanx/Atlantis.",
        "page": "http://arxiv.org/abs/2312.12471",
        "pdf": "http://arxiv.org/pdf/2312.12471.pdf"
    },
    {
        "title": "Asymmetric Masked Distillation for Pre-Training Small Foundation Models",
        "author": "Zhiyu Zhao, Bingkun Huang, Sen Xing, Gangshan Wu, Yu Qiao, Limin Wang",
        "abstract": "Self-supervised foundation models have shown great potential in computer vision thanks to the pre-training paradigm of masked autoencoding. Scale is a primary factor influencing the performance of these foundation models. However, these large foundation models often result in high computational cost. This paper focuses on pre-training relatively small vision transformer models that could be efficiently adapted to downstream tasks. Specifically, taking inspiration from knowledge distillation in model compression, we propose a new asymmetric masked distillation (AMD) framework for pre-training relatively small models with autoencoding. The core of AMD is to devise an asymmetric masking strategy, where the teacher model is enabled to see more context information with a lower masking ratio, while the student model is still equipped with a high masking ratio. We design customized multi-layer feature alignment between the teacher encoder and student encoder to regularize the pre-training of student MAE. To demonstrate the effectiveness and versatility of AMD, we apply it to both ImageMAE and VideoMAE for pre-training relatively small ViT models. AMD achieved 84.6% classification accuracy on IN1K using the ViT-B model. And AMD achieves 73.3% classification accuracy using the ViT-B model on the Something-in-Something V2 dataset, a 3.7% improvement over the original ViT-B model from VideoMAE. We also transfer AMD pre-trained models to downstream tasks and obtain consistent performance improvement over the original masked autoencoding. The code and models are available at https://github.com/MCG-NJU/AMD.",
        "page": "http://arxiv.org/abs/2311.03149",
        "pdf": "http://arxiv.org/pdf/2311.03149.pdf"
    },
    {
        "title": "Faces that Speak: Jointly Synthesising Talking Face and Speech from Text",
        "author": "Youngjoon Jang, Jihoon Kim, Junseok Ahn, Doyeop Kwak, Hongsun Yang, Yooncheol Ju, ILHWAN KIM, Byeong-Yeol Kim, Joon Chung",
        "abstract": "The goal of this work is to simultaneously generate natural talking faces and speech outputs from text. We achieve this by integrating Talking Face Generation (TFG) and Text-to-Speech (TTS) systems into a unified framework. We address the main challenges of each task: (1) generating a range of head poses representative of real-world scenarios, and (2) ensuring voice consistency despite variations in facial motion for the same identity. To tackle these issues, we introduce a motion sampler based on conditional flow matching, which is capable of high-quality motion code generation in an efficient way. Moreover, we introduce a novel conditioning method for the TTS system, which utilises motion-removed features from the TFG model to yield uniform speech outputs. Our extensive experiments demonstrate that our method effectively creates natural-looking talking faces and speech that accurately match the input text. To our knowledge, this is the first effort to build a multimodal synthesis system that can generalise to unseen identities.",
        "page": "http://arxiv.org/abs/2405.10272",
        "pdf": "http://arxiv.org/pdf/2405.10272.pdf"
    },
    {
        "title": "Generative Image Dynamics",
        "author": "Zhengqi Li, Richard Tucker, Noah Snavely, Aleksander Holynski",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Distributionally Generative Augmentation for Fair Facial Attribute Classification",
        "author": "Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu, Jun Xiao, Hanwang Zhang",
        "abstract": "Facial Attribute Classification (FAC) holds substantial promise in widespread applications. However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations. This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the labels of spurious attributes, which may be unavailable in practice. This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation. Initially, we identify the potential spurious attributes based on generative models. Notably, it enhances interpretability by explicitly showing the spurious attributes in image space. Following this, for each image, we first edit the spurious attributes with a random degree sampled from a uniform distribution, while keeping target attribute unchanged. Then we train a fair FAC model by fostering model invariance to these augmentation. Extensive experiments on three common datasets demonstrate the effectiveness of our method in promoting fairness in FAC without compromising accuracy. Codes are in https://github.com/heqianpei/DiGA.",
        "page": "http://arxiv.org/abs/2403.06606",
        "pdf": "http://arxiv.org/pdf/2403.06606.pdf"
    },
    {
        "title": "CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance Fields from Sparse Inputs",
        "author": "Yingji Zhong, Lanqing Hong, Zhenguo Li, Dan Xu",
        "abstract": "Neural Radiance Fields (NeRF) have shown impressive capabilities for photorealistic novel view synthesis when trained on dense inputs. However, when trained on sparse inputs, NeRF typically encounters issues of incorrect density or color predictions, mainly due to insufficient coverage of the scene causing partial and sparse supervision, thus leading to significant performance degradation. While existing works mainly consider ray-level consistency to construct 2D learning regularization based on rendered color, depth, or semantics on image planes, in this paper we propose a novel approach that models 3D spatial field consistency to improve NeRF's performance with sparse inputs. Specifically, we first adopt a voxel-based ray sampling strategy to ensure that the sampled rays intersect with a certain voxel in 3D space. We then randomly sample additional points within the voxel and apply a Transformer to infer the properties of other points on each ray, which are then incorporated into the volume rendering. By backpropagating through the rendering loss, we enhance the consistency among neighboring points. Additionally, we propose to use a contrastive loss on the encoder output of the Transformer to further improve consistency within each voxel. Experiments demonstrate that our method yields significant improvement over different radiance fields in the sparse inputs setting, and achieves comparable performance with current works.",
        "page": "http://arxiv.org/abs/2403.16885",
        "pdf": "http://arxiv.org/pdf/2403.16885.pdf"
    },
    {
        "title": "Bootstrapping SparseFormers from Vision Foundation Models",
        "author": "Ziteng Gao, Zhan Tong, Kevin Qinghong Lin, Joya Chen, Mike Zheng Shou",
        "abstract": "The recently proposed SparseFormer architecture provides an alternative approach to visual understanding by utilizing a significantly lower number of visual tokens via adjusting RoIs, greatly reducing computational costs while still achieving promising performance. However, training SparseFormers from scratch is still expensive, and scaling up the number of parameters can be challenging. In this paper, we propose to bootstrap SparseFormers from ViT-based vision foundation models in a simple and efficient way. Since the majority of SparseFormer blocks are the standard transformer ones, we can inherit weights from large-scale pre-trained vision transformers and freeze them as much as possible. Therefore, we only need to train the SparseFormer-specific lightweight focusing transformer to adjust token RoIs and fine-tune a few early pre-trained blocks to align the final token representation. In such a way, we can bootstrap SparseFormer architectures from various large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or CLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and without labels or captions within just a few hours. As a result, the bootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9% accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from CLIPs also demonstrates notable zero-shot performance with highly reduced computational cost without seeing any caption during the bootstrapping procedure. In addition, CLIP-bootstrapped SparseFormers, which align the output space with language without seeing a word, can serve as efficient vision encoders in multimodal large language models. Code and models are available at https://github.com/showlab/sparseformer",
        "page": "http://arxiv.org/abs/2312.01987",
        "pdf": "http://arxiv.org/pdf/2312.01987.pdf"
    },
    {
        "title": "THRONE: A Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models",
        "author": "Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, CJ Taylor, Stefano Soatto, Stefano Soatto",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Clockwork Diffusion: Efficient Generation With Model-Step Distillation",
        "author": "Amirhossein Habibian, Amir Ghodrati, Noor Fathima, Guillaume Sautiere, Risheek Garrepalli, Fatih Porikli, Jens Petersen",
        "abstract": "This work aims to improve the efficiency of text-to-image diffusion models. While diffusion models use computationally expensive UNet-based denoising operations in every generation step, we identify that not all operations are equally relevant for the final output quality. In particular, we observe that UNet layers operating on high-res feature maps are relatively sensitive to small perturbations. In contrast, low-res feature maps influence the semantic layout of the final image and can often be perturbed with no noticeable change in the output. Based on this observation, we propose Clockwork Diffusion, a method that periodically reuses computation from preceding denoising steps to approximate low-res feature maps at one or more subsequent steps. For multiple baselines, and for both text-to-image generation and image editing, we demonstrate that Clockwork leads to comparable or improved perceptual scores with drastically reduced computational complexity. As an example, for Stable Diffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID and CLIP change.",
        "page": "http://arxiv.org/abs/2312.08128",
        "pdf": "http://arxiv.org/pdf/2312.08128.pdf"
    },
    {
        "title": "Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models",
        "author": "Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis",
        "abstract": "Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, outperform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.",
        "page": "http://arxiv.org/abs/2312.13763",
        "pdf": "http://arxiv.org/pdf/2312.13763.pdf"
    },
    {
        "title": "Inlier Confidence Calibration for Point Cloud Registration",
        "author": "Yongzhe Yuan, Yue Wu, Xiaolong Fan, Maoguo Gong, Qiguang Miao, Wenping Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Memory-Scalable and Simplified Functional Map Learning",
        "author": "Robin Magnet, Maks Ovsjanikov",
        "abstract": "Deep functional maps have emerged in recent years as a prominent learning-based framework for non-rigid shape matching problems. While early methods in this domain only focused on learning in the functional domain, the latest techniques have demonstrated that by promoting consistency between functional and pointwise maps leads to significant improvements in accuracy. Unfortunately, existing approaches rely heavily on the computation of large dense matrices arising from soft pointwise maps, which compromises their efficiency and scalability. To address this limitation, we introduce a novel memory-scalable and efficient functional map learning pipeline. By leveraging the specific structure of functional maps, we offer the possibility to achieve identical results without ever storing the pointwise map in memory. Furthermore, based on the same approach, we present a differentiable map refinement layer adapted from an existing axiomatic refinement algorithm. Unlike many functional map learning methods, which use this algorithm at a post-processing step, ours can be easily used at train time, enabling to enforce consistency between the refined and initial versions of the map. Our resulting approach is both simpler, more efficient and more numerically stable, by avoiding differentiation through a linear system, while achieving close to state-of-the-art results in challenging scenarios.",
        "page": "http://arxiv.org/abs/2404.00330",
        "pdf": "http://arxiv.org/pdf/2404.00330.pdf"
    },
    {
        "title": "HardMo: A Large-Scale Hardcase Dataset for Motion Capture",
        "author": "Jiaqi Liao, Chuanchen Luo, Yinuo Du, Yuxi Wang, Xu-Cheng Yin, Man Zhang, Zhaoxiang Zhang, Junran Peng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions",
        "author": "Hao Xu, Li Haipeng, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu",
        "abstract": "Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on https://github.com/hxwork/HandBooster_Pytorch.",
        "page": "http://arxiv.org/abs/2403.18575",
        "pdf": "http://arxiv.org/pdf/2403.18575.pdf"
    },
    {
        "title": "An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains",
        "author": "George Eskandar",
        "abstract": "3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation. We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location. Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data. We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs.",
        "page": "http://arxiv.org/abs/2402.17562",
        "pdf": "http://arxiv.org/pdf/2402.17562.pdf"
    },
    {
        "title": "Constrained Layout Generation with Factor Graphs",
        "author": "Mohammed Haroon Dupty, Yanfei Dong, Sicong Leng, Guoji Fu, Yong Liang Goh, Wei Lu, Wee Sun Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation",
        "author": "guo, Tianwei Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts",
        "author": "Jialin Wu, Xia Hu, Yaqing Wang, Bo Pang, Radu Soricut",
        "abstract": "Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach helps improve the generalist performance across a broad range of generative vision-and-language tasks, achieving new SoTA generalist performance that often matches or outperforms single specialized LMM baselines, as well as new SoTA specialist performance.",
        "page": "http://arxiv.org/abs/2312.00968",
        "pdf": "http://arxiv.org/pdf/2312.00968.pdf"
    },
    {
        "title": "Distilling CLIP with Dual Guidance for  Learning Discriminative Human Body Shape Representation",
        "author": "Feng Liu, Feng Liu, Minchul Kim, Zhiyuan Ren, Xiaoming Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fully Exploiting Every Real Sample: Super-Pixel Sample Gradient Model Stealing",
        "author": "Yunlong Zhao, Xiaoheng Deng, Yijing Liu, Xinjun Pei, Jiazhi Xia, Wei Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LED: A Large-scale Real-world Paired Dataset for Event Camera Denoising",
        "author": "Yuxing Duan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DePT: Decoupled Prompt Tuning",
        "author": "Ji Zhang, Shihan Wu, Lianli Gao, Heng Tao Shen, Jingkuan Song",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Subspace-Constrained Tyler's Estimator and its Applications to Structure from Motion",
        "author": "Feng Yu, Teng Zhang, Gilad Lerman",
        "abstract": "We present the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers. STE is a fusion of the Tyler's M-estimator (TME) and a variant of the fast median subspace. Our theoretical analysis suggests that, under a common inlier-outlier model, STE can effectively recover the underlying subspace, even when it contains a smaller fraction of inliers relative to other methods in the field of robust subspace recovery. We apply STE in the context of Structure from Motion (SfM) in two ways: for robust estimation of the fundamental matrix and for the removal of outlying cameras, enhancing the robustness of the SfM pipeline. Numerical experiments confirm the state-of-the-art performance of our method in these applications. This research makes significant contributions to the field of robust subspace recovery, particularly in the context of computer vision and 3D reconstruction.",
        "page": "http://arxiv.org/abs/2404.11590",
        "pdf": "http://arxiv.org/pdf/2404.11590.pdf"
    },
    {
        "title": "Bi-level Learning of Task-Specific Decoders for Joint Registration and One-Shot Medical Image Segmentation",
        "author": "Xin Fan, Xiaolin Wang, Jiaxin Gao, Jia Wang, Zhongxuan Luo, Risheng Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation",
        "author": "Sicheng Li, Hao Li, Yiyi Liao, Lu Yu",
        "abstract": "The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB.",
        "page": "http://arxiv.org/abs/2404.02185",
        "pdf": "http://arxiv.org/pdf/2404.02185.pdf"
    },
    {
        "title": "Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of Illumination and Reflectance",
        "author": "Yuto Enyo, Ko Nishino",
        "abstract": "Reflectance bounds the frequency spectrum of illumination in the object appearance. In this paper, we introduce the first stochastic inverse rendering method, which recovers the attenuated frequency spectrum of an illumination jointly with the reflectance of an object of known geometry from a single image. Our key idea is to solve this blind inverse problem in the reflectance map, an appearance representation invariant to the underlying geometry, by learning to reverse the image formation with a novel diffusion model which we refer to as the Diffusion Reflectance Map Network (DRMNet). Given an observed reflectance map converted and completed from the single input image, DRMNet generates a reflectance map corresponding to a perfect mirror sphere while jointly estimating the reflectance. The forward process can be understood as gradually filtering a natural illumination with lower and lower frequency reflectance and additive Gaussian noise. DRMNet learns to invert this process with two subnetworks, IllNet and RefNet, which work in concert towards this joint estimation. The network is trained on an extensive synthetic dataset and is demonstrated to generalize to real images, showing state-of-the-art accuracy on established datasets.",
        "page": "http://arxiv.org/abs/2312.04529",
        "pdf": "http://arxiv.org/pdf/2312.04529.pdf"
    },
    {
        "title": "Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing",
        "author": "Hyelin Nam, Gihyun Kwon, Geon Yeong Park, Jong Chul Ye",
        "abstract": "With the remarkable advent of text-to-image diffusion models, image editing methods have become more diverse and continue to evolve. A promising recent approach in this realm is Delta Denoising Score (DDS) - an image editing technique based on Score Distillation Sampling (SDS) framework that leverages the rich generative prior of text-to-image diffusion models. However, relying solely on the difference between scoring functions is insufficient for preserving specific structural elements from the original image, a crucial aspect of image editing. To address this, here we present an embarrassingly simple yet very powerful modification of DDS, called Contrastive Denoising Score (CDS), for latent diffusion models (LDM). Inspired by the similarities and differences between DDS and the contrastive learning for unpaired image-to-image translation(CUT), we introduce a straightforward approach using CUT loss within the DDS framework. Rather than employing auxiliary networks as in the original CUT approach, we leverage the intermediate features of LDM, specifically those from the self-attention layers, which possesses rich spatial information. Our approach enables zero-shot image-to-image translation and neural radiance field (NeRF) editing, achieving structural correspondence between the input and output while maintaining content controllability. Qualitative results and comparisons demonstrates the effectiveness of our proposed method. Project page: https://hyelinnam.github.io/CDS/",
        "page": "http://arxiv.org/abs/2311.18608",
        "pdf": "http://arxiv.org/pdf/2311.18608.pdf"
    },
    {
        "title": "DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection",
        "author": "Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, Dan Xu",
        "abstract": "Existing open-vocabulary object detectors typically require a predefined set of categories from users, significantly confining their application scenarios. In this paper, we introduce DetCLIPv3, a high-performing detector that excels not only at both open-vocabulary object detection, but also generating hierarchical labels for detected objects. DetCLIPv3 is characterized by three core designs: 1. Versatile model architecture: we derive a robust open-set detection framework which is further empowered with generation ability via the integration of a caption head. 2. High information density data: we develop an auto-annotation pipeline leveraging visual large language model to refine captions for large-scale image-text pairs, providing rich, multi-granular object labels to enhance the training. 3. Efficient training strategy: we employ a pre-training stage with low-resolution inputs that enables the object captioner to efficiently learn a broad spectrum of visual concepts from extensive image-text paired data. This is followed by a fine-tuning stage that leverages a small number of high-resolution samples to further enhance detection performance. With these effective designs, DetCLIPv3 demonstrates superior open-vocabulary detection performance, \\eg, our Swin-T backbone model achieves a notable 47.0 zero-shot fixed AP on the LVIS minival benchmark, outperforming GLIPv2, GroundingDINO, and DetCLIPv2 by 18.0/19.6/6.6 AP, respectively. DetCLIPv3 also achieves a state-of-the-art 19.7 AP in dense captioning task on VG dataset, showcasing its strong generative capability.",
        "page": "http://arxiv.org/abs/2404.09216",
        "pdf": "http://arxiv.org/pdf/2404.09216.pdf"
    },
    {
        "title": "Uncertainty-Guided Never-Ending Learning to Drive",
        "author": "Lei Lai, Eshed Ohn-Bar, Sanjay Arora, John Yi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Koala: Key frame-conditioned long video-LLM",
        "author": "Reuben Tan, Ximeng Sun, Ping Hu, Jui-Hsien Wang, Hanieh Deilamsalehy, Bryan A. Plummer, Bryan Russell, Kate Saenko",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaptation by Combining 3D GANs and Diffusion Priors",
        "author": "Biwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, Xuansong Xie",
        "abstract": "Text-guided domain adaptation and generation of 3D-aware portraits find many applications in various fields. However, due to the lack of training data and the challenges in handling the high variety of geometry and appearance, the existing methods for these tasks suffer from issues like inflexibility, instability, and low fidelity. In this paper, we propose a novel framework DiffusionGAN3D, which boosts text-guided 3D domain adaptation and generation by combining 3D GANs and diffusion priors. Specifically, we integrate the pre-trained 3D generative models (e.g., EG3D) and text-to-image diffusion models. The former provides a strong foundation for stable and high-quality avatar generation from text. And the diffusion models in turn offer powerful priors and guide the 3D generator finetuning with informative direction to achieve flexible and efficient text-guided domain adaptation. To enhance the diversity in domain adaptation and the generation capability in text-to-avatar, we introduce the relative distance loss and case-specific learnable triplane respectively. Besides, we design a progressive texture refinement module to improve the texture quality for both tasks above. Extensive experiments demonstrate that the proposed framework achieves excellent results in both domain adaptation and text-to-avatar tasks, outperforming existing methods in terms of generation quality and efficiency. The project homepage is at https://younglbw.github.io/DiffusionGAN3D-homepage/.",
        "page": "http://arxiv.org/abs/2312.16837",
        "pdf": "http://arxiv.org/pdf/2312.16837.pdf"
    },
    {
        "title": "ZeroShape: Regression-based Zero-shot Shape Reconstruction",
        "author": "Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, James Rehg",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Your Transferability Barrier is Fragile: Free-Lunch for Transferring the Non-Transferable Learning",
        "author": "Ziming Hong, Li Shen, Tongliang Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ARTrackV2: Prompting Autoregressive Tracker Where to Look and How to Describe",
        "author": "Yifan Bai, Zeyang Zhao, Yihong Gong, Xing Wei",
        "abstract": "We present ARTrackV2, which integrates two pivotal aspects of tracking: determining where to look (localization) and how to describe (appearance analysis) the target object across video frames. Building on the foundation of its predecessor, ARTrackV2 extends the concept by introducing a unified generative framework to \"read out\" object's trajectory and \"retell\" its appearance in an autoregressive manner. This approach fosters a time-continuous methodology that models the joint evolution of motion and visual features, guided by previous estimates. Furthermore, ARTrackV2 stands out for its efficiency and simplicity, obviating the less efficient intra-frame autoregression and hand-tuned parameters for appearance updates. Despite its simplicity, ARTrackV2 achieves state-of-the-art performance on prevailing benchmark datasets while demonstrating remarkable efficiency improvement. In particular, ARTrackV2 achieves AO score of 79.5\\% on GOT-10k, and AUC of 86.1\\% on TrackingNet while being $3.6 \\times$ faster than ARTrack. The code will be released.",
        "page": "http://arxiv.org/abs/2312.17133",
        "pdf": "http://arxiv.org/pdf/2312.17133.pdf"
    },
    {
        "title": "CNC-Net: Self-Supervised Learning for CNC Machining Operations",
        "author": "Mohsen Yavartanoo, Sangmin Hong, Reyhaneh Neshatavar, Kyoung Mu Lee",
        "abstract": "CNC manufacturing is a process that employs computer numerical control (CNC) machines to govern the movements of various industrial tools and machinery, encompassing equipment ranging from grinders and lathes to mills and CNC routers. However, the reliance on manual CNC programming has become a bottleneck, and the requirement for expert knowledge can result in significant costs. Therefore, we introduce a pioneering approach named CNC-Net, representing the use of deep neural networks (DNNs) to simulate CNC machines and grasp intricate operations when supplied with raw materials. CNC-Net constitutes a self-supervised framework that exclusively takes an input 3D model and subsequently generates the essential operation parameters required by the CNC machine to construct the object. Our method has the potential to transformative automation in manufacturing by offering a cost-effective alternative to the high costs of manual CNC programming while maintaining exceptional precision in 3D object production. Our experiments underscore the effectiveness of our CNC-Net in constructing the desired 3D objects through the utilization of CNC operations. Notably, it excels in preserving finer local details, exhibiting a marked enhancement in precision compared to the state-of-the-art 3D CAD reconstruction approaches.",
        "page": "http://arxiv.org/abs/2312.09925",
        "pdf": "http://arxiv.org/pdf/2312.09925.pdf"
    },
    {
        "title": "High-Quality Facial Geometry and Appearance Capture at Home",
        "author": "Yuxuan Han, Junfeng Lyu, Feng Xu",
        "abstract": "Facial geometry and appearance capture have demonstrated tremendous success in 3D scanning real humans in studios. Recent works propose to democratize this technique while keeping the results high quality. However, they are still inconvenient for daily usage. In addition, they focus on an easier problem of only capturing facial skin. This paper proposes a novel method for high-quality face capture, featuring an easy-to-use system and the capability to model the complete face with skin, mouth interior, hair, and eyes. We reconstruct facial geometry and appearance from a single co-located smartphone flashlight sequence captured in a dim room where the flashlight is the dominant light source (e.g. rooms with curtains or at night). To model the complete face, we propose a novel hybrid representation to effectively model both eyes and other facial regions, along with novel techniques to learn it from images. We apply a combined lighting model to compactly represent real illuminations and exploit a morphable face albedo model as a reflectance prior to disentangle diffuse and specular. Experiments show that our method can capture high-quality 3D relightable scans.",
        "page": "http://arxiv.org/abs/2312.03442",
        "pdf": "http://arxiv.org/pdf/2312.03442.pdf"
    },
    {
        "title": "Efficient Scene Recovery Using Luminous Flux Prior",
        "author": "ZhongYu Li, Lei Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation",
        "author": "Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, Daniel Aliaga",
        "abstract": "Generative object compositing emerges as a promising new avenue for compositional image editing. However, the requirement of object identity preservation poses a significant challenge, limiting practical usage of most existing methods. In response, this paper introduces IMPRINT, a novel diffusion-based generative model trained with a two-stage learning framework that decouples learning of identity preservation from that of compositing. The first stage is targeted for context-agnostic, identity-preserving pretraining of the object encoder, enabling the encoder to learn an embedding that is both view-invariant and conducive to enhanced detail preservation. The subsequent stage leverages this representation to learn seamless harmonization of the object composited to the background. In addition, IMPRINT incorporates a shape-guidance mechanism offering user-directed control over the compositing process. Extensive experiments demonstrate that IMPRINT significantly outperforms existing methods and various baselines on identity preservation and composition quality.",
        "page": "http://arxiv.org/abs/2403.10701",
        "pdf": "http://arxiv.org/pdf/2403.10701.pdf"
    },
    {
        "title": "Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels",
        "author": "Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang",
        "abstract": "Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer) to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual information. Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.",
        "page": "http://arxiv.org/abs/2403.02746",
        "pdf": "http://arxiv.org/pdf/2403.02746.pdf"
    },
    {
        "title": "Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning",
        "author": "Siteng Huang, Biao Gong, Yutong Feng, Zhang Min, Yiliang Lv, Donglin Wang",
        "abstract": "Recent compositional zero-shot learning (CZSL) methods adapt pre-trained vision-language models (VLMs) by constructing trainable prompts only for composed state-object pairs. Relying on learning the joint representation of seen compositions, these methods ignore the explicit modeling of the state and object, thus limiting the exploitation of pre-trained knowledge and generalization to unseen compositions. With a particular focus on the universality of the solution, in this work, we propose a novel paradigm for CZSL models that establishes three identification branches (i.e., Multi-Path) to jointly model the state, object, and composition. The presented Troika is our implementation that aligns the branch-specific prompt representations with decomposed visual features. To calibrate the bias between semantically similar multi-modal representations, we further devise a Cross-Modal Traction module into Troika that shifts the prompt representation towards the current visual content. We conduct extensive experiments on three popular benchmarks, where our method significantly outperforms existing methods in both closed-world and open-world settings. The code will be available at https://github.com/bighuang624/Troika.",
        "page": "http://arxiv.org/abs/2303.15230",
        "pdf": "http://arxiv.org/pdf/2303.15230.pdf"
    },
    {
        "title": "Hyperbolic Anomaly Detection",
        "author": "Huimin Li, Zhentao Chen, Yunhao Xu, Junlin Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multiple View Geometry Transformers for 3D Human Pose Estimation",
        "author": "Ziwei Liao, jialiang zhu, Chunyu Wang, Han Hu, Steven L. Waslander",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment",
        "author": "Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Jenq-Neng Hwang, Xiaozhong Xu, Shan Liu",
        "abstract": "No-reference point cloud quality assessment (NR-PCQA) aims to automatically evaluate the perceptual quality of distorted point clouds without available reference, which have achieved tremendous improvements due to the utilization of deep neural networks. However, learning-based NR-PCQA methods suffer from the scarcity of labeled data and usually perform suboptimally in terms of generalization. To solve the problem, we propose a novel contrastive pre-training framework tailored for PCQA (CoPA), which enables the pre-trained model to learn quality-aware representations from unlabeled data. To obtain anchors in the representation space, we project point clouds with different distortions into images and randomly mix their local patches to form mixed images with multiple distortions. Utilizing the generated anchors, we constrain the pre-training process via a quality-aware contrastive loss following the philosophy that perceptual quality is closely related to both content and distortion. Furthermore, in the model fine-tuning stage, we propose a semantic-guided multi-view fusion module to effectively integrate the features of projected images from multiple perspectives. Extensive experiments show that our method outperforms the state-of-the-art PCQA methods on popular benchmarks. Further investigations demonstrate that CoPA can also benefit existing learning-based PCQA models.",
        "page": "http://arxiv.org/abs/2403.10066",
        "pdf": "http://arxiv.org/pdf/2403.10066.pdf"
    },
    {
        "title": "Anatomically Constrained Implicit Face Models",
        "author": "Prashanth Chandran, Gaspard Zoss",
        "abstract": "Coordinate based implicit neural representations have gained rapid popularity in recent years as they have been successfully used in image, geometry and scene modeling tasks. In this work, we present a novel use case for such implicit representations in the context of learning anatomically constrained face models. Actor specific anatomically constrained face models are the state of the art in both facial performance capture and performance retargeting. Despite their practical success, these anatomical models are slow to evaluate and often require extensive data capture to be built. We propose the anatomical implicit face model; an ensemble of implicit neural networks that jointly learn to model the facial anatomy and the skin surface with high-fidelity, and can readily be used as a drop in replacement to conventional blendshape models. Given an arbitrary set of skin surface meshes of an actor and only a neutral shape with estimated skull and jaw bones, our method can recover a dense anatomical substructure which constrains every point on the facial surface. We demonstrate the usefulness of our approach in several tasks ranging from shape fitting, shape editing, and performance retargeting.",
        "page": "http://arxiv.org/abs/2312.07538",
        "pdf": "http://arxiv.org/pdf/2312.07538.pdf"
    },
    {
        "title": "Revisiting Global Translation Estimation with Feature Tracks",
        "author": "Peilin Tao, Hainan Cui, Mengqi Rong, Shuhan Shen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "WinSyn: A High Resolution Testbed for Synthetic Data",
        "author": "Tom Kelly, John Femiani, Peter Wonka",
        "abstract": "We present WinSyn, a unique dataset and testbed for creating high-quality synthetic data with procedural modeling techniques. The dataset contains high-resolution photographs of windows, selected from locations around the world, with 89,318 individual window crops showcasing diverse geometric and material characteristics. We evaluate a procedural model by training semantic segmentation networks on both synthetic and real images and then comparing their performances on a shared test set of real images. Specifically, we measure the difference in mean Intersection over Union (mIoU) and determine the effective number of real images to match synthetic data's training performance. We design a baseline procedural model as a benchmark and provide 21,290 synthetically generated images. By tuning the procedural model, key factors are identified which significantly influence the model's fidelity in replicating real-world scenarios. Importantly, we highlight the challenge of procedural modeling using current techniques, especially in their ability to replicate the spatial semantics of real-world scenarios. This insight is critical because of the potential of procedural models to bridge to hidden scene aspects such as depth, reflectivity, material properties, and lighting conditions.",
        "page": "http://arxiv.org/abs/2310.08471",
        "pdf": "http://arxiv.org/pdf/2310.08471.pdf"
    },
    {
        "title": "Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning",
        "author": "Yixiong Zou, Yicong Liu, Yiman Hu, Yuhua Li, Ruixuan Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Neural Super-Resolution for Real-time Rendering with Radiance Demodulation",
        "author": "Jia Li, Ziling Chen, Xiaolong Wu, Lu Wang, Beibei Wang, Lei Zhang",
        "abstract": "It is time-consuming to render high-resolution images in applications such as video games and virtual reality, and thus super-resolution technologies become increasingly popular for real-time rendering. However, it is challenging to preserve sharp texture details, keep the temporal stability and avoid the ghosting artifacts in real-time super-resolution rendering. To address this issue, we introduce radiance demodulation to separate the rendered image or radiance into a lighting component and a material component, considering the fact that the light component is smoother than the rendered image so that the high-resolution material component with detailed textures can be easily obtained. We perform the super-resolution on the lighting component only and re-modulate it with the high-resolution material component to obtain the final super-resolution image with more texture details. A reliable warping module is proposed by explicitly marking the occluded regions to avoid the ghosting artifacts. To further enhance the temporal stability, we design a frame-recurrent neural network and a temporal loss to aggregate the previous and current frames, which can better capture the spatial-temporal consistency among reconstructed frames. As a result, our method is able to produce temporally stable results in real-time rendering with high-quality details, even in the challenging 4 $\\times$ 4 super-resolution scenarios.",
        "page": "http://arxiv.org/abs/2308.06699",
        "pdf": "http://arxiv.org/pdf/2308.06699.pdf"
    },
    {
        "title": "Noisy One-point Homographies are Surprisingly Good",
        "author": "Yaqing Ding, Jonathan Astermark, Magnus Oskarsson, Viktor Larsson",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Alchemist: Parametric Control of Material Properties with Diffusion Models",
        "author": "Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William Freeman, Mark Matthews",
        "abstract": "We propose a method to control material attributes of objects like roughness, metallic, albedo, and transparency in real images. Our method capitalizes on the generative prior of text-to-image models known for photorealism, employing a scalar value and instructions to alter low-level material properties. Addressing the lack of datasets with controlled material attributes, we generated an object-centric synthetic dataset with physically-based materials. Fine-tuning a modified pre-trained text-to-image model on this synthetic dataset enables us to edit material properties in real-world images while preserving all other attributes. We show the potential application of our model to material edited NeRFs.",
        "page": "http://arxiv.org/abs/2312.02970",
        "pdf": "http://arxiv.org/pdf/2312.02970.pdf"
    },
    {
        "title": "PaReNeRF: Toward Fast Large-scale Dynamic NeRF with Patch-based Reference",
        "author": "Xiao Tang, Min Yang, Penghui Sun, Hui Li, Yuchao Dai, feng zhu, Hojae Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval",
        "author": "Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, MAJID RABBANI, Raghuveer Rao, ZHIQIANG TAO",
        "abstract": "The increasing prevalence of video clips has sparked growing interest in text-video retrieval. Recent advances focus on establishing a joint embedding space for text and video, relying on consistent embedding representations to compute similarity. However, the text content in existing datasets is generally short and concise, making it hard to fully describe the redundant semantics of a video. Correspondingly, a single text embedding may be less expressive to capture the video embedding and empower the retrieval. In this study, we propose a new stochastic text modeling method T-MASS, i.e., text is modeled as a stochastic embedding, to enrich text embedding with a flexible and resilient semantic range, yielding a text mass. To be specific, we introduce a similarity-aware radius module to adapt the scale of the text mass upon the given text-video pairs. Plus, we design and develop a support text regularization to further control the text mass during the training. The inference pipeline is also tailored to fully exploit the text mass for accurate retrieval. Empirical evidence suggests that T-MASS not only effectively attracts relevant text-video pairs while distancing irrelevant ones, but also enables the determination of precise text embeddings for relevant pairs. Our experimental results show a substantial improvement of T-MASS over baseline (3% to 6.3% by R@1). Also, T-MASS achieves state-of-the-art performance on five benchmark datasets, including MSRVTT, LSMDC, DiDeMo, VATEX, and Charades.",
        "page": "http://arxiv.org/abs/2403.17998",
        "pdf": "http://arxiv.org/pdf/2403.17998.pdf"
    },
    {
        "title": "Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing",
        "author": "Xun Lin, Shuai Wang, RIZHAO CAI, Yizhong Liu, Ying Fu, Wenzhong Tang, Zitong YU, Alex C. Kot",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Universal Novelty Detection through Adaptive Contrastive Learning",
        "author": "Hossein Mirzaei, Mojtaba Nafez, Mohammad Jafari, Mohammad Soltani, Mohammad Azizmalayeri, Jafar Habibi, Mohammad Sabokrou, Mohammad Rohban",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LAMP: Learn A Motion Pattern for Few-Shot Video Generation",
        "author": "Rui-Qi Wu, Liangyu Chen, Tong Yang, Chun-Le Guo, Chongyi Li, Xiangyu Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CLiC: Concept Learning in Context",
        "author": "Mehdi Safaee, Aryan Mikaeili, Or Patashnik, Daniel Cohen-Or, Ali Mahdavi Amiri",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis",
        "author": "Jiawen Li, Yuxuan Chen, Hongbo Chu, Sun Qiehe, Tian Guan, Anjia Han, Yonghong He",
        "abstract": "Histopathological whole slide images (WSIs) classification has become a foundation task in medical microscopic imaging processing. Prevailing approaches involve learning WSIs as instance-bag representations, emphasizing significant instances but struggling to capture the interactions between instances. Additionally, conventional graph representation methods utilize explicit spatial positions to construct topological structures but restrict the flexible interaction capabilities between instances at arbitrary locations, particularly when spatially distant. In response, we propose a novel dynamic graph representation algorithm that conceptualizes WSIs as a form of the knowledge graph structure. Specifically, we dynamically construct neighbors and directed edge embeddings based on the head and tail relationships between instances. Then, we devise a knowledge-aware attention mechanism that can update the head node features by learning the joint attention score of each neighbor and edge. Finally, we obtain a graph-level embedding through the global pooling process of the updated head, serving as an implicit representation for the WSI classification. Our end-to-end graph representation learning approach has outperformed the state-of-the-art WSI analysis methods on three TCGA benchmark datasets and in-house test sets. Our code is available at https://github.com/WonderLandxD/WiKG.",
        "page": "http://arxiv.org/abs/2403.07719",
        "pdf": "http://arxiv.org/pdf/2403.07719.pdf"
    },
    {
        "title": "Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs",
        "author": "Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Tat-seng Chua",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards CLIP-driven Language-free 3D Visual Grounding via 2D-3D Relational Enhancement and Consistency",
        "author": "Yuqi Zhang, Han Luo, Yinjie Lei",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MR-VNet: Media Restoration using Volterra Networks",
        "author": "Siddharth Roheda, Amit Unde, Loay Rashid",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "WonderJourney: Going from Anywhere to Everywhere",
        "author": "Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, Charles Herrmann",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UFORecon: Generalizable Sparse-View Surface Reconstruction from Arbitrary and Unfavorable Sets",
        "author": "Youngju Na, Woo Jae Kim, Kyu Han, Suhyeon Ha, Sung-Eui Yoon",
        "abstract": "Generalizable neural implicit surface reconstruction aims to obtain an accurate underlying geometry given a limited number of multi-view images from unseen scenes. However, existing methods select only informative and relevant views using predefined scores for training and testing phases. This constraint renders the model impractical in real-world scenarios, where the availability of favorable combinations cannot always be ensured. We introduce and validate a view-combination score to indicate the effectiveness of the input view combination. We observe that previous methods output degenerate solutions under arbitrary and unfavorable sets. Building upon this finding, we propose UFORecon, a robust view-combination generalizable surface reconstruction framework. To achieve this, we apply cross-view matching transformers to model interactions between source images and build correlation frustums to capture global correlations. Additionally, we explicitly encode pairwise feature similarities as view-consistent priors. Our proposed framework significantly outperforms previous methods in terms of view-combination generalizability and also in the conventional generalizable protocol trained with favorable view-combinations. The code is available at https://github.com/Youngju-Na/UFORecon.",
        "page": "http://arxiv.org/abs/2403.05086",
        "pdf": "http://arxiv.org/pdf/2403.05086.pdf"
    },
    {
        "title": "Few-shot Learner Parameterization by Diffusion Time-steps",
        "author": "Zhongqi Yue, Pan Zhou, Richang Hong, Hanwang Zhang, Qianru Sun",
        "abstract": "Even when using large multi-modal foundation models, few-shot learning is still challenging -- if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step Few-shot (TiF) learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt. Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks. Codes are in https://github.com/yue-zhongqi/tif.",
        "page": "http://arxiv.org/abs/2403.02649",
        "pdf": "http://arxiv.org/pdf/2403.02649.pdf"
    },
    {
        "title": "SPIN: Simultaneous Perception, Interaction and Navigation",
        "author": "Shagun Uppal, Ananye Agarwal, Haoyu Xiong, Kenneth Shaw, Deepak Pathak",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Motion Blur Decomposition with Cross-shutter Guidance",
        "author": "Xiang Ji, Haiyang Jiang, Yinqiang Zheng",
        "abstract": "Motion blur is a frequently observed image artifact, especially under insufficient illumination where exposure time has to be prolonged so as to collect more photons for a bright enough image. Rather than simply removing such blurring effects, recent researches have aimed at decomposing a blurry image into multiple sharp images with spatial and temporal coherence. Since motion blur decomposition itself is highly ambiguous, priors from neighbouring frames or human annotation are usually needed for motion disambiguation. In this paper, inspired by the complementary exposure characteristics of a global shutter (GS) camera and a rolling shutter (RS) camera, we propose to utilize the ordered scanline-wise delay in a rolling shutter image to robustify motion decomposition of a single blurry image. To evaluate this novel dual imaging setting, we construct a triaxial system to collect realistic data, as well as a deep network architecture that explicitly addresses temporal and contextual information through reciprocal branches for cross-shutter motion blur decomposition. Experiment results have verified the effectiveness of our proposed algorithm, as well as the validity of our dual imaging setting.",
        "page": "http://arxiv.org/abs/2404.01120",
        "pdf": "http://arxiv.org/pdf/2404.01120.pdf"
    },
    {
        "title": "Real-time Acquisition and Reconstruction of Dynamic Volumes with Neural Structured Illumination",
        "author": "Yixin Zeng, Zoubin Bi, Yin Mingrui, Xiang Feng, Kun Zhou, Hongzhi Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MV-Adapter: Exploring Parameter Efficient Learning for Video Text Retrieval",
        "author": "bowen zhang, Xiaojie Jin, Weibo Gong, Kai Xu, Xueqing Deng, Peng Wang, Zhao Zhang, Xiaohui Shen, Jiashi Feng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Mind marginal non-crack regions: Clustering-inspired representation learning for crack segmentation",
        "author": "zhuangzhuang chen, Zhuonan Lai, Jie Chen, Jianqiang Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SpatialTracker: Tracking Any 2D Pixels in 3D Space",
        "author": "Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou",
        "abstract": "Recovering dense and long-range pixel motion in videos is a challenging problem. Part of the difficulty arises from the 3D-to-2D projection process, leading to occlusions and discontinuities in the 2D motion domain. While 2D motion can be intricate, we posit that the underlying 3D motion can often be simple and low-dimensional. In this work, we propose to estimate point trajectories in 3D space to mitigate the issues caused by image projection. Our method, named SpatialTracker, lifts 2D pixels to 3D using monocular depth estimators, represents the 3D content of each frame efficiently using a triplane representation, and performs iterative updates using a transformer to estimate 3D trajectories. Tracking in 3D allows us to leverage as-rigid-as-possible (ARAP) constraints while simultaneously learning a rigidity embedding that clusters pixels into different rigid parts. Extensive evaluation shows that our approach achieves state-of-the-art tracking performance both qualitatively and quantitatively, particularly in challenging scenarios such as out-of-plane rotation.",
        "page": "http://arxiv.org/abs/2404.04319",
        "pdf": "http://arxiv.org/pdf/2404.04319.pdf"
    },
    {
        "title": "FreePoint: Unsupervised Point Cloud Instance Segmentation",
        "author": "Zhikai Zhang, Jian Ding, Li Jiang, Dengxin Dai, Gui-Song Xia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Projecting Trackable Thermal Patterns for Dynamic Computer Vision",
        "author": "Mark Sheinin, Aswin C. Sankaranarayanan, Srinivasa G. Narasimhan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Overcoming Generic Knowledge Loss with Selective Parameter Update",
        "author": "Wenxuan Zhang, Paul Janson, Rahaf Aljundi, Mohamed Elhoseiny",
        "abstract": "Foundation models encompass an extensive knowledge base and offer remarkable transferability. However, this knowledge becomes outdated or insufficient over time. The challenge lies in continuously updating foundation models to accommodate novel information while retaining their original capabilities. Leveraging the fact that foundation models have initial knowledge on various tasks and domains, we propose a novel approach that, instead of updating all parameters equally, localizes the updates to a sparse set of parameters relevant to the task being learned. We strike a balance between efficiency and new task performance, while maintaining the transferability and generalizability of foundation models. We extensively evaluate our method on foundational vision-language models with a diverse spectrum of continual learning tasks. Our method achieves improvements on the accuracy of the newly learned tasks up to 7% while preserving the pretraining knowledge with a negligible decrease of 0.9% on a representative control set accuracy.",
        "page": "http://arxiv.org/abs/2308.12462",
        "pdf": "http://arxiv.org/pdf/2308.12462.pdf"
    },
    {
        "title": "EventPS: Real-Time Photometric Stereo Using an Event Camera",
        "author": "Bohan Yu, Jieji Ren, Jin Han, Feishi Wang, Jinxiu Liang, Boxin Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Open-Vocabulary 3D Semantic Segmentation with Foundation Models",
        "author": "Li Jiang, Shaoshuai Shi, Bernt Schiele",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Pick-or-Mix: Dynamic Channel Sampling for ConvNets",
        "author": "Ashish Kumar, Daneul Kim, Jaesik Park, Laxmidhar Behera",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers",
        "author": "Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Yan-Pei Cao, Ding Liang, Song-Hai Zhang",
        "abstract": "Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques. Please see our project page at https://zouzx.github.io/TriplaneGaussian/.",
        "page": "http://arxiv.org/abs/2312.09147",
        "pdf": "http://arxiv.org/pdf/2312.09147.pdf"
    },
    {
        "title": "CAMEL: CAusal Motion Enhancement tailored for Lifting Text-driven Video Editing",
        "author": "Guiwei Zhang, Tianyu Zhang, Guanglin Niu, Zichang Tan, Zichang Tan, Yalong Bai, Qing Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery",
        "author": "Mubashir Noman, Muzammal Naseer, Hisham Cholakkal, Rao Anwer, Salman Khan, Fahad Shahbaz Khan",
        "abstract": "Recent advances in unsupervised learning have demonstrated the ability of large vision models to achieve promising results on downstream tasks by pre-training on large amount of unlabelled data. Such pre-training techniques have also been explored recently in the remote sensing domain due to the availability of large amount of unlabelled data. Different from standard natural image datasets, remote sensing data is acquired from various sensor technologies and exhibit diverse range of scale variations as well as modalities. Existing satellite image pre-training methods either ignore the scale information present in the remote sensing imagery or restrict themselves to use only a single type of data modality. In this paper, we re-visit transformers pre-training and leverage multi-scale information that is effectively utilized with multiple modalities. Our proposed approach, named SatMAE++, performs multi-scale pre-training and utilizes convolution based upsampling blocks to reconstruct the image at higher scales making it extensible to include more scales. Compared to existing works, the proposed SatMAE++ with multi-scale pre-training is equally effective for both optical as well as multi-spectral imagery. Extensive experiments on six datasets reveal the merits of proposed contributions, leading to state-of-the-art performance on all datasets. SatMAE++ achieves mean average precision (mAP) gain of 2.5\\% for multi-label classification task on BigEarthNet dataset. Our code and pre-trained models are available at \\url{https://github.com/techmn/satmae_pp}.",
        "page": "http://arxiv.org/abs/2403.05419",
        "pdf": "http://arxiv.org/pdf/2403.05419.pdf"
    },
    {
        "title": "ViTamin: Designing Scalable Vision Models in the Vision-Language Era",
        "author": "Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan L. Yuille, Liang-Chieh Chen",
        "abstract": "Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).",
        "page": "http://arxiv.org/abs/2404.02132",
        "pdf": "http://arxiv.org/pdf/2404.02132.pdf"
    },
    {
        "title": "GraCo: Granularity-Controllable Interactive Segmentation",
        "author": "Yian Zhao, Kehan Li, Zesen Cheng, Pengchong Qiao, Xiawu Zheng, Rongrong Ji, Chang Liu, Li Yuan, Jie Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera",
        "author": "Jiye Lee, Hanbyul Joo",
        "abstract": "We present a lightweight and affordable motion capture method based on two smartwatches and a head-mounted camera. In contrast to the existing approaches that use six or more expert-level IMU devices, our approach is much more cost-effective and convenient. Our method can make wearable motion capture accessible to everyone everywhere, enabling 3D full-body motion capture in diverse environments. As a key idea to overcome the extreme sparsity and ambiguities of sensor inputs with different modalities, we integrate 6D head poses obtained from the head-mounted cameras for motion estimation. To enable capture in expansive indoor and outdoor scenes, we propose an algorithm to track and update floor level changes to define head poses, coupled with a multi-stage Transformer-based regression module. We also introduce novel strategies leveraging visual cues of egocentric images to further enhance the motion capture quality while reducing ambiguities. We demonstrate the performance of our method on various challenging scenarios, including complex outdoor environments and everyday motions including object interactions and social interactions among multiple individuals.",
        "page": "http://arxiv.org/abs/2401.00847",
        "pdf": "http://arxiv.org/pdf/2401.00847.pdf"
    },
    {
        "title": "DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation",
        "author": "Yuanchen Wu, Xichen Ye, KequanYang, Jide Li, Xiaoqiang Li",
        "abstract": "Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets. Code is available at https://github.com/Wu0409/DuPL.",
        "page": "http://arxiv.org/abs/2403.11184",
        "pdf": "http://arxiv.org/pdf/2403.11184.pdf"
    },
    {
        "title": "Image Neural Field Diffusion Models",
        "author": "Yinbo Chen, Oliver Wang, Richard Zhang, Eli Shechtman, Xiaolong Wang, Micha\u00ebl Gharbi, Micha\u00ebl Gharbi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Segment Every Out-of-Distribution Object",
        "author": "Wenjie Zhao, Jia Li, Xin Dong, Yu Xiang, Yunhui Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution",
        "author": "Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, Chen Change Loy",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data",
        "author": "Qihao Liu, Yi Zhang, Song Bai, Adam Kortylewski, Alan L. Yuille",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "An Interactive Navigation Method with Effect-oriented Affordance",
        "author": "Xiaohan Wang, Yuehu LIU, Xinhang Song, Yuyi Liu, Sixian Zhang, Shuqiang Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NAPGuard: Towards Detecting Naturalistic Adversarial Patches",
        "author": "Siyang Wu, Jiakai Wang, Jiejie Zhao, Yazhe Wang, Xianglong Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Flattening the Parent Bias: Hierarchical Semantic Segmentation in the Poincar\u00e9 Ball",
        "author": "Simon Weber, Bar\u0131\u015f Z\u00f6ng\u00fcr, Nikita Araslanov, Daniel Cremers",
        "abstract": "Hierarchy is a natural representation of semantic taxonomies, including the ones routinely used in image segmentation. Indeed, recent work on semantic segmentation reports improved accuracy from supervised training leveraging hierarchical label structures. Encouraged by these results, we revisit the fundamental assumptions behind that work. We postulate and then empirically verify that the reasons for the observed improvement in segmentation accuracy may be entirely unrelated to the use of the semantic hierarchy. To demonstrate this, we design a range of cross-domain experiments with a representative hierarchical approach. We find that on the new testing domains, a flat (non-hierarchical) segmentation network, in which the parents are inferred from the children, has superior segmentation accuracy to the hierarchical approach across the board. Complementing these findings and inspired by the intrinsic properties of hyperbolic spaces, we study a more principled approach to hierarchical segmentation using the Poincar\\'e ball model. The hyperbolic representation largely outperforms the previous (Euclidean) hierarchical approach as well and is on par with our flat Euclidean baseline in terms of segmentation accuracy. However, it additionally exhibits surprisingly strong calibration quality of the parent nodes in the semantic hierarchy, especially on the more challenging domains. Our combined analysis suggests that the established practice of hierarchical segmentation may be limited to in-domain settings, whereas flat classifiers generalize substantially better, especially if they are modeled in the hyperbolic space.",
        "page": "http://arxiv.org/abs/2404.03778",
        "pdf": "http://arxiv.org/pdf/2404.03778.pdf"
    },
    {
        "title": "Generative Region-Language Pretraining for Open-Ended Object Detection",
        "author": "Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, Jianfei Cai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Psychometry: An Omnifit Model for Image Reconstruction from Human Brain Activity",
        "author": "Ruijie Quan, Wenguan Wang, Zhibo Tian, Fan Ma, Yi Yang",
        "abstract": "Reconstructing the viewed images from human brain activity bridges human and computer vision through the Brain-Computer Interface. The inherent variability in brain function between individuals leads existing literature to focus on acquiring separate models for each individual using their respective brain signal data, ignoring commonalities between these data. In this article, we devise Psychometry, an omnifit model for reconstructing images from functional Magnetic Resonance Imaging (fMRI) obtained from different subjects. Psychometry incorporates an omni mixture-of-experts (Omni MoE) module where all the experts work together to capture the inter-subject commonalities, while each expert associated with subject-specific parameters copes with the individual differences. Moreover, Psychometry is equipped with a retrieval-enhanced inference strategy, termed Ecphory, which aims to enhance the learned fMRI representation via retrieving from prestored subject-specific memories. These designs collectively render Psychometry omnifit and efficient, enabling it to capture both inter-subject commonality and individual specificity across subjects. As a result, the enhanced fMRI representations serve as conditional signals to guide a generation model to reconstruct high-quality and realistic images, establishing Psychometry as state-of-the-art in terms of both high-level and low-level metrics.",
        "page": "http://arxiv.org/abs/2403.20022",
        "pdf": "http://arxiv.org/pdf/2403.20022.pdf"
    },
    {
        "title": "A Theory of Joint Light and Heat Transport for Lambertian Scenes",
        "author": "Mani Ramanagopal, Sriram Narayanan, Aswin C. Sankaranarayanan, Srinivasa G. Narasimhan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset",
        "author": "Yujin Jeon, Eunsue Choi, Youngchan Kim, Yunseong Moon, Khalid Omer, Felix Heide, Seung-Hwan Baek",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Efficient Stitchable Task Adaptation",
        "author": "Haoyu He, Zizheng Pan, Jing Liu, Jianfei Cai, Bohan Zhuang",
        "abstract": "The paradigm of pre-training and fine-tuning has laid the foundation for deploying deep learning models. However, most fine-tuning methods are designed to meet a specific resource budget. Recently, considering diverse deployment scenarios with various resource budgets, stitchable neural network (SN-Net) is introduced to quickly obtain numerous new networks (stitches) from the pre-trained models (anchors) in a model family via model stitching. Although promising, SN-Net confronts new challenges when adapting it to new target domains, including huge memory and storage requirements and a long and sub-optimal multistage adaptation process. In this work, we present a novel framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce a palette of fine-tuned models that adhere to diverse resource constraints. Specifically, we first tailor parameter-efficient fine-tuning to share low-rank updates among the stitches while maintaining independent bias terms. In this way, we largely reduce fine-tuning memory burdens and mitigate the interference among stitches that arises in task adaptation. Furthermore, we streamline a simple yet effective one-stage deployment pipeline, which estimates the important stitches to deploy with training-time gradient statistics. By assigning higher sampling probabilities to important stitches, we also get a boosted Pareto frontier. Extensive experiments on 25 downstream visual recognition tasks demonstrate that our ESTA is capable of generating stitches with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net adaptation by remarkable margins with significantly lower training time and fewer trainable parameters. Furthermore, we demonstrate the flexibility and scalability of our ESTA framework by stitching LLMs from LLaMA family, obtaining chatbot stitches of assorted sizes.",
        "page": "http://arxiv.org/abs/2311.17352",
        "pdf": "http://arxiv.org/pdf/2311.17352.pdf"
    },
    {
        "title": "Efficient Multitask Dense Predictor via Binarization",
        "author": "Yuzhang Shang, Dan Xu, Gaowen Liu, Ramana Kompella, Yan Yan",
        "abstract": "Multi-task learning for dense prediction has emerged as a pivotal area in computer vision, enabling simultaneous processing of diverse yet interrelated pixel-wise prediction tasks. However, the substantial computational demands of state-of-the-art (SoTA) models often limit their widespread deployment. This paper addresses this challenge by introducing network binarization to compress resource-intensive multi-task dense predictors. Specifically, our goal is to significantly accelerate multi-task dense prediction models via Binary Neural Networks (BNNs) while maintaining and even improving model performance at the same time. To reach this goal, we propose a Binary Multi-task Dense Predictor, Bi-MTDP, and several variants of Bi-MTDP, in which a multi-task dense predictor is constructed via specified binarized modules. Our systematical analysis of this predictor reveals that performance drop from binarization is primarily caused by severe information degradation. To address this issue, we introduce a deep information bottleneck layer that enforces representations for downstream tasks satisfying Gaussian distribution in forward propagation. Moreover, we introduce a knowledge distillation mechanism to correct the direction of information flow in backward propagation. Intriguingly, one variant of Bi-MTDP outperforms full-precision (FP) multi-task dense prediction SoTAs, ARTC (CNN-based) and InvPT (ViT-Based). This result indicates that Bi-MTDP is not merely a naive trade-off between performance and efficiency, but is rather a benefit of the redundant information flow thanks to the multi-task architecture. Code is available at https://github.com/42Shawn/BiMTDP.",
        "page": "http://arxiv.org/abs/2405.14136",
        "pdf": "http://arxiv.org/pdf/2405.14136.pdf"
    },
    {
        "title": "Novel View Synthesis with View-Dependent Effects from a Single Image",
        "author": "Juan Luis Gonzalez Bello, Munchurl Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation",
        "author": "Hongwei Yan, Liyuan Wang, Kaisheng Ma, Yi Zhong",
        "abstract": "To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervision signals across multiple stages facilitate appropriate convergence of the new task while gathering various strengths from experts by knowledge distillation mitigates the performance decline of old tasks. MOSE demonstrates remarkable efficacy in learning new samples and preserving past knowledge through multi-level experts, thereby significantly advancing OCL performance over state-of-the-art baselines (e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).",
        "page": "http://arxiv.org/abs/2404.00417",
        "pdf": "http://arxiv.org/pdf/2404.00417.pdf"
    },
    {
        "title": "Small Scale Data-Free Knowledge Distillation",
        "author": "He Liu, Yikai Wang, Huaping Liu, Fuchun Sun, Anbang Yao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution",
        "author": "Cheeun Hong, Kyoung Mu Lee",
        "abstract": "Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs. Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks. Specifically, adapting the quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy. However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage. To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time from hours to seconds. We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors. These bit mappings are calibrated and fine-tuned using only a small number of calibration images. We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2000. Codes are available at https://github.com/Cheeun/AdaBM.",
        "page": "http://arxiv.org/abs/2404.03296",
        "pdf": "http://arxiv.org/pdf/2404.03296.pdf"
    },
    {
        "title": "Domain Separation Graph Neural Networks for Saliency Object Ranking",
        "author": "Zijian Wu, Jun Lu, Jing Han, Lianfa Bai, Yi Zhang, Zhuang Zhao, Siyang Song",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Solving the Catastrophic Forgetting Problem in Generalized Category Discovery",
        "author": "Xinzi Cao, Xiawu Zheng, Guanhong Wang, Weijiang Yu, Yunhang Shen, Ke Li, Yutong Lu, Yonghong Tian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Image Restoration through Removing Degradations in Textual Representations",
        "author": "Jingbo Lin, Zhilu Zhang, Yuxiang Wei, Dongwei Ren, Dongsheng Jiang, Qi Tian, Wangmeng Zuo",
        "abstract": "In this paper, we introduce a new perspective for improving image restoration by removing degradation in the textual representations of a given degraded image. Intuitively, restoration is much easier on text modality than image one. For example, it can be easily conducted by removing degradation-related words while keeping the content-aware words. Hence, we combine the advantages of images in detail description and ones of text in degradation removal to perform restoration. To address the cross-modal assistance, we propose to map the degraded images into textual representations for removing the degradations, and then convert the restored textual representations into a guidance image for assisting image restoration. In particular, We ingeniously embed an image-to-text mapper and text restoration module into CLIP-equipped text-to-image models to generate the guidance. Then, we adopt a simple coarse-to-fine approach to dynamically inject multi-scale information from guidance to image restoration networks. Extensive experiments are conducted on various image restoration tasks, including deblurring, dehazing, deraining, and denoising, and all-in-one image restoration. The results showcase that our method outperforms state-of-the-art ones across all these tasks. The codes and models are available at \\url{https://github.com/mrluin/TextualDegRemoval}.",
        "page": "http://arxiv.org/abs/2312.17334",
        "pdf": "http://arxiv.org/pdf/2312.17334.pdf"
    },
    {
        "title": "HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation",
        "author": "Zhiying Leng, Tolga Birdal, Xiaohui Liang, Federico Tombari",
        "abstract": "3D shape generation from text is a fundamental task in 3D representation learning. The text-shape pairs exhibit a hierarchical structure, where a general text like ``chair\" covers all 3D shapes of the chair, while more detailed prompts refer to more specific shapes. Furthermore, both text and 3D shapes are inherently hierarchical structures. However, existing Text2Shape methods, such as SDFusion, do not exploit that. In this work, we propose HyperSDFusion, a dual-branch diffusion model that generates 3D shapes from a given text. Since hyperbolic space is suitable for handling hierarchical data, we propose to learn the hierarchical representations of text and 3D shapes in hyperbolic space. First, we introduce a hyperbolic text-image encoder to learn the sequential and multi-modal hierarchical features of text in hyperbolic space. In addition, we design a hyperbolic text-graph convolution module to learn the hierarchical features of text in hyperbolic space. In order to fully utilize these text features, we introduce a dual-branch structure to embed text features in 3D feature space. At last, to endow the generated 3D shapes with a hierarchical structure, we devise a hyperbolic hierarchical loss. Our method is the first to explore the hyperbolic hierarchical representation for text-to-shape generation. Experimental results on the existing text-to-shape paired dataset, Text2Shape, achieved state-of-the-art results. We release our implementation under HyperSDFusion.github.io.",
        "page": "http://arxiv.org/abs/2403.00372",
        "pdf": "http://arxiv.org/pdf/2403.00372.pdf"
    },
    {
        "title": "MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images",
        "author": "Junwen Huang, Hao Yu, Kuan-Ting Yu, Nassir Navab, Slobodan Ilic, Benjamin Busam",
        "abstract": "Recent learning methods for object pose estimation require resource-intensive training for each individual object instance or category, hampering their scalability in real applications when confronted with previously unseen objects. In this paper, we propose MatchU, a Fuse-Describe-Match strategy for 6D pose estimation from RGB-D images. MatchU is a generic approach that fuses 2D texture and 3D geometric cues for 6D pose prediction of unseen objects. We rely on learning geometric 3D descriptors that are rotation-invariant by design. By encoding pose-agnostic geometry, the learned descriptors naturally generalize to unseen objects and capture symmetries. To tackle ambiguous associations using 3D geometry only, we fuse additional RGB information into our descriptor. This is achieved through a novel attention-based mechanism that fuses cross-modal information, together with a matching loss that leverages the latent space learned from RGB data to guide the descriptor learning process. Extensive experiments reveal the generalizability of both the RGB-D fusion strategy as well as the descriptor efficacy. Benefiting from the novel designs, MatchU surpasses all existing methods by a significant margin in terms of both accuracy and speed, even without the requirement of expensive re-training or rendering.",
        "page": "http://arxiv.org/abs/2403.01517",
        "pdf": "http://arxiv.org/pdf/2403.01517.pdf"
    },
    {
        "title": "Towards Variable and Coordinated Holistic Co-Speech Motion Generation",
        "author": "Yifei Liu, Qiong Cao, Yandong Wen, Huaiguang Jiang, Changxing Ding",
        "abstract": "This paper addresses the problem of generating lifelike holistic co-speech motions for 3D avatars, focusing on two key aspects: variability and coordination. Variability allows the avatar to exhibit a wide range of motions even with similar speech content, while coordination ensures a harmonious alignment among facial expressions, hand gestures, and body poses. We aim to achieve both with ProbTalk, a unified probabilistic framework designed to jointly model facial, hand, and body movements in speech. ProbTalk builds on the variational autoencoder (VAE) architecture and incorporates three core designs. First, we introduce product quantization (PQ) to the VAE, which enriches the representation of complex holistic motion. Second, we devise a novel non-autoregressive model that embeds 2D positional encoding into the product-quantized representation, thereby preserving essential structure information of the PQ codes. Last, we employ a secondary stage to refine the preliminary prediction, further sharpening the high-frequency details. Coupling these three designs enables ProbTalk to generate natural and diverse holistic co-speech motions, outperforming several state-of-the-art methods in qualitative and quantitative evaluations, particularly in terms of realism. Our code and model will be released for research purposes at https://feifeifeiliu.github.io/probtalk/.",
        "page": "http://arxiv.org/abs/2404.00368",
        "pdf": "http://arxiv.org/pdf/2404.00368.pdf"
    },
    {
        "title": "Fast ODE-based Sampling for Diffusion Models in Around 5 Steps",
        "author": "Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen",
        "abstract": "Sampling from diffusion models can be treated as solving the corresponding ordinary differential equations (ODEs), with the aim of obtaining an accurate solution with as few number of function evaluations (NFE) as possible. Recently, various fast samplers utilizing higher-order ODE solvers have emerged and achieved better performance than the initial first-order one. However, these numerical methods inherently result in certain approximation errors, which significantly degrades sample quality with extremely small NFE (e.g., around 5). In contrast, based on the geometric observation that each sampling trajectory almost lies in a two-dimensional subspace embedded in the ambient space, we propose Approximate MEan-Direction Solver (AMED-Solver) that eliminates truncation errors by directly learning the mean direction for fast diffusion sampling. Besides, our method can be easily used as a plugin to further improve existing ODE-based samplers. Extensive experiments on image synthesis with the resolution ranging from 32 to 512 demonstrate the effectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10, 10.74 FID on ImageNet 64$\\times$64, and 13.20 FID on LSUN Bedroom. Our code is available at https://github.com/zju-pi/diff-sampler.",
        "page": "http://arxiv.org/abs/2312.00094",
        "pdf": "http://arxiv.org/pdf/2312.00094.pdf"
    },
    {
        "title": "Going Beyond Multi-Task Dense Prediction with Synergy Embedding Models",
        "author": "Huimin Huang, Yawen Huang, Lanfen Lin, Ruofeng Tong, Yen-Wei Chen, Hao Zheng, Yuexiang Li, Yefeng Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concept",
        "author": "Yong Hyun Ahn, Hyeon Bae Kim, Seong Tae Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ToonerGAN: Reinforcing GANs for Obfuscating Automated Facial Indexing",
        "author": "Kartik Thakral, Shashikant Prasad, Stuti Aswani, Mayank Vatsa, Richa Singh",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction",
        "author": "Junuk Cha, Jihyeon Kim, Jae Shin Yoon, Seungryul Baek",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation",
        "author": "Xiao Lin, Wenfei Yang, Yuan Gao, Tianzhu Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Open-Set Domain Adaptation for Semantic Segmentation",
        "author": "Seun-An Choe, Ah-Hyung Shin, Keon Hee Park, Jinwoo Choi, Gyeong-Moon Park",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge",
        "author": "Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, Liqiang Nie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Pixel Aligned Language Models",
        "author": "Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, Cordelia Schmid",
        "abstract": "Large language models have achieved great success in recent years, so as their variants in vision. Existing vision-language models can describe images in natural languages, answer visual-related questions, or perform complex reasoning about the image. However, it is yet unclear how localization tasks, such as word grounding or referring localization, can be performed using large language models. In this work, we aim to develop a vision-language model that can take locations, for example, a set of points or boxes, as either inputs or outputs. When taking locations as inputs, the model performs location-conditioned captioning, which generates captions for the indicated object or region. When generating locations as outputs, our model regresses pixel coordinates for each output word generated by the language model, and thus performs dense word grounding. Our model is pre-trained on the Localized Narrative dataset, which contains pixel-word-aligned captioning from human attention. We show our model can be applied to various location-aware vision-language tasks, including referring localization, location-conditioned captioning, and dense object captioning, archiving state-of-the-art performance on RefCOCO and Visual Genome. Project page: https://jerryxu.net/PixelLLM .",
        "page": "http://arxiv.org/abs/2312.09237",
        "pdf": "http://arxiv.org/pdf/2312.09237.pdf"
    },
    {
        "title": "Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection",
        "author": "Zhiyuan Yan, Yuhao Luo, Siwei Lyu, Qingshan Liu, Baoyuan Wu",
        "abstract": "Deepfake detection faces a critical generalization hurdle, with performance deteriorating when there is a mismatch between the distributions of training and testing data. A broadly received explanation is the tendency of these detectors to be overfitted to forgery-specific artifacts, rather than learning features that are widely applicable across various forgeries. To address this issue, we propose a simple yet effective detector called LSDA (\\underline{L}atent \\underline{S}pace \\underline{D}ata \\underline{A}ugmentation), which is based on a heuristic idea: representations with a wider variety of forgeries should be able to learn a more generalizable decision boundary, thereby mitigating the overfitting of method-specific features (see Fig.~\\ref{fig:toy}). Following this idea, we propose to enlarge the forgery space by constructing and simulating variations within and across forgery features in the latent space. This approach encompasses the acquisition of enriched, domain-specific features and the facilitation of smoother transitions between different forgery types, effectively bridging domain gaps. Our approach culminates in refining a binary classifier that leverages the distilled knowledge from the enhanced features, striving for a generalizable deepfake detector. Comprehensive experiments show that our proposed method is surprisingly effective and transcends state-of-the-art detectors across several widely used benchmarks.",
        "page": "http://arxiv.org/abs/2311.11278",
        "pdf": "http://arxiv.org/pdf/2311.11278.pdf"
    },
    {
        "title": "PFStorer: Personalized Face Restoration and Super-Resolution",
        "author": "Tuomas Varanka, Tapani Toivonen, Soumya Tripathy, Guoying Zhao, Erman Acar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adapters Strike Back",
        "author": "Jan-Martin Steitz, Stefan Roth",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Eclipse: Disambiguating Illumination and Materials using Unintended Shadows",
        "author": "Dor Verbin, Ben Mildenhall, Peter Hedman, Jonathan T. Barron, Todd Zickler, Pratul P. Srinivasan",
        "abstract": "Decomposing an object's appearance into representations of its materials and the surrounding illumination is difficult, even when the object's 3D shape is known beforehand. This problem is especially challenging for diffuse objects: it is ill-conditioned because diffuse materials severely blur incoming light, and it is ill-posed because diffuse materials under high-frequency lighting can be indistinguishable from shiny materials under low-frequency lighting. We show that it is possible to recover precise materials and illumination -- even from diffuse objects -- by exploiting unintended shadows, like the ones cast onto an object by the photographer who moves around it. These shadows are a nuisance in most previous inverse rendering pipelines, but here we exploit them as signals that improve conditioning and help resolve material-lighting ambiguities. We present a method based on differentiable Monte Carlo ray tracing that uses images of an object to jointly recover its spatially-varying materials, the surrounding illumination environment, and the shapes of the unseen light occluders who inadvertently cast shadows upon it.",
        "page": "http://arxiv.org/abs/2305.16321",
        "pdf": "http://arxiv.org/pdf/2305.16321.pdf"
    },
    {
        "title": "ASAM: Boosting Segment Anything Model with Adversarial Tuning",
        "author": "Bo Li, Haoke Xiao, Lv Tang",
        "abstract": "In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This paper introduces ASAM, a novel methodology that amplifies SAM's performance through adversarial tuning. We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in https://asam2024.github.io/.",
        "page": "http://arxiv.org/abs/2405.00256",
        "pdf": "http://arxiv.org/pdf/2405.00256.pdf"
    },
    {
        "title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects",
        "author": "Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach",
        "author": "Beichen Zhang, Xiaoxing Wang, Xiaohan Qin, Junchi Yan",
        "abstract": "Supernet is a core component in many recent Neural Architecture Search (NAS) methods. It not only helps embody the search space but also provides a (relative) estimation of the final performance of candidate architectures. Thus, it is critical that the top architectures ranked by a supernet should be consistent with those ranked by true performance, which is known as the order-preserving ability. In this work, we analyze the order-preserving ability on the whole search space (global) and a sub-space of top architectures (local), and empirically show that the local order-preserving for current two-stage NAS methods still need to be improved. To rectify this, we propose a novel concept of Supernet Shifting, a refined search strategy combining architecture searching with supernet fine-tuning. Specifically, apart from evaluating, the training loss is also accumulated in searching and the supernet is updated every iteration. Since superior architectures are sampled more frequently in evolutionary searching, the supernet is encouraged to focus on top architectures, thus improving local order-preserving. Besides, a pre-trained supernet is often un-reusable for one-shot methods. We show that Supernet Shifting can fulfill transferring supernet to a new dataset. Specifically, the last classifier layer will be unset and trained through evolutionary searching. Comprehensive experiments show that our method has better order-preserving ability and can find a dominating architecture. Moreover, the pre-trained supernet can be easily transferred into a new dataset with no loss of performance.",
        "page": "http://arxiv.org/abs/2403.11380",
        "pdf": "http://arxiv.org/pdf/2403.11380.pdf"
    },
    {
        "title": "ScanFormer: Referring Expression Comprehension by Iteratively Scanning",
        "author": "Wei Su, Peihan Miao, Huanzhang Dou, Xi Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text",
        "author": "Junshu Tang, Yanhong Zeng, Ke Fan, Xuheng Wang, Bo Dai, Kai Chen, Lizhuang Ma",
        "abstract": "Creating and animating 3D biped cartoon characters is crucial and valuable in various applications. Compared with geometry, the diverse texture design plays an important role in making 3D biped cartoon characters vivid and charming. Therefore, we focus on automatic texture design for cartoon characters based on input instructions. This is challenging for domain-specific requirements and a lack of high-quality data. To address this challenge, we propose Make-It-Vivid, the first attempt to enable high-quality texture generation from text in UV space. We prepare a detailed text-texture paired data for 3D characters by using vision-question-answering agents. Then we customize a pretrained text-to-image model to generate texture map with template structure while preserving the natural 2D image knowledge. Furthermore, to enhance fine-grained details, we propose a novel adversarial learning scheme to shorten the domain gap between original dataset and realistic texture domain. Extensive experiments show that our approach outperforms current texture generation methods, resulting in efficient character texturing and faithful generation with prompts. Besides, we showcase various applications such as out of domain generation and texture stylization. We also provide an efficient generation system for automatic text-guided textured character generation and animation.",
        "page": "http://arxiv.org/abs/2403.16897",
        "pdf": "http://arxiv.org/pdf/2403.16897.pdf"
    },
    {
        "title": "Exploiting Diffusion Prior for Generalizable Dense Prediction",
        "author": "Hsin-Ying Lee, Hung-Yu Tseng, Hsin-Ying Lee, Ming-Hsuan Yang",
        "abstract": "Contents generated by recent advanced Text-to-Image (T2I) diffusion models are sometimes too imaginative for existing off-the-shelf dense predictors to estimate due to the immitigable domain gap. We introduce DMP, a pipeline utilizing pre-trained T2I models as a prior for dense prediction tasks. To address the misalignment between deterministic prediction tasks and stochastic T2I models, we reformulate the diffusion process through a sequence of interpolations, establishing a deterministic mapping between input RGB images and output prediction distributions. To preserve generalizability, we use low-rank adaptation to fine-tune pre-trained models. Extensive experiments across five tasks, including 3D property estimation, semantic segmentation, and intrinsic image decomposition, showcase the efficacy of the proposed method. Despite limited-domain training data, the approach yields faithful estimations for arbitrary images, surpassing existing state-of-the-art algorithms.",
        "page": "http://arxiv.org/abs/2311.18832",
        "pdf": "http://arxiv.org/pdf/2311.18832.pdf"
    },
    {
        "title": "ElasticDiffusion: Training-free Arbitrary Size Image Generation",
        "author": "Moayed Haji Ali, Guha Balakrishnan, Vicente Ordonez",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Uncertainty Visualization via Low-Dimensional Posterior Projections",
        "author": "Omer Yair, Tomer Michaeli, Elias Nehme",
        "abstract": "In ill-posed inverse problems, it is commonly desirable to obtain insight into the full spectrum of plausible solutions, rather than extracting only a single reconstruction. Information about the plausible solutions and their likelihoods is encoded in the posterior distribution. However, for high-dimensional data, this distribution is challenging to visualize. In this work, we introduce a new approach for estimating and visualizing posteriors by employing energy-based models (EBMs) over low-dimensional subspaces. Specifically, we train a conditional EBM that receives an input measurement and a set of directions that span some low-dimensional subspace of solutions, and outputs the probability density function of the posterior within that space. We demonstrate the effectiveness of our method across a diverse range of datasets and image restoration problems, showcasing its strength in uncertainty quantification and visualization. As we show, our method outperforms a baseline that projects samples from a diffusion-based posterior sampler, while being orders of magnitude faster. Furthermore, it is more accurate than a baseline that assumes a Gaussian posterior.",
        "page": "http://arxiv.org/abs/2312.07804",
        "pdf": "http://arxiv.org/pdf/2312.07804.pdf"
    },
    {
        "title": "Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval",
        "author": "Young Kyun Jang, Donghyun Kim, Zihang Meng, Dat Huynh, Ser-Nam Lim",
        "abstract": "Composed Image Retrieval (CIR) is a task that retrieves images similar to a query, based on a provided textual modification. Current techniques rely on supervised learning for CIR models using labeled triplets of the reference image, text, target image. These specific triplets are not as commonly available as simple image-text pairs, limiting the widespread use of CIR and its scalability. On the other hand, zero-shot CIR can be relatively easily trained with image-caption pairs without considering the image-to-image relation, but this approach tends to yield lower accuracy. We propose a new semi-supervised CIR approach where we search for a reference and its related target images in auxiliary data and learn our large language model-based Visual Delta Generator (VDG) to generate text describing the visual difference (i.e., visual delta) between the two. VDG, equipped with fluent language knowledge and being model agnostic, can generate pseudo triplets to boost the performance of CIR models. Our approach significantly improves the existing supervised learning approaches and achieves state-of-the-art results on the CIR benchmarks.",
        "page": "http://arxiv.org/abs/2404.15516",
        "pdf": "http://arxiv.org/pdf/2404.15516.pdf"
    },
    {
        "title": "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs",
        "author": "Penghao Wu, Saining Xie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Real-Time Neural BRDF with Spherically Distributed Primitives",
        "author": "Yishun Dou, Zhong Zheng, Qiaoqiao Jin, Bingbing Ni, Yugang Chen, Junxiang Ke",
        "abstract": "We propose a novel compact and efficient neural BRDF offering highly versatile material representation, yet with very-light memory and neural computation consumption towards achieving real-time rendering. The results in Figure 1, rendered at full HD resolution on a current desktop machine, show that our system achieves real-time rendering with a wide variety of appearances, which is approached by the following two designs. On the one hand, noting that bidirectional reflectance is distributed in a very sparse high-dimensional subspace, we propose to project the BRDF into two low-dimensional components, i.e., two hemisphere feature-grids for incoming and outgoing directions, respectively. On the other hand, learnable neural reflectance primitives are distributed on our highly-tailored spherical surface grid, which offer informative features for each component and alleviate the conventional heavy feature learning network to a much smaller one, leading to very fast evaluation. These primitives are centrally stored in a codebook and can be shared across multiple grids and even across materials, based on the low-cost indices stored in material-specific spherical surface grids. Our neural BRDF, which is agnostic to the material, provides a unified framework that can represent a variety of materials in consistent manner. Comprehensive experimental results on measured BRDF compression, Monte Carlo simulated BRDF acceleration, and extension to spatially varying effect demonstrate the superior quality and generalizability achieved by the proposed scheme.",
        "page": "http://arxiv.org/abs/2310.08332",
        "pdf": "http://arxiv.org/pdf/2310.08332.pdf"
    },
    {
        "title": "RCL: Reliable Continual Learning for Unified Failure Detection",
        "author": "Fei Zhu, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu, Zhaoxiang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models",
        "author": "Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James Rehg, Pinar Yanardag",
        "abstract": "Recent advancements in diffusion-based models have demonstrated significant success in generating images from text. However, video editing models have not yet reached the same level of visual quality and user control. To address this, we introduce RAVE, a zero-shot video editing method that leverages pre-trained text-to-image diffusion models without additional training. RAVE takes an input video and a text prompt to produce high-quality videos while preserving the original motion and semantic structure. It employs a novel noise shuffling strategy, leveraging spatio-temporal interactions between frames, to produce temporally consistent videos faster than existing methods. It is also efficient in terms of memory requirements, allowing it to handle longer videos. RAVE is capable of a wide range of edits, from local attribute modifications to shape transformations. In order to demonstrate the versatility of RAVE, we create a comprehensive video evaluation dataset ranging from object-focused scenes to complex human activities like dancing and typing, and dynamic scenes featuring swimming fish and boats. Our qualitative and quantitative experiments highlight the effectiveness of RAVE in diverse video editing scenarios compared to existing methods. Our code, dataset and videos can be found in https://rave-video.github.io.",
        "page": "http://arxiv.org/abs/2312.04524",
        "pdf": "http://arxiv.org/pdf/2312.04524.pdf"
    },
    {
        "title": "Geometry Transfer for Stylizing Radiance Fields",
        "author": "Hyunyoung Jung, Seonghyeon Nam, Nikolaos Sarafianos, Sungjoo Yoo, Alexander Sorkine-Hornung, Rakesh Ranjan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diffusion Model Alignment Using Direct Preference Optimization",
        "author": "Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik",
        "abstract": "Large language models (LLMs) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback (RLHF) methods to make them better aligned with users' preferences. In contrast to LLMs, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-DPO, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-DPO is adapted from the recently developed Direct Preference Optimization (DPO), a simpler alternative to RLHF which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate DPO to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our fine-tuned base model significantly outperforms both base SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses AI feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.",
        "page": "http://arxiv.org/abs/2311.12908",
        "pdf": "http://arxiv.org/pdf/2311.12908.pdf"
    },
    {
        "title": "Sieve: Multimodal Dataset Pruning using Image-Captioning Models",
        "author": "Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, Ari Morcos",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AMU-Tuning: Learning Effective Bias for CLIP-based Few-shot Classification",
        "author": "Yuwei Tang, ZhenYi Lin, Qilong Wang, Pengfei Zhu, Qinghua Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Not All Voxels Are Equal: Hardness-Aware Semantic Scene Completion with Self-Distillation",
        "author": "Song Wang, Jiawei Yu, Wentong Li, Wenyu Liu, Xiaolu Liu, Junbo Chen, Jianke Zhu",
        "abstract": "Semantic scene completion, also known as semantic occupancy prediction, can provide dense geometric and semantic information for autonomous vehicles, which attracts the increasing attention of both academia and industry. Unfortunately, existing methods usually formulate this task as a voxel-wise classification problem and treat each voxel equally in 3D space during training. As the hard voxels have not been paid enough attention, the performance in some challenging regions is limited. The 3D dense space typically contains a large number of empty voxels, which are easy to learn but require amounts of computation due to handling all the voxels uniformly for the existing models. Furthermore, the voxels in the boundary region are more challenging to differentiate than those in the interior. In this paper, we propose HASSC approach to train the semantic scene completion model with hardness-aware design. The global hardness from the network optimization process is defined for dynamical hard voxel selection. Then, the local hardness with geometric anisotropy is adopted for voxel-wise refinement. Besides, self-distillation strategy is introduced to make training process stable and consistent. Extensive experiments show that our HASSC scheme can effectively promote the accuracy of the baseline model without incurring the extra inference cost. Source code is available at: https://github.com/songw-zju/HASSC.",
        "page": "http://arxiv.org/abs/2404.11958",
        "pdf": "http://arxiv.org/pdf/2404.11958.pdf"
    },
    {
        "title": "Towards Fairness-Aware Adversarial Learning",
        "author": "Yanghao Zhang, Tianle Zhang, Ronghui Mu, Ronghui Mu, Xiaowei Huang, Wenjie Ruan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Retrieval-Augmented Egocentric Video Captioning",
        "author": "Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie",
        "abstract": "Understanding human actions from videos of first-person view poses significant challenges. Most prior approaches explore representation learning on egocentric videos only, while overlooking the potential benefit of exploiting existing large-scale third-person videos. In this paper, (1) we develop EgoInstructor, a retrieval-augmented multimodal captioning model that automatically retrieves semantically relevant third-person instructional videos to enhance the video captioning of egocentric videos. (2) For training the cross-view retrieval module, we devise an automatic pipeline to discover ego-exo video pairs from distinct large-scale egocentric and exocentric datasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE loss that pulls egocentric and exocentric video features closer by aligning them to shared text features that describe similar actions. (4) Through extensive experiments, our cross-view retrieval module demonstrates superior performance across seven benchmarks. Regarding egocentric video captioning, EgoInstructor exhibits significant improvements by leveraging third-person videos as references.",
        "page": "http://arxiv.org/abs/2401.00789",
        "pdf": "http://arxiv.org/pdf/2401.00789.pdf"
    },
    {
        "title": "Low-Rank Knowledge Decomposition for Medical Foundation Models",
        "author": "Yuhang Zhou, Haolin li, Siyuan Du, Jiangchao Yao, Ya Zhang, Yanfeng Wang",
        "abstract": "The popularity of large-scale pre-training has promoted the development of medical foundation models. However, some studies have shown that although foundation models exhibit strong general feature extraction capabilities, their performance on specific tasks is still inferior to task-specific methods. In this paper, we explore a new perspective called ``Knowledge Decomposition'' to improve the performance on specific medical tasks, which deconstruct the foundation model into multiple lightweight expert models, each dedicated to a particular task, with the goal of improving specialization while concurrently mitigating resource expenditure. To accomplish the above objective, we design a novel framework named Low-Rank Knowledge Decomposition (LoRKD), which explicitly separates graidents by incorporating low-rank expert modules and the efficient knowledge separation convolution. Extensive experimental results demonstrate that the decomposed models perform well in terms of performance and transferability, even surpassing the original foundation models.",
        "page": "http://arxiv.org/abs/2404.17184",
        "pdf": "http://arxiv.org/pdf/2404.17184.pdf"
    },
    {
        "title": "FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models",
        "author": "Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nie\u00dfner",
        "abstract": "We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the NPHM shape space. Our experimental results substantiate the effectiveness of FaceTalk, consistently achieving superior and visually natural motion, encompassing diverse facial expressions and styles, outperforming existing methods by 75% in perceptual user study evaluation.",
        "page": "http://arxiv.org/abs/2312.08459",
        "pdf": "http://arxiv.org/pdf/2312.08459.pdf"
    },
    {
        "title": "CPR: Retrieval Augmented Generation for Copyright Protection",
        "author": "Aditya Golatkar, Alessandro Achille, Luca Zancato, Yu-Xiang Wang, Ashwin Swaminathan, Stefano Soatto, Stefano Soatto",
        "abstract": "Retrieval Augmented Generation (RAG) is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient machine unlearning at scale. However, RAG techniques for image generation may lead to parts of the retrieved samples being copied in the model's output. To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with Retrieval (CPR), a new method for RAG with strong copyright protection guarantees in a mixed-private setting for diffusion models.CPR allows to condition the output of diffusion models on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs. In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their diffusion scores at inference. We prove that CPR satisfies Near Access Freeness (NAF) which bounds the amount of information an attacker may be able to extract from the generated images. We provide two algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously proposed rejection-sampling-based NAF methods, our methods enable efficient copyright-protected sampling with a single run of backward diffusion. We show that our method can be applied to any pre-trained conditional diffusion model, such as Stable Diffusion or unCLIP. In particular, we empirically show that applying CPR on top of unCLIP improves quality and text-to-image alignment of the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit attribution, copy-right protection, and deterministic, constant time, unlearning.",
        "page": "http://arxiv.org/abs/2403.18920",
        "pdf": "http://arxiv.org/pdf/2403.18920.pdf"
    },
    {
        "title": "Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding",
        "author": "Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, Zuxuan Wu",
        "abstract": "Vision language models (VLM) have demonstrated remarkable performance across various downstream tasks. However, understanding fine-grained visual-linguistic concepts, such as attributes and inter-object relationships, remains a significant challenge. While several benchmarks aim to evaluate VLMs in finer granularity, their primary focus remains on the linguistic aspect, neglecting the visual dimension. Here, we highlight the importance of evaluating VLMs from both a textual and visual perspective. We introduce a progressive pipeline to synthesize images that vary in a specific attribute while ensuring consistency in all other aspects. Utilizing this data engine, we carefully design a benchmark, SPEC, to diagnose the comprehension of object size, position, existence, and count. Subsequently, we conduct a thorough evaluation of four leading VLMs on SPEC. Surprisingly, their performance is close to random guess, revealing significant limitations. With this in mind, we propose a simple yet effective approach to optimize VLMs in fine-grained understanding, achieving significant improvements on SPEC without compromising the zero-shot performance. Results on two additional fine-grained benchmarks also show consistent improvements, further validating the transferability of our approach. Code and data are available at https://github.com/wjpoom/SPEC.",
        "page": "http://arxiv.org/abs/2312.00081",
        "pdf": "http://arxiv.org/pdf/2312.00081.pdf"
    },
    {
        "title": "DeIl: Direct and Inverse CLIP for Open-World Few-Shot Learning",
        "author": "Shuai Shao, Yu Bai, Yan WANG, Bao-di Liu, Yicong Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FedAS: Bridging Inconsistency in Personalized Federated Learning",
        "author": "Xiyuan Yang, Wenke Huang, Mang Ye",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GPT4Point: A Unified Framework for Point-Language Understanding and Generation",
        "author": "Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao",
        "abstract": "Multimodal Large Language Models (MLLMs) have excelled in 2D image-text comprehension and image generation, but their understanding of the 3D world is notably deficient, limiting progress in 3D language understanding and generation. To solve this problem, we introduce GPT4Point, an innovative groundbreaking point-language multimodal model designed specifically for unified 3D object understanding and generation within the MLLM framework. GPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text reference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point is equipped with advanced capabilities for controllable 3D generation, it can get high-quality results through a low-quality point-text feature maintaining the geometric shapes and colors. To support the expansive needs of 3D object-text pairs, we develop Pyramid-XL, a point-language dataset annotation engine. It constructs a large-scale database over 1M objects of varied text granularity levels from the Objaverse-XL dataset, essential for training GPT4Point. A comprehensive benchmark has been proposed to evaluate 3D point-language understanding capabilities. In extensive evaluations, GPT4Point has demonstrated superior performance in understanding and generation.",
        "page": "http://arxiv.org/abs/2312.02980",
        "pdf": "http://arxiv.org/pdf/2312.02980.pdf"
    },
    {
        "title": "Rendering Every Pixel for High-Fidelity Geometry in 3D GANs",
        "author": "Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Residual Learning in Diffusion Models",
        "author": "Junyu Zhang, Daochang Liu, Eunbyung Park, Shichao Zhang, Chang Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains",
        "author": "Bang-Dang Pham, Phong Tran, Anh Tran, Cuong Pham, Rang Nguyen, Minh Hoai",
        "abstract": "This paper presents an innovative framework designed to train an image deblurring algorithm tailored to a specific camera device. This algorithm works by transforming a blurry input image, which is challenging to deblur, into another blurry image that is more amenable to deblurring. The transformation process, from one blurry state to another, leverages unpaired data consisting of sharp and blurry images captured by the target camera device. Learning this blur-to-blur transformation is inherently simpler than direct blur-to-sharp conversion, as it primarily involves modifying blur patterns rather than the intricate task of reconstructing fine image details. The efficacy of the proposed approach has been demonstrated through comprehensive experiments on various benchmarks, where it significantly outperforms state-of-the-art methods both quantitatively and qualitatively. Our code and data are available at https://zero1778.github.io/blur2blur/",
        "page": "http://arxiv.org/abs/2403.16205",
        "pdf": "http://arxiv.org/pdf/2403.16205.pdf"
    },
    {
        "title": "FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization",
        "author": "Jiahui Zhang, Fangneng Zhan, MUYU XU, Shijian Lu, Eric P. Xing",
        "abstract": "3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently.",
        "page": "http://arxiv.org/abs/2403.06908",
        "pdf": "http://arxiv.org/pdf/2403.06908.pdf"
    },
    {
        "title": "PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns",
        "author": "Shuliang Ning, Duomin Wang, Yipeng Qin, Zirong Jin, Baoyuan Wang, Xiaoguang Han",
        "abstract": "In this paper, we propose a novel virtual try-on from unconstrained designs (ucVTON) task to enable photorealistic synthesis of personalized composite clothing on input human images. Unlike prior arts constrained by specific input types, our method allows flexible specification of style (text or image) and texture (full garment, cropped sections, or texture patches) conditions. To address the entanglement challenge when using full garment images as conditions, we develop a two-stage pipeline with explicit disentanglement of style and texture. In the first stage, we generate a human parsing map reflecting the desired style conditioned on the input. In the second stage, we composite textures onto the parsing map areas based on the texture input. To represent complex and non-stationary textures that have never been achieved in previous fashion editing works, we first propose extracting hierarchical and balanced CLIP features and applying position encoding in VTON. Experiments demonstrate superior synthesis quality and personalization enabled by our method. The flexible control over style and texture mixing brings virtual try-on to a new level of user experience for online shopping and fashion design.",
        "page": "http://arxiv.org/abs/2312.04534",
        "pdf": "http://arxiv.org/pdf/2312.04534.pdf"
    },
    {
        "title": "Revisiting Sampson Approximations for Geometric Estimation Problems",
        "author": "Felix Rydell, Angelica Torres, Viktor Larsson",
        "abstract": "Many problems in computer vision can be formulated as geometric estimation problems, i.e. given a collection of measurements (e.g. point correspondences) we wish to fit a model (e.g. an essential matrix) that agrees with our observations. This necessitates some measure of how much an observation ``agrees\" with a given model. A natural choice is to consider the smallest perturbation that makes the observation exactly satisfy the constraints. However, for many problems, this metric is expensive or otherwise intractable to compute. The so-called Sampson error approximates this geometric error through a linearization scheme. For epipolar geometry, the Sampson error is a popular choice and in practice known to yield very tight approximations of the corresponding geometric residual (the reprojection error). In this paper we revisit the Sampson approximation and provide new theoretical insights as to why and when this approximation works, as well as provide explicit bounds on the tightness under some mild assumptions. Our theoretical results are validated in several experiments on real data and in the context of different geometric estimation tasks.",
        "page": "http://arxiv.org/abs/2401.07114",
        "pdf": "http://arxiv.org/pdf/2401.07114.pdf"
    },
    {
        "title": "Neural 3D Strokes: Creating Stylized 3D Scenes with Vectorized 3D Strokes",
        "author": "Haobin Duan, Miao Wang, Yanxun Li, Yong-Liang Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception",
        "author": "Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Bin Luo, Jun-Yan He, Jin-Peng Lan, Xuansong Xie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Flexible Depth Completion for Sparse and Varying Point Densities",
        "author": "Jinhyung Park, Yu-Jhe Li, Kris Kitani",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Generalization via Meta-Learning on Hard Samples",
        "author": "Nishant Jain, Arun Suggala, Pradeep Shenoy",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes",
        "author": "Chi-Hsi Kung, \u66f8\u7def \u5442, Yi-Hsuan Tsai, Yi-Ting Chen",
        "abstract": "In this paper, we study multi-label atomic activity recognition. Despite the notable progress in action recognition, it is still challenging to recognize atomic activities due to a deficiency in a holistic understanding of both multiple road users' motions and their contextual information. In this paper, we introduce Action-slot, a slot attention-based approach that learns visual action-centric representations, capturing both motion and contextual information. Our key idea is to design action slots that are capable of paying attention to regions where atomic activities occur, without the need for explicit perception guidance. To further enhance slot attention, we introduce a background slot that competes with action slots, aiding the training process in avoiding unnecessary focus on background regions devoid of activities. Yet, the imbalanced class distribution in the existing dataset hampers the assessment of rare activities. To address the limitation, we collect a synthetic dataset called TACO, which is four times larger than OATS and features a balanced distribution of atomic activities. To validate the effectiveness of our method, we conduct comprehensive experiments and ablation studies against various action recognition baselines. We also show that the performance of multi-label atomic activity recognition on real-world datasets can be improved by pretraining representations on TACO. We will release our source code and dataset. See the videos of visualization on the project page: https://hcis-lab.github.io/Action-slot/",
        "page": "http://arxiv.org/abs/2311.17948",
        "pdf": "http://arxiv.org/pdf/2311.17948.pdf"
    },
    {
        "title": "CLIP as RNN:  Segment Countless Visual Concepts without Training Endeavor",
        "author": "Shuyang Sun, Runjia Li, Philip H.S. Torr, Xiuye Gu, Siyang Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition",
        "author": "Zhonglin Sun, Chen Feng, Ioannis Patras, Georgios Tzimiropoulos",
        "abstract": "In this work we focus on learning facial representations that can be adapted to train effective face recognition models, particularly in the absence of labels. Firstly, compared with existing labelled face datasets, a vastly larger magnitude of unlabeled faces exists in the real world. We explore the learning strategy of these unlabeled facial images through self-supervised pretraining to transfer generalized face recognition performance. Moreover, motivated by one recent finding, that is, the face saliency area is critical for face recognition, in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks. This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition. We also incorporate two landmark-specific augmentations which introduce more diversity of landmark information to further regularize the learning. With learned landmark-based facial representations, we further adapt the representation for face recognition with regularization mitigating variations in landmark positions. Our method achieves significant improvement over the state-of-the-art on multiple face recognition benchmarks, especially on more challenging few-shot scenarios.",
        "page": "http://arxiv.org/abs/2403.08161",
        "pdf": "http://arxiv.org/pdf/2403.08161.pdf"
    },
    {
        "title": "SinSR: Diffusion-Based Image Super-Resolution in a Single Step",
        "author": "Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex C. Kot, Bihan Wen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF",
        "author": "Jie Long Lee, Chen Li, Gim Hee Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Relightable and Animatable Neural Avatar from Sparse-View Video",
        "author": "Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, Xiaowei Zhou",
        "abstract": "This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination. Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem. Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting. While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering. To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses. Specifically, we estimate coarse distances based on a parametric human model and compute fine distances by exploiting the local deformation invariance of SDF. Based on the HDQ algorithm, we leverage sphere tracing to efficiently estimate the surface intersection and light visibility. This allows us to develop the first system to recover animatable and relightable neural avatars from sparse view (or monocular) inputs. Experiments demonstrate that our approach is able to produce superior results compared to state-of-the-art methods. Our code will be released for reproducibility.",
        "page": "http://arxiv.org/abs/2308.07903",
        "pdf": "http://arxiv.org/pdf/2308.07903.pdf"
    },
    {
        "title": "VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction",
        "author": "Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang",
        "abstract": "Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.",
        "page": "http://arxiv.org/abs/2402.17427",
        "pdf": "http://arxiv.org/pdf/2402.17427.pdf"
    },
    {
        "title": "WANDR: Intention-guided Human Motion Generation",
        "author": "Markos Diomataris, Nikos Athanasiou, Omid Taheri, Xi Wang, Otmar Hilliges, Michael J. Black",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering",
        "author": "Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, Bo Dai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GART: Gaussian Articulated Template Models",
        "author": "Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, Kostas Daniilidis",
        "abstract": "We introduce Gaussian Articulated Template Model GART, an explicit, efficient, and expressive representation for non-rigid articulated subject capturing and rendering from monocular videos. GART utilizes a mixture of moving 3D Gaussians to explicitly approximate a deformable subject's geometry and appearance. It takes advantage of a categorical template model prior (SMPL, SMAL, etc.) with learnable forward skinning while further generalizing to more complex non-rigid deformations with novel latent bones. GART can be reconstructed via differentiable rendering from monocular videos in seconds or minutes and rendered in novel poses faster than 150fps.",
        "page": "http://arxiv.org/abs/2311.16099",
        "pdf": "http://arxiv.org/pdf/2311.16099.pdf"
    },
    {
        "title": "Learning from Observer Gaze: Zero-shot Attention Prediction Oriented by Human-Object Interaction Recognition",
        "author": "Yuchen Zhou, Linkai Liu, Chao Gou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Anchor-based Robust Finetuning of Vision-Language Models",
        "author": "Jinwei Han, Zhiwen Lin, Zhongyisun Sun, Yingguo Gao, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D",
        "author": "Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, Niloy J. Mitra",
        "abstract": "Diffusion Handles is a novel approach to enabling 3D object edits on diffusion images. We accomplish these edits using existing pre-trained diffusion models, and 2D image depth estimation, without any fine-tuning or 3D object retrieval. The edited results remain plausible, photo-real, and preserve object identity. Diffusion Handles address a critically missing facet of generative image based creative design, and significantly advance the state-of-the-art in generative image editing. Our key insight is to lift diffusion activations for an object to 3D using a proxy depth, 3D-transform the depth and associated activations, and project them back to image space. The diffusion process applied to the manipulated activations with identity control, produces plausible edited images showing complex 3D occlusion and lighting effects. We evaluate Diffusion Handles: quantitatively, on a large synthetic data benchmark; and qualitatively by a user study, showing our output to be more plausible, and better than prior art at both, 3D editing and identity control. Project Webpage: https://diffusionhandles.github.io/",
        "page": "http://arxiv.org/abs/2312.02190",
        "pdf": "http://arxiv.org/pdf/2312.02190.pdf"
    },
    {
        "title": "DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization",
        "author": "Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, Seunggyu Chang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Language-driven All-in-one Adverse Weather Removal",
        "author": "Hao Yang, Liyuan Pan, Yan Yang, Wei Liang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Volumetric Environment Representation for Vision-Language Navigation",
        "author": "Liu, Wenguan Wang, Yi Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CG-HOI: Contact-Guided 3D Human-Object Interaction Generation",
        "author": "Christian Diller, Angela Dai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Contrastive Mean-Shift Learning for Generalized Category Discovery",
        "author": "Sua Choi, Dahyun Kang, Minsu Cho",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?",
        "author": "Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen",
        "abstract": "Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure, however, when fewer known views are given (i.e., few-shot view synthesis), the model is prone to overfit the given views. To handle this issue, previous efforts have been made towards leveraging learned priors or introducing additional regularizations. In contrast, in this paper, we for the first time provide an orthogonal method from the perspective of network structure. Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue, but at the cost of missing details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e., location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis. To further reduce the artifacts, we propose to model colors and volume density separately and present two regularization terms. Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement, it is surprisingly effective as it boosts the PSNR of the baseline from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art results on a wide range of benchmarks. We will release the code upon publication.",
        "page": "http://arxiv.org/abs/2403.06092",
        "pdf": "http://arxiv.org/pdf/2403.06092.pdf"
    },
    {
        "title": "Iterated Learning Improves Compositionality in Large Vision-Language Models",
        "author": "Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, Ranjay Krishna",
        "abstract": "A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, recent investigations find that most-if not all-our state-of-the-art vision-language models struggle at compositionality. They are unable to distinguish between images of \" a girl in white facing a man in black\" and \"a girl in black facing a man in white\". Moreover, prior work suggests that compositionality doesn't arise with scale: larger model sizes or training data don't help. This paper develops a new iterated training algorithm that incentivizes compositionality. We draw on decades of cognitive science research that identifies cultural transmission-the need to teach a new generation-as a necessary inductive prior that incentivizes humans to develop compositional languages. Specifically, we reframe vision-language contrastive learning as the Lewis Signaling Game between a vision agent and a language agent, and operationalize cultural transmission by iteratively resetting one of the agent's weights during training. After every iteration, this training paradigm induces representations that become \"easier to learn\", a property of compositional languages: e.g. our model trained on CC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the SugarCrepe benchmark.",
        "page": "http://arxiv.org/abs/2404.02145",
        "pdf": "http://arxiv.org/pdf/2404.02145.pdf"
    },
    {
        "title": "Detours for Navigating Instructional Videos",
        "author": "Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman",
        "abstract": "We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.",
        "page": "http://arxiv.org/abs/2401.01823",
        "pdf": "http://arxiv.org/pdf/2401.01823.pdf"
    },
    {
        "title": "Domain Gap Embeddings for Generative Dataset Augmentation",
        "author": "Yinong Wang, Younjoon Chung, Chen Henry Wu, Fernando De la Torre",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TransLoc4D: Transformer-based 4D Radar Place Recognition",
        "author": "Guohao Peng, Heshan Li, Yangyang Zhao, Jun Zhang, Zhenyu Wu, Pengyu Zheng, Danwei Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification",
        "author": "Sravanti Addepalli, Ashish Asokan, Lakshay Sharma, R. Venkatesh Babu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Small Steps and Level Sets: Fitting Neural Surface Models with Point Guidance",
        "author": "Chamin Hewa Koneputugodage, Yizhak Ben-Shabat, Dylan Campbell, Stephen Gould",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Absolute Pose from One or Two Scaled and Oriented Features",
        "author": "Jonathan Ventura, Zuzana Kukelova, Torsten Sattler, Daniel Barath",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "\\emph{RealCustom}: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization",
        "author": "Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian HE, Yongdong Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Driving Everywhere with Large Language Model Policy Adaptation",
        "author": "Boyi Li, Yue Wang, Jiageng Mao, Boris Ivanovic, Sushant Veer, Karen Leung, Marco Pavone",
        "abstract": "Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.",
        "page": "http://arxiv.org/abs/2402.05932",
        "pdf": "http://arxiv.org/pdf/2402.05932.pdf"
    },
    {
        "title": "SANeRF-HQ: Segment Anything for NeRF in High Quality",
        "author": "Yichen Liu, Benran Hu, Chi-Keung Tang, Yu-Wing Tai",
        "abstract": "Recently, the Segment Anything Model (SAM) has showcased remarkable capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has gained popularity as a method for various 3D problems beyond novel view synthesis. Though there exist initial attempts to incorporate these two methods into 3D segmentation, they face the challenge of accurately and consistently segmenting objects in complex scenarios. In this paper, we introduce the Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high-quality 3D segmentation of any target object in a given scene. SANeRF-HQ utilizes SAM for open-world object segmentation guided by user-supplied prompts, while leveraging NeRF to aggregate information from different viewpoints. To overcome the aforementioned challenges, we employ density field and RGB similarity to enhance the accuracy of segmentation boundary during the aggregation. Emphasizing on segmentation accuracy, we evaluate our method on multiple NeRF datasets where high-quality ground-truths are available or manually annotated. SANeRF-HQ shows a significant quality improvement over state-of-the-art methods in NeRF object segmentation, provides higher flexibility for object localization, and enables more consistent object segmentation across multiple views. Results and code are available at the project site: https://lyclyc52.github.io/SANeRF-HQ/.",
        "page": "http://arxiv.org/abs/2312.01531",
        "pdf": "http://arxiv.org/pdf/2312.01531.pdf"
    },
    {
        "title": "APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic Segmentation",
        "author": "Weizhao He, Yang Zhang, Wei Zhuo, Linlin Shen, Jiaqi Yang, Songhe Deng, Liang Sun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "InstanceDiffusion: Instance-level Control for Image Generation",
        "author": "Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Shadow Generation for Composite Image Using Diffusion Model",
        "author": "Qingyang Liu, Junqi You, Jian-Ting Wang, Xinhao Tao, Bo Zhang, Li Niu",
        "abstract": "In the realm of image composition, generating realistic shadow for the inserted foreground remains a formidable challenge. Previous works have developed image-to-image translation models which are trained on paired training data. However, they are struggling to generate shadows with accurate shapes and intensities, hindered by data scarcity and inherent task complexity. In this paper, we resort to foundation model with rich prior knowledge of natural shadow images. Specifically, we first adapt ControlNet to our task and then propose intensity modulation modules to improve the shadow intensity. Moreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel data acquisition pipeline. Experimental results on both DESOBA and DESOBAv2 datasets as well as real composite images demonstrate the superior capability of our model for shadow generation task. The dataset, code, and model are released at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.",
        "page": "http://arxiv.org/abs/2403.15234",
        "pdf": "http://arxiv.org/pdf/2403.15234.pdf"
    },
    {
        "title": "DS-NeRV: Implicit Neural Video Representation with Decomposed Static and Dynamic Codes",
        "author": "Hao Yan, Zhihui Ke, Xiaobo Zhou, Tie Qiu, Xidong Shi, DaDong Jiang",
        "abstract": "Implicit neural representations for video (NeRV) have recently become a novel way for high-quality video representation. However, existing works employ a single network to represent the entire video, which implicitly confuse static and dynamic information. This leads to an inability to effectively compress the redundant static information and lack the explicitly modeling of global temporal-coherent dynamic details. To solve above problems, we propose DS-NeRV, which decomposes videos into sparse learnable static codes and dynamic codes without the need for explicit optical flow or residual supervision. By setting different sampling rates for two codes and applying weighted sum and interpolation sampling methods, DS-NeRV efficiently utilizes redundant static information while maintaining high-frequency details. Additionally, we design a cross-channel attention-based (CCA) fusion module to efficiently fuse these two codes for frame decoding. Our approach achieves a high quality reconstruction of 31.2 PSNR with only 0.35M parameters thanks to separate static and dynamic codes representation and outperforms existing NeRV methods in many downstream tasks. Our project website is at https://haoyan14.github.io/DS-NeRV.",
        "page": "http://arxiv.org/abs/2403.15679",
        "pdf": "http://arxiv.org/pdf/2403.15679.pdf"
    },
    {
        "title": "Towards Transferable Targeted 3D Adversarial Attack in the Physical World",
        "author": "Yao Huang, Yinpeng Dong, Shouwei Ruan, Xiao Yang, Hang Su, Xingxing Wei",
        "abstract": "Compared with transferable untargeted attacks, transferable targeted adversarial attacks could specify the misclassification categories of adversarial samples, posing a greater threat to security-critical tasks. In the meanwhile, 3D adversarial samples, due to their potential of multi-view robustness, can more comprehensively identify weaknesses in existing deep learning systems, possessing great application value. However, the field of transferable targeted 3D adversarial attacks remains vacant. The goal of this work is to develop a more effective technique that could generate transferable targeted 3D adversarial examples, filling the gap in this field. To achieve this goal, we design a novel framework named TT3D that could rapidly reconstruct from few multi-view images into Transferable Targeted 3D textured meshes. While existing mesh-based texture optimization methods compute gradients in the high-dimensional mesh space and easily fall into local optima, leading to unsatisfactory transferability and distinct distortions, TT3D innovatively performs dual optimization towards both feature grid and Multi-layer Perceptron (MLP) parameters in the grid-based NeRF space, which significantly enhances black-box transferability while enjoying naturalness. Experimental results show that TT3D not only exhibits superior cross-model transferability but also maintains considerable adaptability across different renders and vision tasks. More importantly, we produce 3D adversarial examples with 3D printing techniques in the real world and verify their robust performance under various scenarios.",
        "page": "http://arxiv.org/abs/2312.09558",
        "pdf": "http://arxiv.org/pdf/2312.09558.pdf"
    },
    {
        "title": "Aligning Logits Generatively for Principled Black-Box Knowledge Distillation",
        "author": "Jing Ma, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li",
        "abstract": "Black-Box Knowledge Distillation (B2KD) is a formulated problem for cloud-to-edge model compression with invisible data and models hosted on the server. B2KD faces challenges such as limited Internet exchange and edge-cloud disparity of data distributions. In this paper, we formalize a two-step workflow consisting of deprivatization and distillation, and theoretically provide a new optimization direction from logits to cell boundary different from direct logits alignment. With its guidance, we propose a new method Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a lightweight one. Our method does not differentiate between treating soft or hard responses, and consists of: 1) deprivatization: emulating the inverse mapping of the teacher function with a generator, and 2) distillation: aligning low-dimensional logits of the teacher and student models by reducing the distance of high-dimensional image points. For different teacher-student pairs, our method yields inspiring distillation performance on various benchmarks, and outperforms the previous state-of-the-art approaches.",
        "page": "http://arxiv.org/abs/2205.10490",
        "pdf": "http://arxiv.org/pdf/2205.10490.pdf"
    },
    {
        "title": "ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers",
        "author": "Narges Norouzi, Svetlana Orlova, Daan de Geus, Gijs Dubbelman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed",
        "author": "Yifan Wang, Xingyi He, Sida Peng, Dongli Tan, Xiaowei Zhou",
        "abstract": "We present a novel method for efficiently producing semi-dense matches across images. Previous detector-free matcher LoFTR has shown remarkable matching capability in handling large-viewpoint change and texture-poor scenarios but suffers from low efficiency. We revisit its design choices and derive multiple improvements for both efficiency and accuracy. One key observation is that performing the transformer over the entire feature map is redundant due to shared local information, therefore we propose an aggregated attention mechanism with adaptive token selection for efficiency. Furthermore, we find spatial variance exists in LoFTR's fine correlation module, which is adverse to matching accuracy. A novel two-stage correlation layer is proposed to achieve accurate subpixel correspondences for accuracy improvement. Our efficiency optimized model is $\\sim 2.5\\times$ faster than LoFTR which can even surpass state-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue. Moreover, extensive experiments show that our method can achieve higher accuracy compared with competitive semi-dense matchers, with considerable efficiency benefits. This opens up exciting prospects for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction. Project page: https://zju3dv.github.io/efficientloftr.",
        "page": "http://arxiv.org/abs/2403.04765",
        "pdf": "http://arxiv.org/pdf/2403.04765.pdf"
    },
    {
        "title": "Language-guided Image Reflection Separation",
        "author": "Haofeng Zhong, Yuchen Hong, Shuchen Weng, Jinxiu Liang, Boxin Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models",
        "author": "Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen",
        "abstract": "Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles. While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text. In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning. To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model. A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame affinity as input to transfer appearance information guided by the affinity hint for individual frame synthesis in the latent space. This design mitigates the challenges of appearance-related image alignment within and allows for a stronger focus on aligning with motion-related guidance.",
        "page": "http://arxiv.org/abs/2312.13964",
        "pdf": "http://arxiv.org/pdf/2312.13964.pdf"
    },
    {
        "title": "Motion Diversification Networks",
        "author": "Hee Jae Kim, Eshed Ohn-Bar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
        "author": "Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R. Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, Stefano Soatto",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation",
        "author": "Jiahao Lu, Jiacheng Deng, Tianzhu Zhang",
        "abstract": "3D instance segmentation (3DIS) is a crucial task, but point-level annotations are tedious in fully supervised settings. Thus, using bounding boxes (bboxes) as annotations has shown great potential. The current mainstream approach is a two-step process, involving the generation of pseudo-labels from box annotations and the training of a 3DIS network with the pseudo-labels. However, due to the presence of intersections among bboxes, not every point has a determined instance label, especially in overlapping areas. To generate higher quality pseudo-labels and achieve more precise weakly supervised 3DIS results, we propose the Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation (BSNet), which devises a novel pseudo-labeler called Simulation-assisted Transformer. The labeler consists of two main components. The first is Simulation-assisted Mean Teacher, which introduces Mean Teacher for the first time in this task and constructs simulated samples to assist the labeler in acquiring prior knowledge about overlapping areas. To better model local-global structure, we also propose Local-Global Aware Attention as the decoder for teacher and student labelers. Extensive experiments conducted on the ScanNetV2 and S3DIS datasets verify the superiority of our designs. Code is available at \\href{https://github.com/peoplelu/BSNet}{https://github.com/peoplelu/BSNet}.",
        "page": "http://arxiv.org/abs/2403.15019",
        "pdf": "http://arxiv.org/pdf/2403.15019.pdf"
    },
    {
        "title": "Unlocking Pretrained Image Backbones for Semantic Image Synthesis",
        "author": "Tariq Berrada, Jakob Verbeek, camille couprie, Karteek Alahari",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D",
        "author": "Sangmin Woo, byeongjun park, Hyojun Go, Jin-Young Kim, Changick Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation",
        "author": "Jonas Herzog",
        "abstract": "Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.",
        "page": "http://arxiv.org/abs/2402.17614",
        "pdf": "http://arxiv.org/pdf/2402.17614.pdf"
    },
    {
        "title": "From Variance to Veracity: Unbundling and Mitigating Gradient Variance in Differentiable Bundle Adjustment Layers",
        "author": "Swaminathan Gurumurthy, Karnik Ram, Bingqing Chen, Zachary Manchester, Zico Kolter",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Image Restoration by Denoising Diffusion Models With Iteratively Preconditioned Guidance",
        "author": "Tomer Garber, Tom Tirer",
        "abstract": "Training deep neural networks has become a common approach for addressing image restoration problems. An alternative for training a \"task-specific\" network for each observation model is to use pretrained deep denoisers for imposing only the signal's prior within iterative algorithms, without additional training. Recently, a sampling-based variant of this approach has become popular with the rise of diffusion/score-based generative models. Using denoisers for general purpose restoration requires guiding the iterations to ensure agreement of the signal with the observations. In low-noise settings, guidance that is based on back-projection (BP) has been shown to be a promising strategy (used recently also under the names \"pseudoinverse\" or \"range/null-space\" guidance). However, the presence of noise in the observations hinders the gains from this approach. In this paper, we propose a novel guidance technique, based on preconditioning that allows traversing from BP-based guidance to least squares based guidance along the restoration scheme. The proposed approach is robust to noise while still having much simpler implementation than alternative methods (e.g., it does not require SVD or a large number of iterations). We use it within both an optimization scheme and a sampling-based scheme, and demonstrate its advantages over existing methods for image deblurring and super-resolution.",
        "page": "http://arxiv.org/abs/2312.16519",
        "pdf": "http://arxiv.org/pdf/2312.16519.pdf"
    },
    {
        "title": "Mean-Shift Feature Transformer",
        "author": "Takumi Kobayashi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "RegionGPT: Towards Region Understanding Vision Language Model",
        "author": "Qiushan Guo, Shalini De Mello, Danny Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, Sifei Liu",
        "abstract": "Vision language models (VLMs) have experienced rapid advancements through the integration of large language models (LLMs) with image-text pairs, yet they struggle with detailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse-grained training data that lacks detailed, region-specific captions. To address this, we introduce RegionGPT (short as RGPT), a novel framework designed for complex region-level captioning and understanding. RGPT enhances the spatial awareness of regional representation with simple yet effective modifications to existing visual encoders in VLMs. We further improve performance on tasks requiring a specific output scope by integrating task-guided instruction prompts during both training and inference phases, while maintaining the model's versatility for general-purpose tasks. Additionally, we develop an automated region caption data generation pipeline, enriching the training set with detailed region-level captions. We demonstrate that a universal RGPT model can be effectively applied and significantly enhancing performance across a range of region-level tasks, including but not limited to complex region descriptions, reasoning, object classification, and referring expressions comprehension.",
        "page": "http://arxiv.org/abs/2403.02330",
        "pdf": "http://arxiv.org/pdf/2403.02330.pdf"
    },
    {
        "title": "Unlocking the Potential of Pre-trained Vision Transformers for Few-Shot Semantic Segmentation through Relationship Descriptors",
        "author": "Ziqin Zhou, Hai-Ming Xu, Yangyang Shu, Lingqiao Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Relational Matching for Weakly Semi-Supervised Oriented Object Detection",
        "author": "Wenhao Wu, Hau San Wong, Si Wu, Tianyou Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "JointSQ: Joint Sparsification-Quantization for Distributed Learning",
        "author": "Weiying Xie, Haowei Li, Ma Jitao, Yunsong Li, Jie Lei, donglai Liu, Leyuan Fang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Endow SAM with Keen Eyes: Temporal-spatial Prompt Learning for Video Camouflaged Object Detection",
        "author": "Wenjun Hui, Zhenfeng Zhu, Shuai Zheng, Yao Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NICE: Neurogenesis Inspired Contextual Encoding for Replay-free Class Incremental Learning",
        "author": "Mustafa B Gurbuz, Jean Moorman, Constantine Dovrolis",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences",
        "author": "Axel Barroso-Laguna, Sowmya Munukutla, Victor Adrian Prisacariu, Eric Brachmann",
        "abstract": "Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences. Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale. Some applications, aiming at instant augmented reality anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale. We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space. By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements. Depth measurements are also not required for training, nor are scene reconstructions or image overlap information. MicKey is supervised only by pairs of images and their relative poses. MicKey achieves state-of-the-art performance on the Map-Free Relocalisation benchmark while requiring less supervision than competing approaches.",
        "page": "http://arxiv.org/abs/2404.06337",
        "pdf": "http://arxiv.org/pdf/2404.06337.pdf"
    },
    {
        "title": "Learning for Transductive Threshold Calibration in Open-World Recognition",
        "author": "Qin ZHANG, DONGSHENG An, Tianjun Xiao, Tong He, Qingming Tang, Ying Nian Wu, Joseph Tighe, Yifan Xing",
        "abstract": "In deep metric learning for visual recognition, the calibration of distance thresholds is crucial for achieving desired model performance in the true positive rates (TPR) or true negative rates (TNR). However, calibrating this threshold presents challenges in open-world scenarios, where the test classes can be entirely disjoint from those encountered during training. We define the problem of finding distance thresholds for a trained embedding model to achieve target performance metrics over unseen open-world test classes as open-world threshold calibration. Existing posthoc threshold calibration methods, reliant on inductive inference and requiring a calibration dataset with a similar distance distribution as the test data, often prove ineffective in open-world scenarios. To address this, we introduce OpenGCN, a Graph Neural Network-based transductive threshold calibration method with enhanced adaptability and robustness. OpenGCN learns to predict pairwise connectivity for the unlabeled test instances embedded in a graph to determine its TPR and TNR at various distance thresholds, allowing for transductive inference of the distance thresholds which also incorporates test-time information. Extensive experiments across open-world visual recognition benchmarks validate OpenGCN's superiority over existing posthoc calibration methods for open-world threshold calibration.",
        "page": "http://arxiv.org/abs/2305.12039",
        "pdf": "http://arxiv.org/pdf/2305.12039.pdf"
    },
    {
        "title": "LightOctree: Lightweight 3D Spatially-Coherent Indoor Lighting Estimation",
        "author": "Xuecan Wang, Shibang Xiao, Xiaohui Liang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
        "author": "Ege Ozguroglu, Ruoshi Liu, D\u00eddac Sur\u00eds, Dian Chen, Achal Dave, Pavel Tokmakov, Carl Vondrick",
        "abstract": "We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions.",
        "page": "http://arxiv.org/abs/2401.14398",
        "pdf": "http://arxiv.org/pdf/2401.14398.pdf"
    },
    {
        "title": "TextCraftor: Your Text Encoder Can be Image Quality Controller",
        "author": "Yanyu Li, Xian Liu, Anil Kag, Ju Hu, Yerlan Idelbayev, Dhritiman Sagar, Yanzhi Wang, Sergey Tulyakov, Jian Ren",
        "abstract": "Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in Stable Diffusion with other large language models, we can enhance it through our proposed fine-tuning approach, TextCraftor, leading to substantial improvements in quantitative benchmarks and human assessments. Interestingly, our technique also empowers controllable image generation through the interpolation of different text encoders fine-tuned with various rewards. We also demonstrate that TextCraftor is orthogonal to UNet finetuning, and can be combined to further improve generative quality.",
        "page": "http://arxiv.org/abs/2403.18978",
        "pdf": "http://arxiv.org/pdf/2403.18978.pdf"
    },
    {
        "title": "OTE: Exploring Accurate Scene Text Recognition Using One Token",
        "author": "Jianjun Xu, Yuxin Wang, Hongtao Xie, Yongdong Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation",
        "author": "Biao Gong, Siteng Huang, Yutong Feng, Shiwei Zhang, Yuyuan Li, Yu Liu",
        "abstract": "Diffusion models have recently achieved remarkable progress in generating realistic images. However, challenges remain in accurately understanding and synthesizing the layout requirements in the textual prompts. To align the generated image with layout instructions, we present a training-free layout calibration system SimM that intervenes in the generative process on the fly during inference time. Specifically, following a \"check-locate-rectify\" pipeline, the system first analyses the prompt to generate the target layout and compares it with the intermediate outputs to automatically detect errors. Then, by moving the located activations and making intra- and inter-map adjustments, the rectification process can be performed with negligible computational overhead. To evaluate SimM over a range of layout requirements, we present a benchmark SimMBench that compensates for the lack of superlative spatial relations in existing datasets. And both quantitative and qualitative results demonstrate the effectiveness of the proposed SimM in calibrating the layout inconsistencies. Our project page is at https://simm-t2i.github.io/SimM.",
        "page": "http://arxiv.org/abs/2311.15773",
        "pdf": "http://arxiv.org/pdf/2311.15773.pdf"
    },
    {
        "title": "$\\mathsf{LQMFormer}$:~Language-aware Query Mask Transformer for Referring Image Segmentation",
        "author": "Nisarg Shah, Vibashan VS, Vishal M. Patel",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Latent Modulated Function for Computational Optimal Continuous Image Representation",
        "author": "Zongyao He, Zhi Jin",
        "abstract": "The recent work Local Implicit Image Function (LIIF) and subsequent Implicit Neural Representation (INR) based works have achieved remarkable success in Arbitrary-Scale Super-Resolution (ASSR) by using MLP to decode Low-Resolution (LR) features. However, these continuous image representations typically implement decoding in High-Resolution (HR) High-Dimensional (HD) space, leading to a quadratic increase in computational cost and seriously hindering the practical applications of ASSR. To tackle this problem, we propose a novel Latent Modulated Function (LMF), which decouples the HR-HD decoding process into shared latent decoding in LR-HD space and independent rendering in HR Low-Dimensional (LD) space, thereby realizing the first computational optimal paradigm of continuous image representation. Specifically, LMF utilizes an HD MLP in latent space to generate latent modulations of each LR feature vector. This enables a modulated LD MLP in render space to quickly adapt to any input feature vector and perform rendering at arbitrary resolution. Furthermore, we leverage the positive correlation between modulation intensity and input image complexity to design a Controllable Multi-Scale Rendering (CMSR) algorithm, offering the flexibility to adjust the decoding efficiency based on the rendering precision. Extensive experiments demonstrate that converting existing INR-based ASSR methods to LMF can reduce the computational cost by up to 99.9%, accelerate inference by up to 57 times, and save up to 76% of parameters, while maintaining competitive performance. The code is available at https://github.com/HeZongyao/LMF.",
        "page": "http://arxiv.org/abs/2404.16451",
        "pdf": "http://arxiv.org/pdf/2404.16451.pdf"
    },
    {
        "title": "Shallow-Deep Collaborative Learning for Unsupervised Visible-Infrared Person Re-Identification",
        "author": "Bin Yang, Jun Chen, Mang Ye",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Spherical Mask: Coarse-to-Fine 3D Point Cloud Instance Segmentation with Spherical Representation",
        "author": "Sangyun Shin, Kaichen Zhou, Madhu Vankadari, Andrew Markham, Niki Trigoni",
        "abstract": "Coarse-to-fine 3D instance segmentation methods show weak performances compared to recent Grouping-based, Kernel-based and Transformer-based methods. We argue that this is due to two limitations: 1) Instance size overestimation by axis-aligned bounding box(AABB) 2) False negative error accumulation from inaccurate box to the refinement phase. In this work, we introduce Spherical Mask, a novel coarse-to-fine approach based on spherical representation, overcoming those two limitations with several benefits. Specifically, our coarse detection estimates each instance with a 3D polygon using a center and radial distance predictions, which avoids excessive size estimation of AABB. To cut the error propagation in the existing coarse-to-fine approaches, we virtually migrate points based on the polygon, allowing all foreground points, including false negatives, to be refined. During inference, the proposal and point migration modules run in parallel and are assembled to form binary masks of instances. We also introduce two margin-based losses for the point migration to enforce corrections for the false positives/negatives and cohesion of foreground points, significantly improving the performance. Experimental results from three datasets, such as ScanNetV2, S3DIS, and STPLS3D, show that our proposed method outperforms existing works, demonstrating the effectiveness of the new instance representation with spherical coordinates.",
        "page": "http://arxiv.org/abs/2312.11269",
        "pdf": "http://arxiv.org/pdf/2312.11269.pdf"
    },
    {
        "title": "Neural Spline Fields for Burst Image Fusion and Layer Separation",
        "author": "Ilya Chugunov, David Shustin, Ruyu Yan, Chenyang Lei, Felix Heide",
        "abstract": "Each photo in an image burst can be considered a sample of a complex 3D scene: the product of parallax, diffuse and specular materials, scene motion, and illuminant variation. While decomposing all of these effects from a stack of misaligned images is a highly ill-conditioned task, the conventional align-and-merge burst pipeline takes the other extreme: blending them into a single image. In this work, we propose a versatile intermediate representation: a two-layer alpha-composited image plus flow model constructed with neural spline fields -- networks trained to map input coordinates to spline control points. Our method is able to, during test-time optimization, jointly fuse a burst image capture into one high-resolution reconstruction and decompose it into transmission and obstruction layers. Then, by discarding the obstruction layer, we can perform a range of tasks including seeing through occlusions, reflection suppression, and shadow removal. Validated on complex synthetic and in-the-wild captures we find that, with no post-processing steps or learned priors, our generalizable model is able to outperform existing dedicated single-image and multi-view obstruction removal approaches.",
        "page": "http://arxiv.org/abs/2312.14235",
        "pdf": "http://arxiv.org/pdf/2312.14235.pdf"
    },
    {
        "title": "L2B: Learning to Bootstrap Robust Models for Combating Label Noise",
        "author": "Yuyin Zhou, Xianhang li, Fengze Liu, Qingyue Wei, Xuxi Chen, Lequan Yu, Cihang Xie, Matthew P. Lungren, Lei Xing",
        "abstract": "Deep neural networks have shown great success in representation learning. However, when learning with noisy labels (LNL), they can easily overfit and fail to generalize to new data. This paper introduces a simple and effective method, named Learning to Bootstrap (L2B), which enables models to bootstrap themselves using their own predictions without being adversely affected by erroneous pseudo-labels. It achieves this by dynamically adjusting the importance weight between real observed and generated labels, as well as between different samples through meta-learning. Unlike existing instance reweighting methods, the key to our method lies in a new, versatile objective that enables implicit relabeling concurrently, leading to significant improvements without incurring additional costs. L2B offers several benefits over the baseline methods. It yields more robust models that are less susceptible to the impact of noisy labels by guiding the bootstrapping procedure more effectively. It better exploits the valuable information contained in corrupted instances by adapting the weights of both instances and labels. Furthermore, L2B is compatible with existing LNL methods and delivers competitive results spanning natural and medical imaging tasks including classification and segmentation under both synthetic and real-world noise. Extensive experiments demonstrate that our method effectively mitigates the challenges of noisy labels, often necessitating few to no validation samples, and is well generalized to other tasks such as image segmentation. This not only positions it as a robust complement to existing LNL techniques but also underscores its practical applicability. The code and models are available at https://github.com/yuyinzhou/l2b.",
        "page": "http://arxiv.org/abs/2202.04291",
        "pdf": "http://arxiv.org/pdf/2202.04291.pdf"
    },
    {
        "title": "Deep Video Inverse Tone Mapping Based on Temporal Clues",
        "author": "Yuyao Ye, Ning Zhang, Yang Zhao, Hongbin Cao, Ronggang Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Attack To Defend: Exploiting Adversarial Attacks for Detecting Poisoned Models",
        "author": "Samar Fares, Karthik Nandakumar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Non-autoregressive Sequence-to-Sequence Vision-Language Models",
        "author": "Kunyu Shi, Qi Dong, Luis Goncalves, Zhuowen Tu, Stefano Soatto",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Seeing the Unseen: Visual Common Sense for Semantic Placement",
        "author": "Ram Ramrakhya, Aniruddha Kembhavi, Dhruv Batra, Zsolt Kira, Kuo-Hao Zeng, Luca Weihs",
        "abstract": "Computer vision tasks typically involve describing what is present in an image (e.g. classification, detection, segmentation, and captioning). We study a visual common sense task that requires understanding what is not present. Specifically, given an image (e.g. of a living room) and name of an object (\"cushion\"), a vision system is asked to predict semantically-meaningful regions (masks or bounding boxes) in the image where that object could be placed or is likely be placed by humans (e.g. on the sofa). We call this task: Semantic Placement (SP) and believe that such common-sense visual understanding is critical for assitive robots (tidying a house), and AR devices (automatically rendering an object in the user's space). Studying the invisible is hard. Datasets for image description are typically constructed by curating relevant images and asking humans to annotate the contents of the image; neither of those two steps are straightforward for objects not present in the image. We overcome this challenge by operating in the opposite direction: we start with an image of an object in context from web, and then remove that object from the image via inpainting. This automated pipeline converts unstructured web data into a dataset comprising pairs of images with/without the object. Using this, we collect a novel dataset, with ${\\sim}1.3$M images across $9$ object categories, and train a SP prediction model called CLIP-UNet. CLIP-UNet outperforms existing VLMs and baselines that combine semantic priors with object detectors on real-world and simulated images. In our user studies, we find that the SP masks predicted by CLIP-UNet are favored $43.7\\%$ and $31.3\\%$ times when comparing against the $4$ SP baselines on real and simulated images. In addition, we demonstrate leveraging SP mask predictions from CLIP-UNet enables downstream applications like building tidying robots in indoor environments.",
        "page": "http://arxiv.org/abs/2401.07770",
        "pdf": "http://arxiv.org/pdf/2401.07770.pdf"
    },
    {
        "title": "Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields",
        "author": "Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W.H. Lau",
        "abstract": "Inverse rendering aims at recovering both geometry and materials of objects. It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs). On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only. Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation. We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields. Our method has two stages: the geometry of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy. Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects. Project webpage: https://whyy.site/paper/nep",
        "page": "http://arxiv.org/abs/2403.16224",
        "pdf": "http://arxiv.org/pdf/2403.16224.pdf"
    },
    {
        "title": "3D LiDAR Mapping in Dynamic Environments using a 4D Implicit Neural Representation",
        "author": "Xingguang Zhong, Yue Pan, Cyrill Stachniss, Jens Behley",
        "abstract": "Building accurate maps is a key building block to enable reliable localization, planning, and navigation of autonomous vehicles. We propose a novel approach for building accurate maps of dynamic environments utilizing a sequence of LiDAR scans. To this end, we propose encoding the 4D scene into a novel spatio-temporal implicit neural map representation by fitting a time-dependent truncated signed distance function to each point. Using our representation, we extract the static map by filtering the dynamic parts. Our neural representation is based on sparse feature grids, a globally shared decoder, and time-dependent basis functions, which we jointly optimize in an unsupervised fashion. To learn this representation from a sequence of LiDAR scans, we design a simple yet efficient loss function to supervise the map optimization in a piecewise way. We evaluate our approach on various scenes containing moving objects in terms of the reconstruction quality of static maps and the segmentation of dynamic point clouds. The experimental results demonstrate that our method is capable of removing the dynamic part of the input point clouds while reconstructing accurate and complete 3D maps, outperforming several state-of-the-art methods. Codes are available at: https://github.com/PRBonn/4dNDF",
        "page": "http://arxiv.org/abs/2405.03388",
        "pdf": "http://arxiv.org/pdf/2405.03388.pdf"
    },
    {
        "title": "SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction",
        "author": "Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, Jiwen Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SUGAR: Pre-training 3D Visual Representation for Robotics",
        "author": "Shizhe Chen, Ricardo Garcia Pinel, Ivan Laptev, Cordelia Schmid",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models",
        "author": "Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang",
        "abstract": "In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.",
        "page": "http://arxiv.org/abs/2310.08529",
        "pdf": "http://arxiv.org/pdf/2310.08529.pdf"
    },
    {
        "title": "A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives",
        "author": "Simone Peirone, Francesca Pistilli, Antonio Alliegro, Giuseppe Averta",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Compact 3D Gaussian Representation for Radiance Field",
        "author": "Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations",
        "author": "Christian Diller, Thomas Funkhouser, Angela Dai",
        "abstract": "We present a generative approach to forecast long-term future human behavior in 3D, requiring only weak supervision from readily available 2D human action data. This is a fundamental task enabling many downstream applications. The required ground-truth data is hard to capture in 3D (mocap suits, expensive setups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our method to only require 2D RGB data at inference time while being able to generate 3D human motion sequences. We use a differentiable 2D projection scheme in an autoregressive manner for weak supervision, and an adversarial loss for 3D regularization. Our method predicts long and complex human behavior sequences (e.g., cooking, assembly) consisting of multiple sub-actions. We tackle this in a semantically hierarchical manner, jointly predicting high-level coarse action labels together with their low-level fine-grained realizations as characteristic 3D human poses. We observe that these two action representations are coupled in nature, and joint prediction benefits both action and pose forecasting. Our experiments demonstrate the complementary nature of joint action and 3D pose prediction: our joint approach outperforms each task treated individually, enables robust longer-term sequence prediction, and improves over alternative approaches to forecast actions and characteristic 3D poses.",
        "page": "http://arxiv.org/abs/2211.14309",
        "pdf": "http://arxiv.org/pdf/2211.14309.pdf"
    },
    {
        "title": "FlowIE\uff1aEfficient Image Enhancement via Rectified Flow",
        "author": "Yixuan Zhu, Wenliang Zhao, Ao Li, Yansong Tang, Jie Zhou, Jiwen Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Combining Frame and GOP Embeddings for Neural Video Representation",
        "author": "Jens Eirik Saethre, Roberto Azevedo, Christopher Schroers",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation",
        "author": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu",
        "abstract": "Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment. Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources, incurring inevitable additional costs. In this paper, we present OPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy, serving as a nearly free lunch to alleviate the hallucination issue without additional data, knowledge, or training. Our approach begins with an interesting observation that, most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens, but not all the previous tokens. Such partial over-trust inclination results in the neglecting of image tokens and describes the image content with hallucination. Based on the observation, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue, along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens, and re-allocate the token selection if necessary. With extensive experiments, OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality. Our code is available at: https://github.com/shikiw/OPERA.",
        "page": "http://arxiv.org/abs/2311.17911",
        "pdf": "http://arxiv.org/pdf/2311.17911.pdf"
    },
    {
        "title": "Not All Classes Stand on Same Embeddings: Calibrating a Semantic Distance with Metric Tensor",
        "author": "Jae Hyeon Park, Gyoomin Lee, Seunggi Park, Sung In Cho",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models",
        "author": "Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao P. Huang, Tuanfeng Y. Wang, Gordon Wetzstein",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Out-of-Distribution Generalization in Graphs via Hierarchical Semantic Environments",
        "author": "Yinhua Piao, Sangseon Lee, Yijingxiu Lu, Sun Kim",
        "abstract": "Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attention mechanisms are employed to re-extract the subgraphs for regenerating global environments in a hierarchical manner. In addition, we introduce a new learning objective that guides our model to learn the diversity of environments within the same hierarchy while maintaining consistency across different hierarchies. This approach enables our model to consider the relationships between environments and facilitates robust graph invariant learning. Extensive experiments on real-world graph data have demonstrated the effectiveness of our framework. Particularly, in the challenging dataset DrugOOD, our method achieves up to 1.29\\% and 2.83\\% improvement over the best baselines on IC50 and EC50 prediction tasks, respectively.",
        "page": "http://arxiv.org/abs/2403.01773",
        "pdf": "http://arxiv.org/pdf/2403.01773.pdf"
    },
    {
        "title": "Towards Understanding and Improving Adversarial Robustness of Vision Transformers",
        "author": "Samyak Jain, Tanima Dutta",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval",
        "author": "Fang Kaipeng, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Zhi-Qi Cheng, Xiyao LI, Heng Tao Shen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improved Self-Training for Test-Time Adaptation",
        "author": "Jing Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LangSplat: 3D Language Gaussian Splatting",
        "author": "Minghan Qin, Wanhua Li, Jiawei ZHOU, Haoqian Wang, Hanspeter Pfister",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Retrieval-Augmented Embodied Agents",
        "author": "Yichen Zhu, Zhicai Ou, Xiaofeng Mou, Jian Tang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Positive-Unlabeled Learning by Latent Group-Aware Meta Disambiguation",
        "author": "Lin Long, Haobo Wang, Zhijie Jiang, Lei Feng, Chang Yao, Gang Chen, Junbo Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Contextrast: Contextual Contrastive Learning for Semantic Segmentation",
        "author": "Changki Sung, Wanhee Kim, Jungho An, WooJu Lee, Hyungtae Lim, Hyun Myung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data",
        "author": "Hanrong Ye, Dan Xu",
        "abstract": "Recently, there has been an increased interest in the practical problem of learning multiple dense scene understanding tasks from partially annotated data, where each training sample is only labeled for a subset of the tasks. The missing of task labels in training leads to low-quality and noisy predictions, as can be observed from state-of-the-art methods. To tackle this issue, we reformulate the partially-labeled multi-task dense prediction as a pixel-level denoising problem, and propose a novel multi-task denoising diffusion framework coined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to model a potential noisy distribution in the task prediction or feature maps and generate rectified outputs for different tasks. To exploit multi-task consistency in denoising, we further introduce a Multi-Task Conditioning strategy, which can implicitly utilize the complementary nature of the tasks to help learn the unlabeled tasks, leading to an improvement in the denoising performance of the different tasks. Extensive quantitative and qualitative experiments demonstrate that the proposed multi-task denoising diffusion model can significantly improve multi-task prediction maps, and outperform the state-of-the-art methods on three challenging multi-task benchmarks, under two different partial-labeling evaluation settings. The code is available at https://prismformore.github.io/diffusionmtl/.",
        "page": "http://arxiv.org/abs/2403.15389",
        "pdf": "http://arxiv.org/pdf/2403.15389.pdf"
    },
    {
        "title": "JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation",
        "author": "Yu Zeng, Vishal M. Patel, Haochen Wang, Xun Huang, Ting-Chun Wang, Ming-Yu Liu, Yogesh Balaji",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "An N-Point Linear Solver for Line and Motion Estimation with Event Cameras",
        "author": "Ling Gao, Daniel Gehrig, Hang Su, Davide Scaramuzza, Laurent Kneip",
        "abstract": "Event cameras respond primarily to edges--formed by strong gradients--and are thus particularly well-suited for line-based motion estimation. Recent work has shown that events generated by a single line each satisfy a polynomial constraint which describes a manifold in the space-time volume. Multiple such constraints can be solved simultaneously to recover the partial linear velocity and line parameters. In this work, we show that, with a suitable line parametrization, this system of constraints is actually linear in the unknowns, which allows us to design a novel linear solver. Unlike existing solvers, our linear solver (i) is fast and numerically stable since it does not rely on expensive root finding, (ii) can solve both minimal and overdetermined systems with more than 5 events, and (iii) admits the characterization of all degenerate cases and multiple solutions. The found line parameters are singularity-free and have a fixed scale, which eliminates the need for auxiliary constraints typically encountered in previous work. To recover the full linear camera velocity we fuse observations from multiple lines with a novel velocity averaging scheme that relies on a geometrically-motivated residual, and thus solves the problem more efficiently than previous schemes which minimize an algebraic residual. Extensive experiments in synthetic and real-world settings demonstrate that our method surpasses the previous work in numerical stability, and operates over 600 times faster.",
        "page": "http://arxiv.org/abs/2404.00842",
        "pdf": "http://arxiv.org/pdf/2404.00842.pdf"
    },
    {
        "title": "Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction",
        "author": "Zilin Du, Haoxin Li, Xu Guo, Boyang Li",
        "abstract": "The task of multimodal relation extraction has attracted significant research attention, but progress is constrained by the scarcity of available training data. One natural thought is to extend existing datasets with cross-modal generative models. In this paper, we consider a novel problem setting, where only unimodal data, either text or image, are available during training. We aim to train a multimodal classifier from synthetic data that perform well on real multimodal test data. However, training with synthetic data suffers from two obstacles: lack of data diversity and label information loss. To alleviate the issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to promote diversity in the generated data and exploits a teacher network to select valuable training samples with high mutual information with the ground-truth labels. Comparing our method to direct training on synthetic data, we observed a significant improvement of 24.06% F1 with synthetic text and 26.42% F1 with synthetic images. Notably, our best model trained on completely synthetic images outperforms prior state-of-the-art models trained on real multimodal data by a margin of 3.76% in F1. Our codebase will be made available upon acceptance.",
        "page": "http://arxiv.org/abs/2312.03025",
        "pdf": "http://arxiv.org/pdf/2312.03025.pdf"
    },
    {
        "title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering",
        "author": "Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang",
        "abstract": "Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency, we propose 4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps. Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800$\\times$800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.",
        "page": "http://arxiv.org/abs/2310.08528",
        "pdf": "http://arxiv.org/pdf/2310.08528.pdf"
    },
    {
        "title": "Differentiable Information Bottleneck for Deterministic Multi-view Clustering",
        "author": "Xiaoqiang Yan, Zhixiang Jin, Fengshou Han, Yangdong Ye",
        "abstract": "In recent several years, the information bottleneck (IB) principle provides an information-theoretic framework for deep multi-view clustering (MVC) by compressing multi-view observations while preserving the relevant information of multiple views. Although existing IB-based deep MVC methods have achieved huge success, they rely on variational approximation and distribution assumption to estimate the lower bound of mutual information, which is a notoriously hard and impractical problem in high-dimensional multi-view spaces. In this work, we propose a new differentiable information bottleneck (DIB) method, which provides a deterministic and analytical MVC solution by fitting the mutual information without the necessity of variational approximation. Specifically, we first propose to directly fit the mutual information of high-dimensional spaces by leveraging normalized kernel Gram matrix, which does not require any auxiliary neural estimator to estimate the lower bound of mutual information. Then, based on the new mutual information measurement, a deterministic multi-view neural network with analytical gradients is explicitly trained to parameterize IB principle, which derives a deterministic compression of input variables from different views. Finally, a triplet consistency discovery mechanism is devised, which is capable of mining the feature consistency, cluster consistency and joint consistency based on the deterministic and compact representations. Extensive experimental results show the superiority of our DIB method on 6 benchmarks compared with 13 state-of-the-art baselines.",
        "page": "http://arxiv.org/abs/2403.15681",
        "pdf": "http://arxiv.org/pdf/2403.15681.pdf"
    },
    {
        "title": "SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering",
        "author": "Antoine Gu\u00e9don, Vincent Lepetit",
        "abstract": "We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality. Our project page is the following: https://anttwo.github.io/sugar/",
        "page": "http://arxiv.org/abs/2311.12775",
        "pdf": "http://arxiv.org/pdf/2311.12775.pdf"
    },
    {
        "title": "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models",
        "author": "Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, Xiang Bai",
        "abstract": "Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative results validate the effectiveness of our designs. Additionally, experiments on 18 datasets further demonstrate that Monkey surpasses existing LMMs in many tasks like Image Captioning and various Visual Question Answering formats. Specially, in qualitative tests focused on dense text question answering, Monkey has exhibited encouraging results compared with GPT4V. Code is available at https://github.com/Yuliang-Liu/Monkey.",
        "page": "http://arxiv.org/abs/2311.06607",
        "pdf": "http://arxiv.org/pdf/2311.06607.pdf"
    },
    {
        "title": "Zero-Reference Low-Light Enhancement via Physical Quadruple Priors",
        "author": "Wenjing Wang, Huan Yang, Jianlong Fu, Jiaying Liu",
        "abstract": "Understanding illumination and reducing the need for supervision pose a significant challenge in low-light enhancement. Current approaches are highly sensitive to data usage during training and illumination-specific hyper-parameters, limiting their ability to handle unseen scenarios. In this paper, we propose a new zero-reference low-light enhancement framework trainable solely with normal light images. To accomplish this, we devise an illumination-invariant prior inspired by the theory of physical light transfer. This prior serves as the bridge between normal and low-light images. Then, we develop a prior-to-image framework trained without low-light data. During testing, this framework is able to restore our illumination-invariant prior back to images, automatically achieving low-light enhancement. Within this framework, we leverage a pretrained generative diffusion model for model ability, introduce a bypass decoder to handle detail distortion, as well as offer a lightweight version for practicality. Extensive experiments demonstrate our framework's superiority in various scenarios as well as good interpretability, robustness, and efficiency. Code is available on our project homepage: http://daooshee.github.io/QuadPrior-Website/",
        "page": "http://arxiv.org/abs/2403.12933",
        "pdf": "http://arxiv.org/pdf/2403.12933.pdf"
    },
    {
        "title": "Hybrid Proposal Refiner: Revisiting DETR Series from the Faster R-CNN Perspective",
        "author": "Jinjing Zhao, Fangyun Wei, Chang Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion",
        "author": "Tom Van Wouwe, Seunghwan Lee, Antoine Falisse, Scott Delp, Karen Liu",
        "abstract": "Motion capture from a limited number of body-worn sensors, such as inertial measurement units (IMUs) and pressure insoles, has important applications in health, human performance, and entertainment. Recent work has focused on accurately reconstructing whole-body motion from a specific sensor configuration using six IMUs. While a common goal across applications is to use the minimal number of sensors to achieve required accuracy, the optimal arrangement of the sensors might differ from application to application. We propose a single diffusion model, DiffusionPoser, which reconstructs human motion in real-time from an arbitrary combination of sensors, including IMUs placed at specified locations, and, pressure insoles. Unlike existing methods, our model grants users the flexibility to determine the number and arrangement of sensors tailored to the specific activity of interest, without the need for retraining. A novel autoregressive inferencing scheme ensures real-time motion reconstruction that closely aligns with measured sensor signals. The generative nature of DiffusionPoser ensures realistic behavior, even for degrees-of-freedom not directly measured. Qualitative results can be found on our website: https://diffusionposer.github.io/.",
        "page": "http://arxiv.org/abs/2308.16682",
        "pdf": "http://arxiv.org/pdf/2308.16682.pdf"
    },
    {
        "title": "HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion",
        "author": "Jingbo Zhang, Xiaoyu Li, Qi Zhang, Yan-Pei Cao, Ying Shan, Jing Liao",
        "abstract": "Generating a 3D human model from a single reference image is challenging because it requires inferring textures and geometries in invisible views while maintaining consistency with the reference image. Previous methods utilizing 3D generative models are limited by the availability of 3D training data. Optimization-based methods that lift text-to-image diffusion models to 3D generation often fail to preserve the texture details of the reference image, resulting in inconsistent appearances in different views. In this paper, we propose HumanRef, a 3D human generation framework from a single-view input. To ensure the generated 3D model is photorealistic and consistent with the input image, HumanRef introduces a novel method called reference-guided score distillation sampling (Ref-SDS), which effectively incorporates image guidance into the generation process. Furthermore, we introduce region-aware attention to Ref-SDS, ensuring accurate correspondence between different body regions. Experimental results demonstrate that HumanRef outperforms state-of-the-art methods in generating 3D clothed humans with fine geometry, photorealistic textures, and view-consistent appearances.",
        "page": "http://arxiv.org/abs/2311.16961",
        "pdf": "http://arxiv.org/pdf/2311.16961.pdf"
    },
    {
        "title": "CurveCloudNet: Processing Point Clouds with 1D Structure",
        "author": "Colton Stearns, Alex Fu, Jiateng Liu, Jeong Joon Park, Davis Rempe, Despoina Paschalidou, Leonidas Guibas",
        "abstract": "Modern depth sensors such as LiDAR operate by sweeping laser-beams across the scene, resulting in a point cloud with notable 1D curve-like structures. In this work, we introduce a new point cloud processing scheme and backbone, called CurveCloudNet, which takes advantage of the curve-like structure inherent to these sensors. While existing backbones discard the rich 1D traversal patterns and rely on generic 3D operations, CurveCloudNet parameterizes the point cloud as a collection of polylines (dubbed a \"curve cloud\"), establishing a local surface-aware ordering on the points. By reasoning along curves, CurveCloudNet captures lightweight curve-aware priors to efficiently and accurately reason in several diverse 3D environments. We evaluate CurveCloudNet on multiple synthetic and real datasets that exhibit distinct 3D size and structure. We demonstrate that CurveCloudNet outperforms both point-based and sparse-voxel backbones in various segmentation settings, notably scaling to large scenes better than point-based alternatives while exhibiting improved single-object performance over sparse-voxel alternatives. In all, CurveCloudNet is an efficient and accurate backbone that can handle a larger variety of 3D environments than past works.",
        "page": "http://arxiv.org/abs/2303.12050",
        "pdf": "http://arxiv.org/pdf/2303.12050.pdf"
    },
    {
        "title": "Learning Visual Prompt for Gait Recognition",
        "author": "Kang Ma, Ying Fu, Chunshui Cao, Saihui Hou, Yongzhen Huang, Dezhi Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Discovering and Mitigating Visual Biases through Keyword Explanation",
        "author": "Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin",
        "abstract": "Addressing biases in computer vision models is crucial for real-world AI deployments. However, mitigating visual biases is challenging due to their unexplainable nature, often identified indirectly through visualization or sample statistics, which necessitates additional human supervision for interpretation. To tackle this issue, we propose the Bias-to-Text (B2T) framework, which interprets visual biases as keywords. Specifically, we extract common keywords from the captions of mispredicted images to identify potential biases in the model. We then validate these keywords by measuring their similarity to the mispredicted images using a vision-language scoring model. The keyword explanation form of visual bias offers several advantages, such as a clear group naming for bias discovery and a natural extension for debiasing using these group names. Our experiments demonstrate that B2T can identify known biases, such as gender bias in CelebA, background bias in Waterbirds, and distribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in larger datasets, such as Dollar Street and ImageNet. For example, we discovered a contextual bias between \"bee\" and \"flower\" in ImageNet. We also highlight various applications of B2T keywords, including debiased training, CLIP prompting, and model comparison.",
        "page": "http://arxiv.org/abs/2301.11104",
        "pdf": "http://arxiv.org/pdf/2301.11104.pdf"
    },
    {
        "title": "MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning",
        "author": "Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, Lijuan Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GreedyViG: Dynamic Axial Graph Construction for Efficient Vision GNNs",
        "author": "Mustafa Munir, William Avery, Md Mostafijur Rahman, Radu Marculescu",
        "abstract": "Vision graph neural networks (ViG) offer a new avenue for exploration in computer vision. A major bottleneck in ViGs is the inefficient k-nearest neighbor (KNN) operation used for graph construction. To solve this issue, we propose a new method for designing ViGs, Dynamic Axial Graph Construction (DAGC), which is more efficient than KNN as it limits the number of considered graph connections made within an image. Additionally, we propose a novel CNN-GNN architecture, GreedyViG, which uses DAGC. Extensive experiments show that GreedyViG beats existing ViG, CNN, and ViT architectures in terms of accuracy, GMACs, and parameters on image classification, object detection, instance segmentation, and semantic segmentation tasks. Our smallest model, GreedyViG-S, achieves 81.1% top-1 accuracy on ImageNet-1K, 2.9% higher than Vision GNN and 2.2% higher than Vision HyperGraph Neural Network (ViHGNN), with less GMACs and a similar number of parameters. Our largest model, GreedyViG-B obtains 83.9% top-1 accuracy, 0.2% higher than Vision GNN, with a 66.6% decrease in parameters and a 69% decrease in GMACs. GreedyViG-B also obtains the same accuracy as ViHGNN with a 67.3% decrease in parameters and a 71.3% decrease in GMACs. Our work shows that hybrid CNN-GNN architectures not only provide a new avenue for designing efficient models, but that they can also exceed the performance of current state-of-the-art models.",
        "page": "http://arxiv.org/abs/2405.06849",
        "pdf": "http://arxiv.org/pdf/2405.06849.pdf"
    },
    {
        "title": "MoML: Online Meta Adaptation for 3D Human Motion Prediction",
        "author": "Xiaoning Sun, Huaijiang Sun, Bin Li, Dong Wei, Weiqing Li, Jianfeng Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VILA: On Pre-training for Visual Language Models",
        "author": "Ji Lin, Danny Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, Song Han",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dr2Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning",
        "author": "Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng Qian, Fatimah Zohra, Abdulmohsen Alghannam, Jitendra Malik, Bernard Ghanem",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Vision-and-Language Navigation via Causal Learning",
        "author": "Liuyi Wang, Zongtao He, Ronghao Dang, mengjiao shen, Chengju Liu, Qijun Chen",
        "abstract": "In the pursuit of robust and generalizable environment perception and language understanding, the ubiquitous challenge of dataset bias continues to plague vision-and-language navigation (VLN) agents, hindering their performance in unseen environments. This paper introduces the generalized cross-modal causal transformer (GOAT), a pioneering solution rooted in the paradigm of causal inference. By delving into both observable and unobservable confounders within vision, language, and history, we propose the back-door and front-door adjustment causal learning (BACL and FACL) modules to promote unbiased learning by comprehensively mitigating potential spurious correlations. Additionally, to capture global confounder features, we propose a cross-modal feature pooling (CFP) module supervised by contrastive learning, which is also shown to be effective in improving cross-modal representations during pre-training. Extensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and SOON) underscore the superiority of our proposed method over previous state-of-the-art approaches. Code is available at https://github.com/CrystalSixone/VLN-GOAT.",
        "page": "http://arxiv.org/abs/2404.10241",
        "pdf": "http://arxiv.org/pdf/2404.10241.pdf"
    },
    {
        "title": "A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?",
        "author": "Galadrielle Humblot-Renaux, Sergio Escalera, Thomas B. Moeslund",
        "abstract": "The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types & levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show that poor separation between incorrectly classified ID samples vs. OOD samples is an overlooked yet important limitation of existing methods. Code: https://github.com/glhr/ood-labelnoise",
        "page": "http://arxiv.org/abs/2404.01775",
        "pdf": "http://arxiv.org/pdf/2404.01775.pdf"
    },
    {
        "title": "Learning with Structural Labels for Learning with Noisy Labels",
        "author": "Noo-ri Kim, Jin-Seop Lee, Jee-Hyong Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models",
        "author": "Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, Bingchen Zhao",
        "abstract": "Counterfactual reasoning, a fundamental aspect of human cognition, involves contemplating alternatives to established facts or past events, significantly enhancing our abilities in planning and decision-making. In light of the advancements in current multi-modal large language models, we explore their effectiveness in counterfactual reasoning. To facilitate this investigation, we introduce a novel dataset, C-VQA, specifically designed to test the counterfactual reasoning capabilities of modern multi-modal large language models. This dataset is constructed by infusing original questions with counterfactual presuppositions, spanning various types such as numerical and boolean queries. It encompasses a mix of real and synthetic data, representing a wide range of difficulty levels. Our thorough evaluations of contemporary vision-language models using this dataset have revealed substantial performance drops, with some models showing up to a 40% decrease, highlighting a significant gap between current models and human-like vision reasoning capabilities. We hope our dataset will serve as a vital benchmark for evaluating the counterfactual reasoning capabilities of models. Code and dataset are publicly available at https://bzhao.me/C-VQA/.",
        "page": "http://arxiv.org/abs/2310.06627",
        "pdf": "http://arxiv.org/pdf/2310.06627.pdf"
    },
    {
        "title": "Bayesian Exploration of Pre-trained Models for Low-shot Image Classification",
        "author": "Yibo Miao, Yu lei, Feng Zhou, Zhijie Deng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment",
        "author": "Tianchen Deng, Guole Shen, Tong Qin, jianyu wang, Wentao Zhao, Jingchuan Wang, Danwei Wang, Weidong Chen",
        "abstract": "Neural implicit scene representations have recently shown encouraging results in dense visual SLAM. However, existing methods produce low-quality scene reconstruction and low-accuracy localization performance when scaling up to large indoor scenes and long sequences. These limitations are mainly due to their single, global radiance field with finite capacity, which does not adapt to large scenarios. Their end-to-end pose networks are also not robust enough with the growth of cumulative errors in large scenes. To this end, we introduce PLGSLAM, a neural visual SLAM system capable of high-fidelity surface reconstruction and robust camera tracking in real-time. To handle large-scale indoor scenes, PLGSLAM proposes a progressive scene representation method which dynamically allocates new local scene representation trained with frames within a local sliding window. This allows us to scale up to larger indoor scenes and improves robustness (even under pose drifts). In local scene representation, PLGSLAM utilizes tri-planes for local high-frequency features with multi-layer perceptron (MLP) networks for the low-frequency feature, achieving smoothness and scene completion in unobserved areas. Moreover, we propose local-to-global bundle adjustment method with a global keyframe database to address the increased pose drifts on long sequences. Experimental results demonstrate that PLGSLAM achieves state-of-the-art scene reconstruction results and tracking performance across various datasets and scenarios (both in small and large-scale indoor environments).",
        "page": "http://arxiv.org/abs/2312.09866",
        "pdf": "http://arxiv.org/pdf/2312.09866.pdf"
    },
    {
        "title": "RecDiffusion: Rectangling for Image Stitching with Diffusion Models",
        "author": "Tianhao Zhou, Li Haipeng, Ziyi Wang, Ao Luo, Chenlin Zhang, Jiajun Li, Bing Zeng, Shuaicheng Liu",
        "abstract": "Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing. To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts. To overcome these issues, we introduce a novel diffusion-based learning framework, \\textbf{RecDiffusion}, for image stitching rectangling. This framework combines Motion Diffusion Models (MDM) to generate motion fields, effectively transitioning from the stitched image's irregular borders to a geometrically corrected intermediary. Followed by Content Diffusion Models (CDM) for image detail refinement. Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM. Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public benchmarks. Code is released at https://github.com/lhaippp/RecDiffusion.",
        "page": "http://arxiv.org/abs/2403.19164",
        "pdf": "http://arxiv.org/pdf/2403.19164.pdf"
    },
    {
        "title": "Incremental Nuclei Segmentation from Histopathological Images via Future-class Awareness and Compatibility-inspired Distillation",
        "author": "Huyong Wang, Huisi Wu, Jing Qin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder",
        "author": "Jinseok Kim, Tae-Kyun Kim",
        "abstract": "Super-resolution (SR) and image generation are important tasks in computer vision and are widely adopted in real-world applications. Most existing methods, however, generate images only at fixed-scale magnification and suffer from over-smoothing and artifacts. Additionally, they do not offer enough diversity of output images nor image consistency at different scales. Most relevant work applied Implicit Neural Representation (INR) to the denoising diffusion model to obtain continuous-resolution yet diverse and high-quality SR results. Since this model operates in the image space, the larger the resolution of image is produced, the more memory and inference time is required, and it also does not maintain scale-specific consistency. We propose a novel pipeline that can super-resolve an input image or generate from a random noise a novel image at arbitrary scales. The method consists of a pretrained auto-encoder, a latent diffusion model, and an implicit neural decoder, and their learning strategies. The proposed method adopts diffusion processes in a latent space, thus efficient, yet aligned with output image space decoded by MLPs at arbitrary scales. More specifically, our arbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling from the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in series. The latent diffusion process is learnt by the denoising and the alignment losses jointly. Errors in output images are backpropagated via the fixed decoder, improving the quality of output images. In the extensive experiments using multiple public benchmarks on the two tasks i.e. image super-resolution and novel image generation at arbitrary scales, the proposed method outperforms relevant methods in metrics of image quality, diversity and scale consistency. It is significantly better than the relevant prior-art in the inference speed and memory usage.",
        "page": "http://arxiv.org/abs/2403.10255",
        "pdf": "http://arxiv.org/pdf/2403.10255.pdf"
    },
    {
        "title": "Three Pillars improving Vision Foundation Model Distillation for Lidar",
        "author": "Gilles Puy, Spyros Gidaris, Alexandre Boulch, Oriane Sim\u00e9oni, Corentin Sautier, Patrick P\u00e9rez, Andrei Bursuc, Renaud Marlet",
        "abstract": "Self-supervised image backbones can be used to address complex 2D tasks (e.g., semantic segmentation, object discovery) very efficiently and with little or no downstream supervision. Ideally, 3D backbones for lidar should be able to inherit these properties after distillation of these powerful 2D features. The most recent methods for image-to-lidar distillation on autonomous driving data show promising results, obtained thanks to distillation methods that keep improving. Yet, we still notice a large performance gap when measuring the quality of distilled and fully supervised features by linear probing. In this work, instead of focusing only on the distillation method, we study the effect of three pillars for distillation: the 3D backbone, the pretrained 2D backbones, and the pretraining dataset. In particular, thanks to our scalable distillation method named ScaLR, we show that scaling the 2D and 3D backbones and pretraining on diverse datasets leads to a substantial improvement of the feature quality. This allows us to significantly reduce the gap between the quality of distilled and fully-supervised 3D features, and to improve the robustness of the pretrained backbones to domain gaps and perturbations.",
        "page": "http://arxiv.org/abs/2310.17504",
        "pdf": "http://arxiv.org/pdf/2310.17504.pdf"
    },
    {
        "title": "Retraining-free Model Quantization via One-Shot Weight-Coupling Learning",
        "author": "Chen Tang, Yuan Meng, Jiacheng Jiang, Shuzhao Xie, Rongwei Lu, Xinzhu Ma, Zhi Wang, Wenwu Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Model Inversion Robustness: Can Transfer Learning Help?",
        "author": "Sy-Tuyen Ho, Koh Jun Hao, Keshigeyan Chandrasegaran, Ngoc-Bao Nguyen, Ngai-Man Cheung",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Seamless Human Motion Composition with Blended Positional Encodings",
        "author": "German Barquero, Sergio Escalera, Cristina Palmero",
        "abstract": "Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.",
        "page": "http://arxiv.org/abs/2402.15509",
        "pdf": "http://arxiv.org/pdf/2402.15509.pdf"
    },
    {
        "title": "SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation",
        "author": "Junyan Ye, Qiyan Luo, Jinhua Yu, Huaping Zhong, Zhimeng Zheng, Conghui He, Weijia Li",
        "abstract": "This paper aims at achieving fine-grained building attribute segmentation in a cross-view scenario, i.e., using satellite and street-view image pairs. The main challenge lies in overcoming the significant perspective differences between street views and satellite views. In this work, we introduce SG-BEV, a novel approach for satellite-guided BEV fusion for cross-view semantic segmentation. To overcome the limitations of existing cross-view projection methods in capturing the complete building facade features, we innovatively incorporate Bird's Eye View (BEV) method to establish a spatially explicit mapping of street-view features. Moreover, we fully leverage the advantages of multiple perspectives by introducing a novel satellite-guided reprojection module, optimizing the uneven feature distribution issues associated with traditional BEV methods. Our method demonstrates significant improvements on four cross-view datasets collected from multiple cities, including New York, San Francisco, and Boston. On average across these datasets, our method achieves an increase in mIOU by 10.13% and 5.21% compared with the state-of-the-art satellite-based and cross-view methods. The code and datasets of this work will be released at https://github.com/yejy53/SG-BEV.",
        "page": "http://arxiv.org/abs/2404.02638",
        "pdf": "http://arxiv.org/pdf/2404.02638.pdf"
    },
    {
        "title": "GLaMM: Pixel Grounding Large Multimodal Model",
        "author": "Hanoona Rasheed, Muhammad Maaz, Sahal Shaji Mullappilly, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric P. Xing, Ming-Hsuan Yang, Fahad Shahbaz Khan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners",
        "author": "Keon Hee Park, Kyungwoo Song, Gyeong-Moon Park",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Large-Factor EM Image Super-Resolution with Generative Priors",
        "author": "Jiateng Shou, Zeyu Xiao, Shiyu Deng, Wei Huang, ShiPeiyao, Ruobing Zhang, Zhiwei Xiong, Feng Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval",
        "author": "Yucheng Suo, Fan Ma, Linchao Zhu, Yi Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Functional Diffusion",
        "author": "Biao Zhang, Peter Wonka",
        "abstract": "We propose a new class of generative diffusion models, called functional diffusion. In contrast to previous work, functional diffusion works on samples that are represented by functions with a continuous domain. Functional diffusion can be seen as an extension of classical diffusion models to an infinite-dimensional domain. Functional diffusion is very versatile as images, videos, audio, 3D shapes, deformations, \\etc, can be handled by the same framework with minimal changes. In addition, functional diffusion is especially suited for irregular data or data defined in non-standard domains. In our work, we derive the necessary foundations for functional diffusion and propose a first implementation based on the transformer architecture. We show generative results on complicated signed distance functions and deformation functions defined on 3D surfaces.",
        "page": "http://arxiv.org/abs/2311.15435",
        "pdf": "http://arxiv.org/pdf/2311.15435.pdf"
    },
    {
        "title": "VideoRF: Rendering Dynamic Radiance Fields as 2D Feature Video Streams",
        "author": "Liao Wang, Kaixin Yao, Chengcheng Guo, Zhirui Zhang, Qiang Hu, Jingyi Yu, Lan Xu, Minye Wu",
        "abstract": "Neural Radiance Fields (NeRFs) excel in photorealistically rendering static scenes. However, rendering dynamic, long-duration radiance fields on ubiquitous devices remains challenging, due to data storage and computational constraints. In this paper, we introduce VideoRF, the first approach to enable real-time streaming and rendering of dynamic radiance fields on mobile platforms. At the core is a serialized 2D feature image stream representing the 4D radiance field all in one. We introduce a tailored training scheme directly applied to this 2D domain to impose the temporal and spatial redundancy of the feature image stream. By leveraging the redundancy, we show that the feature image stream can be efficiently compressed by 2D video codecs, which allows us to exploit video hardware accelerators to achieve real-time decoding. On the other hand, based on the feature image stream, we propose a novel rendering pipeline for VideoRF, which has specialized space mappings to query radiance properties efficiently. Paired with a deferred shading model, VideoRF has the capability of real-time rendering on mobile devices thanks to its efficiency. We have developed a real-time interactive player that enables online streaming and rendering of dynamic scenes, offering a seamless and immersive free-viewpoint experience across a range of devices, from desktops to mobile phones.",
        "page": "http://arxiv.org/abs/2312.01407",
        "pdf": "http://arxiv.org/pdf/2312.01407.pdf"
    },
    {
        "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models",
        "author": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, Kfir Aberman",
        "abstract": "Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10000x smaller than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io",
        "page": "http://arxiv.org/abs/2307.06949",
        "pdf": "http://arxiv.org/pdf/2307.06949.pdf"
    },
    {
        "title": "Clustering Propagation for Universal Medical Image Segmentation",
        "author": "Yuhang Ding, Liulei Li, Wenguan Wang, Yi Yang",
        "abstract": "Prominent solutions for medical image segmentation are typically tailored for automatic or interactive setups, posing challenges in facilitating progress achieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$ necessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both training time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$ issues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$ universal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$ Slice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive segmentation within a single model and one training session. Inspired by clustering-based segmentation techniques, S2VNet makes full use of the slice-wise structure of volumetric data by initializing cluster centers from the cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$ This enables knowledge acquired from prior slices to assist in the segmentation of the current slice, further efficiently bridging the communication between remote slices using mere 2D networks. Moreover, such a framework readily accommodates interactive segmentation with no architectural change, simply by initializing centroids from user inputs. S2VNet distinguishes itself by swift inference speeds and reduced memory consumption compared to prevailing 3D solutions. It can also handle multi-class interactions with each of them serving to initialize different centroids. Experiments on three benchmarks demonstrate S2VNet surpasses task-specified solutions on both automatic/interactive setups.",
        "page": "http://arxiv.org/abs/2403.16646",
        "pdf": "http://arxiv.org/pdf/2403.16646.pdf"
    },
    {
        "title": "Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening",
        "author": "Yule Duan, Xiao Wu, Haoyu Deng, Liang-Jian Deng",
        "abstract": "Currently, machine learning-based methods for remote sensing pansharpening have progressed rapidly. However, existing pansharpening methods often do not fully exploit differentiating regional information in non-local spaces, thereby limiting the effectiveness of the methods and resulting in redundant learning parameters. In this paper, we introduce a so-called content-adaptive non-local convolution (CANConv), a novel method tailored for remote sensing image pansharpening. Specifically, CANConv employs adaptive convolution, ensuring spatial adaptability, and incorporates non-local self-similarity through the similarity relationship partition (SRP) and the partition-wise adaptive convolution (PWAC) sub-modules. Furthermore, we also propose a corresponding network architecture, called CANNet, which mainly utilizes the multi-scale self-similarity. Extensive experiments demonstrate the superior performance of CANConv, compared with recent promising fusion methods. Besides, we substantiate the method's effectiveness through visualization, ablation experiments, and comparison with existing methods on multiple test sets. The source code is publicly available at https://github.com/duanyll/CANConv.",
        "page": "http://arxiv.org/abs/2404.07543",
        "pdf": "http://arxiv.org/pdf/2404.07543.pdf"
    },
    {
        "title": "A Versatile Framework for Continual Test-Time Domain Adaptation: Balancing Discriminability and Generalizability",
        "author": "Xu Yang, Xuan chen, Moqi Li, Kun Wei, Cheng Deng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Device-Wise Federated Network Pruning",
        "author": "Shangqian Gao, Junyi Li, Zeyu Zhang, Yanfu Zhang, Weidong Cai, Heng Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "D$^4$M: Dataset Distillation via Disentangled Diffusion Model",
        "author": "Duo Su, Junjie Hou, Weizhi Gao, Yingjie Tian, Bowen Tang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Logarithmic Lenses: Exploring Log RGB Data for Image Classification",
        "author": "Bruce Maxwell, Bruce Maxwell, Sumegha Singhania, Avnish Patel, Rahul Kumar, Heather Fryling, Sihan Li, Haonan Sun, Ping He, Zewen Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Score-Guided Diffusion for 3D Human Recovery",
        "author": "Anastasis Stathopoulos, Ligong Han, Dimitris N. Metaxas",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Draw Step by Step: Reconstructing CAD Construction Sequences from Point Clouds via Multimodal Diffusion.",
        "author": "Weijian Ma, Shuaiqi Chen, Yunzhong Lou, Xueyang Li, Xiangdong Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation",
        "author": "Yining Shi, Kun JIANG, Ke Wang, Jiusi Li, Yunlong Wang, Mengmeng Yang, Diange Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Specularity Factorization for Low Light Enhancement",
        "author": "Saurabh Saini, P. J. Narayanan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Just Add $\\pi$! Pose Induced Video Transformers for Understanding Activities of Daily Living",
        "author": "Dominick Reilly, Srijan Das",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "3DInAction: Understanding Human Actions in 3D Point Clouds",
        "author": "Yizhak Ben-Shabat, Oren Shrout, Stephen Gould",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VideoDistill: Language-aware Vision Distillation for Video Question Answering",
        "author": "Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Embracing Unimodal Aleatoric Uncertainty for Robust Multimodal Fusion",
        "author": "Zixian Gao, Xun Jiang, Xing Xu, Fumin Shen, Yujie Li, Heng Tao Shen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DGC-GNN: Leveraging Geometry and Color Cues for Visual Descriptor-Free 2D-3D Matching",
        "author": "Shuzhe Wang, Juho Kannala, Daniel Barath",
        "abstract": "Matching 2D keypoints in an image to a sparse 3D point cloud of the scene without requiring visual descriptors has garnered increased interest due to its low memory requirements, inherent privacy preservation, and reduced need for expensive 3D model maintenance compared to visual descriptor-based methods. However, existing algorithms often compromise on performance, resulting in a significant deterioration compared to their descriptor-based counterparts. In this paper, we introduce DGC-GNN, a novel algorithm that employs a global-to-local Graph Neural Network (GNN) that progressively exploits geometric and color cues to represent keypoints, thereby improving matching accuracy. Our procedure encodes both Euclidean and angular relations at a coarse level, forming the geometric embedding to guide the point matching. We evaluate DGC-GNN on both indoor and outdoor datasets, demonstrating that it not only doubles the accuracy of the state-of-the-art visual descriptor-free algorithm but also substantially narrows the performance gap between descriptor-based and descriptor-free methods.",
        "page": "http://arxiv.org/abs/2306.12547",
        "pdf": "http://arxiv.org/pdf/2306.12547.pdf"
    },
    {
        "title": "Multiplane Prior Guided Few-Shot Aerial Scene Rendering",
        "author": "Zihan Gao, Licheng Jiao, Lingling Li, Xu Liu, Fang Liu, Puhua Chen, Yuwei Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "4K4D: Real-Time 4D View Synthesis at 4K Resolution",
        "author": "Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, Xiaowei Zhou",
        "abstract": "This paper targets high-fidelity and real-time view synthesis of dynamic 3D scenes at 4K resolution. Recently, some methods on dynamic view synthesis have shown impressive rendering quality. However, their speed is still limited when rendering high-resolution images. To overcome this problem, we propose 4K4D, a 4D point cloud representation that supports hardware rasterization and enables unprecedented rendering speed. Our representation is built on a 4D feature grid so that the points are naturally regularized and can be robustly optimized. In addition, we design a novel hybrid appearance model that significantly boosts the rendering quality while preserving efficiency. Moreover, we develop a differentiable depth peeling algorithm to effectively learn the proposed model from RGB videos. Experiments show that our representation can be rendered at over 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on the ENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30x faster than previous methods and achieves the state-of-the-art rendering quality. Our project page is available at https://zju3dv.github.io/4k4d/.",
        "page": "http://arxiv.org/abs/2310.11448",
        "pdf": "http://arxiv.org/pdf/2310.11448.pdf"
    },
    {
        "title": "Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis",
        "author": "Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, Xiang Bai",
        "abstract": "Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal trade-off between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pre-trained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.",
        "page": "http://arxiv.org/abs/2403.01439",
        "pdf": "http://arxiv.org/pdf/2403.01439.pdf"
    },
    {
        "title": "Reconstruction-free Cascaded Adaptive Compressive Sensing",
        "author": "Chenxi Qiu, Tao Yue, Xuemei Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Progressive Divide-and-Conquer via Subsampling Decomposition for Accelerated MRI",
        "author": "Chong Wang, Lanqing Guo, Yufei Wang, Hao Cheng, Yi Yu, Bihan Wen",
        "abstract": "Deep unfolding networks (DUN) have emerged as a popular iterative framework for accelerated magnetic resonance imaging (MRI) reconstruction. However, conventional DUN aims to reconstruct all the missing information within the entire null space in each iteration. Thus it could be challenging when dealing with highly ill-posed degradation, usually leading to unsatisfactory reconstruction. In this work, we propose a Progressive Divide-And-Conquer (PDAC) strategy, aiming to break down the subsampling process in the actual severe degradation and thus perform reconstruction sequentially. Starting from decomposing the original maximum-a-posteriori problem of accelerated MRI, we present a rigorous derivation of the proposed PDAC framework, which could be further unfolded into an end-to-end trainable network. Specifically, each iterative stage in PDAC focuses on recovering a distinct moderate degradation according to the decomposition. Furthermore, as part of the PDAC iteration, such decomposition is adaptively learned as an auxiliary task through a degradation predictor which provides an estimation of the decomposed sampling mask. Following this prediction, the sampling mask is further integrated via a severity conditioning module to ensure awareness of the degradation severity at each stage. Extensive experiments demonstrate that our proposed method achieves superior performance on the publicly available fastMRI and Stanford2D FSE datasets in both multi-coil and single-coil settings.",
        "page": "http://arxiv.org/abs/2403.10064",
        "pdf": "http://arxiv.org/pdf/2403.10064.pdf"
    },
    {
        "title": "A Unified Approach for Text- and Image-guided 4D Scene Generation",
        "author": "Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, Shalini De Mello",
        "abstract": "Large-scale diffusion generative models are greatly simplifying image, video and 3D asset creation from user-provided text prompts and images. However, the challenging problem of text-to-4D dynamic 3D scene generation with diffusion guidance remains largely unexplored. We propose Dream-in-4D, which features a novel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2D diffusion guidance to effectively learn a high-quality static 3D asset in the first stage; (2) a deformable neural radiance field that explicitly disentangles the learned static asset from its deformation, preserving quality during motion learning; and (3) a multi-resolution feature grid for the deformation field with a displacement total variation loss to effectively learn motion with video diffusion guidance in the second stage. Through a user preference study, we demonstrate that our approach significantly advances image and motion quality, 3D consistency and text fidelity for text-to-4D generation compared to baseline approaches. Thanks to its motion-disentangled representation, Dream-in-4D can also be easily adapted for controllable generation where appearance is defined by one or multiple images, without the need to modify the motion learning stage. Thus, our method offers, for the first time, a unified approach for text-to-4D, image-to-4D and personalized 4D generation tasks.",
        "page": "http://arxiv.org/abs/2311.16854",
        "pdf": "http://arxiv.org/pdf/2311.16854.pdf"
    },
    {
        "title": "Intrinsic Image Diffusion for Indoor Single-view Material Estimation",
        "author": "Peter Kocsis, Vincent Sitzmann, Matthias Nie\u00dfner",
        "abstract": "We present Intrinsic Image Diffusion, a generative model for appearance decomposition of indoor scenes. Given a single input view, we sample multiple possible material explanations represented as albedo, roughness, and metallic maps. Appearance decomposition poses a considerable challenge in computer vision due to the inherent ambiguity between lighting and material properties and the lack of real datasets. To address this issue, we advocate for a probabilistic formulation, where instead of attempting to directly predict the true material properties, we employ a conditional generative model to sample from the solution space. Furthermore, we show that utilizing the strong learned prior of recent diffusion models trained on large-scale real-world images can be adapted to material estimation and highly improves the generalization to real images. Our method produces significantly sharper, more consistent, and more detailed materials, outperforming state-of-the-art methods by $1.5dB$ on PSNR and by $45\\%$ better FID score on albedo prediction. We demonstrate the effectiveness of our approach through experiments on both synthetic and real-world datasets.",
        "page": "http://arxiv.org/abs/2312.12274",
        "pdf": "http://arxiv.org/pdf/2312.12274.pdf"
    },
    {
        "title": "Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?",
        "author": "Zhengyue Zhao, Jinhao Duan, Kaidi Xu, Chenan Wang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu",
        "abstract": "Stable Diffusion has established itself as a foundation model in generative AI artistic applications, receiving widespread research and application. Some recent fine-tuning methods have made it feasible for individuals to implant personalized concepts onto the basic Stable Diffusion model with minimal computational costs on small datasets. However, these innovations have also given rise to issues like facial privacy forgery and artistic copyright infringement. In recent studies, researchers have explored the addition of imperceptible adversarial perturbations to images to prevent potential unauthorized exploitation and infringements when personal data is used for fine-tuning Stable Diffusion. Although these studies have demonstrated the ability to protect images, it is essential to consider that these methods may not be entirely applicable in real-world scenarios. In this paper, we systematically evaluate the use of perturbations to protect images within a practical threat model. The results suggest that these approaches may not be sufficient to safeguard image privacy and copyright effectively. Furthermore, we introduce a purification method capable of removing protected perturbations while preserving the original image structure to the greatest extent possible. Experiments reveal that Stable Diffusion can effectively learn from purified images over all protective methods.",
        "page": "http://arxiv.org/abs/2312.00084",
        "pdf": "http://arxiv.org/pdf/2312.00084.pdf"
    },
    {
        "title": "CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment",
        "author": "Hyeongmin Lee, Kyoungkook Kang, Jungseul Ok, Sunghyun Cho",
        "abstract": "Recent image tone adjustment (or enhancement) approaches have predominantly adopted supervised learning for learning human-centric perceptual assessment. However, these approaches are constrained by intrinsic challenges of supervised learning. Primarily, the requirement for expertly-curated or retouched images escalates the data acquisition expenses. Moreover, their coverage of target style is confined to stylistic variants inferred from the training data. To surmount the above challenges, we propose an unsupervised learning-based approach for text-based image tone adjustment method, CLIPtone, that extends an existing image enhancement method to accommodate natural language descriptions. Specifically, we design a hyper-network to adaptively modulate the pretrained parameters of the backbone model based on text description. To assess whether the adjusted image aligns with the text description without ground truth image, we utilize CLIP, which is trained on a vast set of language-image pairs and thus encompasses knowledge of human perception. The major advantages of our approach are three fold: (i) minimal data collection expenses, (ii) support for a range of adjustments, and (iii) the ability to handle novel text descriptions unseen in training. Our approach's efficacy is demonstrated through comprehensive experiments, including a user study.",
        "page": "http://arxiv.org/abs/2404.01123",
        "pdf": "http://arxiv.org/pdf/2404.01123.pdf"
    },
    {
        "title": "VGGSfM: Visual Geometry Grounded Deep Structure From Motion",
        "author": "Jianyuan Wang, Nikita Karaev, Christian Rupprecht, David Novotny",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Video Recognition in Portrait Mode",
        "author": "Mingfei Han, Linjie Yang, Xiaojie Jin, Jiashi Feng, Xiaojun Chang, Heng Wang",
        "abstract": "The creation of new datasets often presents new challenges for video recognition and can inspire novel ideas while addressing these challenges. While existing datasets mainly comprise landscape mode videos, our paper seeks to introduce portrait mode videos to the research community and highlight the unique challenges associated with this video format. With the growing popularity of smartphones and social media applications, recognizing portrait mode videos is becoming increasingly important. To this end, we have developed the first dataset dedicated to portrait mode video recognition, namely PortraitMode-400. The taxonomy of PortraitMode-400 was constructed in a data-driven manner, comprising 400 fine-grained categories, and rigorous quality assurance was implemented to ensure the accuracy of human annotations. In addition to the new dataset, we conducted a comprehensive analysis of the impact of video format (portrait mode versus landscape mode) on recognition accuracy and spatial bias due to the different formats. Furthermore, we designed extensive experiments to explore key aspects of portrait mode video recognition, including the choice of data augmentation, evaluation procedure, the importance of temporal information, and the role of audio modality. Building on the insights from our experimental results and the introduction of PortraitMode-400, our paper aims to inspire further research efforts in this emerging research area.",
        "page": "http://arxiv.org/abs/2312.13746",
        "pdf": "http://arxiv.org/pdf/2312.13746.pdf"
    },
    {
        "title": "Versatile Navigation under Partial Observability via Value-Guided Diffusion Policy",
        "author": "Gengyu Zhang, Hao Tang, Yan Yan",
        "abstract": "Route planning for navigation under partial observability plays a crucial role in modern robotics and autonomous driving. Existing route planning approaches can be categorized into two main classes: traditional autoregressive and diffusion-based methods. The former often fails due to its myopic nature, while the latter either assumes full observability or struggles to adapt to unfamiliar scenarios, due to strong couplings with behavior cloning from experts. To address these deficiencies, we propose a versatile diffusion-based approach for both 2D and 3D route planning under partial observability. Specifically, our value-guided diffusion policy first generates plans to predict actions across various timesteps, providing ample foresight to the planning. It then employs a differentiable planner with state estimations to derive a value function, directing the agent's exploration and goal-seeking behaviors without seeking experts while explicitly addressing partial observability. During inference, our policy is further enhanced by a best-plan-selection strategy, substantially boosting the planning success rate. Moreover, we propose projecting point clouds, derived from RGB-D inputs, onto 2D grid-based bird-eye-view maps via semantic segmentation, generalizing to 3D environments. This simple yet effective adaption enables zero-shot transfer from 2D-trained policy to 3D, cutting across the laborious training for 3D policy, and thus certifying our versatility. Experimental results demonstrate our superior performance, particularly in navigating situations beyond expert demonstrations, surpassing state-of-the-art autoregressive and diffusion-based baselines for both 2D and 3D scenarios.",
        "page": "http://arxiv.org/abs/2404.02176",
        "pdf": "http://arxiv.org/pdf/2404.02176.pdf"
    },
    {
        "title": "Point, Segment and Count: A Generalized Framework for Object Counting",
        "author": "Zhizhong Huang, Mingliang Dai, Yi Zhang, Junping Zhang, Hongming Shan",
        "abstract": "Class-agnostic object counting aims to count all objects in an image with respect to example boxes or class names, \\emph{a.k.a} few-shot and zero-shot counting. In this paper, we propose a generalized framework for both few-shot and zero-shot object counting based on detection. Our framework combines the superior advantages of two foundation models without compromising their zero-shot capability: (\\textbf{i}) SAM to segment all possible objects as mask proposals, and (\\textbf{ii}) CLIP to classify proposals to obtain accurate object counts. However, this strategy meets the obstacles of efficiency overhead and the small crowded objects that cannot be localized and distinguished. To address these issues, our framework, termed PseCo, follows three steps: point, segment, and count. Specifically, we first propose a class-agnostic object localization to provide accurate but least point prompts for SAM, which consequently not only reduces computation costs but also avoids missing small objects. Furthermore, we propose a generalized object classification that leverages CLIP image/text embeddings as the classifier, following a hierarchical knowledge distillation to obtain discriminative classifications among hierarchical mask proposals. Extensive experimental results on FSC-147, COCO, and LVIS demonstrate that PseCo achieves state-of-the-art performance in both few-shot/zero-shot object counting/detection. Code: https://github.com/Hzzone/PseCo",
        "page": "http://arxiv.org/abs/2311.12386",
        "pdf": "http://arxiv.org/pdf/2311.12386.pdf"
    },
    {
        "title": "Normalizing Flows on the Product Space of SO(3) Manifolds for Probabilistic Human Pose Modeling",
        "author": "Olaf D\u00fcnkel, Tim Salzmann, Florian Pfaff",
        "abstract": "Normalizing flows have proven their efficacy for density estimation in Euclidean space, but their application to rotational representations, crucial in various domains such as robotics or human pose modeling, remains underexplored. Probabilistic models of the human pose can benefit from approaches that rigorously consider the rotational nature of human joints. For this purpose, we introduce HuProSO3, a normalizing flow model that operates on a high-dimensional product space of SO(3) manifolds, modeling the joint distribution for human joints with three degrees of freedom. HuProSO3's advantage over state-of-the-art approaches is demonstrated through its superior modeling accuracy in three different applications and its capability to evaluate the exact likelihood. This work not only addresses the technical challenge of learning densities on SO(3) manifolds, but it also has broader implications for domains where the probabilistic regression of correlated 3D rotations is of importance.",
        "page": "http://arxiv.org/abs/2404.05675",
        "pdf": "http://arxiv.org/pdf/2404.05675.pdf"
    },
    {
        "title": "GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions",
        "author": "Junjie Wang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Qi Tian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers",
        "author": "Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, Matthias Nie\u00dfner",
        "abstract": "We introduce MeshGPT, a new approach for generating triangle meshes that reflects the compactness typical of artist-created meshes, in contrast to dense triangle meshes extracted by iso-surfacing methods from neural fields. Inspired by recent advances in powerful large language models, we adopt a sequence-based approach to autoregressively generate triangle meshes as sequences of triangles. We first learn a vocabulary of latent quantized embeddings, using graph convolutions, which inform these embeddings of the local mesh geometry and topology. These embeddings are sequenced and decoded into triangles by a decoder, ensuring that they can effectively reconstruct the mesh. A transformer is then trained on this learned vocabulary to predict the index of the next embedding given previous embeddings. Once trained, our model can be autoregressively sampled to generate new triangle meshes, directly generating compact meshes with sharp edges, more closely imitating the efficient triangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable improvement over state of the art mesh generation methods, with a 9% increase in shape coverage and a 30-point enhancement in FID scores across various categories.",
        "page": "http://arxiv.org/abs/2311.15475",
        "pdf": "http://arxiv.org/pdf/2311.15475.pdf"
    },
    {
        "title": "RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features",
        "author": "Geonho Bang, Kwangjin Choi, Jisong Kim, Dongsuk Kum, Jun Won Choi, Jun Won Choi",
        "abstract": "The inherent noisy and sparse characteristics of radar data pose challenges in finding effective representations for 3D object detection. In this paper, we propose RadarDistill, a novel knowledge distillation (KD) method, which can improve the representation of radar data by leveraging LiDAR data. RadarDistill successfully transfers desirable characteristics of LiDAR features into radar features using three key components: Cross-Modality Alignment (CMA), Activation-based Feature Distillation (AFD), and Proposal-based Feature Distillation (PFD). CMA enhances the density of radar features by employing multiple layers of dilation operations, effectively addressing the challenge of inefficient knowledge transfer from LiDAR to radar. AFD selectively transfers knowledge based on regions of the LiDAR features, with a specific focus on areas where activation intensity exceeds a predefined threshold. PFD similarly guides the radar network to selectively mimic features from the LiDAR network within the object proposals. Our comparative analyses conducted on the nuScenes datasets demonstrate that RadarDistill achieves state-of-the-art (SOTA) performance for radar-only object detection task, recording 20.5% in mAP and 43.7% in NDS. Also, RadarDistill significantly improves the performance of the camera-radar fusion model.",
        "page": "http://arxiv.org/abs/2403.05061",
        "pdf": "http://arxiv.org/pdf/2403.05061.pdf"
    },
    {
        "title": "Global Latent Neural Rendering",
        "author": "Thomas Tanay, Matteo Maggioni",
        "abstract": "A recent trend among generalizable novel view synthesis methods is to learn a rendering operator acting over single camera rays. This approach is promising because it removes the need for explicit volumetric rendering, but it effectively treats target images as collections of independent pixels. Here, we propose to learn a global rendering operator acting over all camera rays jointly. We show that the right representation to enable such rendering is a 5-dimensional plane sweep volume consisting of the projection of the input images on a set of planes facing the target camera. Based on this understanding, we introduce our Convolutional Global Latent Renderer (ConvGLR), an efficient convolutional architecture that performs the rendering operation globally in a low-resolution latent space. Experiments on various datasets under sparse and generalizable setups show that our approach consistently outperforms existing methods by significant margins.",
        "page": "http://arxiv.org/abs/2312.08338",
        "pdf": "http://arxiv.org/pdf/2312.08338.pdf"
    },
    {
        "title": "Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer",
        "author": "Junyi Wu, Bin Duan, Weitai Kang, Hao Tang, Yan Yan",
        "abstract": "While Transformers have rapidly gained popularity in various computer vision applications, post-hoc explanations of their internal mechanisms remain largely unexplored. Vision Transformers extract visual information by representing image regions as transformed tokens and integrating them via attention weights. However, existing post-hoc explanation methods merely consider these attention weights, neglecting crucial information from the transformed tokens, which fails to accurately illustrate the rationales behind the models' predictions. To incorporate the influence of token transformation into interpretation, we propose TokenTM, a novel post-hoc explanation method that utilizes our introduced measurement of token transformation effects. Specifically, we quantify token transformation effects by measuring changes in token lengths and correlations in their directions pre- and post-transformation. Moreover, we develop initialization and aggregation rules to integrate both attention weights and token transformation effects across all layers, capturing holistic token contributions throughout the model. Experimental results on segmentation and perturbation tests demonstrate the superiority of our proposed TokenTM compared to state-of-the-art Vision Transformer explanation methods.",
        "page": "http://arxiv.org/abs/2403.14552",
        "pdf": "http://arxiv.org/pdf/2403.14552.pdf"
    },
    {
        "title": "Cache Me if You Can: Accelerating Diffusion Models through Block Caching",
        "author": "Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, Christian Rupprecht, Daniel Cremers, Peter Vajda, Jialiang Wang",
        "abstract": "Diffusion models have recently revolutionized the field of image synthesis due to their ability to generate photorealistic images. However, one of the major drawbacks of diffusion models is that the image generation process is costly. A large image-to-image network has to be applied many times to iteratively refine an image from random noise. While many recent works propose techniques to reduce the number of required steps, they generally treat the underlying denoising network as a black box. In this work, we investigate the behavior of the layers within the network and find that 1) the layers' output changes smoothly over time, 2) the layers show distinct patterns of change, and 3) the change from step to step is often very small. We hypothesize that many layer computations in the denoising network are redundant. Leveraging this, we introduce block caching, in which we reuse outputs from layer blocks of previous steps to speed up inference. Furthermore, we propose a technique to automatically determine caching schedules based on each block's changes over timesteps. In our experiments, we show through FID, human evaluation and qualitative analysis that Block Caching allows to generate images with higher visual quality at the same computational cost. We demonstrate this for different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).",
        "page": "http://arxiv.org/abs/2312.03209",
        "pdf": "http://arxiv.org/pdf/2312.03209.pdf"
    },
    {
        "title": "ESR-NeRF: Emissive Source Reconstruction Using LDR Multi-view Images",
        "author": "Jinseo Jeong, Junseo Koo, Qimeng Zhang, Gunhee Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Epistemic Uncertainty Quantification For Pre-trained Neural Networks",
        "author": "Hanjing Wang, Qiang Ji",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning",
        "author": "Haiyang Ying, Yixuan Yin, Jinzhi Zhang, Fan Wang, Tao Yu, Ruqi Huang, Lu Fang",
        "abstract": "Towards holistic understanding of 3D scenes, a general 3D segmentation method is needed that can segment diverse objects without restrictions on object quantity or categories, while also reflecting the inherent hierarchical structure. To achieve this, we propose OmniSeg3D, an omniversal segmentation method aims for segmenting anything in 3D all at once. The key insight is to lift multi-view inconsistent 2D segmentations into a consistent 3D feature field through a hierarchical contrastive learning framework, which is accomplished by two steps. Firstly, we design a novel hierarchical representation based on category-agnostic 2D segmentations to model the multi-level relationship among pixels. Secondly, image features rendered from the 3D feature field are clustered at different levels, which can be further drawn closer or pushed apart according to the hierarchical relationship between different levels. In tackling the challenges posed by inconsistent 2D segmentations, this framework yields a global consistent 3D feature field, which further enables hierarchical segmentation, multi-object selection, and global discretization. Extensive experiments demonstrate the effectiveness of our method on high-quality 3D segmentation and accurate hierarchical structure understanding. A graphical user interface further facilitates flexible interaction for omniversal 3D segmentation.",
        "page": "http://arxiv.org/abs/2311.11666",
        "pdf": "http://arxiv.org/pdf/2311.11666.pdf"
    },
    {
        "title": "MRFS: Mutually Reinforcing Image Fusion and Segmentation",
        "author": "HAO ZHANG, Xuhui Zuo, Jie Jiang, Chunchao Guo, Jiayi Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation",
        "author": "Dale Decatur, Itai Lang, Kfir Aberman, Rana Hanocka",
        "abstract": "In this work we develop 3D Paintbrush, a technique for automatically texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly integrate into standard graphics pipelines. We opt to simultaneously produce a localization map (to specify the edit region) and a texture map which conforms to it. This synergistic approach improves the quality of both the localization and the stylization. To enhance the details and resolution of the textured area, we leverage multiple stages of a cascaded diffusion model to supervise our local editing technique with generative priors learned from images at different resolutions. Our technique, referred to as Cascaded Score Distillation (CSD), simultaneously distills scores at multiple resolutions in a cascaded fashion, enabling control over both the granularity and global understanding of the supervision. We demonstrate the effectiveness of 3D Paintbrush to locally texture a variety of shapes within different semantic regions. Project page: https://threedle.github.io/3d-paintbrush",
        "page": "http://arxiv.org/abs/2311.09571",
        "pdf": "http://arxiv.org/pdf/2311.09571.pdf"
    },
    {
        "title": "3D-LFM: Lifting Foundation Model",
        "author": "Mosam Dabhi, L\u00e1szl\u00f3 A. Jeni, Simon Lucey",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It",
        "author": "Adam Lilja, Junsheng Fu, Erik Stenborg, Lars Hammarstrand",
        "abstract": "The task of online mapping is to predict a local map using current sensor observations, e.g. from lidar and camera, without relying on a pre-built map. State-of-the-art methods are based on supervised learning and are trained predominantly using two datasets: nuScenes and Argoverse 2. However, these datasets revisit the same geographic locations across training, validation, and test sets. Specifically, over $80$% of nuScenes and $40$% of Argoverse 2 validation and test samples are less than $5$ m from a training sample. At test time, the methods are thus evaluated more on how well they localize within a memorized implicit map built from the training data than on extrapolating to unseen locations. Naturally, this data leakage causes inflated performance numbers and we propose geographically disjoint data splits to reveal the true performance in unseen environments. Experimental results show that methods perform considerably worse, some dropping more than $45$ mAP, when trained and evaluated on proper data splits. Additionally, a reassessment of prior design choices reveals diverging conclusions from those based on the original split. Notably, the impact of lifting methods and the support from auxiliary tasks (e.g., depth supervision) on performance appears less substantial or follows a different trajectory than previously perceived. Splits can be found at https://github.com/LiljaAdam/geographical-splits",
        "page": "http://arxiv.org/abs/2312.06420",
        "pdf": "http://arxiv.org/pdf/2312.06420.pdf"
    },
    {
        "title": "Masked AutoDecoder is Effective Multi-Task Vision Generalist",
        "author": "Han Qiu, Jiaxing Huang, Peng Gao, Lewei Lu, Xiaoqin Zhang, Shijian Lu",
        "abstract": "Inspired by the success of general-purpose models in NLP, recent studies attempt to unify different vision tasks in the same sequence format and employ autoregressive Transformers for sequence prediction. They apply uni-directional attention to capture sequential dependencies and generate task sequences recursively. However, such autoregressive Transformers may not fit vision tasks well, as vision task sequences usually lack the sequential dependencies typically observed in natural languages. In this work, we design Masked AutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of two core designs. First, we develop a parallel decoding framework that introduces bi-directional attention to capture contextual dependencies comprehensively and decode vision task sequences in parallel. Second, we design a masked sequence modeling approach that learns rich task contexts by masking and reconstructing task sequences. In this way, MAD handles all the tasks by a single network branch and a simple cross-entropy loss with minimal task-specific designs. Extensive experiments demonstrate the great potential of MAD as a new paradigm for unifying various vision tasks. MAD achieves superior performance and inference efficiency compared to autoregressive counterparts while obtaining competitive accuracy with task-specific models. Code will be released.",
        "page": "http://arxiv.org/abs/2403.07692",
        "pdf": "http://arxiv.org/pdf/2403.07692.pdf"
    },
    {
        "title": "PerceptionGPT: Effectively Fusing Visual Perception into LLM",
        "author": "Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, Tong Zhang",
        "abstract": "The integration of visual inputs with large language models (LLMs) has led to remarkable advancements in multi-modal capabilities, giving rise to visual large language models (VLLMs). However, effectively harnessing VLLMs for intricate visual perception tasks remains a challenge. In this paper, we present a novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding. Our proposed method treats the token embedding of the LLM as the carrier of spatial information, then leverage lightweight visual task encoders and decoders to perform visual perception tasks (e.g., detection, segmentation). Our approach significantly alleviates the training difficulty suffered by previous approaches that formulate the visual outputs as discrete tokens, and enables achieving superior performance with fewer trainable parameters, less training data and shorted training time. Moreover, as only one token embedding is required to decode the visual outputs, the resulting sequence length during inference is significantly reduced. Consequently, our approach enables accurate and flexible representations, seamless integration of visual perception tasks, and efficient handling of a multiple of visual outputs. We validate the effectiveness and efficiency of our approach through extensive experiments. The results demonstrate significant improvements over previous methods with much fewer trainable parameters and GPU hours, which facilitates future research in enabling LLMs with visual perception abilities.",
        "page": "http://arxiv.org/abs/2311.06612",
        "pdf": "http://arxiv.org/pdf/2311.06612.pdf"
    },
    {
        "title": "Probing the 3D Awareness of Visual Foundation Models",
        "author": "Mohamed El Banani, Amit Raj, Kevis-kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, Varun Jampani",
        "abstract": "Recent advances in large-scale pretraining have yielded visual foundation models with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and localize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual foundation models. We posit that 3D awareness implies that representations (1) encode the 3D structure of the scene and (2) consistently represent the surface across views. We conduct a series of experiments using task-specific probes and zero-shot inference procedures on frozen features. Our experiments reveal several limitations of the current models. Our code and analysis can be found at https://github.com/mbanani/probe3d.",
        "page": "http://arxiv.org/abs/2404.08636",
        "pdf": "http://arxiv.org/pdf/2404.08636.pdf"
    },
    {
        "title": "View-Category Interactive Sharing Transformer for Incomplete Multi-View Multi-Label Learning",
        "author": "Shilong Ou, Zhe Xue, Yawen Li, Meiyu Liang, Yuanqiang Cai, junjiang wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Generating Illustrated Instructions",
        "author": "Sachit Menon, Ishan Misra, Rohit Girdhar",
        "abstract": "We introduce the new task of generating Illustrated Instructions, i.e., visual instructions customized to a user's needs. We identify desiderata unique to this task, and formalize it through a suite of automatic and human evaluation metrics, designed to measure the validity, consistency, and efficacy of the generations. We combine the power of large language models (LLMs) together with strong text-to-image generation diffusion models to propose a simple approach called StackedDiffusion, which generates such illustrated instructions given text as input. The resulting model strongly outperforms baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases, users even prefer it to human-generated articles. Most notably, it enables various new and exciting applications far beyond what static articles on the web can provide, such as personalized instructions complete with intermediate steps and pictures in response to a user's individual situation.",
        "page": "http://arxiv.org/abs/2312.04552",
        "pdf": "http://arxiv.org/pdf/2312.04552.pdf"
    },
    {
        "title": "GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning",
        "author": "Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, Umar Iqbal",
        "abstract": "Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries and extract detailed meshes, we propose a novel SDF-based implicit mesh learning approach for 3D Gaussians that regularizes the underlying geometries and extracts highly detailed textured meshes. Our proposed method, GAvatar, enables the large-scale generation of diverse animatable avatars using only text prompts. GAvatar significantly surpasses existing methods in terms of both appearance and geometry quality, and achieves extremely fast rendering (100 fps) at 1K resolution.",
        "page": "http://arxiv.org/abs/2312.11461",
        "pdf": "http://arxiv.org/pdf/2312.11461.pdf"
    },
    {
        "title": "TexTile: A Differentiable Metric for Texture Tileability",
        "author": "Carlos Rodriguez-Pardo, Dan Casas, Elena Garces, Jorge Lopez-Moreno",
        "abstract": "We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability). Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture. In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures. Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing robustness and accuracy. We demonstrate that TexTile can be plugged into different state-of-the-art texture synthesis methods, including diffusion-based strategies, and generate tileable textures while keeping or even improving the overall texture quality. Furthermore, we show that TexTile can objectively evaluate any tileable texture synthesis method, whereas the current mix of existing metrics produces uncorrelated scores which heavily hinders progress in the field.",
        "page": "http://arxiv.org/abs/2403.12961",
        "pdf": "http://arxiv.org/pdf/2403.12961.pdf"
    },
    {
        "title": "Image Processing GNN: Breaking Rigidity in Super-Resolution",
        "author": "Yuchuan Tian, Hanting Chen, Chao Xu, Yunhe Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network",
        "author": "Hao Yang, Liyuan Pan, Yan Yang, Richard Hartley, Miaomiao Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation",
        "author": "Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang",
        "abstract": "Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multi-view-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks. The code for our project is publicly available.",
        "page": "http://arxiv.org/abs/2401.04728",
        "pdf": "http://arxiv.org/pdf/2401.04728.pdf"
    },
    {
        "title": "LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content",
        "author": "Qihao Zhao, Yalun Dai, Hao Li, Wei Hu, Fan Zhang, Jun Liu",
        "abstract": "Long-tail recognition is challenging because it requires the model to learn good representations from tail categories and address imbalances across all categories. In this paper, we propose a novel generative and fine-tuning framework, LTGC, to handle long-tail recognition via leveraging generated content. Firstly, inspired by the rich implicit knowledge in large-scale models (e.g., large language models, LLMs), LTGC leverages the power of these models to parse and reason over the original tail data to produce diverse tail-class content. We then propose several novel designs for LTGC to ensure the quality of the generated data and to efficiently fine-tune the model using both the generated and original data. The visualization demonstrates the effectiveness of the generation module in LTGC, which produces accurate and diverse tail data. Additionally, the experimental results demonstrate that our LTGC outperforms existing state-of-the-art methods on popular long-tailed benchmarks.",
        "page": "http://arxiv.org/abs/2403.05854",
        "pdf": "http://arxiv.org/pdf/2403.05854.pdf"
    },
    {
        "title": "S$^2$MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering",
        "author": "Zhen Long, Qiyuan Wang, Yazhou Ren, Yipeng Liu, Ce Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adapting Short-Term Transformers for Action Detection in Untrimmed Videos",
        "author": "Min Yang, gaohuan, Ping Guo, Limin Wang",
        "abstract": "Vision Transformer (ViT) has shown high potential in video recognition, owing to its flexible design, adaptable self-attention mechanisms, and the efficacy of masked pre-training. Yet, it remains unclear how to adapt these pre-trained short-term ViTs for temporal action detection (TAD) in untrimmed videos. The existing works treat them as off-the-shelf feature extractors for each short-trimmed snippet without capturing the fine-grained relation among different snippets in a broader temporal context. To mitigate this issue, this paper focuses on designing a new mechanism for adapting these pre-trained ViT models as a unified long-form video transformer to fully unleash its modeling power in capturing inter-snippet relation, while still keeping low computation overhead and memory consumption for efficient TAD. To this end, we design effective cross-snippet propagation modules to gradually exchange short-term video information among different snippets from two levels. For inner-backbone information propagation, we introduce a cross-snippet propagation strategy to enable multi-snippet temporal feature interaction inside the backbone.For post-backbone information propagation, we propose temporal transformer layers for further clip-level modeling. With the plain ViT-B pre-trained with VideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very competitive performance to previous temporal action detectors, riching up to 69.5 average mAP on THUMOS14, 37.40 average mAP on ActivityNet-1.3 and 17.20 average mAP on FineAction.",
        "page": "http://arxiv.org/abs/2312.01897",
        "pdf": "http://arxiv.org/pdf/2312.01897.pdf"
    },
    {
        "title": "MAFA: Managing False Negatives for Vision-Language Pre-training",
        "author": "Jaeseok Byun, Dohoon Kim, Taesup Moon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization",
        "author": "Khiem Le, Tuan Long Ho, Cuong Do, Danh Le-Phuoc, KOK SENG WONG",
        "abstract": "Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. Federated Domain Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the Federated Learning paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while retaining discrimination of those features. Then, we incorporate a simple yet effective regularizer to guide these models in directly capturing domain-invariant representations that the global model's classifier can leverage. Extensive experimental results on two benchmark datasets, i.e., PACS and Office-Home, and a real-world medical dataset, Camelyon17, indicate that our proposed method outperforms other existing methods in addressing this particular problem.",
        "page": "http://arxiv.org/abs/2403.15605",
        "pdf": "http://arxiv.org/pdf/2403.15605.pdf"
    },
    {
        "title": "Unsupervised Gaze Representation Learning from Multi-view Face Images",
        "author": "Yiwei Bao, Feng Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PEEKABOO: Interactive Video Generation via Masked-Diffusion",
        "author": "Yash Jain, Anshul Nasery, Vibhav Vineet, Harkirat Behl",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video Question-Answering",
        "author": "Zhaohe Liao, Jiangtong Li, Li Niu, Liqing Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion",
        "author": "Roy Kapon, Guy Tevet, Daniel Cohen-Or, Amit H. Bermano",
        "abstract": "We introduce Multi-view Ancestral Sampling (MAS), a method for 3D motion generation, using 2D diffusion models that were trained on motions obtained from in-the-wild videos. As such, MAS opens opportunities to exciting and diverse fields of motion previously under-explored as 3D data is scarce and hard to collect. MAS works by simultaneously denoising multiple 2D motion sequences representing different views of the same 3D motion. It ensures consistency across all views at each diffusion step by combining the individual generations into a unified 3D sequence, and projecting it back to the original views. We demonstrate MAS on 2D pose data acquired from videos depicting professional basketball maneuvers, rhythmic gymnastic performances featuring a ball apparatus, and horse races. In each of these domains, 3D motion capture is arduous, and yet, MAS generates diverse and realistic 3D sequences. Unlike the Score Distillation approach, which optimizes each sample by repeatedly applying small fixes, our method uses a sampling process that was constructed for the diffusion framework. As we demonstrate, MAS avoids common issues such as out-of-domain sampling and mode-collapse. https://guytevet.github.io/mas-page/",
        "page": "http://arxiv.org/abs/2310.14729",
        "pdf": "http://arxiv.org/pdf/2310.14729.pdf"
    },
    {
        "title": "Reg-PTQ: Regression-specialized Post-training Quantization for Fully Quantized Object Detector",
        "author": "Yifu Ding, Weilun Feng, Chuyan Chen, Jinyang Guo, Xianglong Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "From Coarse to Fine-Grained Open-Set Recognition",
        "author": "Nico Lang, V\u00e9steinn Sn\u00e6bjarnarson, Elijah Cole, Oisin Mac Aodha, Christian Igel, Serge Belongie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DSL-FIQA: Assessing Facial Image Quality via Dual-Set Degradation Learning and Landmark-Guided Transformer",
        "author": "Wei-Ting Chen, Gurunandan Krishnan, Qiang Gao, Sy-Yen Kuo, Sizhuo Ma, Jian Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Discriminative Pattern Calibration Mechanism for Source-Free Domain Adaptation",
        "author": "Haifeng Xia, Siyu Xia, Zhengming Ding",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "RAM-Avatar: Real-time Photo-Realistic Avatar from Monocular Videos with Full-body Control",
        "author": "xiang deng, Zerong Zheng, Yuxiang Zhang, Jingxiang Sun, Chao Xu, Xiaodong Yang, Lizhen Wang, Yebin Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EMCAD: Efficient Multi-scale Convolutional Attention Decoding for Medical Image Segmentation",
        "author": "Md Mostafijur Rahman, Mustafa Munir, Radu Marculescu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction",
        "author": "Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin",
        "abstract": "Implicit neural representation has paved the way for new approaches to dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic neural rendering methods rely heavily on these implicit representations, which frequently struggle to capture the intricate details of objects in the scene. Furthermore, implicit methods have difficulty achieving real-time rendering in general dynamic scenes, limiting their use in a variety of tasks. To address the issues, we propose a deformable 3D Gaussians Splatting method that reconstructs scenes using 3D Gaussians and learns them in canonical space with a deformation field to model monocular dynamic scenes. We also introduce an annealing smoothing training mechanism with no extra overhead, which can mitigate the impact of inaccurate poses on the smoothness of time interpolation tasks in real-world datasets. Through a differential Gaussian rasterizer, the deformable 3D Gaussians not only achieve higher rendering quality but also real-time rendering speed. Experiments show that our method outperforms existing methods significantly in terms of both rendering quality and speed, making it well-suited for tasks such as novel-view synthesis, time interpolation, and real-time rendering.",
        "page": "http://arxiv.org/abs/2309.13101",
        "pdf": "http://arxiv.org/pdf/2309.13101.pdf"
    },
    {
        "title": "TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation",
        "author": "Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, Michael J. Black",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals",
        "author": "Jiangnan Tang, Jingya Wang, Kaiyang Ji, Lan Xu, Jingyi Yu, Ye Shi",
        "abstract": "Estimating full-body human motion via sparse tracking signals from head-mounted displays and hand controllers in 3D scenes is crucial to applications in AR/VR. One of the biggest challenges to this task is the one-to-many mapping from sparse observations to dense full-body motions, which endowed inherent ambiguities. To help resolve this ambiguous problem, we introduce a new framework to combine rich contextual information provided by scenes to benefit full-body motion tracking from sparse observations. To estimate plausible human motions given sparse tracking signals and 3D scenes, we develop $\\text{S}^2$Fusion, a unified framework fusing \\underline{S}cene and sparse \\underline{S}ignals with a conditional dif\\underline{Fusion} model. $\\text{S}^2$Fusion first extracts the spatial-temporal relations residing in the sparse signals via a periodic autoencoder, and then produces time-alignment feature embedding as additional inputs. Subsequently, by drawing initial noisy motion from a pre-trained prior, $\\text{S}^2$Fusion utilizes conditional diffusion to fuse scene geometry and sparse tracking signals to generate full-body scene-aware motions. The sampling procedure of $\\text{S}^2$Fusion is further guided by a specially designed scene-penetration loss and phase-matching loss, which effectively regularizes the motion of the lower body even in the absence of any tracking signals, making the generated motion much more plausible and coherent. Extensive experimental results have demonstrated that our $\\text{S}^2$Fusion outperforms the state-of-the-art in terms of estimation quality and smoothness.",
        "page": "http://arxiv.org/abs/2404.04890",
        "pdf": "http://arxiv.org/pdf/2404.04890.pdf"
    },
    {
        "title": "How to Make Cross Encoder a Good Teacher for Efficient Image-Text Retrieval?",
        "author": "Yuxin Chen, Zongyang Ma, Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Ying Shan, Xiaojuan Qi, Weiming Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Locally Adaptive Neural 3D Morphable Models",
        "author": "Michail Tarasiou, Rolandos Alexandros Potamias, Eimear O' Sullivan, Stylianos Ploumpis, Stefanos Zafeiriou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Revisiting Adversarial Training at Scale",
        "author": "Zeyu Wang, Xianhang li, Hongru Zhu, Cihang Xie",
        "abstract": "The machine learning community has witnessed a drastic change in the training pipeline, pivoted by those ''foundation models'' with unprecedented scales. However, the field of adversarial training is lagging behind, predominantly centered around small model sizes like ResNet-50, and tiny and low-resolution datasets like CIFAR-10. To bridge this transformation gap, this paper provides a modern re-examination with adversarial training, investigating its potential benefits when applied at scale. Additionally, we introduce an efficient and effective training strategy to enable adversarial training with giant models and web-scale data at an affordable computing cost. We denote this newly introduced framework as AdvXL. Empirical results demonstrate that AdvXL establishes new state-of-the-art robust accuracy records under AutoAttack on ImageNet-1K. For example, by training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and $l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively. This achievement posits AdvXL as a pioneering approach, charting a new trajectory for the efficient training of robust visual representations at significantly larger scales. Our code is available at https://github.com/UCSC-VLAA/AdvXL.",
        "page": "http://arxiv.org/abs/2401.04727",
        "pdf": "http://arxiv.org/pdf/2401.04727.pdf"
    },
    {
        "title": "Benchmarking Segmentation Models with Mask-Preserved Attribute Editing",
        "author": "Zijin Yin, Kongming Liang, Bing Li, Zhanyu Ma, Jun Guo",
        "abstract": "When deploying segmentation models in practice, it is critical to evaluate their behaviors in varied and complex scenes. Different from the previous evaluation paradigms only in consideration of global attribute variations (e.g. adverse weather), we investigate both local and global attribute variations for robustness evaluation. To achieve this, we construct a mask-preserved attribute editing pipeline to edit visual attributes of real images with precise control of structural information. Therefore, the original segmentation labels can be reused for the edited images. Using our pipeline, we construct a benchmark covering both object and image attributes (e.g. color, material, pattern, style). We evaluate a broad variety of semantic segmentation models, spanning from conventional close-set models to recent open-vocabulary large models on their robustness to different types of variations. We find that both local and global attribute variations affect segmentation performances, and the sensitivity of models diverges across different variation types. We argue that local attributes have the same importance as global attributes, and should be considered in the robustness evaluation of segmentation models. Code: https://github.com/PRIS-CV/Pascal-EA.",
        "page": "http://arxiv.org/abs/2403.01231",
        "pdf": "http://arxiv.org/pdf/2403.01231.pdf"
    },
    {
        "title": "MaskCLR: Attention-Guided Contrastive Learning for Robust Action Representation Learning",
        "author": "Mohamed Abdelfattah, Mariam Hassan, Alex Alahi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fair Federated Learning under Domain Skew with Local Consistency and Domain Diversity",
        "author": "Yuhang Chen, Wenke Huang, Mang Ye",
        "abstract": "Federated learning (FL) has emerged as a new paradigm for privacy-preserving collaborative training. Under domain skew, the current FL approaches are biased and face two fairness problems. 1) Parameter Update Conflict: data disparity among clients leads to varying parameter importance and inconsistent update directions. These two disparities cause important parameters to potentially be overwhelmed by unimportant ones of dominant updates. It consequently results in significant performance decreases for lower-performing clients. 2) Model Aggregation Bias: existing FL approaches introduce unfair weight allocation and neglect domain diversity. It leads to biased model convergence objective and distinct performance among domains. We discover a pronounced directional update consistency in Federated Learning and propose a novel framework to tackle above issues. First, leveraging the discovered characteristic, we selectively discard unimportant parameter updates to prevent updates from clients with lower performance overwhelmed by unimportant parameters, resulting in fairer generalization performance. Second, we propose a fair aggregation objective to prevent global model bias towards some domains, ensuring that the global model continuously aligns with an unbiased model. The proposed method is generic and can be combined with other existing FL methods to enhance fairness. Comprehensive experiments on Digits and Office-Caltech demonstrate the high fairness and performance of our method.",
        "page": "http://arxiv.org/abs/2405.16585",
        "pdf": "http://arxiv.org/pdf/2405.16585.pdf"
    },
    {
        "title": "Visual In-Context Prompting",
        "author": "Feng Li, Qing Jiang, Hao Zhang, Shilong Liu, Huaizhe Xu, Xueyan Zou, Tianhe Ren, Hongyang Li, Lei Zhang, Chunyuan Li, Jianwei Yang, Jianfeng Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dual DETRs for Multi-Label Temporal Action Detection",
        "author": "Yuhan Zhu, Guozhen Zhang, Jing Tan, Gangshan Wu, Limin Wang",
        "abstract": "Temporal Action Detection (TAD) aims to identify the action boundaries and the corresponding category within untrimmed videos. Inspired by the success of DETR in object detection, several methods have adapted the query-based framework to the TAD task. However, these approaches primarily followed DETR to predict actions at the instance level (i.e., identify each action by its center point), leading to sub-optimal boundary localization. To address this issue, we propose a new Dual-level query-based TAD framework, namely DualDETR, to detect actions from both instance-level and boundary-level. Decoding at different levels requires semantics of different granularity, therefore we introduce a two-branch decoding structure. This structure builds distinctive decoding processes for different levels, facilitating explicit capture of temporal cues and semantics at each level. On top of the two-branch design, we present a joint query initialization strategy to align queries from both levels. Specifically, we leverage encoder proposals to match queries from each level in a one-to-one manner. Then, the matched queries are initialized using position and content prior from the matched action proposal. The aligned dual-level queries can refine the matched proposal with complementary cues during subsequent decoding. We evaluate DualDETR on three challenging multi-label TAD benchmarks. The experimental results demonstrate the superior performance of DualDETR to the existing state-of-the-art methods, achieving a substantial improvement under det-mAP and delivering impressive results under seg-mAP.",
        "page": "http://arxiv.org/abs/2404.00653",
        "pdf": "http://arxiv.org/pdf/2404.00653.pdf"
    },
    {
        "title": "Symphonize 3D Semantic Scene Completion with Contextual Instance Queries",
        "author": "Haoyi Jiang, Tianheng Cheng, Naiyu Gao, Haoyang Zhang, Tianwei Lin, Wenyu Liu, Xinggang Wang",
        "abstract": "`3D Semantic Scene Completion (SSC) has emerged as a nascent and pivotal undertaking in autonomous driving, aiming to predict voxel occupancy within volumetric scenes. However, prevailing methodologies primarily focus on voxel-wise feature aggregation, while neglecting instance semantics and scene context. In this paper, we present a novel paradigm termed Symphonies (Scene-from-Insts), that delves into the integration of instance queries to orchestrate 2D-to-3D reconstruction and 3D scene modeling. Leveraging our proposed Serial Instance-Propagated Attentions, Symphonies dynamically encodes instance-centric semantics, facilitating intricate interactions between image-based and volumetric domains. Simultaneously, Symphonies enables holistic scene comprehension by capturing context through the efficient fusion of instance queries, alleviating geometric ambiguity such as occlusion and perspective errors through contextual scene reasoning. Experimental results demonstrate that Symphonies achieves state-of-the-art performance on challenging benchmarks SemanticKITTI and SSCBench-KITTI-360, yielding remarkable mIoU scores of 15.04 and 18.58, respectively. These results showcase the paradigm's promising advancements. The code is available at https://github.com/hustvl/Symphonies.",
        "page": "http://arxiv.org/abs/2306.15670",
        "pdf": "http://arxiv.org/pdf/2306.15670.pdf"
    },
    {
        "title": "End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames",
        "author": "Shuming Liu, Chenlin Zhang, Chen Zhao, Bernard Ghanem",
        "abstract": "Recently, temporal action detection (TAD) has seen significant performance improvement with end-to-end training. However, due to the memory bottleneck, only models with limited scales and limited data volumes can afford end-to-end training, which inevitably restricts TAD performance. In this paper, we reduce the memory consumption for end-to-end training, and manage to scale up the TAD backbone to 1 billion parameters and the input video to 1,536 frames, leading to significant detection performance. The key to our approach lies in our proposed temporal-informative adapter (TIA), which is a novel lightweight module that reduces training memory. Using TIA, we free the humongous backbone from learning to adapt to the TAD task by only updating the parameters in TIA. TIA also leads to better TAD representation by temporally aggregating context from adjacent frames throughout the backbone. We evaluate our model across four representative datasets. Owing to our efficient design, we are able to train end-to-end on VideoMAEv2-giant and achieve 75.4% mAP on THUMOS14, being the first end-to-end model to outperform the best feature-based methods. Code is available at https://github.com/sming256/AdaTAD.",
        "page": "http://arxiv.org/abs/2311.17241",
        "pdf": "http://arxiv.org/pdf/2311.17241.pdf"
    },
    {
        "title": "AlignSAM: Aligning Segment Anything Model to Open Context via Reinforcement Learning",
        "author": "Duojun Huang, Xinyu Xiong, Jie Ma, Jichang Li, Zequn Jie, Lin Ma, Guanbin Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Adaptive VIO: Deep Visual-Inertial Odometry with Online Continual Learning",
        "author": "Youqi Pan, Wugen Zhou, Yingdian Cao, Hongbin Zha",
        "abstract": "Visual-inertial odometry (VIO) has demonstrated remarkable success due to its low-cost and complementary sensors. However, existing VIO methods lack the generalization ability to adjust to different environments and sensor attributes. In this paper, we propose Adaptive VIO, a new monocular visual-inertial odometry that combines online continual learning with traditional nonlinear optimization. Adaptive VIO comprises two networks to predict visual correspondence and IMU bias. Unlike end-to-end approaches that use networks to fuse the features from two modalities (camera and IMU) and predict poses directly, we combine neural networks with visual-inertial bundle adjustment in our VIO system. The optimized estimates will be fed back to the visual and IMU bias networks, refining the networks in a self-supervised manner. Such a learning-optimization-combined framework and feedback mechanism enable the system to perform online continual learning. Experiments demonstrate that our Adaptive VIO manifests adaptive capability on EuRoC and TUM-VI datasets. The overall performance exceeds the currently known learning-based VIO methods and is comparable to the state-of-the-art optimization-based methods.",
        "page": "http://arxiv.org/abs/2405.16754",
        "pdf": "http://arxiv.org/pdf/2405.16754.pdf"
    },
    {
        "title": "Tri-Modal Motion Retrieval by Learning a Joint Embedding Space",
        "author": "Kangning Yin, Shihao Zou, Yuxuan Ge, Zheng Tian",
        "abstract": "Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignment and synergistic effects among text, video, and motion modalities. Empirically, our results on the HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art performance in various motion-related cross-modal retrieval tasks, including text-to-motion, motion-to-text, video-to-motion and motion-to-video.",
        "page": "http://arxiv.org/abs/2403.00691",
        "pdf": "http://arxiv.org/pdf/2403.00691.pdf"
    },
    {
        "title": "The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN Inversion and High Quality Image Editing",
        "author": "Denis Bobkov, Vadim Titov, Aibek Alanov, Dmitry Vetrov",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Interactive Continual Learning: Fast and Slow Thinking",
        "author": "Biqing Qi, Xinquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "In-Context Matting",
        "author": "He Guo, Zixuan Ye, Zhiguo Cao, Hao Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EscherNet: A Generative Model for Scalable View Synthesis",
        "author": "Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison",
        "abstract": "We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis -- it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: https://kxhit.github.io/EscherNet.",
        "page": "http://arxiv.org/abs/2402.03908",
        "pdf": "http://arxiv.org/pdf/2402.03908.pdf"
    },
    {
        "title": "FlowTrack: Revisiting Optical Flow for Long-Range Dense Tracking",
        "author": "Seokju Cho, Gabriel Huang, Seungryong Kim, Joon-Young Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MVCPS-NeuS: Multi-view Constrained Photometric Stereo for Neural Surface Reconstruction",
        "author": "Hiroaki Santo, Fumio Okura, Yasuyuki Matsushita",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LLaFS: When Large Language Models Meet Few-Shot Segmentation",
        "author": "Lanyun Zhu, Tianrun Chen, Deyi Ji, Deyi Ji, Jieping Ye, Jun Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Memorization-Free Diffusion Models",
        "author": "Chen Chen, Daochang Liu, Chang Xu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unified Entropy Optimization for Open-Set Test-Time Adaptation",
        "author": "Zhengqing Gao, Xu-Yao Zhang, Cheng-Lin Liu",
        "abstract": "Test-time adaptation (TTA) aims at adapting a model pre-trained on the labeled source domain to the unlabeled target domain. Existing methods usually focus on improving TTA performance under covariate shifts, while neglecting semantic shifts. In this paper, we delve into a realistic open-set TTA setting where the target domain may contain samples from unknown classes. Many state-of-the-art closed-set TTA methods perform poorly when applied to open-set scenarios, which can be attributed to the inaccurate estimation of data distribution and model confidence. To address these issues, we propose a simple but effective framework called unified entropy optimization (UniEnt), which is capable of simultaneously adapting to covariate-shifted in-distribution (csID) data and detecting covariate-shifted out-of-distribution (csOOD) data. Specifically, UniEnt first mines pseudo-csID and pseudo-csOOD samples from test data, followed by entropy minimization on the pseudo-csID data and entropy maximization on the pseudo-csOOD data. Furthermore, we introduce UniEnt+ to alleviate the noise caused by hard data partition leveraging sample-level confidence. Extensive experiments on CIFAR benchmarks and Tiny-ImageNet-C show the superiority of our framework. The code is available at https://github.com/gaozhengqing/UniEnt",
        "page": "http://arxiv.org/abs/2404.06065",
        "pdf": "http://arxiv.org/pdf/2404.06065.pdf"
    },
    {
        "title": "SEED-Bench: Benchmarking Multimodal Large Language Models",
        "author": "Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels",
        "author": "Tuo Feng, Wenguan Wang, Fan Ma, Yi Yang",
        "abstract": "Autonomous systems need to process large-scale, sparse, and irregular point clouds with limited compute resources. Consequently, it is essential to develop LiDAR perception methods that are both efficient and effective. Although naively enlarging 3D kernel size can enhance performance, it will also lead to a cubically-increasing overhead. Therefore, it is crucial to develop streamlined 3D large kernel designs that eliminate redundant weights and work effectively with larger kernels. In this paper, we propose an efficient and effective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages dynamic pruning to amplify the 3D kernel size. Our method comprises two core components: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight Selection (CWS). SDS dynamically prunes and regrows volumetric weights from the beginning to learn a large sparse 3D kernel. It not only boosts performance but also significantly reduces model size and computational cost. Moreover, CWS selects the most important channels for 3D convolution during training and subsequently prunes the redundant channels to accelerate inference for 3D vision tasks. We demonstrate the effectiveness of LSK3DNet on three benchmark datasets and five tracks compared with classical models and large kernel designs. Notably, LSK3DNet achieves the state-of-the-art performance on SemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with roughly 40% model size reduction and 60% computing operations reduction compared to the naive large 3D kernel model.",
        "page": "http://arxiv.org/abs/2403.15173",
        "pdf": "http://arxiv.org/pdf/2403.15173.pdf"
    },
    {
        "title": "Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model",
        "author": "Zelin Peng, Zhengqin Xu, Zhilin Zeng, Lingxi Xie, Qi Tian, Wei Shen",
        "abstract": "Parameter-efficient fine-tuning (PEFT) is an effective methodology to unleash the potential of large foundation models in novel scenarios with limited training data. In the computer vision community, PEFT has shown effectiveness in image classification, but little research has studied its ability for image segmentation. Fine-tuning segmentation models usually require a heavier adjustment of parameters to align the proper projection directions in the parameter space for new scenarios. This raises a challenge to existing PEFT algorithms, as they often inject a limited number of individual parameters into each block, which prevents substantial adjustment of the projection direction of the parameter space due to the limitation of Hidden Markov Chain along blocks. In this paper, we equip PEFT with a cross-block orchestration mechanism to enable the adaptation of the Segment Anything Model (SAM) to various downstream scenarios. We introduce a novel inter-block communication module, which integrates a learnable relation matrix to facilitate communication among different coefficient sets of each PEFT block's parameter space. Moreover, we propose an intra-block enhancement module, which introduces a linear projection head whose weights are generated from a hyper-complex layer, further enhancing the impact of the adjustment of projection directions on the entire parameter space. Extensive experiments on diverse benchmarks demonstrate that our proposed approach consistently improves the segmentation performance significantly on novel scenarios with only around 1K additional parameters.",
        "page": "http://arxiv.org/abs/2311.17112",
        "pdf": "http://arxiv.org/pdf/2311.17112.pdf"
    },
    {
        "title": "MGMap: Mask-Guided Learning for Online Vectorized HD Map Construction",
        "author": "Xiaolu Liu, Song Wang, Wentong Li, Ruizi Yang, Junbo Chen, Jianke Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ViT-Lens: Towards Omni-modal Representations",
        "author": "Stan Weixian Lei, Yixiao Ge, Kun Yi, Jianfeng Zhang, Difei Gao, Dylan Sun, Yuying Ge, Ying Shan, Mike Zheng Shou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rewrite the stars",
        "author": "Xu Ma, Xiyang Dai, Yue Bai, Yizhou Wang, Yun Fu",
        "abstract": "Recent studies have drawn attention to the untapped potential of the \"star operation\" (element-wise multiplication) in network design. While intuitive explanations abound, the foundational rationale behind its application remains largely unexplored. Our study attempts to reveal the star operation's ability to map inputs into high-dimensional, non-linear feature spaces -- akin to kernel tricks -- without widening the network. We further introduce StarNet, a simple yet powerful prototype, demonstrating impressive performance and low latency under compact network structure and efficient budget. Like stars in the sky, the star operation appears unremarkable but holds a vast universe of potential. Our work encourages further exploration across tasks, with codes available at https://github.com/ma-xu/Rewrite-the-Stars.",
        "page": "http://arxiv.org/abs/2403.19967",
        "pdf": "http://arxiv.org/pdf/2403.19967.pdf"
    },
    {
        "title": "MultiPhys: Multi-Person Physics-aware 3D Motion Estimation",
        "author": "Nicol\u00e1s Ugrinovic, Boxiao Pan, Georgios Pavlakos, Despoina Paschalidou, Bokui Shen, Jordi Sanchez-Riera, Francesc Moreno-Noguer, Leonidas Guibas",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection",
        "author": "Hanshi Wang, Zhipeng Zhang, Jin Gao, Weiming Hu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
        "author": "Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan",
        "abstract": "We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.",
        "page": "http://arxiv.org/abs/2311.06242",
        "pdf": "http://arxiv.org/pdf/2311.06242.pdf"
    },
    {
        "title": "Adversarial Score Distillation: When score distillation meets GAN",
        "author": "Min Wei, Jingkai Zhou, Junyao Sun, Xuesong Zhang",
        "abstract": "Existing score distillation methods are sensitive to classifier-free guidance (CFG) scale: manifested as over-smoothness or instability at small CFG scales, while over-saturation at large ones. To explain and analyze these issues, we revisit the derivation of Score Distillation Sampling (SDS) and decipher existing score distillation with the Wasserstein Generative Adversarial Network (WGAN) paradigm. With the WGAN paradigm, we find that existing score distillation either employs a fixed sub-optimal discriminator or conducts incomplete discriminator optimization, resulting in the scale-sensitive issue. We propose the Adversarial Score Distillation (ASD), which maintains an optimizable discriminator and updates it using the complete optimization objective. Experiments show that the proposed ASD performs favorably in 2D distillation and text-to-3D tasks against existing methods. Furthermore, to explore the generalization ability of our WGAN paradigm, we extend ASD to the image editing task, which achieves competitive results. The project page and code are at https://github.com/2y7c3/ASD.",
        "page": "http://arxiv.org/abs/2312.00739",
        "pdf": "http://arxiv.org/pdf/2312.00739.pdf"
    },
    {
        "title": "HumanNeRF-SE: A Simple yet Effective Approach to Animate HumanNeRF with Diverse Poses",
        "author": "Caoyuan Ma, Yu-Lun Liu, Zhixiang Wang, Wu Liu, Xinchen Liu, Zheng Wang",
        "abstract": "We present HumanNeRF-SE, a simple yet effective method that synthesizes diverse novel pose images with simple input. Previous HumanNeRF works require a large number of optimizable parameters to fit the human images. Instead, we reload these approaches by combining explicit and implicit human representations to design both generalized rigid deformation and specific non-rigid deformation. Our key insight is that explicit shape can reduce the sampling points used to fit implicit representation, and frozen blending weights from SMPL constructing a generalized rigid deformation can effectively avoid overfitting and improve pose generalization performance. Our architecture involving both explicit and implicit representation is simple yet effective. Experiments demonstrate our model can synthesize images under arbitrary poses with few-shot input and increase the speed of synthesizing images by 15 times through a reduction in computational complexity without using any existing acceleration modules. Compared to the state-of-the-art HumanNeRF studies, HumanNeRF-SE achieves better performance with fewer learnable parameters and less training time.",
        "page": "http://arxiv.org/abs/2312.02232",
        "pdf": "http://arxiv.org/pdf/2312.02232.pdf"
    },
    {
        "title": "Communication-Efficient Collaborative Perception via Information Filling with Codebook",
        "author": "Yue Hu, Juntong Peng, Sifei Liu, Junhao Ge, Si Liu, Siheng Chen",
        "abstract": "Collaborative perception empowers each agent to improve its perceptual ability through the exchange of perceptual messages with other agents. It inherently results in a fundamental trade-off between perception ability and communication cost. To address this bottleneck issue, our core idea is to optimize the collaborative messages from two key aspects: representation and selection. The proposed codebook-based message representation enables the transmission of integer codes, rather than high-dimensional feature maps. The proposed information-filling-driven message selection optimizes local messages to collectively fill each agent's information demand, preventing information overflow among multiple agents. By integrating these two designs, we propose CodeFilling, a novel communication-efficient collaborative perception system, which significantly advances the perception-communication trade-off and is inclusive to both homogeneous and heterogeneous collaboration settings. We evaluate CodeFilling in both a real-world dataset, DAIR-V2X, and a new simulation dataset, OPV2VH+. Results show that CodeFilling outperforms previous SOTA Where2comm on DAIR-V2X/OPV2VH+ with 1,333/1,206 times lower communication volume. Our code is available at https://github.com/PhyllisH/CodeFilling.",
        "page": "http://arxiv.org/abs/2405.04966",
        "pdf": "http://arxiv.org/pdf/2405.04966.pdf"
    },
    {
        "title": "EventDance: Unsupervised Cross-modal Source-free Adaptation for Event-based Object Recognition",
        "author": "Xu Zheng, Lin Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion",
        "author": "Xunpeng Yi, Han Xu, HAO ZHANG, Linfeng Tang, Jiayi Ma",
        "abstract": "Image fusion aims to combine information from different source images to create a comprehensively representative image. Existing fusion methods are typically helpless in dealing with degradations in low-quality source images and non-interactive to multiple subjective and objective needs. To solve them, we introduce a novel approach that leverages semantic text guidance image fusion model for degradation-aware and interactive image fusion task, termed as Text-IF. It innovatively extends the classical image fusion to the text guided image fusion along with the ability to harmoniously address the degradation and interaction issues during fusion. Through the text semantic encoder and semantic interaction fusion decoder, Text-IF is accessible to the all-in-one infrared and visible image degradation-aware processing and the interactive flexible fusion outcomes. In this way, Text-IF achieves not only multi-modal image fusion, but also multi-modal information fusion. Extensive experiments prove that our proposed text guided image fusion strategy has obvious advantages over SOTA methods in the image fusion performance and degradation treatment. The code is available at https://github.com/XunpengYi/Text-IF.",
        "page": "http://arxiv.org/abs/2403.16387",
        "pdf": "http://arxiv.org/pdf/2403.16387.pdf"
    },
    {
        "title": "Semantics-aware Motion Retargeting with Vision-Language Models",
        "author": "Haodong Zhang, ZhiKe Chen, Haocheng Xu, Lei Hao, Xiaofei Wu, Songcen Xu, Zhensong Zhang, Yue Wang, Rong Xiong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PanoPose: Self-supervised Relative Pose Estimation for Panoramic Images",
        "author": "Diantao Tu, Hainan Cui, Xianwei Zheng, Shuhan Shen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Enhancing Post-training Quantization Calibration through Contrastive Learning",
        "author": "Yuzhang Shang, Gaowen Liu, Ramana Kompella, Yan Yan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Global and Local Prompts Cooperation via Optimal Transport for Federated Learning",
        "author": "Hongxia Li, Wei Huang, Jingya Wang, Ye Shi",
        "abstract": "Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific category characteristics. Unbalanced Optimal Transport is then employed to align local visual features with these prompts, striking a balance between global consensus and local personalization. By relaxing one of the equality constraints, FedOTP enables prompts to focus solely on the core regions of image patches. Extensive experiments on datasets with various types of heterogeneities have demonstrated that our FedOTP outperforms the state-of-the-art methods.",
        "page": "http://arxiv.org/abs/2403.00041",
        "pdf": "http://arxiv.org/pdf/2403.00041.pdf"
    },
    {
        "title": "Classes Are Not Equal: An Empirical Study on Image Recognition Fairness",
        "author": "Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, Hanwang Zhang",
        "abstract": "In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further, we conclude that data augmentation and representation learning algorithms improve overall performance by promoting fairness to some degree in image classification. The Code is available at https://github.com/dvlab-research/Parametric-Contrastive-Learning.",
        "page": "http://arxiv.org/abs/2402.18133",
        "pdf": "http://arxiv.org/pdf/2402.18133.pdf"
    },
    {
        "title": "Dense Optical Tracking: Connecting the Dots",
        "author": "Guillaume Le Moing, Jean Ponce, Cordelia Schmid",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-agent Collaborative Perception via Motion-aware Robust Communication Network",
        "author": "Shixin Hong, Yu LIU, Zhi Li, Shaohui Li, You He",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial Training",
        "author": "Qian Li, Yuxiao Hu, Yinpeng Dong, Dongxiao Zhang, Yuntian Chen",
        "abstract": "Adversarial training is often formulated as a min-max problem, however, concentrating only on the worst adversarial examples causes alternating repetitive confusion of the model, i.e., previously defended or correctly classified samples are not defensible or accurately classifiable in subsequent adversarial training. We characterize such non-ignorable samples as \"hiders\", which reveal the hidden high-risk regions within the secure area obtained through adversarial training and prevent the model from finding the real worst cases. We demand the model to prevent hiders when defending against adversarial examples for improving accuracy and robustness simultaneously. By rethinking and redefining the min-max optimization problem for adversarial training, we propose a generalized adversarial training algorithm called Hider-Focused Adversarial Training (HFAT). HFAT introduces the iterative evolution optimization strategy to simplify the optimization problem and employs an auxiliary model to reveal hiders, effectively combining the optimization directions of standard adversarial training and prevention hiders. Furthermore, we introduce an adaptive weighting mechanism that facilitates the model in adaptively adjusting its focus between adversarial examples and hiders during different training periods. We demonstrate the effectiveness of our method based on extensive experiments, and ensure that HFAT can provide higher robustness and accuracy.",
        "page": "http://arxiv.org/abs/2312.07067",
        "pdf": "http://arxiv.org/pdf/2312.07067.pdf"
    },
    {
        "title": "ColorPCR: Color Point Cloud Registration with Multi-Stage Geometric-Color Fusion",
        "author": "Juncheng Mu, Lin Bie, Shaoyi Du, Yue Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Person-in-WiFi 3D: End-to-End Multi-Person 3D Pose Estimation with Wi-Fi",
        "author": "Kangwei Yan, Fei Wang, Bo Qian, Han Ding, Jinsong Han, Xing Wei",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsupervised Feature Learning with Emergent Data-Driven Prototypicality",
        "author": "Yunhui Guo, Youren Zhang, Yubei Chen, Stella X. Yu",
        "abstract": "Given an image set without any labels, our goal is to train a model that maps each image to a point in a feature space such that, not only proximity indicates visual similarity, but where it is located directly encodes how prototypical the image is according to the dataset. Our key insight is to perform unsupervised feature learning in hyperbolic instead of Euclidean space, where the distance between points still reflect image similarity, and yet we gain additional capacity for representing prototypicality with the location of the point: The closer it is to the origin, the more prototypical it is. The latter property is simply emergent from optimizing the usual metric learning objective: The image similar to many training instances is best placed at the center of corresponding points in Euclidean space, but closer to the origin in hyperbolic space. We propose an unsupervised feature learning algorithm in Hyperbolic space with sphere pACKing. HACK first generates uniformly packed particles in the Poincar\\'e ball of hyperbolic space and then assigns each image uniquely to each particle. Images after congealing are regarded more typical of the dataset it belongs to. With our feature mapper simply trained to spread out training instances in hyperbolic space, we observe that images move closer to the origin with congealing, validating our idea of unsupervised prototypicality discovery. We demonstrate that our data-driven prototypicality provides an easy and superior unsupervised instance selection to reduce sample complexity, increase model generalization with atypical instances and robustness with typical ones.",
        "page": "http://arxiv.org/abs/2307.01421",
        "pdf": "http://arxiv.org/pdf/2307.01421.pdf"
    },
    {
        "title": "3DiffTection: 3D Object Detection with Geometry-aware Diffusion Features",
        "author": "Chenfeng Xu, Huan Ling, Sanja Fidler, Or Litany",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Hierarchical Patch Diffusion Models for High-Resolution Video Generation",
        "author": "Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies",
        "author": "Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, Francis Williams",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Probabilistic Human Mesh Estimation with Hypothesis Scoring",
        "author": "Yuan Xu, Xiaoxuan Ma, Jiajun Su, Wentao Zhu, Yu Qiao, Yizhou Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models",
        "author": "Shitian Zhao, Zhuowan Li, YadongLu, Alan L. Yuille, Yan Wang",
        "abstract": "While Multi-modal Language Models (MLMs) demonstrate impressive multimodal ability, they still struggle on providing factual and precise responses for tasks like visual question answering (VQA). In this paper, we address this challenge from the perspective of contextual information. We propose Causal Context Generation, Causal-CoG, which is a prompting strategy that engages contextual information to enhance precise VQA during inference. Specifically, we prompt MLMs to generate contexts, i.e, text description of an image, and engage the generated contexts for question answering. Moreover, we investigate the advantage of contexts on VQA from a causality perspective, introducing causality filtering to select samples for which contextual information is helpful. To show the effectiveness of Causal-CoG, we run extensive experiments on 10 multimodal benchmarks and show consistent improvements, e.g., +6.30% on POPE, +13.69% on Vizwiz and +6.43% on VQAv2 compared to direct decoding, surpassing existing methods. We hope Casual-CoG inspires explorations of context knowledge in multimodal models, and serves as a plug-and-play strategy for MLM decoding.",
        "page": "http://arxiv.org/abs/2312.06685",
        "pdf": "http://arxiv.org/pdf/2312.06685.pdf"
    },
    {
        "title": "Unlocking the Potential of Prompt-Tuning in Bridging Generalized and Personalized Federated Learning",
        "author": "wenlong deng, Christos Thrampoulidis, Xiaoxiao Li",
        "abstract": "Vision Transformers (ViT) and Visual Prompt Tuning (VPT) achieve state-of-the-art performance with improved efficiency in various computer vision tasks. This suggests a promising paradigm shift of adapting pre-trained ViT models to Federated Learning (FL) settings. However, the challenge of data heterogeneity among FL clients presents a significant hurdle in effectively deploying ViT models. Existing Generalized FL (GFL) and Personalized FL (PFL) methods have limitations in balancing performance across both global and local data distributions. In this paper, we present a novel algorithm, SGPT, that integrates GFL and PFL approaches by employing a unique combination of both shared and group-specific prompts. This design enables SGPT to capture both common and group-specific features. A key feature of SGPT is its prompt selection module, which facilitates the training of a single global model capable of automatically adapting to diverse local client data distributions without the need for local fine-tuning. To effectively train the prompts, we utilize block coordinate descent (BCD), learning from common feature information (shared prompts), and then more specialized knowledge (group prompts) iteratively. Theoretically, we justify that learning the proposed prompts can reduce the gap between global and local performance. Empirically, we conduct experiments on both label and feature heterogeneity settings in comparison with state-of-the-art baselines, along with extensive ablation studies, to substantiate the superior performance of SGPT.",
        "page": "http://arxiv.org/abs/2310.18285",
        "pdf": "http://arxiv.org/pdf/2310.18285.pdf"
    },
    {
        "title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?",
        "author": "Maxime Zanella, Ismail Ben Ayed",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation",
        "author": "Pengchong Qiao, Lei Shang, Chang Liu, Baigui Sun, Xiangyang Ji, Jie Chen",
        "abstract": "Subject-driven generation has garnered significant interest recently due to its ability to personalize text-to-image generation. Typical works focus on learning the new subject's private attributes. However, an important fact has not been taken seriously that a subject is not an isolated new concept but should be a specialization of a certain category in the pre-trained model. This results in the subject failing to comprehensively inherit the attributes in its category, causing poor attribute-related generations. In this paper, motivated by object-oriented programming, we model the subject as a derived class whose base class is its semantic category. This modeling enables the subject to inherit public attributes from its category while learning its private attributes from the user-provided example. Specifically, we propose a plug-and-play method, Subject-Derived regularization (SuDe). It constructs the base-derived class modeling by constraining the subject-driven generated images to semantically belong to the subject's category. Extensive experiments under three baselines and two backbones on various subjects show that our SuDe enables imaginative attribute-related generations while maintaining subject fidelity. Codes will be open sourced soon at FaceChain (https://github.com/modelscope/facechain).",
        "page": "http://arxiv.org/abs/2403.06775",
        "pdf": "http://arxiv.org/pdf/2403.06775.pdf"
    },
    {
        "title": "SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction",
        "author": "Pin Tang, Zhongdao Wang, Guoqing Wang, Jilai Zheng, Xiangxuan Ren, Bailan Feng, Chao Ma",
        "abstract": "Vision-based perception for autonomous driving requires an explicit modeling of a 3D space, where 2D latent representations are mapped and subsequent 3D operators are applied. However, operating on dense latent spaces introduces a cubic time and space complexity, which limits scalability in terms of perception range or spatial resolution. Existing approaches compress the dense representation using projections like Bird's Eye View (BEV) or Tri-Perspective View (TPV). Although efficient, these projections result in information loss, especially for tasks like semantic occupancy prediction. To address this, we propose SparseOcc, an efficient occupancy network inspired by sparse point cloud processing. It utilizes a lossless sparse latent representation with three key innovations. Firstly, a 3D sparse diffuser performs latent completion using spatially decomposed 3D sparse convolutional kernels. Secondly, a feature pyramid and sparse interpolation enhance scales with information from others. Finally, the transformer head is redesigned as a sparse variant. SparseOcc achieves a remarkable 74.9% reduction on FLOPs over the dense baseline. Interestingly, it also improves accuracy, from 12.8% to 14.1% mIOU, which in part can be attributed to the sparse representation's ability to avoid hallucinations on empty voxels.",
        "page": "http://arxiv.org/abs/2404.09502",
        "pdf": "http://arxiv.org/pdf/2404.09502.pdf"
    },
    {
        "title": "Extreme Point Supervised Instance Segmentation",
        "author": "Hyeonjun Lee, Sehyun Hwang, Suha Kwak",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DreamComposer: Controllable 3D Object Generation via Multi-View Conditions",
        "author": "Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Degree-of-Freedom Matters: Inferring Dynamics from Point Trajectories",
        "author": "Yan Zhang, Sergey Prokudin, Marko Mihajlovic, Qianli Ma, Siyu Tang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ActiveDC: Distribution Calibration for Active Finetuning",
        "author": "Wenshuai Xu, Zhenghui Hu, Yu Lu, Jinzhou Meng, Qingjie Liu, Yunhong Wang",
        "abstract": "The pretraining-finetuning paradigm has gained popularity in various computer vision tasks. In this paradigm, the emergence of active finetuning arises due to the abundance of large-scale data and costly annotation requirements. Active finetuning involves selecting a subset of data from an unlabeled pool for annotation, facilitating subsequent finetuning. However, the use of a limited number of training samples can lead to a biased distribution, potentially resulting in model overfitting. In this paper, we propose a new method called ActiveDC for the active finetuning tasks. Firstly, we select samples for annotation by optimizing the distribution similarity between the subset to be selected and the entire unlabeled pool in continuous space. Secondly, we calibrate the distribution of the selected samples by exploiting implicit category information in the unlabeled pool. The feature visualization provides an intuitive sense of the effectiveness of our approach to distribution calibration. We conducted extensive experiments on three image classification datasets with different sampling ratios. The results indicate that ActiveDC consistently outperforms the baseline performance in all image classification tasks. The improvement is particularly significant when the sampling ratio is low, with performance gains of up to 10%. Our code will be released.",
        "page": "http://arxiv.org/abs/2311.07634",
        "pdf": "http://arxiv.org/pdf/2311.07634.pdf"
    },
    {
        "title": "InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning",
        "author": "Yan-Shuo Liang, Wu-Jun Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer",
        "author": "Jiwoo Chung, Sangeek Hyun, Jae-Pil Heo",
        "abstract": "Despite the impressive generative capabilities of diffusion models, existing diffusion model-based style transfer methods require inference-stage optimization (e.g. fine-tuning or textual inversion of style) which is time-consuming, or fails to leverage the generative ability of large-scale diffusion models. To address these issues, we introduce a novel artistic style transfer method based on a pre-trained large-scale diffusion model without any optimization. Specifically, we manipulate the features of self-attention layers as the way the cross-attention mechanism works; in the generation process, substituting the key and value of content with those of style image. This approach provides several desirable characteristics for style transfer including 1) preservation of content by transferring similar styles into similar image patches and 2) transfer of style based on similarity of local texture (e.g. edge) between content and style images. Furthermore, we introduce query preservation and attention temperature scaling to mitigate the issue of disruption of original content, and initial latent Adaptive Instance Normalization (AdaIN) to deal with the disharmonious color (failure to transfer the colors of style). Our experimental results demonstrate that our proposed method surpasses state-of-the-art methods in both conventional and diffusion-based style transfer baselines.",
        "page": "http://arxiv.org/abs/2312.09008",
        "pdf": "http://arxiv.org/pdf/2312.09008.pdf"
    },
    {
        "title": "Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model",
        "author": "Dian Zheng, Xiao-Ming Wu, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, Wei-Shi Zheng",
        "abstract": "Universal image restoration is a practical and potential computer vision task for real-world applications. The main challenge of this task is handling the different degradation distributions at once. Existing methods mainly utilize task-specific conditions (e.g., prompt) to guide the model to learn different distributions separately, named multi-partite mapping. However, it is not suitable for universal model learning as it ignores the shared information between different tasks. In this work, we propose an advanced selective hourglass mapping strategy based on diffusion model, termed DiffUIR. Two novel considerations make our DiffUIR non-trivial. Firstly, we equip the model with strong condition guidance to obtain accurate generation direction of diffusion model (selective). More importantly, DiffUIR integrates a flexible shared distribution term (SDT) into the diffusion algorithm elegantly and naturally, which gradually maps different distributions into a shared one. In the reverse process, combined with SDT and strong condition guidance, DiffUIR iteratively guides the shared distribution to the task-specific distribution with high image quality (hourglass). Without bells and whistles, by only modifying the mapping strategy, we achieve state-of-the-art performance on five image restoration tasks, 22 benchmarks in the universal setting and zero-shot generalization setting. Surprisingly, by only using a lightweight model (only 0.89M), we could achieve outstanding performance. The source code and pre-trained models are available at https://github.com/iSEE-Laboratory/DiffUIR",
        "page": "http://arxiv.org/abs/2403.11157",
        "pdf": "http://arxiv.org/pdf/2403.11157.pdf"
    },
    {
        "title": "Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning",
        "author": "Xinshun Wang, Zhongbin Fang, Xia Li, Xiangtai Li, Chen Chen, Mengyuan Liu",
        "abstract": "In-context learning provides a new perspective for multi-task modeling for vision and NLP. Under this setting, the model can perceive tasks from prompts and accomplish them without any extra task-specific head predictions or model fine-tuning. However, Skeleton sequence modeling via in-context learning remains unexplored. Directly applying existing in-context models from other areas onto skeleton sequences fails due to the inter-frame and cross-task pose similarity that makes it outstandingly hard to perceive the task correctly from a subtle context. To address this challenge, we propose Skeleton-in-Context (SiC), an effective framework for in-context skeleton sequence modeling. Our SiC is able to handle multiple skeleton-based tasks simultaneously after a single training process and accomplish each task from context according to the given prompt. It can further generalize to new, unseen tasks according to customized prompts. To facilitate context perception, we additionally propose a task-unified prompt, which adaptively learns tasks of different natures, such as partial joint-level generation, sequence-level prediction, or 2D-to-3D motion prediction. We conduct extensive experiments to evaluate the effectiveness of our SiC on multiple tasks, including motion prediction, pose estimation, joint completion, and future pose estimation. We also evaluate its generalization capability on unseen tasks such as motion-in-between. These experiments show that our model achieves state-of-the-art multi-task performance and even outperforms single-task methods on certain tasks.",
        "page": "http://arxiv.org/abs/2312.03703",
        "pdf": "http://arxiv.org/pdf/2312.03703.pdf"
    },
    {
        "title": "Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training",
        "author": "Arun Reddy, William Paul, Corban Rivera, Ketul Shah, Celso M. de Melo, Rama Chellappa",
        "abstract": "In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.",
        "page": "http://arxiv.org/abs/2312.02914",
        "pdf": "http://arxiv.org/pdf/2312.02914.pdf"
    },
    {
        "title": "Fast Adaptation for Human Pose Estimation via Meta-Optimization",
        "author": "Shengxiang Hu, Huaijiang Sun, Bin Li, Dong Wei, Weiqing Li, Jianfeng Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "\"Previously on ...\" From Recaps to Story Summarization",
        "author": "Aditya Kumar Singh, Dhruv Srivastava, Dhruv Srivastava, Makarand Tapaswi",
        "abstract": "We introduce multimodal story summarization by leveraging TV episode recaps - short video sequences interweaving key story moments from previous episodes to bring viewers up to speed. We propose PlotSnap, a dataset featuring two crime thriller TV shows with rich recaps and long episodes of 40 minutes. Story summarization labels are unlocked by matching recap shots to corresponding sub-stories in the episode. We propose a hierarchical model TaleSumm that processes entire episodes by creating compact shot and dialog representations, and predicts importance scores for each video shot and dialog utterance by enabling interactions between local story groups. Unlike traditional summarization, our method extracts multiple plot points from long videos. We present a thorough evaluation on story summarization, including promising cross-series generalization. TaleSumm also shows good results on classic video summarization benchmarks.",
        "page": "http://arxiv.org/abs/2405.11487",
        "pdf": "http://arxiv.org/pdf/2405.11487.pdf"
    },
    {
        "title": "SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection",
        "author": "JUNSU KIM, Hoseong Cho, Jihyeon Kim, Yihalem Tiruneh, Seungryul Baek",
        "abstract": "In the field of class incremental learning (CIL), generative replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the continuous improvements in generative models. However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the complexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text-to-diffusion networks to generate realistic and diverse synthetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 knowledge distillation technique to improve the retention of prior knowledge in synthetic images. Furthermore, our approach includes pseudo-labeling for old objects within new task images, preventing misclassification as background elements. Extensive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.",
        "page": "http://arxiv.org/abs/2402.17323",
        "pdf": "http://arxiv.org/pdf/2402.17323.pdf"
    },
    {
        "title": "Frozen Feature Augmentation for Few-Shot Image Classification",
        "author": "Andreas B\u00e4r, Neil Houlsby, Mostafa Dehghani, Manoj Kumar",
        "abstract": "Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called 'frozen features', leads to impressive performance on a number of downstream few-shot tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, data augmentation is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an extensive pilot study on few-shot image classification that explores applying data augmentations in the frozen feature space, dubbed 'frozen feature augmentation (FroFA)', covering twenty augmentations in total. Our study demonstrates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve few-shot performance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets.",
        "page": "http://arxiv.org/abs/2403.10519",
        "pdf": "http://arxiv.org/pdf/2403.10519.pdf"
    },
    {
        "title": "1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness",
        "author": "Bernd Prach, Fabio Brau, Giorgio Buttazzo, Christoph Lampert",
        "abstract": "The robustness of neural networks against input perturbations with bounded magnitude represents a serious concern in the deployment of deep learning models in safety-critical systems. Recently, the scientific community has focused on enhancing certifiable robustness guarantees by crafting 1-Lipschitz neural networks that leverage Lipschitz bounded dense and convolutional layers. Although different methods have been proposed in the literature to achieve this goal, understanding the performance of such methods is not straightforward, since different metrics can be relevant (e.g., training time, memory usage, accuracy, certifiable robustness) for different applications. For this reason, this work provides a thorough theoretical and empirical comparison between methods by evaluating them in terms of memory usage, speed, and certifiable robust accuracy. The paper also provides some guidelines and recommendations to support the user in selecting the methods that work best depending on the available resources. We provide code at https://github.com/berndprach/1LipschitzLayersCompared.",
        "page": "http://arxiv.org/abs/2311.16833",
        "pdf": "http://arxiv.org/pdf/2311.16833.pdf"
    },
    {
        "title": "VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models",
        "author": "Hyeonho Jeong, Geon Yeong Park, Jong Chul Ye",
        "abstract": "Text-to-video diffusion models have advanced video generation significantly. However, customizing these models to generate videos with tailored motions presents a substantial challenge. In specific, they encounter hurdles in (a) accurately reproducing motion from a target video, and (b) creating diverse visual variations. For example, straightforward extensions of static image customization methods to video often lead to intricate entanglements of appearance and motion data. To tackle this, here we present the Video Motion Customization (VMC) framework, a novel one-shot tuning approach crafted to adapt temporal attention layers within video diffusion models. Our approach introduces a novel motion distillation objective using residual vectors between consecutive frames as a motion reference. The diffusion process then preserves low-frequency motion trajectories while mitigating high-frequency motion-unrelated noise in image space. We validate our method against state-of-the-art video generative models across diverse real-world motions and contexts. Our codes, data and the project demo can be found at https://video-motion-customization.github.io",
        "page": "http://arxiv.org/abs/2312.00845",
        "pdf": "http://arxiv.org/pdf/2312.00845.pdf"
    },
    {
        "title": "Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection",
        "author": "Jiawen Zhu, Choubo Ding, Yu Tian, Guansong Pang",
        "abstract": "Open-set supervised anomaly detection (OSAD) - a recently emerging anomaly detection area - aims at utilizing a few samples of anomaly classes seen during training to detect unseen anomalies (i.e., samples from open-set anomaly classes), while effectively identifying the seen anomalies. Benefiting from the prior knowledge illustrated by the seen anomalies, current OSAD methods can often largely reduce false positive errors. However, these methods are trained in a closed-set setting and treat the anomaly examples as from a homogeneous distribution, rendering them less effective in generalizing to unseen anomalies that can be drawn from any distribution. This paper proposes to learn heterogeneous anomaly distributions using the limited anomaly examples to address this issue. To this end, we introduce a novel approach, namely Anomaly Heterogeneity Learning (AHL), that simulates a diverse set of heterogeneous anomaly distributions and then utilizes them to learn a unified heterogeneous abnormality model in surrogate open-set environments. Further, AHL is a generic framework that existing OSAD models can plug and play for enhancing their abnormality modeling. Extensive experiments on nine real-world anomaly detection datasets show that AHL can 1) substantially enhance different state-of-the-art OSAD models in detecting seen and unseen anomalies, and 2) effectively generalize to unseen anomalies in new domains. Code is available at https://github.com/mala-lab/AHL.",
        "page": "http://arxiv.org/abs/2310.12790",
        "pdf": "http://arxiv.org/pdf/2310.12790.pdf"
    },
    {
        "title": "L4D-Track: Language-to-4D Modeling Towards 6-DoF Tracking and Shape Reconstruction in 3D Point Cloud Stream",
        "author": "Jingtao Sun, Yaonan Wang, Mingtao Feng, Yulan Guo, Ajmal Mian, Mike Zheng Shou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation",
        "author": "Qihang Zhang, Yinghao Xu, Yujun Shen, Bo Dai, Bolei Zhou, Ceyuan Yang",
        "abstract": "Generating large-scale 3D scenes cannot simply apply existing 3D object synthesis technique since 3D scenes usually hold complex spatial configurations and consist of a number of objects at varying scales. We thus propose a practical and efficient 3D representation that incorporates an equivariant radiance field with the guidance of a bird's-eye view (BEV) map. Concretely, objects of synthesized 3D scenes could be easily manipulated through steering the corresponding BEV maps. Moreover, by adequately incorporating positional encoding and low-pass filters into the generator, the representation becomes equivariant to the given BEV map. Such equivariance allows us to produce large-scale, even infinite-scale, 3D scenes via synthesizing local scenes and then stitching them with smooth consistency. Extensive experiments on 3D scene datasets demonstrate the effectiveness of our approach. Our project website is at https://zqh0253.github.io/BerfScene/.",
        "page": "http://arxiv.org/abs/2312.02136",
        "pdf": "http://arxiv.org/pdf/2312.02136.pdf"
    },
    {
        "title": "GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces",
        "author": "Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, Yuexin Ma",
        "abstract": "The advent of neural 3D Gaussians has recently brought about a revolution in the field of neural rendering, facilitating the generation of high-quality renderings at real-time speeds. However, the explicit and discrete representation encounters challenges when applied to scenes featuring reflective surfaces. In this paper, we present GaussianShader, a novel method that applies a simplified shading function on 3D Gaussians to enhance the neural rendering in scenes with reflective surfaces while preserving the training and rendering efficiency. The main challenge in applying the shading function lies in the accurate normal estimation on discrete 3D Gaussians. Specifically, we proposed a novel normal estimation framework based on the shortest axis directions of 3D Gaussians with a delicately designed loss to make the consistency between the normals and the geometries of Gaussian spheres. Experiments show that GaussianShader strikes a commendable balance between efficiency and visual quality. Our method surpasses Gaussian Splatting in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When compared to prior works handling reflective surfaces, such as Ref-NeRF, our optimization time is significantly accelerated (23h vs. 0.58h). Please click on our project website to see more results.",
        "page": "http://arxiv.org/abs/2311.17977",
        "pdf": "http://arxiv.org/pdf/2311.17977.pdf"
    },
    {
        "title": "Scaling Laws for Data Filtering: Data Curation cannot be Compute Agnostic",
        "author": "Sachin Goyal, Pratyush Maini, Zachary Lipton, Aditi Raghunathan, Zico Kolter",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Representing Signs as Language: A New Method for Sign Language Translation from Videos",
        "author": "Jia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani, Jun Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HIPTrack: Visual Tracking with Historical Prompts",
        "author": "Wenrui Cai, Qingjie Liu, Yunhong Wang",
        "abstract": "Trackers that follow Siamese paradigm utilize similarity matching between template and search region features for tracking. Many methods have been explored to enhance tracking performance by incorporating tracking history to better handle scenarios involving target appearance variations such as deformation and occlusion. However, the utilization of historical information in existing methods is insufficient and incomprehensive, which typically requires repetitive training and introduces a large amount of computation. In this paper, we show that by providing a tracker that follows Siamese paradigm with precise and updated historical information, a significant performance improvement can be achieved with completely unchanged parameters. Based on this, we propose a historical prompt network that uses refined historical foreground masks and historical visual features of the target to provide comprehensive and precise prompts for the tracker. We build a novel tracker called HIPTrack based on the historical prompt network, which achieves considerable performance improvements without the need to retrain the entire model. We conduct experiments on seven datasets and experimental results demonstrate that our method surpasses the current state-of-the-art trackers on LaSOT, LaSOText, GOT-10k and NfS. Furthermore, the historical prompt network can seamlessly integrate as a plug-and-play module into existing trackers, providing performance enhancements. The source code is available at https://github.com/WenRuiCai/HIPTrack.",
        "page": "http://arxiv.org/abs/2311.02072",
        "pdf": "http://arxiv.org/pdf/2311.02072.pdf"
    },
    {
        "title": "Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation",
        "author": "Siteng Huang, Biao Gong, Yutong Feng, Xi Chen, Yuqian Fu, Yu Liu, Donglin Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Regularized Parameter Uncertainty for Improving Generalization in Reinforcement Learning",
        "author": "Pehuen Moure, Longbiao Cheng, Joachim Ott, Zuowen Wang, Shih-Chii Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Robust Noisy Correspondence Learning with Equivariant Similarity Consistency",
        "author": "Yuchen Yang, Erkun Yang, Likai Wang, Cheng Deng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PanoRecon: Real-Time Panoptic 3D Reconstruction from Monocular Video",
        "author": "Dong Wu, Zike Yan, Hongbin Zha",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Situational Awareness Matters in 3D Vision Language Reasoning",
        "author": "Yunze Man, Liang-Yan Gui, Yu-Xiong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Task-Driven Wavelets using Constrained Empirical Risk Minimization",
        "author": "Eric Marcus, Ray Sheombarsing, Jan-Jakob Sonke, Jonas Teuwen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AHIVE: Anatomy-aware Hierarchical Vision Encoding for Interactive Radiology Report Retrieval",
        "author": "Sixing Yan, William K. Cheung, Ivor Tsang, Wan Hang Keith Chiu, Tong Terence, Ka Chun Cheung, Simon See",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Scaling Laws of Synthetic Images for Model Training ... for Now",
        "author": "Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, Yonglong Tian",
        "abstract": "Recent significant advances in text-to-image models unlock the possibility of training vision systems using synthetic images, potentially overcoming the difficulty of collecting curated data at scale. It is unclear, however, how these models behave at scale, as more synthetic data is added to the training set. In this paper we study the scaling laws of synthetic images generated by state of the art text-to-image models, for the training of supervised models: image classifiers with label supervision, and CLIP with language supervision. We identify several factors, including text prompts, classifier-free guidance scale, and types of text-to-image models, that significantly affect scaling behavior. After tuning these factors, we observe that synthetic images demonstrate a scaling trend similar to, but slightly less effective than, real images in CLIP training, while they significantly underperform in scaling when training supervised image classifiers. Our analysis indicates that the main reason for this underperformance is the inability of off-the-shelf text-to-image models to generate certain concepts, a limitation that significantly impairs the training of image classifiers. Our findings also suggest that scaling synthetic data can be particularly effective in scenarios such as: (1) when there is a limited supply of real images for a supervised problem (e.g., fewer than 0.5 million images in ImageNet), (2) when the evaluation dataset diverges significantly from the training data, indicating the out-of-distribution scenario, or (3) when synthetic data is used in conjunction with real images, as demonstrated in the training of CLIP models.",
        "page": "http://arxiv.org/abs/2312.04567",
        "pdf": "http://arxiv.org/pdf/2312.04567.pdf"
    },
    {
        "title": "MMA: Multi-Modal Adapter for Vision-Language Models",
        "author": "Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, Xiaohua Xie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unsupervised Deep Unrolling Networks for Phase Unwrapping",
        "author": "Zhile Chen, Yuhui Quan, Hui Ji",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Would Deep Generative Models Amplify Bias in Future Models?",
        "author": "Tianwei Chen, Yusuke Hirota, Mayu Otani, Noa Garcia, Yuta Nakashima",
        "abstract": "We investigate the impact of deep generative models on potential social biases in upcoming computer vision models. As the internet witnesses an increasing influx of AI-generated images, concerns arise regarding inherent biases that may accompany them, potentially leading to the dissemination of harmful content. This paper explores whether a detrimental feedback loop, resulting in bias amplification, would occur if generated images were used as the training data for future models. We conduct simulations by progressively substituting original images in COCO and CC3M datasets with images generated through Stable Diffusion. The modified datasets are used to train OpenCLIP and image captioning models, which we evaluate in terms of quality and bias. Contrary to expectations, our findings indicate that introducing generated images during training does not uniformly amplify bias. Instead, instances of bias mitigation across specific tasks are observed. We further explore the factors that may influence these phenomena, such as artifacts in image generation (e.g., blurry faces) or pre-existing biases in the original datasets.",
        "page": "http://arxiv.org/abs/2404.03242",
        "pdf": "http://arxiv.org/pdf/2404.03242.pdf"
    },
    {
        "title": "SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World",
        "author": "Kiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng, Kunal Singh Singh, Yejin Kim, Winson Han, Alvaro Herrasti, Ranjay Krishna, Dustin Schwenk, Eli VanderBilt, Aniruddha Kembhavi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation",
        "author": "Yihua Cheng, Yaning Zhu, Zongji Wang, hongquan hao, Liu wei, Shiqing Cheng, Xi Wang, Hyung Jin Chang",
        "abstract": "Driver's eye gaze holds a wealth of cognitive and intentional cues crucial for intelligent vehicles. Despite its significance, research on in-vehicle gaze estimation remains limited due to the scarcity of comprehensive and well-annotated datasets in real driving scenarios. In this paper, we present three novel elements to advance in-vehicle gaze research. Firstly, we introduce IVGaze, a pioneering dataset capturing in-vehicle gaze, collected from 125 subjects and covering a large range of gaze and head poses within vehicles. Conventional gaze collection systems are inadequate for in-vehicle use. In this dataset, we propose a new vision-based solution for in-vehicle gaze collection, introducing a refined gaze target calibration method to tackle annotation challenges. Second, our research focuses on in-vehicle gaze estimation leveraging the IVGaze. In-vehicle face images often suffer from low resolution, prompting our introduction of a gaze pyramid transformer that leverages transformer-based multilevel features integration. Expanding upon this, we introduce the dual-stream gaze pyramid transformer (GazeDPTR). Employing perspective transformation, we rotate virtual cameras to normalize images, utilizing camera pose to merge normalized and original images for accurate gaze estimation. GazeDPTR shows state-of-the-art performance on the IVGaze dataset. Thirdly, we explore a novel strategy for gaze zone classification by extending the GazeDPTR. A foundational tri-plane and project gaze onto these planes are newly defined. Leveraging both positional features from the projection points and visual attributes from images, we achieve superior performance compared to relying solely on visual features, substantiating the advantage of gaze estimation. Our project is available at https://yihua.zone/work/ivgaze.",
        "page": "http://arxiv.org/abs/2403.15664",
        "pdf": "http://arxiv.org/pdf/2403.15664.pdf"
    },
    {
        "title": "HUGS: Human Gaussian Splatting",
        "author": "Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rapid Motor Adaptation for Robotic Manipulator Arms",
        "author": "Yichao Liang, Kevin Ellis, Jo\u00e3o F. Henriques",
        "abstract": "Developing generalizable manipulation skills is a core challenge in embodied AI. This includes generalization across diverse task configurations, encompassing variations in object shape, density, friction coefficient, and external disturbances such as forces applied to the robot. Rapid Motor Adaptation (RMA) offers a promising solution to this challenge. It posits that essential hidden variables influencing an agent's task performance, such as object mass and shape, can be effectively inferred from the agent's action and proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand rotation, we use depth perception to develop agents tailored for rapid motor adaptation in a variety of manipulation tasks. We evaluated our agents on four challenging tasks from the Maniskill2 benchmark, namely pick-and-place operations with hundreds of objects from the YCB and EGAD datasets, peg insertion with precise position and orientation, and operating a variety of faucets and handles, with customized environment variations. Empirical results demonstrate that our agents surpass state-of-the-art methods like automatic domain randomization and vision-based policies, obtaining better generalization performance and sample efficiency.",
        "page": "http://arxiv.org/abs/2312.04670",
        "pdf": "http://arxiv.org/pdf/2312.04670.pdf"
    },
    {
        "title": "TurboSL: Dense, Accurate and Fast 3D by Neural Inverse Structured Light",
        "author": "Parsa Mirdehghan, Maxx Wu, Wenzheng Chen, Wenzheng Chen, David B. Lindell, Kiriakos Kutulakos",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric",
        "author": "Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, Zhenan Sun",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Revisiting Non-Autoregressive Transformers for Efficient Image Synthesis",
        "author": "Zanlin Ni, Yulin Wang, Renping Zhou, Jiayi Guo, Jinyi Hu, Zhiyuan Liu, Shiji Song, Yuan Yao, Gao Huang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis",
        "author": "Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, Yi Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DiffPerformer: Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation",
        "author": "Chenyang Wang, Zerong Zheng, Tao Yu, Xiaoqian Lv, Bineng Zhong, Shengping Zhang, Liqiang Nie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LiSA: LiDAR Localization with Semantic Awareness",
        "author": "Bochun Yang, Zijun Li, Wen Li, zhipeng cai, Chenglu Wen, Yu Zang, Matthias Mueller, Cheng Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Coupled Dictionaries from Unpaired Data for Image Super-Resolution",
        "author": "Longguang Wang, Juncheng Li, Yingqian Wang, Qingyong Hu, Yulan Guo",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking Human Motion Prediction with Symplectic Integral",
        "author": "Haipeng Chen, Kedi L\u2006yu, Zhenguang Liu, Yifang Yin, Xun Yang, Yingda Lyu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Unleashing Network Potentials for Semantic Scene Completion",
        "author": "Fengyun Wang, Qianru Sun, Dong Zhang, Jinhui Tang",
        "abstract": "Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy and semantics from a single-view RGB-D image, and recent SSC methods commonly adopt multi-modal inputs. However, our investigation reveals two limitations: ineffective feature learning from single modalities and overfitting to limited datasets. To address these issues, this paper proposes a novel SSC framework - Adversarial Modality Modulation Network (AMMNet) - with a fresh perspective of optimizing gradient updates. The proposed AMMNet introduces two core modules: a cross-modal modulation enabling the interdependence of gradient flows between modalities, and a customized adversarial training scheme leveraging dynamic gradient competition. Specifically, the cross-modal modulation adaptively re-calibrates the features to better excite representation potentials from each single modality. The adversarial training employs a minimax game of evolving gradients, with customized guidance to strengthen the generator's perception of visual fidelity from both geometric completeness and semantic correctness. Extensive experimental results demonstrate that AMMNet outperforms state-of-the-art SSC methods by a large margin, providing a promising direction for improving the effectiveness and generalization of SSC methods.",
        "page": "http://arxiv.org/abs/2403.07560",
        "pdf": "http://arxiv.org/pdf/2403.07560.pdf"
    },
    {
        "title": "Video Super-Resolution Transformer with Masked Inter&Intra-Frame Attention",
        "author": "Xingyu Zhou, Leheng Zhang, Xiaorui Zhao, Keze Wang, Leida Li, Shuhang Gu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Robust 3D Pose Transfer with Adversarial Learning",
        "author": "Haoyu Chen, Hao Tang, Ehsan Adeli, Guoying Zhao",
        "abstract": "3D pose transfer that aims to transfer the desired pose to a target mesh is one of the most challenging 3D generation tasks. Previous attempts rely on well-defined parametric human models or skeletal joints as driving pose sources. However, to obtain those clean pose sources, cumbersome but necessary pre-processing pipelines are inevitable, hindering implementations of the real-time applications. This work is driven by the intuition that the robustness of the model can be enhanced by introducing adversarial samples into the training, leading to a more invulnerable model to the noisy inputs, which even can be further extended to directly handling the real-world data like raw point clouds/scans without intermediate processing. Furthermore, we propose a novel 3D pose Masked Autoencoder (3D-PoseMAE), a customized MAE that effectively learns 3D extrinsic presentations (i.e., pose). 3D-PoseMAE facilitates learning from the aspect of extrinsic attributes by simultaneously generating adversarial samples that perturb the model and learning the arbitrary raw noisy poses via a multi-scale masking strategy. Both qualitative and quantitative studies show that the transferred meshes given by our network result in much better quality. Besides, we demonstrate the strong generalizability of our method on various poses, different domains, and even raw scans. Experimental results also show meaningful insights that the intermediate adversarial samples generated in the training can successfully attack the existing pose transfer models.",
        "page": "http://arxiv.org/abs/2404.02242",
        "pdf": "http://arxiv.org/pdf/2404.02242.pdf"
    },
    {
        "title": "Building Vision-Language Models on Solid Foundations with Masked Distillation",
        "author": "Sepehr Sameni, Kushal Kafle, Hao Tan, Simon Jenni",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "author": "Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, Jie Tang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
        "author": "Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao",
        "abstract": "This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.",
        "page": "http://arxiv.org/abs/2401.10891",
        "pdf": "http://arxiv.org/pdf/2401.10891.pdf"
    },
    {
        "title": "Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network",
        "author": "wenqiao Li, Xiaohao Xu, Yao Gu, BoZhong Zheng, Shenghua Gao, Yingna Wu",
        "abstract": "Recently, 3D anomaly detection, a crucial problem involving fine-grained geometry discrimination, is getting more attention. However, the lack of abundant real 3D anomaly data limits the scalability of current models. To enable scalable anomaly data collection, we propose a 3D anomaly synthesis pipeline to adapt existing large-scale 3Dmodels for 3D anomaly detection. Specifically, we construct a synthetic dataset, i.e., Anomaly-ShapeNet, basedon ShapeNet. Anomaly-ShapeNet consists of 1600 point cloud samples under 40 categories, which provides a rich and varied collection of data, enabling efficient training and enhancing adaptability to industrial scenarios. Meanwhile,to enable scalable representation learning for 3D anomaly localization, we propose a self-supervised method, i.e., Iterative Mask Reconstruction Network (IMRNet). During training, we propose a geometry-aware sample module to preserve potentially anomalous local regions during point cloud down-sampling. Then, we randomly mask out point patches and sent the visible patches to a transformer for reconstruction-based self-supervision. During testing, the point cloud repeatedly goes through the Mask Reconstruction Network, with each iteration's output becoming the next input. By merging and contrasting the final reconstructed point cloud with the initial input, our method successfully locates anomalies. Experiments show that IMRNet outperforms previous state-of-the-art methods, achieving 66.1% in I-AUC on Anomaly-ShapeNet dataset and 72.5% in I-AUC on Real3D-AD dataset. Our dataset will be released at https://github.com/Chopper-233/Anomaly-ShapeNet",
        "page": "http://arxiv.org/abs/2311.14897",
        "pdf": "http://arxiv.org/pdf/2311.14897.pdf"
    },
    {
        "title": "PAPR in Motion: Seamless Point-level 3D Scene Interpolation",
        "author": "Shichong Peng, Yanshu Zhang, Ke Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Modern Image Manipulation Localization: A Large-Scale Dataset and Novel Methods",
        "author": "Chenfan Qu, Yiwu Zhong, Chongyu Liu, Guitao Xu, Dezhi Peng, Fengjun Guo, Lianwen Jin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AM-RADIO: Agglomerative Models - Reduce All Domains Into One",
        "author": "Mike Ranzinger, Greg Heinrich, Jan Kautz, Pavlo Molchanov",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Tune-An-Ellipse: CLIP Has Potential to Find What You Want",
        "author": "Jinheng Xie, Songhe Deng, Bing Li, Haozhe Liu, Yawen Huang, Yefeng Zheng, J\u00fcrgen Schmidhuber, Bernard Ghanem, Linlin Shen, Mike Zheng Shou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LISA: Reasoning Segmentation via Large Language Model",
        "author": "Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM",
        "author": "Pingping Zhang, Tianyu Yan, Yang Liu, Huchuan Lu",
        "abstract": "As an important pillar of underwater intelligence, Marine Animal Segmentation (MAS) involves segmenting animals within marine environments. Previous methods don't excel in extracting long-range contextual features and overlook the connectivity between discrete pixels. Recently, Segment Anything Model (SAM) offers a universal framework for general segmentation tasks. Unfortunately, trained with natural images, SAM does not obtain the prior knowledge from marine images. In addition, the single-position prompt of SAM is very insufficient for prior guidance. To address these issues, we propose a novel feature learning framework, named Dual-SAM for high-performance MAS. To this end, we first introduce a dual structure with SAM's paradigm to enhance feature learning of marine images. Then, we propose a Multi-level Coupled Prompt (MCP) strategy to instruct comprehensive underwater prior information, and enhance the multi-level features of SAM's encoder with adapters. Subsequently, we design a Dilated Fusion Attention Module (DFAM) to progressively integrate multi-level features from SAM's encoder. Finally, instead of directly predicting the masks of marine animals, we propose a Criss-Cross Connectivity Prediction (C$^3$P) paradigm to capture the inter-connectivity between discrete pixels. With dual decoders, it generates pseudo-labels and achieves mutual supervision for complementary feature representations, resulting in considerable improvements over previous techniques. Extensive experiments verify that our proposed method achieves state-of-the-art performances on five widely-used MAS datasets. The code is available at https://github.com/Drchip61/Dual_SAM.",
        "page": "http://arxiv.org/abs/2404.04996",
        "pdf": "http://arxiv.org/pdf/2404.04996.pdf"
    },
    {
        "title": "IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing",
        "author": "Shaofei Wang, Bozidar Antic, Andreas Geiger, Siyu Tang",
        "abstract": "We present IntrinsicAvatar, a novel approach to recovering the intrinsic properties of clothed human avatars including geometry, albedo, material, and environment lighting from only monocular videos. Recent advancements in human-based neural rendering have enabled high-quality geometry and appearance reconstruction of clothed humans from just monocular videos. However, these methods bake intrinsic properties such as albedo, material, and environment lighting into a single entangled neural representation. On the other hand, only a handful of works tackle the problem of estimating geometry and disentangled appearance properties of clothed humans from monocular videos. They usually achieve limited quality and disentanglement due to approximations of secondary shading effects via learned MLPs. In this work, we propose to model secondary shading effects explicitly via Monte-Carlo ray tracing. We model the rendering process of clothed humans as a volumetric scattering process, and combine ray tracing with body articulation. Our approach can recover high-quality geometry, albedo, material, and lighting properties of clothed humans from a single monocular video, without requiring supervised pre-training using ground truth materials. Furthermore, since we explicitly model the volumetric scattering process and ray tracing, our model naturally generalizes to novel poses, enabling animation of the reconstructed avatar in novel lighting conditions.",
        "page": "http://arxiv.org/abs/2312.05210",
        "pdf": "http://arxiv.org/pdf/2312.05210.pdf"
    },
    {
        "title": "Multi-modal learning for geospatial vegetation forecasting",
        "author": "Vitus Benson, Claire Robin, Christian Requena-Mesa, LAZARO ALONSO SILVA, M\u00e9lanie Weynants, Nora Linscheid, Jose Cortes, Zhihan Gao, Nuno Carvalhais, Markus Reichstein",
        "abstract": "The innovative application of precise geospatial vegetation forecasting holds immense potential across diverse sectors, including agriculture, forestry, humanitarian aid, and carbon accounting. To leverage the vast availability of satellite imagery for this task, various works have applied deep neural networks for predicting multispectral images in photorealistic quality. However, the important area of vegetation dynamics has not been thoroughly explored. Our study breaks new ground by introducing GreenEarthNet, the first dataset specifically designed for high-resolution vegetation forecasting, and Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images with fine resolution across Europe. Our multi-modal transformer model Contextformer leverages spatial context through a vision backbone and predicts the temporal dynamics on local context patches incorporating meteorological time series in a parameter-efficient manner. The GreenEarthNet dataset features a learned cloud mask and an appropriate evaluation scheme for vegetation modeling. It also maintains compatibility with the existing satellite imagery forecasting dataset EarthNet2021, enabling cross-dataset model comparisons. Our extensive qualitative and quantitative analyses reveal that our methods outperform a broad range of baseline techniques. This includes surpassing previous state-of-the-art models on EarthNet2021, as well as adapted models from time series forecasting and video prediction. To the best of our knowledge, this work presents the first models for continental-scale vegetation modeling at fine resolution able to capture anomalies beyond the seasonal cycle, thereby paving the way for predicting vegetation health and behaviour in response to climate variability and extremes.",
        "page": "http://arxiv.org/abs/2303.16198",
        "pdf": "http://arxiv.org/pdf/2303.16198.pdf"
    },
    {
        "title": "All in One Framework for Multimodal Re-identification in the Wild",
        "author": "He Li, Mang Ye, Ming Zhang, Bo Du",
        "abstract": "In Re-identification (ReID), recent advancements yield noteworthy progress in both unimodal and cross-modal retrieval tasks. However, the challenge persists in developing a unified framework that could effectively handle varying multimodal data, including RGB, infrared, sketches, and textual information. Additionally, the emergence of large-scale models shows promising performance in various vision tasks but the foundation model in ReID is still blank. In response to these challenges, a novel multimodal learning paradigm for ReID is introduced, referred to as All-in-One (AIO), which harnesses a frozen pre-trained big model as an encoder, enabling effective multimodal retrieval without additional fine-tuning. The diverse multimodal data in AIO are seamlessly tokenized into a unified space, allowing the modality-shared frozen encoder to extract identity-consistent features comprehensively across all modalities. Furthermore, a meticulously crafted ensemble of cross-modality heads is designed to guide the learning trajectory. AIO is the \\textbf{first} framework to perform all-in-one ReID, encompassing four commonly used modalities. Experiments on cross-modal and multimodal ReID reveal that AIO not only adeptly handles various modal data but also excels in challenging contexts, showcasing exceptional performance in zero-shot and domain generalization scenarios.",
        "page": "http://arxiv.org/abs/2405.04741",
        "pdf": "http://arxiv.org/pdf/2405.04741.pdf"
    },
    {
        "title": "Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification",
        "author": "Chao Yi, Lu Ren, De-Chuan Zhan, Han-Jia Ye",
        "abstract": "CLIP showcases exceptional cross-modal matching capabilities due to its training on image-text contrastive learning tasks. However, without specific optimization for unimodal scenarios, its performance in single-modality feature extraction might be suboptimal. Despite this, some studies have directly used CLIP's image encoder for tasks like few-shot classification, introducing a misalignment between its pre-training objectives and feature extraction methods. This inconsistency can diminish the quality of the image's feature representation, adversely affecting CLIP's effectiveness in target tasks. In this paper, we view text features as precise neighbors of image features in CLIP's space and present a novel CrOss-moDal nEighbor Representation(CODER) based on the distance structure between images and their neighbor texts. This feature extraction method aligns better with CLIP's pre-training objectives, thereby fully leveraging CLIP's robust cross-modal capabilities. The key to construct a high-quality CODER lies in how to create a vast amount of high-quality and diverse texts to match with images. We introduce the Auto Text Generator(ATG) to automatically generate the required texts in a data-free and training-free manner. We apply CODER to CLIP's zero-shot and few-shot image classification tasks. Experiment results across various datasets and models confirm CODER's effectiveness. Code is available at:https://github.com/YCaigogogo/CVPR24-CODER.",
        "page": "http://arxiv.org/abs/2404.17753",
        "pdf": "http://arxiv.org/pdf/2404.17753.pdf"
    },
    {
        "title": "Bilateral Adaptation for Human-Object Interaction Detection with Occlusion-Robustness",
        "author": "Guangzhi Wang, Yangyang Guo, Ziwei Xu, Mohan Kankanhalli",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs",
        "author": "Michael Dorkenwald, Nimrod Barazani, Cees G. M. Snoek, Yuki Asano",
        "abstract": "Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.",
        "page": "http://arxiv.org/abs/2402.08657",
        "pdf": "http://arxiv.org/pdf/2402.08657.pdf"
    },
    {
        "title": "MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior",
        "author": "Honghua Chen, Chen Change Loy, Xingang Pan",
        "abstract": "Despite the emergence of successful NeRF inpainting methods built upon explicit RGB and depth 2D inpainting supervisions, these methods are inherently constrained by the capabilities of their underlying 2D inpainters. This is due to two key reasons: (i) independently inpainting constituent images results in view-inconsistent imagery, and (ii) 2D inpainters struggle to ensure high-quality geometry completion and alignment with inpainted RGB images. To overcome these limitations, we propose a novel approach called MVIP-NeRF that harnesses the potential of diffusion priors for NeRF inpainting, addressing both appearance and geometry aspects. MVIP-NeRF performs joint inpainting across multiple views to reach a consistent solution, which is achieved via an iterative optimization process based on Score Distillation Sampling (SDS). Apart from recovering the rendered RGB images, we also extract normal maps as a geometric representation and define a normal SDS loss that motivates accurate geometry inpainting and alignment with the appearance. Additionally, we formulate a multi-view SDS score function to distill generative priors simultaneously from different view images, ensuring consistent visual completion when dealing with large view variations. Our experimental results show better appearance and geometry recovery than previous NeRF inpainting methods.",
        "page": "http://arxiv.org/abs/2405.02859",
        "pdf": "http://arxiv.org/pdf/2405.02859.pdf"
    },
    {
        "title": "Composed Video Retrieval via Enriched Context and Discriminative Embeddings",
        "author": "Omkar Thawakar, Muzammal Naseer, Rao Anwer, Salman Khan, Michael Felsberg, Mubarak Shah, Fahad Shahbaz Khan",
        "abstract": "Composed video retrieval (CoVR) is a challenging problem in computer vision which has recently highlighted the integration of modification text with visual queries for more sophisticated video search in large databases. Existing works predominantly rely on visual queries combined with modification text to distinguish relevant videos. However, such a strategy struggles to fully preserve the rich query-specific context in retrieved target videos and only represents the target video using visual embedding. We introduce a novel CoVR framework that leverages detailed language descriptions to explicitly encode query-specific contextual information and learns discriminative embeddings of vision only, text only and vision-text for better alignment to accurately retrieve matched target videos. Our proposed framework can be flexibly employed for both composed video (CoVR) and image (CoIR) retrieval tasks. Experiments on three datasets show that our approach obtains state-of-the-art performance for both CovR and zero-shot CoIR tasks, achieving gains as high as around 7% in terms of recall@K=1 score. Our code, models, detailed language descriptions for WebViD-CoVR dataset are available at \\url{https://github.com/OmkarThawakar/composed-video-retrieval}",
        "page": "http://arxiv.org/abs/2403.16997",
        "pdf": "http://arxiv.org/pdf/2403.16997.pdf"
    },
    {
        "title": "Low-Rank Approximation for Sparse Attention in Multi-Modal LLMs",
        "author": "Lin Song, Yukang Chen, Shuai Yang, Xiaohan Ding, Yixiao Ge, Ying-Cong Chen, Ying Shan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis",
        "author": "Zicheng Zhang, RUOBING ZHENG, Bonan Li, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Ziwen Liu, Ming Yang",
        "abstract": "Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications.",
        "page": "http://arxiv.org/abs/2402.17364",
        "pdf": "http://arxiv.org/pdf/2402.17364.pdf"
    },
    {
        "title": "PairDETR : Joint Detection and Association of Human Bodies and Faces",
        "author": "Ammar Ali, Georgii Gaikov, Denis Rybalchenko, Alexander Chigorin, Ivan Laptev, Sergey Zagoruyko",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Steerers: A framework for rotation equivariant keypoint descriptors",
        "author": "Georg B\u00f6kman, Johan Edstedt, Michael Felsberg, Fredrik Kahl",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "On the Faithfulness of Vision Transformer Explanations",
        "author": "Junyi Wu, Weitai Kang, Hao Tang, Yuan Hong, Yan Yan",
        "abstract": "To interpret Vision Transformers, post-hoc explanations assign salience scores to input pixels, providing human-understandable heatmaps. However, whether these interpretations reflect true rationales behind the model's output is still underexplored. To address this gap, we study the faithfulness criterion of explanations: the assigned salience scores should represent the influence of the corresponding input pixels on the model's predictions. To evaluate faithfulness, we introduce Salience-guided Faithfulness Coefficient (SaCo), a novel evaluation metric leveraging essential information of salience distribution. Specifically, we conduct pair-wise comparisons among distinct pixel groups and then aggregate the differences in their salience scores, resulting in a coefficient that indicates the explanation's degree of faithfulness. Our explorations reveal that current metrics struggle to differentiate between advanced explanation methods and Random Attribution, thereby failing to capture the faithfulness property. In contrast, our proposed SaCo offers a reliable faithfulness measurement, establishing a robust metric for interpretations. Furthermore, our SaCo demonstrates that the use of gradient and multi-layer aggregation can markedly enhance the faithfulness of attention-based explanation, shedding light on potential paths for advancing Vision Transformer explainability.",
        "page": "http://arxiv.org/abs/2404.01415",
        "pdf": "http://arxiv.org/pdf/2404.01415.pdf"
    },
    {
        "title": "Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss",
        "author": "Jaeha Kim, Junghun Oh, Kyoung Mu Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Boosting Image Quality Assessment through Efficient Transformer Adaptation with Local Feature Enhancement",
        "author": "Kangmin Xu, Liang Liao, Jing Xiao, Chaofeng Chen, Haoning Wu, Qiong Yan, Weisi Lin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "COLMAP-Free 3D Gaussian Splatting",
        "author": "Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Point-VOS: Pointing Up Video Object Segmentation",
        "author": "Sabarinath Mahadevan, Idil Esen Zulfikar, Paul Voigtlaender, Bastian Leibe",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion",
        "author": "Linzhan Mou, Jun-Kun Chen, Yu-Xiong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
        "author": "Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig",
        "abstract": "The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs. Code: https://github.com/chancharikmitra/CCoT",
        "page": "http://arxiv.org/abs/2311.17076",
        "pdf": "http://arxiv.org/pdf/2311.17076.pdf"
    },
    {
        "title": "As-Plausible-As-Possible: Plausibility-Aware Mesh Deformation Using 2D Diffusion Priors",
        "author": "Seungwoo Yoo, Kunho Kim, Vladimir G. Kim, Minhyuk Sung",
        "abstract": "We present As-Plausible-as-Possible (APAP) mesh deformation technique that leverages 2D diffusion priors to preserve the plausibility of a mesh under user-controlled deformation. Our framework uses per-face Jacobians to represent mesh deformations, where mesh vertex coordinates are computed via a differentiable Poisson Solve. The deformed mesh is rendered, and the resulting 2D image is used in the Score Distillation Sampling (SDS) process, which enables extracting meaningful plausibility priors from a pretrained 2D diffusion model. To better preserve the identity of the edited mesh, we fine-tune our 2D diffusion model with LoRA. Gradients extracted by SDS and a user-prescribed handle displacement are then backpropagated to the per-face Jacobians, and we use iterative gradient descent to compute the final deformation that balances between the user edit and the output plausibility. We evaluate our method with 2D and 3D meshes and demonstrate qualitative and quantitative improvements when using plausibility priors over geometry-preservation or distortion-minimization priors used by previous techniques. Our project page is at: https://as-plausible-aspossible.github.io/",
        "page": "http://arxiv.org/abs/2311.16739",
        "pdf": "http://arxiv.org/pdf/2311.16739.pdf"
    },
    {
        "title": "Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts",
        "author": "Qin Liu, Jaemin Cho, Mohit Bansal, Marc Niethammer",
        "abstract": "The goal of interactive image segmentation is to delineate specific regions within an image via visual or language prompts. Low-latency and high-quality interactive segmentation with diverse prompts remain challenging for existing specialist and generalist models. Specialist models, with their limited prompts and task-specific designs, experience high latency because the image must be recomputed every time the prompt is updated, due to the joint encoding of image and visual prompts. Generalist models, exemplified by the Segment Anything Model (SAM), have recently excelled in prompt diversity and efficiency, lifting image segmentation to the foundation model era. However, for high-quality segmentations, SAM still lags behind state-of-the-art specialist models despite SAM being trained with x100 more segmentation masks. In this work, we delve deep into the architectural differences between the two types of models. We observe that dense representation and fusion of visual prompts are the key design choices contributing to the high segmentation quality of specialist models. In light of this, we reintroduce this dense design into the generalist models, to facilitate the development of generalist models with high segmentation quality. To densely represent diverse visual prompts, we propose to use a dense map to capture five types: clicks, boxes, polygons, scribbles, and masks. Thus, we propose SegNext, a next-generation interactive segmentation approach offering low latency, high quality, and diverse prompt support. Our method outperforms current state-of-the-art methods on HQSeg-44K and DAVIS, both quantitatively and qualitatively.",
        "page": "http://arxiv.org/abs/2404.00741",
        "pdf": "http://arxiv.org/pdf/2404.00741.pdf"
    },
    {
        "title": "NViST: In the Wild New View Synthesis from a Single Image with Transformers",
        "author": "Wonbong Jang, Lourdes Agapito",
        "abstract": "We propose NViST, a transformer-based model for efficient and generalizable novel-view synthesis from a single image for real-world scenes. In contrast to many methods that are trained on synthetic data, object-centred scenarios, or in a category-specific manner, NViST is trained on MVImgNet, a large-scale dataset of casually-captured real-world videos of hundreds of object categories with diverse backgrounds. NViST transforms image inputs directly into a radiance field, conditioned on camera parameters via adaptive layer normalisation. In practice, NViST exploits fine-tuned masked autoencoder (MAE) features and translates them to 3D output tokens via cross-attention, while addressing occlusions with self-attention. To move away from object-centred datasets and enable full scene synthesis, NViST adopts a 6-DOF camera pose model and only requires relative pose, dropping the need for canonicalization of the training data, which removes a substantial barrier to it being used on casually captured datasets. We show results on unseen objects and categories from MVImgNet and even generalization to casual phone captures. We conduct qualitative and quantitative evaluations on MVImgNet and ShapeNet to show that our model represents a step forward towards enabling true in-the-wild generalizable novel-view synthesis from a single image. Project webpage: https://wbjang.github.io/nvist_webpage.",
        "page": "http://arxiv.org/abs/2312.08568",
        "pdf": "http://arxiv.org/pdf/2312.08568.pdf"
    },
    {
        "title": "Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation",
        "author": "Yunhao Ge, Xiaohui Zeng, Jacob Huffman, Tsung-Yi Lin, Ming-Yu Liu, Yin Cui",
        "abstract": "Existing automatic captioning methods for visual content face challenges such as lack of detail, content hallucination, and poor instruction following. In this work, we propose VisualFactChecker (VFC), a flexible training-free pipeline that generates high-fidelity and detailed captions for both 2D images and 3D objects. VFC consists of three steps: 1) proposal, where image-to-text captioning models propose multiple initial captions; 2) verification, where a large language model (LLM) utilizes tools such as object detection and VQA models to fact-check proposed captions; 3) captioning, where an LLM generates the final caption by summarizing caption proposals and the fact check verification results. In this step, VFC can flexibly generate captions in various styles following complex instructions. We conduct comprehensive captioning evaluations using four metrics: 1) CLIP-Score for image-text similarity; 2) CLIP-Image-Score for measuring the image-image similarity between the original and the reconstructed image generated by a text-to-image model using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4V for fine-grained evaluation. Evaluation results show that VFC outperforms state-of-the-art open-sourced captioning methods for 2D images on the COCO dataset and 3D assets on the Objaverse dataset. Our study demonstrates that by combining open-source models into a pipeline, we can attain captioning capability comparable to proprietary models such as GPT-4V, despite being over 10x smaller in model size.",
        "page": "http://arxiv.org/abs/2404.19752",
        "pdf": "http://arxiv.org/pdf/2404.19752.pdf"
    },
    {
        "title": "Latency Correction for Event-guided Deblurring and Frame Interpolation",
        "author": "Yixin Yang, Jinxiu Liang, Bohan Yu, Yan Chen, Jimmy S. Ren, Boxin Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Visual Recognition with Hyperbolical Visual Hierarchy Mapping",
        "author": "Hyeongjun Kwon, Jinhyun Jang, Jin Kim, Kwonyoung Kim, Kwanghoon Sohn",
        "abstract": "Visual scenes are naturally organized in a hierarchy, where a coarse semantic is recursively comprised of several fine details. Exploring such a visual hierarchy is crucial to recognize the complex relations of visual elements, leading to a comprehensive scene understanding. In this paper, we propose a Visual Hierarchy Mapper (Hi-Mapper), a novel approach for enhancing the structured understanding of the pre-trained Deep Neural Networks (DNNs). Hi-Mapper investigates the hierarchical organization of the visual scene by 1) pre-defining a hierarchy tree through the encapsulation of probability densities; and 2) learning the hierarchical relations in hyperbolic space with a novel hierarchical contrastive loss. The pre-defined hierarchy tree recursively interacts with the visual features of the pre-trained DNNs through hierarchy decomposition and encoding procedures, thereby effectively identifying the visual hierarchy and enhancing the recognition of an entire scene. Extensive experiments demonstrate that Hi-Mapper significantly enhances the representation capability of DNNs, leading to an improved performance on various tasks, including image classification and dense prediction tasks.",
        "page": "http://arxiv.org/abs/2404.00974",
        "pdf": "http://arxiv.org/pdf/2404.00974.pdf"
    },
    {
        "title": "Self-supervised Representation Learning from Arbitrary Scenarios",
        "author": "Zhaowen Li, Yousong Zhu, Zhiyang Chen, Zongxin Gao, Rui Zhao, Chaoyang Zhao, Ming Tang, Jinqiao Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NEAT: Distilling 3D Wireframes from Neural Attraction Fields",
        "author": "Nan Xue, Bin Tan, Yuxi Xiao, Liang Dong, Gui-Song Xia, Tianfu Wu, Yujun Shen",
        "abstract": "This paper studies the problem of structured 3D reconstruction using wireframes that consist of line segments and junctions, focusing on the computation of structured boundary geometries of scenes. Instead of leveraging matching-based solutions from 2D wireframes (or line segments) for 3D wireframe reconstruction as done in prior arts, we present NEAT, a rendering-distilling formulation using neural fields to represent 3D line segments with 2D observations, and bipartite matching for perceiving and distilling of a sparse set of 3D global junctions. The proposed {NEAT} enjoys the joint optimization of the neural fields and the global junctions from scratch, using view-dependent 2D observations without precomputed cross-view feature matching. Comprehensive experiments on the DTU and BlendedMVS datasets demonstrate our NEAT's superiority over state-of-the-art alternatives for 3D wireframe reconstruction. Moreover, the distilled 3D global junctions by NEAT, are a better initialization than SfM points, for the recently-emerged 3D Gaussian Splatting for high-fidelity novel view synthesis using about 20 times fewer initial 3D points. Project page: \\url{https://xuenan.net/neat}.",
        "page": "http://arxiv.org/abs/2307.10206",
        "pdf": "http://arxiv.org/pdf/2307.10206.pdf"
    },
    {
        "title": "Generating Content for HDR Deghosting from Frequency View",
        "author": "Tao Hu, Qingsen Yan, Yuankai Qi, Yanning Zhang",
        "abstract": "Recovering ghost-free High Dynamic Range (HDR) images from multiple Low Dynamic Range (LDR) images becomes challenging when the LDR images exhibit saturation and significant motion. Recent Diffusion Models (DMs) have been introduced in HDR imaging field, demonstrating promising performance, particularly in achieving visually perceptible results compared to previous DNN-based methods. However, DMs require extensive iterations with large models to estimate entire images, resulting in inefficiency that hinders their practical application. To address this challenge, we propose the Low-Frequency aware Diffusion (LF-Diff) model for ghost-free HDR imaging. The key idea of LF-Diff is implementing the DMs in a highly compacted latent space and integrating it into a regression-based model to enhance the details of reconstructed images. Specifically, as low-frequency information is closely related to human visual perception we propose to utilize DMs to create compact low-frequency priors for the reconstruction process. In addition, to take full advantage of the above low-frequency priors, the Dynamic HDR Reconstruction Network (DHRNet) is carried out in a regression-based manner to obtain final HDR images. Extensive experiments conducted on synthetic and real-world benchmark datasets demonstrate that our LF-Diff performs favorably against several state-of-the-art methods and is 10$\\times$ faster than previous DM-based methods.",
        "page": "http://arxiv.org/abs/2404.00849",
        "pdf": "http://arxiv.org/pdf/2404.00849.pdf"
    },
    {
        "title": "Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model",
        "author": "Tian Liang, Jing Huang, Ming Kong, Luyuan Chen, Qiang Zhu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GeoChat: Grounded Large Vision-Language Model for Remote Sensing",
        "author": "Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, Fahad Shahbaz Khan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection",
        "author": "Zhiwei Yang, Jing Liu, Peng Wu",
        "abstract": "Weakly supervised video anomaly detection (WSVAD) is a challenging task. Generating fine-grained pseudo-labels based on weak-label and then self-training a classifier is currently a promising solution. However, since the existing methods use only RGB visual modality and the utilization of category text information is neglected, thus limiting the generation of more accurate pseudo-labels and affecting the performance of self-training. Inspired by the manual labeling process based on the event description, in this paper, we propose a novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer the rich language-visual knowledge of the contrastive language-image pre-training (CLIP) model for aligning the video event description text and corresponding video frames to generate pseudo-labels. Specifically, We first fine-tune the CLIP for domain adaptation by designing two ranking losses and a distributional inconsistency loss. Further, we propose a learnable text prompt mechanism with the assist of a normality visual prompt to further improve the matching accuracy of video event description text and video frames. Then, we design a pseudo-label generation module based on the normality guidance to infer reliable frame-level pseudo-labels. Finally, we introduce a temporal context self-adaptive learning module to learn the temporal dependencies of different video events more flexibly and accurately. Extensive experiments show that our method achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Viole",
        "page": "http://arxiv.org/abs/2404.08531",
        "pdf": "http://arxiv.org/pdf/2404.08531.pdf"
    },
    {
        "title": "Prompt Learning via Meta-Regularization",
        "author": "Jinyoung Park, Juyeon Ko, Hyunwoo J. Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Addressing Background Context Bias in Few-Shot Segmentation through Iterative Modulation",
        "author": "Lanyun Zhu, Tianrun Chen, Jianxiong Yin, Simon See, Jun Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Rethinking the Region Classification in Open-Vocabulary Semantic Segmentation: An Image-to-Image View",
        "author": "Yuan Wang, Rui Sun, Naisong Luo, Yuwen Pan, Tianzhu Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SuperNormal: Neural Surface Reconstruction via Multi-View Normal Integration",
        "author": "Xu Cao, Takafumi Taketomi",
        "abstract": "We present SuperNormal, a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. With a few minutes, SuperNormal produces detailed surfaces on par with 3D scanners. We harness volume rendering to optimize a neural signed distance function (SDF) powered by multi-resolution hash encoding. To accelerate training, we propose directional finite difference and patch-based ray marching to approximate the SDF gradients numerically. While not compromising reconstruction quality, this strategy is nearly twice as efficient as analytical gradients and about three times faster than axis-aligned finite difference. Experiments on the benchmark dataset demonstrate the superiority of SuperNormal in efficiency and accuracy compared to existing multi-view photometric stereo methods. On our captured objects, SuperNormal produces more fine-grained geometry than recent neural 3D reconstruction methods.",
        "page": "http://arxiv.org/abs/2312.04803",
        "pdf": "http://arxiv.org/pdf/2312.04803.pdf"
    },
    {
        "title": "Navigating Beyond Dropout: An Intriguing Solution towards Generalizable Image Super-Resolution",
        "author": "Hongjun Wang, Jiyuan Chen, Yinqiang Zheng, Tieyong Zeng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation",
        "author": "Xiaoyang Wang, Huihui Bai, Limin Yu, Yao Zhao, Jimin Xiao",
        "abstract": "Semi-supervised semantic segmentation allows model to mine effective supervision from unlabeled data to complement label-guided training. Recent research has primarily focused on consistency regularization techniques, exploring perturbation-invariant training at both the image and feature levels. In this work, we proposed a novel feature-level consistency learning framework named Density-Descending Feature Perturbation (DDFP). Inspired by the low-density separation assumption in semi-supervised learning, our key insight is that feature density can shed a light on the most promising direction for the segmentation classifier to explore, which is the regions with lower density. We propose to shift features with confident predictions towards lower-density regions by perturbation injection. The perturbed features are then supervised by the predictions on the original features, thereby compelling the classifier to explore less dense regions to effectively regularize the decision boundary. Central to our method is the estimation of feature density. To this end, we introduce a lightweight density estimator based on normalizing flow, allowing for efficient capture of the feature density distribution in an online manner. By extracting gradients from the density estimator, we can determine the direction towards less dense regions for each feature. The proposed DDFP outperforms other designs on feature-level perturbations and shows state of the art performances on both Pascal VOC and Cityscapes dataset under various partition protocols. The project is available at https://github.com/Gavinwxy/DDFP.",
        "page": "http://arxiv.org/abs/2403.06462",
        "pdf": "http://arxiv.org/pdf/2403.06462.pdf"
    },
    {
        "title": "General Object Foundation Model for Images and Videos at Scale",
        "author": "Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai",
        "abstract": "We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos. Through a unified framework, GLEE accomplishes detection, segmentation, tracking, grounding, and identification of arbitrary objects in the open world scenario for various object perception tasks. Adopting a cohesive learning strategy, GLEE acquires knowledge from diverse data sources with varying supervision levels to formulate general object representations, excelling in zero-shot transfer to new data and tasks. Specifically, we employ an image encoder, text encoder, and visual prompter to handle multi-modal inputs, enabling to simultaneously solve various object-centric downstream tasks while maintaining state-of-the-art performance. Demonstrated through extensive training on over five million images from diverse benchmarks, GLEE exhibits remarkable versatility and improved generalization performance, efficiently tackling downstream tasks without the need for task-specific adaptation. By integrating large volumes of automatically labeled data, we further enhance its zero-shot generalization capabilities. Additionally, GLEE is capable of being integrated into Large Language Models, serving as a foundational model to provide universal object-level information for multi-modal tasks. We hope that the versatility and universality of our method will mark a significant step in the development of efficient visual foundation models for AGI systems. The model and code will be released at https://glee-vision.github.io .",
        "page": "http://arxiv.org/abs/2312.09158",
        "pdf": "http://arxiv.org/pdf/2312.09158.pdf"
    },
    {
        "title": "Emotional Speech-Driven 3D Body Animation via Disentangled Latent Diffusion",
        "author": "Kiran Chhatre, Radek Danecek, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J. Black, Timo Bolkart",
        "abstract": "Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content, and better represent the emotion expressed by the input speech. Our code is available at amuse.is.tue.mpg.de.",
        "page": "http://arxiv.org/abs/2312.04466",
        "pdf": "http://arxiv.org/pdf/2312.04466.pdf"
    },
    {
        "title": "Deciphering \u2018What\u2019 and \u2018Where\u2019 Visual Pathways from Spectral Clustering of Layer-Distributed Neural Representations",
        "author": "Xiao Zhang, David Yunis, Michael Maire",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Distribution-aware Knowledge Prototyping for Non-exemplar Lifelong Person Re-identification",
        "author": "Kunlun Xu, Xu Zou, Yuxin Peng, Jiahuan Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced Transformer for 3D Human Pose Estimation",
        "author": "Jihua Peng, Yanghong Zhou, Tracy P Y Mok",
        "abstract": "This paper presents a novel Kinematics and Trajectory Prior Knowledge-Enhanced Transformer (KTPFormer), which overcomes the weakness in existing transformer-based methods for 3D human pose estimation that the derivation of Q, K, V vectors in their self-attention mechanisms are all based on simple linear mapping. We propose two prior attention modules, namely Kinematics Prior Attention (KPA) and Trajectory Prior Attention (TPA) to take advantage of the known anatomical structure of the human body and motion trajectory information, to facilitate effective learning of global dependencies and features in the multi-head self-attention. KPA models kinematic relationships in the human body by constructing a topology of kinematics, while TPA builds a trajectory topology to learn the information of joint motion trajectory across frames. Yielding Q, K, V vectors with prior knowledge, the two modules enable KTPFormer to model both spatial and temporal correlations simultaneously. Extensive experiments on three benchmarks (Human3.6M, MPI-INF-3DHP and HumanEva) show that KTPFormer achieves superior performance in comparison to state-of-the-art methods. More importantly, our KPA and TPA modules have lightweight plug-and-play designs and can be integrated into various transformer-based networks (i.e., diffusion-based) to improve the performance with only a very small increase in the computational overhead. The code is available at: https://github.com/JihuaPeng/KTPFormer.",
        "page": "http://arxiv.org/abs/2404.00658",
        "pdf": "http://arxiv.org/pdf/2404.00658.pdf"
    },
    {
        "title": "Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering",
        "author": "Zaid Khan, Yun Fu",
        "abstract": "The goal of selective prediction is to allow an a model to abstain when it may not be able to deliver a reliable prediction, which is important in safety-critical contexts. Existing approaches to selective prediction typically require access to the internals of a model, require retraining a model or study only unimodal models. However, the most powerful models (e.g. GPT-4) are typically only available as black boxes with inaccessible internals, are not retrainable by end-users, and are frequently used for multimodal tasks. We study the possibility of selective prediction for vision-language models in a realistic, black-box setting. We propose using the principle of \\textit{neighborhood consistency} to identify unreliable responses from a black-box vision-language model in question answering tasks. We hypothesize that given only a visual question and model response, the consistency of the model's responses over the neighborhood of a visual question will indicate reliability. It is impossible to directly sample neighbors in feature space in a black-box setting. Instead, we show that it is possible to use a smaller proxy model to approximately sample from the neighborhood. We find that neighborhood consistency can be used to identify model responses to visual questions that are likely unreliable, even in adversarial settings or settings that are out-of-distribution to the proxy model.",
        "page": "http://arxiv.org/abs/2404.10193",
        "pdf": "http://arxiv.org/pdf/2404.10193.pdf"
    },
    {
        "title": "Optimal Transport Aggregation for Visual Place Recognition",
        "author": "Sergio Izquierdo, Javier Civera",
        "abstract": "The task of Visual Place Recognition (VPR) aims to match a query image against references from an extensive database of images from different places, relying solely on visual cues. State-of-the-art pipelines focus on the aggregation of features extracted from a deep backbone, in order to form a global descriptor for each image. In this context, we introduce SALAD (Sinkhorn Algorithm for Locally Aggregated Descriptors), which reformulates NetVLAD's soft-assignment of local features to clusters as an optimal transport problem. In SALAD, we consider both feature-to-cluster and cluster-to-feature relations and we also introduce a 'dustbin' cluster, designed to selectively discard features deemed non-informative, enhancing the overall descriptor quality. Additionally, we leverage and fine-tune DINOv2 as a backbone, which provides enhanced description power for the local features, and dramatically reduces the required training time. As a result, our single-stage method not only surpasses single-stage baselines in public VPR datasets, but also surpasses two-stage methods that add a re-ranking with significantly higher cost. Code and models are available at https://github.com/serizba/salad.",
        "page": "http://arxiv.org/abs/2311.15937",
        "pdf": "http://arxiv.org/pdf/2311.15937.pdf"
    },
    {
        "title": "HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting",
        "author": "Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao",
        "abstract": "Holistic understanding of urban scenes based on RGB images is a challenging yet important problem. It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects. Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints. Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach.",
        "page": "http://arxiv.org/abs/2403.12722",
        "pdf": "http://arxiv.org/pdf/2403.12722.pdf"
    },
    {
        "title": "Human Motion Prediction under Unexpected Perturbation",
        "author": "Jiangbei Yue, Baiyi Li, Julien Pettr\u00e9, Armin Seyfried, He Wang",
        "abstract": "We investigate a new task in human motion prediction, which is predicting motions under unexpected physical perturbation potentially involving multiple people. Compared with existing research, this task involves predicting less controlled, unpremeditated and pure reactive motions in response to external impact and how such motions can propagate through people. It brings new challenges such as data scarcity and predicting complex interactions. To this end, we propose a new method capitalizing differential physics and deep neural networks, leading to an explicit Latent Differential Physics (LDP) model. Through experiments, we demonstrate that LDP has high data efficiency, outstanding prediction accuracy, strong generalizability and good explainability. Since there is no similar research, a comprehensive comparison with 11 adapted baselines from several relevant domains is conducted, showing LDP outperforming existing research both quantitatively and qualitatively, improving prediction accuracy by as much as 70%, and demonstrating significantly stronger generalization.",
        "page": "http://arxiv.org/abs/2403.15891",
        "pdf": "http://arxiv.org/pdf/2403.15891.pdf"
    },
    {
        "title": "MFP: Making Full use of Probability Maps for Interactive Image Segmentation",
        "author": "Chaewon Lee, Seon-Ho Lee, Chang-Su Kim",
        "abstract": "In recent interactive segmentation algorithms, previous probability maps are used as network input to help predictions in the current segmentation round. However, despite the utilization of previous masks, useful information contained in the probability maps is not well propagated to the current predictions. In this paper, to overcome this limitation, we propose a novel and effective algorithm for click-based interactive image segmentation, called MFP, which attempts to make full use of probability maps. We first modulate previous probability maps to enhance their representations of user-specified objects. Then, we feed the modulated probability maps as additional input to the segmentation network. We implement the proposed MFP algorithm based on the ResNet-34, HRNet-18, and ViT-B backbones and assess the performance extensively on various datasets. It is demonstrated that MFP meaningfully outperforms the existing algorithms using identical backbones. The source codes are available at https://github.com/cwlee00/MFP.",
        "page": "http://arxiv.org/abs/2404.18448",
        "pdf": "http://arxiv.org/pdf/2404.18448.pdf"
    },
    {
        "title": "Instantaneous Perception of Moving Objects in 3D",
        "author": "Di Liu, Bingbing Zhuang, Dimitris N. Metaxas, Manmohan Chandraker",
        "abstract": "The perception of 3D motion of surrounding traffic participants is crucial for driving safety. While existing works primarily focus on general large motions, we contend that the instantaneous detection and quantification of subtle motions is equally important as they indicate the nuances in driving behavior that may be safety critical, such as behaviors near a stop sign of parking positions. We delve into this under-explored task, examining its unique challenges and developing our solution, accompanied by a carefully designed benchmark. Specifically, due to the lack of correspondences between consecutive frames of sparse Lidar point clouds, static objects might appear to be moving - the so-called swimming effect. This intertwines with the true object motion, thereby posing ambiguity in accurate estimation, especially for subtle motions. To address this, we propose to leverage local occupancy completion of object point clouds to densify the shape cue, and mitigate the impact of swimming artifacts. The occupancy completion is learned in an end-to-end fashion together with the detection of moving objects and the estimation of their motion, instantaneously as soon as objects start to move. Extensive experiments demonstrate superior performance compared to standard 3D motion estimation approaches, particularly highlighting our method's specialized treatment of subtle motions.",
        "page": "http://arxiv.org/abs/2405.02781",
        "pdf": "http://arxiv.org/pdf/2405.02781.pdf"
    },
    {
        "title": "Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning",
        "author": "Zhengwei Fang, Rui Wang, Tao Huang, Liping Jing",
        "abstract": "Strong adversarial examples are crucial for evaluating and enhancing the robustness of deep neural networks. However, the performance of popular attacks is usually sensitive, for instance, to minor image transformations, stemming from limited information -- typically only one input example, a handful of white-box source models, and undefined defense strategies. Hence, the crafted adversarial examples are prone to overfit the source model, which hampers their transferability to unknown architectures. In this paper, we propose an approach named Multiple Asymptotically Normal Distribution Attacks (MultiANDA) which explicitly characterize adversarial perturbations from a learned distribution. Specifically, we approximate the posterior distribution over the perturbations by taking advantage of the asymptotic normality property of stochastic gradient ascent (SGA), then employ the deep ensemble strategy as an effective proxy for Bayesian marginalization in this process, aiming to estimate a mixture of Gaussians that facilitates a more thorough exploration of the potential optimization space. The approximated posterior essentially describes the stationary distribution of SGA iterations, which captures the geometric information around the local optimum. Thus, MultiANDA allows drawing an unlimited number of adversarial perturbations for each input and reliably maintains the transferability. Our proposed method outperforms ten state-of-the-art black-box attacks on deep learning models with or without defenses through extensive experiments on seven normally trained and seven defense models.",
        "page": "http://arxiv.org/abs/2209.11964",
        "pdf": "http://arxiv.org/pdf/2209.11964.pdf"
    },
    {
        "title": "Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction",
        "author": "Yizhi Wang, Wallace Lira, Wenqi Wang, Ali Mahdavi Amiri, Hao Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Differentiable Neural Surface Refinement for Transparent Objects",
        "author": "Weijian Deng, Dylan Campbell, Chunyi Sun, Shubham Kanitkar, Matthew Shaffer, Stephen Gould",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything",
        "author": "Yunyang Xiong, Balakrishnan Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra",
        "abstract": "Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.",
        "page": "http://arxiv.org/abs/2312.00863",
        "pdf": "http://arxiv.org/pdf/2312.00863.pdf"
    },
    {
        "title": "Look-Up Table Compression for Efficient Image Restoration",
        "author": "Yinglong Li, Jiacheng Li, Zhiwei Xiong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation",
        "author": "Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe",
        "abstract": "Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and estimation accuracy compared to the original VPT models. For instance, applying to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop, respectively. Code and models are available at https://github.com/NationalGAILab/HoT.",
        "page": "http://arxiv.org/abs/2311.12028",
        "pdf": "http://arxiv.org/pdf/2311.12028.pdf"
    },
    {
        "title": "RepAn: Enhanced Annealing through Re-parameterization",
        "author": "Xiang Fei, Xiawu Zheng, Yan Wang, Fei Chao, Chenglin Wu, Liujuan Cao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Vector Graphics Generation via Mutually Impulsed Dual-domain Diffusion",
        "author": "Zhongyin Zhao, Ye Chen, Zhangli Hu, Xuanhong Chen, Bingbing Ni",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FocSAM: Delving Deeply into Focused Objects in Segmenting Anything",
        "author": "You Huang, Zongyu Lan, Liujuan Cao, Xianming Lin, Shengchuan Zhang, Guannan Jiang, Rongrong Ji",
        "abstract": "The Segment Anything Model (SAM) marks a notable milestone in segmentation models, highlighted by its robust zero-shot capabilities and ability to handle diverse prompts. SAM follows a pipeline that separates interactive segmentation into image preprocessing through a large encoder and interactive inference via a lightweight decoder, ensuring efficient real-time performance. However, SAM faces stability issues in challenging samples upon this pipeline. These issues arise from two main factors. Firstly, the image preprocessing disables SAM from dynamically using image-level zoom-in strategies to refocus on the target object during interaction. Secondly, the lightweight decoder struggles to sufficiently integrate interactive information with image embeddings. To address these two limitations, we propose FocSAM with a pipeline redesigned on two pivotal aspects. First, we propose Dynamic Window Multi-head Self-Attention (Dwin-MSA) to dynamically refocus SAM's image embeddings on the target object. Dwin-MSA localizes attention computations around the target object, enhancing object-related embeddings with minimal computational overhead. Second, we propose Pixel-wise Dynamic ReLU (P-DyReLU) to enable sufficient integration of interactive information from a few initial clicks that have significant impacts on the overall segmentation results. Experimentally, FocSAM augments SAM's interactive segmentation performance to match the existing state-of-the-art method in segmentation quality, requiring only about 5.6% of this method's inference time on CPUs.",
        "page": "http://arxiv.org/abs/2405.18706",
        "pdf": "http://arxiv.org/pdf/2405.18706.pdf"
    },
    {
        "title": "ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting",
        "author": "Chen Duan, Pei Fu, Shan Guo, Qianyi Jiang, Xiaoming Wei",
        "abstract": "In recent years, text-image joint pre-training techniques have shown promising results in various tasks. However, in Optical Character Recognition (OCR) tasks, aligning text instances with their corresponding text regions in images poses a challenge, as it requires effective alignment between text and OCR-Text (referring to the text in images as OCR-Text to distinguish from the text in natural language) rather than a holistic understanding of the overall image content. In this paper, we propose a new pre-training method called OCR-Text Destylization Modeling (ODM) that transfers diverse styles of text found in images to a uniform style based on the text prompt. With ODM, we achieve better alignment between text and OCR-Text and enable pre-trained models to adapt to the complex and diverse styles of scene text detection and spotting tasks. Additionally, we have designed a new labeling generation method specifically for ODM and combined it with our proposed Text-Controller module to address the challenge of annotation costs in OCR tasks, allowing a larger amount of unlabeled data to participate in pre-training. Extensive experiments on multiple public datasets demonstrate that our method significantly improves performance and outperforms current pre-training methods in scene text detection and spotting tasks. Code is available at https://github.com/PriNing/ODM.",
        "page": "http://arxiv.org/abs/2403.00303",
        "pdf": "http://arxiv.org/pdf/2403.00303.pdf"
    },
    {
        "title": "CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow",
        "author": "Chenbin Pan, Burhaneddin Yaman, Senem Velipasalar, Liu Ren",
        "abstract": "Autonomous driving stands as a pivotal domain in computer vision, shaping the future of transportation. Within this paradigm, the backbone of the system plays a crucial role in interpreting the complex environment. However, a notable challenge has been the loss of clear supervision when it comes to Bird's Eye View elements. To address this limitation, we introduce CLIP-BEVFormer, a novel approach that leverages the power of contrastive learning techniques to enhance the multi-view image-derived BEV backbones with ground truth information flow. We conduct extensive experiments on the challenging nuScenes dataset and showcase significant and consistent improvements over the SOTA. Specifically, CLIP-BEVFormer achieves an impressive 8.5\\% and 9.2\\% enhancement in terms of NDS and mAP, respectively, over the previous best BEV model on the 3D object detection task.",
        "page": "http://arxiv.org/abs/2403.08919",
        "pdf": "http://arxiv.org/pdf/2403.08919.pdf"
    },
    {
        "title": "Hyperspherical Classification with Dynamic Label-to-Prototype Assignment",
        "author": "Mohammad Saadabadi Saadabadi, Ali Dabouei, Sahar Rahimi Malakshan, Nasser Nasrabadi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts",
        "author": "Jiawen Zhu, Guansong Pang",
        "abstract": "This paper explores the problem of Generalist Anomaly Detection (GAD), aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data. Some recent studies have shown that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text prompts about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images. In this work, we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly. To this end, we introduce a novel approach that learns an in-context residual learning model for GAD, termed InCTRL. It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts. Regardless of the datasets, per definition of anomaly, larger residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training. Comprehensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulate the detection of industrial defect anomalies, medical anomalies, and semantic anomalies in both one-vs-all and multi-class setting, on which InCTRL is the best performer and significantly outperforms state-of-the-art competing methods. Code is available at https://github.com/mala-lab/InCTRL.",
        "page": "http://arxiv.org/abs/2403.06495",
        "pdf": "http://arxiv.org/pdf/2403.06495.pdf"
    },
    {
        "title": "VideoGrounding-DINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding",
        "author": "Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "ODIN: A Single Model for 2D and 3D Segmentation",
        "author": "Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Prompt Augmentation for Self-supervised Text-guided Image Manipulation",
        "author": "Rumeysa Bodur, Binod Bhattarai, Tae-Kyun Kim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Open-Vocabulary Segmentation with Semantic-Assisted Calibration",
        "author": "Yong Liu, Sule Bai, Guanbin Li, Yitong Wang, Yansong Tang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via Diffusion Models",
        "author": "Jinglin Xu, Yijie Guo, Yuxin Peng",
        "abstract": "The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \\textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024.",
        "page": "http://arxiv.org/abs/2405.05216",
        "pdf": "http://arxiv.org/pdf/2405.05216.pdf"
    },
    {
        "title": "MaskClustering:  View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation",
        "author": "Mi Yan, Jiazhao Zhang, Yan Zhu, He Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PointBeV: A Sparse Approach for BeV Predictions",
        "author": "Loick Chambon, \u00c9loi Zablocki, Micka\u00ebl Chen, Florent Bartoccioni, Patrick P\u00e9rez, Matthieu Cord",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification",
        "author": "kaijie ren, Lei Zhang",
        "abstract": "Visible-Infrared Person Re-identification (VI-ReID) is a challenging cross-modal pedestrian retrieval task, due to significant intra-class variations and cross-modal discrepancies among different cameras. Existing works mainly focus on embedding images of different modalities into a unified space to mine modality-shared features. They only seek distinctive information within these shared features, while ignoring the identity-aware useful information that is implicit in the modality-specific features. To address this issue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL) network to uncover and leverage the implicit discriminative information contained within the modality-specific. First, we extract modality-specific and modality-shared features using a novel dual-stream network. Then, the modality-specific features undergo purification to reduce their modality style discrepancies while preserving identity-aware discriminative knowledge. Subsequently, this kind of implicit knowledge is distilled into the modality-shared feature to enhance its distinctiveness. Finally, an alignment loss is proposed to minimize modality discrepancy on enhanced modality-shared features. Extensive experiments on multiple public datasets demonstrate the superiority of IDKL network over the state-of-the-art methods. Code is available at https://github.com/1KK077/IDKL.",
        "page": "http://arxiv.org/abs/2403.11708",
        "pdf": "http://arxiv.org/pdf/2403.11708.pdf"
    },
    {
        "title": "On the Content Bias in Fr\u00e9chet Video Distance",
        "author": "Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, Jia-Bin Huang",
        "abstract": "Fr\\'echet Video Distance (FVD), a prominent metric for evaluating video generation models, is known to conflict with human perception occasionally. In this paper, we aim to explore the extent of FVD's bias toward per-frame quality over temporal realism and identify its sources. We first quantify the FVD's sensitivity to the temporal axis by decoupling the frame and motion quality and find that the FVD increases only slightly with large temporal corruption. We then analyze the generated videos and show that via careful sampling from a large set of generated videos that do not contain motions, one can drastically decrease FVD without improving the temporal quality. Both studies suggest FVD's bias towards the quality of individual frames. We further observe that the bias can be attributed to the features extracted from a supervised video classifier trained on the content-biased dataset. We show that FVD with features extracted from the recent large-scale self-supervised video models is less biased toward image quality. Finally, we revisit a few real-world examples to validate our hypothesis.",
        "page": "http://arxiv.org/abs/2404.12391",
        "pdf": "http://arxiv.org/pdf/2404.12391.pdf"
    },
    {
        "title": "Sheared Backpropagation for Finetuning Foundation Models",
        "author": "Zhiyuan Yu, Li Shen, Liang Ding, Xinmei Tian, Yixin Chen, Dacheng Tao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Hyperbolic Learning with Synthetic Captions for Open-World Detection",
        "author": "Fanjie Kong, Yanbei Chen, Jiarui Cai, Davide Modolo",
        "abstract": "Open-world detection poses significant challenges, as it requires the detection of any object using either object class labels or free-form texts. Existing related works often use large-scale manual annotated caption datasets for training, which are extremely expensive to collect. Instead, we propose to transfer knowledge from vision-language models (VLMs) to enrich the open-vocabulary descriptions automatically. Specifically, we bootstrap dense synthetic captions using pre-trained VLMs to provide rich descriptions on different regions in images, and incorporate these captions to train a novel detector that generalizes to novel concepts. To mitigate the noise caused by hallucination in synthetic captions, we also propose a novel hyperbolic vision-language learning approach to impose a hierarchy between visual and caption embeddings. We call our detector ``HyperLearner''. We conduct extensive experiments on a wide variety of open-world detection benchmarks (COCO, LVIS, Object Detection in the Wild, RefCOCO) and our results show that our model consistently outperforms existing state-of-the-art methods, such as GLIP, GLIPv2 and Grounding DINO, when using the same backbone.",
        "page": "http://arxiv.org/abs/2404.05016",
        "pdf": "http://arxiv.org/pdf/2404.05016.pdf"
    },
    {
        "title": "High Fidelity Person-centric Subject-to-Image Synthesis",
        "author": "Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Fixed Point Diffusion Models",
        "author": "Luke Melas-Kyriazi, Xingjian Bai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Contextual Augmented Global Contrast for Multimodal Intent Recognition",
        "author": "Kaili Sun, Zhiwen Xie, Mang Ye, Huyin Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System",
        "author": "Yunfei Fan, Tianyu Zhao, Guidong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MACE: Mass Concept Erasure in Diffusion Models",
        "author": "Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, Adams Wai-Kin Kong",
        "abstract": "The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of mass concept erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE integrates multiple LoRAs without mutual interference. We conduct extensive evaluations of MACE against prior methods across four different tasks: object erasure, celebrity erasure, explicit content erasure, and artistic style erasure. Our results reveal that MACE surpasses prior methods in all evaluated tasks. Code is available at https://github.com/Shilin-LU/MACE.",
        "page": "http://arxiv.org/abs/2403.06135",
        "pdf": "http://arxiv.org/pdf/2403.06135.pdf"
    },
    {
        "title": "XFeat: Accelerated Features for Lightweight Image Matching",
        "author": "Guilherme Potje, Felipe Cadar, Andr\u00e9 Araujo, Renato Martins, Erickson R. Nascimento",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation",
        "author": "WEIMING ZHANG, Yexin Liu, Xu Zheng, Lin Wang",
        "abstract": "This paper tackles a novel yet challenging problem: how to transfer knowledge from the emerging Segment Anything Model (SAM) -- which reveals impressive zero-shot instance segmentation capacity -- to learn a compact panoramic semantic segmentation model, i.e., student, without requiring any labeled data. This poses considerable challenges due to SAM's inability to provide semantic labels and the large capacity gap between SAM and the student. To this end, we propose a novel framework, called GoodSAM, that introduces a teacher assistant (TA) to provide semantic information, integrated with SAM to generate ensemble logits to achieve knowledge transfer. Specifically, we propose a Distortion-Aware Rectification (DAR) module that first addresses the distortion problem of panoramic images by imposing prediction-level consistency and boundary enhancement. This subtly enhances TA's prediction capacity on panoramic images. DAR then incorporates a cross-task complementary fusion block to adaptively merge the predictions of SAM and TA to obtain more reliable ensemble logits. Moreover, we introduce a Multi-level Knowledge Adaptation (MKA) module to efficiently transfer the multi-level feature knowledge from TA and ensemble logits to learn a compact student model. Extensive experiments on two benchmarks show that our GoodSAM achieves a remarkable +3.75\\% mIoU improvement over the state-of-the-art (SOTA) domain adaptation methods. Also, our most lightweight model achieves comparable performance to the SOTA methods with only 3.7M parameters.",
        "page": "http://arxiv.org/abs/2403.16370",
        "pdf": "http://arxiv.org/pdf/2403.16370.pdf"
    },
    {
        "title": "VideoBooth: Diffusion-based Video Generation with Image Prompts",
        "author": "Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, Ziwei Liu",
        "abstract": "Text-driven video generation witnesses rapid progress. However, merely using text prompts is not enough to depict the desired subject appearance that accurately aligns with users' intents, especially for customized content creation. In this paper, we study the task of video generation with image prompts, which provide more accurate and direct content control beyond the text prompts. Specifically, we propose a feed-forward framework VideoBooth, with two dedicated designs: 1) We propose to embed image prompts in a coarse-to-fine manner. Coarse visual embeddings from image encoder provide high-level encodings of image prompts, while fine visual embeddings from the proposed attention injection module provide multi-scale and detailed encoding of image prompts. These two complementary embeddings can faithfully capture the desired appearance. 2) In the attention injection module at fine level, multi-scale image prompts are fed into different cross-frame attention layers as additional keys and values. This extra spatial information refines the details in the first frame and then it is propagated to the remaining frames, which maintains temporal consistency. Extensive experiments demonstrate that VideoBooth achieves state-of-the-art performance in generating customized high-quality videos with subjects specified in image prompts. Notably, VideoBooth is a generalizable framework where a single model works for a wide range of image prompts with feed-forward pass.",
        "page": "http://arxiv.org/abs/2312.00777",
        "pdf": "http://arxiv.org/pdf/2312.00777.pdf"
    },
    {
        "title": "CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization",
        "author": "Yao Ni, Piotr Koniusz",
        "abstract": "Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution few-shot image datasets. Code: https://github.com/MaxwellYaoNi/CHAIN",
        "page": "http://arxiv.org/abs/2404.00521",
        "pdf": "http://arxiv.org/pdf/2404.00521.pdf"
    },
    {
        "title": "SaCo Loss: Sample-wise Affinity Consistency for Vision-Language Pre-training",
        "author": "WU Sitong, Haoru Tan, Zhuotao Tian, Yukang Chen, Xiaojuan Qi, Jiaya Jia",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "StrokeFaceNeRF: Stroke-based Facial Appearance Editing in Neural Radiance Field",
        "author": "Xiao-juan Li, Dingxi Zhang, Shu-Yu Chen, Feng-Lin Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
        "author": "Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel",
        "abstract": "Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$ improved learning efficiency when compared with non-reinforced CLIP training. Code and models are available at https://github.com/apple/ml-mobileclip .",
        "page": "http://arxiv.org/abs/2311.17049",
        "pdf": "http://arxiv.org/pdf/2311.17049.pdf"
    },
    {
        "title": "WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion",
        "author": "Soyong Shin, Juyong Kim, Eni Halilaj, Michael J. Black",
        "abstract": "The estimation of 3D human motion from video has progressed rapidly but current methods still have several key limitations. First, most methods estimate the human in camera coordinates. Second, prior work on estimating humans in global coordinates often assumes a flat ground plane and produces foot sliding. Third, the most accurate methods rely on computationally expensive optimization pipelines, limiting their use to offline applications. Finally, existing video-based methods are surprisingly less accurate than single-frame methods. We address these limitations with WHAM (World-grounded Humans with Accurate Motion), which accurately and efficiently reconstructs 3D human motion in a global coordinate system from video. WHAM learns to lift 2D keypoint sequences to 3D using motion capture data and fuses this with video features, integrating motion context and visual information. WHAM exploits camera angular velocity estimated from a SLAM method together with human motion to estimate the body's global trajectory. We combine this with a contact-aware trajectory refinement method that lets WHAM capture human motion in diverse conditions, such as climbing stairs. WHAM outperforms all existing 3D human motion recovery methods across multiple in-the-wild benchmarks. Code will be available for research purposes at http://wham.is.tue.mpg.de/",
        "page": "http://arxiv.org/abs/2312.07531",
        "pdf": "http://arxiv.org/pdf/2312.07531.pdf"
    },
    {
        "title": "YOLO-World: Real-Time Open-Vocabulary Object Detection",
        "author": "Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment",
        "author": "Alireza Ganjdanesh, Shangqian Gao, Heng Huang",
        "abstract": "Structural model pruning is a prominent approach used for reducing the computational cost of Convolutional Neural Networks (CNNs) before their deployment on resource-constrained devices. Yet, the majority of proposed ideas require a pretrained model before pruning, which is costly to secure. In this paper, we propose a novel structural pruning approach to jointly learn the weights and structurally prune architectures of CNN models. The core element of our method is a Reinforcement Learning (RL) agent whose actions determine the pruning ratios of the CNN model's layers, and the resulting model's accuracy serves as its reward. We conduct the joint training and pruning by iteratively training the model's weights and the agent's policy, and we regularize the model's weights to align with the selected structure by the agent. The evolving model's weights result in a dynamic reward function for the agent, which prevents using prominent episodic RL methods with stationary environment assumption for our purpose. We address this challenge by designing a mechanism to model the complex changing dynamics of the reward function and provide a representation of it to the RL agent. To do so, we take a learnable embedding for each training epoch and employ a recurrent model to calculate a representation of the changing environment. We train the recurrent model and embeddings using a decoder model to reconstruct observed rewards. Such a design empowers our agent to effectively leverage episodic observations along with the environment representations to learn a proper policy to determine performant sub-networks of the CNN model. Our extensive experiments on CIFAR-10 and ImageNet using ResNets and MobileNets demonstrate the effectiveness of our method.",
        "page": "http://arxiv.org/abs/2403.19490",
        "pdf": "http://arxiv.org/pdf/2403.19490.pdf"
    },
    {
        "title": "Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion",
        "author": "Yuanxun Lu, Jingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao",
        "abstract": "Recent advances in generative AI have unveiled significant potential for the creation of 3D content. However, current methods either apply a pre-trained 2D diffusion model with the time-consuming score distillation sampling (SDS), or a direct 3D diffusion model trained on limited 3D data losing generation diversity. In this work, we approach the problem by employing a multi-view 2.5D diffusion fine-tuned from a pre-trained 2D diffusion model. The multi-view 2.5D diffusion directly models the structural distribution of 3D data, while still maintaining the strong generalization ability of the original 2D diffusion model, filling the gap between 2D diffusion-based and direct 3D diffusion-based methods for 3D content generation. During inference, multi-view normal maps are generated using the 2.5D diffusion, and a novel differentiable rasterization scheme is introduced to fuse the almost consistent multi-view normal maps into a consistent 3D model. We further design a normal-conditioned multi-view image generation module for fast appearance generation given the 3D geometry. Our method is a one-pass diffusion process and does not require any SDS optimization as post-processing. We demonstrate through extensive experiments that, our direct 2.5D generation with the specially-designed fusion scheme can achieve diverse, mode-seeking-free, and high-fidelity 3D content generation in only 10 seconds. Project page: https://nju-3dv.github.io/projects/direct25.",
        "page": "http://arxiv.org/abs/2311.15980",
        "pdf": "http://arxiv.org/pdf/2311.15980.pdf"
    },
    {
        "title": "B\u00e9zier Everywhere All at Once: Learning Drivable Lanes as B\u00e9zier Graphs",
        "author": "Hugh Blayney, Hanlin Tian, Hamish Scott, Nils Goldbeck, Chess Stetson, Panagiotis Angeloudis",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes",
        "author": "Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang",
        "abstract": "3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.",
        "page": "http://arxiv.org/abs/2404.01543",
        "pdf": "http://arxiv.org/pdf/2404.01543.pdf"
    },
    {
        "title": "FaceCom: Towards High-fidelity 3D Facial Shape Completion via Optimization and Inpainting Guidance",
        "author": "Yinglong Li, Hongyu Wu, Wang, Qingzhao Qin, yijiao zhao, Yong Wang, Aimin Hao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "RankMatch: Exploring the Better Consistency Regularization for Semi-supervised Semantic Segmentation",
        "author": "Huayu Mai, Rui Sun, Tianzhu Zhang, Feng Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Revisiting Adversarial Training under Long-Tailed Distributions",
        "author": "Xinli Yue, Ningping Mou, Qian Wang, Lingchen Zhao",
        "abstract": "Deep neural networks are vulnerable to adversarial attacks, often leading to erroneous outputs. Adversarial training has been recognized as one of the most effective methods to counter such attacks. However, existing adversarial training techniques have predominantly been tested on balanced datasets, whereas real-world data often exhibit a long-tailed distribution, casting doubt on the efficacy of these methods in practical scenarios. In this paper, we delve into adversarial training under long-tailed distributions. Through an analysis of the previous work \"RoBal\", we discover that utilizing Balanced Softmax Loss alone can achieve performance comparable to the complete RoBal approach while significantly reducing training overheads. Additionally, we reveal that, similar to uniform distributions, adversarial training under long-tailed distributions also suffers from robust overfitting. To address this, we explore data augmentation as a solution and unexpectedly discover that, unlike results obtained with balanced data, data augmentation not only effectively alleviates robust overfitting but also significantly improves robustness. We further investigate the reasons behind the improvement of robustness through data augmentation and identify that it is attributable to the increased diversity of examples. Extensive experiments further corroborate that data augmentation alone can significantly improve robustness. Finally, building on these findings, we demonstrate that compared to RoBal, the combination of BSL and data augmentation leads to a +6.66% improvement in model robustness under AutoAttack on CIFAR-10-LT. Our code is available at https://github.com/NISPLab/AT-BSL .",
        "page": "http://arxiv.org/abs/2403.10073",
        "pdf": "http://arxiv.org/pdf/2403.10073.pdf"
    },
    {
        "title": "From SAM to CAMs: Exploring Segment Anything Model for Weakly Supervised Semantic Segmentation",
        "author": "Hyeokjun Kweon, Kuk-Jin Yoon",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "VINECS: Video-based Neural Character Skinning",
        "author": "Zhouyingcheng Liao, Vladislav Golyanik, Marc Habermann, Christian Theobalt",
        "abstract": "Rigging and skinning clothed human avatars is a challenging task and traditionally requires a lot of manual work and expertise. Recent methods addressing it either generalize across different characters or focus on capturing the dynamics of a single character observed under different pose configurations. However, the former methods typically predict solely static skinning weights, which perform poorly for highly articulated poses, and the latter ones either require dense 3D character scans in different poses or cannot generate an explicit mesh with vertex correspondence over time. To address these challenges, we propose a fully automated approach for creating a fully rigged character with pose-dependent skinning weights, which can be solely learned from multi-view video. Therefore, we first acquire a rigged template, which is then statically skinned. Next, a coordinate-based MLP learns a skinning weights field parameterized over the position in a canonical pose space and the respective pose. Moreover, we introduce our pose- and view-dependent appearance field allowing us to differentiably render and supervise the posed mesh using multi-view imagery. We show that our approach outperforms state-of-the-art while not relying on dense 4D scans.",
        "page": "http://arxiv.org/abs/2307.00842",
        "pdf": "http://arxiv.org/pdf/2307.00842.pdf"
    },
    {
        "title": "Plug and Play Active Learning for Object Detection",
        "author": "Chenhongyi Yang, Lichao Huang, Elliot Crowley",
        "abstract": "Annotating datasets for object detection is an expensive and time-consuming endeavor. To minimize this burden, active learning (AL) techniques are employed to select the most informative samples for annotation within a constrained \"annotation budget\". Traditional AL strategies typically rely on model uncertainty or sample diversity for query sampling, while more advanced methods have focused on developing AL-specific object detector architectures to enhance performance. However, these specialized approaches are not readily adaptable to different object detectors due to the significant engineering effort required for integration. To overcome this challenge, we introduce Plug and Play Active Learning (PPAL), a simple and effective AL strategy for object detection. PPAL is a two-stage method comprising uncertainty-based and diversity-based sampling phases. In the first stage, our Difficulty Calibrated Uncertainty Sampling leverage a category-wise difficulty coefficient that combines both classification and localisation difficulties to re-weight instance uncertainties, from which we sample a candidate pool for the subsequent diversity-based sampling. In the second stage, we propose Category Conditioned Matching Similarity to better compute the similarities of multi-instance images as ensembles of their instance similarities, which is used by the k-Means++ algorithm to sample the final AL queries. PPAL makes no change to model architectures or detector training pipelines; hence it can be easily generalized to different object detectors. We benchmark PPAL on the MS-COCO and Pascal VOC datasets using different detector architectures and show that our method outperforms prior work by a large margin. Code is available at https://github.com/ChenhongyiYang/PPAL",
        "page": "http://arxiv.org/abs/2211.11612",
        "pdf": "http://arxiv.org/pdf/2211.11612.pdf"
    },
    {
        "title": "Learning Structure-from-Motion with Graph Attention Networks",
        "author": "Lucas Brynte, Jos\u00e9 Pedro Iglesias, Carl Olsson, Fredrik Kahl",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation",
        "author": "Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Insights from the Use of Previously Unseen Neural Architecture Search Datasets",
        "author": "Rob Geada, David Towers, Matthew Forshaw, Amir Atapour-Abarghouei, Stephen McGough",
        "abstract": "The boundless possibility of neural networks which can be used to solve a problem -- each with different performance -- leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning methods as well as the best results from challenge participants.",
        "page": "http://arxiv.org/abs/2404.02189",
        "pdf": "http://arxiv.org/pdf/2404.02189.pdf"
    },
    {
        "title": "$L_0$-Sampler: An $L_{0}$ Model Guided Volume Sampling for NeRF",
        "author": "Liangchen Li, Juyong Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Priors",
        "author": "Zhipeng Hu, Minda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li, Zeng Zhao, Changjie Fan, Xiaowei Zhou, Xin Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features",
        "author": "Niladri Shekhar Dutt, Sanjeev Muralikrishnan, Niloy J. Mitra",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SGC-Occ: Semantic-Geometry Consistent 3D Occupancy Prediction for Autonomous Driving",
        "author": "Zhiwen Yang, Xiangteng He, Yuxin Peng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "De-Diffusion Makes Text a Strong Cross-Modal Interface",
        "author": "Chen Wei, Chenxi Liu, Siyuan Qiao, Zhishuai Zhang, Alan L. Yuille, Jiahui Yu",
        "abstract": "We demonstrate text as a strong cross-modal interface. Rather than relying on deep embeddings to connect image and language as the interface representation, our approach represents an image as text, from which we enjoy the interpretability and flexibility inherent to natural language. We employ an autoencoder that uses a pre-trained text-to-image diffusion model for decoding. The encoder is trained to transform an input image into text, which is then fed into the fixed text-to-image diffusion decoder to reconstruct the original input -- a process we term De-Diffusion. Experiments validate both the precision and comprehensiveness of De-Diffusion text representing images, such that it can be readily ingested by off-the-shelf text-to-image tools and LLMs for diverse multi-modal tasks. For example, a single De-Diffusion model can generalize to provide transferable prompts for different text-to-image tools, and also achieves a new state of the art on open-ended vision-language tasks by simply prompting large language models with few-shot examples.",
        "page": "http://arxiv.org/abs/2311.00618",
        "pdf": "http://arxiv.org/pdf/2311.00618.pdf"
    },
    {
        "title": "Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis",
        "author": "Yanzuo Lu, Manlin Zhang, Jinhua Ma, Xiaohua Xie, Jianhuang Lai",
        "abstract": "Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of a pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at https://github.com/YanzuoLu/CFLD.",
        "page": "http://arxiv.org/abs/2402.18078",
        "pdf": "http://arxiv.org/pdf/2402.18078.pdf"
    },
    {
        "title": "Unsupervised Occupancy Learning from Sparse Point Cloud",
        "author": "Amine Ouasfi, Adnane Boukhayma",
        "abstract": "Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs. We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud. We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while maximizing its entropy at the input point cloud. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve implicit shape inference with respect to baselines and the state-of-the-art using synthetic and real data.",
        "page": "http://arxiv.org/abs/2404.02759",
        "pdf": "http://arxiv.org/pdf/2404.02759.pdf"
    },
    {
        "title": "Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM",
        "author": "Tongyan Hua, Lin Wang",
        "abstract": "Implicit neural representation (INR), in combination with geometric rendering, has recently been employed in real-time dense RGB-D SLAM. Despite active research endeavors being made, there lacks a unified protocol for fair evaluation, impeding the evolution of this area. In this work, we establish, to our knowledge, the first open-source benchmark framework to evaluate the performance of a wide spectrum of commonly used INRs and rendering functions for mapping and localization. The goal of our benchmark is to 1) gain an intuition of how different INRs and rendering functions impact mapping and localization and 2) establish a unified evaluation protocol w.r.t. the design choices that may impact the mapping and localization. With the framework, we conduct a large suite of experiments, offering various insights in choosing the INRs and geometric rendering functions: for example, the dense feature grid outperforms other INRs (e.g. tri-plane and hash grid), even when geometric and color features are jointly encoded for memory efficiency. To extend the findings into the practical scenario, a hybrid encoding strategy is proposed to bring the best of the accuracy and completion from the grid-based and decomposition-based INRs. We further propose explicit hybrid encoding for high-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts the premise on robustness and computation efficiency.",
        "page": "http://arxiv.org/abs/2403.19473",
        "pdf": "http://arxiv.org/pdf/2403.19473.pdf"
    },
    {
        "title": "GLOW: Global Layout Aware Attacks on Object Detection",
        "author": "Jun Bao, Buyu Liu, Kui Ren, Jun Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DeepCache: Accelerating Diffusion Models for Free",
        "author": "Xinyin Ma, Gongfan Fang, Xinchao Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "HPNet: Dynamic Trajectory Forecasting with Historical Prediction Attention",
        "author": "Xiaolong Tang, Meina Kan, Shiguang Shan, Zhilong Ji, Jinfeng Bai, Xilin Chen",
        "abstract": "Predicting the trajectories of road agents is essential for autonomous driving systems. The recent mainstream methods follow a static paradigm, which predicts the future trajectory by using a fixed duration of historical frames. These methods make the predictions independently even at adjacent time steps, which leads to potential instability and temporal inconsistency. As successive time steps have largely overlapping historical frames, their forecasting should have intrinsic correlation, such as overlapping predicted trajectories should be consistent, or be different but share the same motion goal depending on the road situation. Motivated by this, in this work, we introduce HPNet, a novel dynamic trajectory forecasting method. Aiming for stable and accurate trajectory forecasting, our method leverages not only historical frames including maps and agent states, but also historical predictions. Specifically, we newly design a Historical Prediction Attention module to automatically encode the dynamic relationship between successive predictions. Besides, it also extends the attention range beyond the currently visible window benefitting from the use of historical predictions. The proposed Historical Prediction Attention together with the Agent Attention and Mode Attention is further formulated as the Triple Factorized Attention module, serving as the core design of HPNet.Experiments on the Argoverse and INTERACTION datasets show that HPNet achieves state-of-the-art performance, and generates accurate and stable future trajectories. Our code are available at https://github.com/XiaolongTang23/HPNet.",
        "page": "http://arxiv.org/abs/2404.06351",
        "pdf": "http://arxiv.org/pdf/2404.06351.pdf"
    },
    {
        "title": "Neural Underwater Scene Representation",
        "author": "Yunkai Tang, Chengxuan Zhu, Renjie Wan, Chao Xu, Boxin Shi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PolarMatte: Fully Computational Ground-Truth-Quality Alpha Matte Extraction for Images and Video using Polarized Screen Matting",
        "author": "Kenji Enomoto, TJ Rhodes, Brian Price, Gavin Miller",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Traceable Federated Continual Learning",
        "author": "Qiang Wang, Bingyan Liu, Yawen Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CrossMAE: Cross Modality Masked Autoencoders For Region-Aware Audio-Visual Pre-Training",
        "author": "Yuxin Guo, Siyang Sun, Shuailei Ma, Kecheng Zheng, Xiaoyi Bao, Shijie Ma, Wei Zou, Yun Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models",
        "author": "Gihyun Kwon, Simon Jenni, Ding Li, Joon-Young Lee, Jong Chul Ye, Fabian Caba Heilbron",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Vista-LLaMA: Reliable Video Teller via Equal Distance to Visual Tokens",
        "author": "Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, Yi Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Real-World Mobile Image Denoising Dataset with Efficient Baselines",
        "author": "Roman Flepp, Andrey Ignatov, Radu Timofte, Luc Van Gool",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving",
        "author": "Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, Marco Pavone",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SRTube: Video-Language Pre-Training with Action-Centric Video Tube Features and Semantic Role Labeling",
        "author": "Juhee Lee, Jewon Kang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PoseIRM: Enhance 3D Human Pose Estimation on Unseen  Camera Settings via Invariant Risk Minimization",
        "author": "Yanlu Cai, Weizhong Zhang, Yuan Wu, Cheng Jin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UniHuman: A Unified Model For Editing Human Images in the Wild",
        "author": "Nannan Li, Qing Liu, Krishna Kumar Singh, Yilin Wang, Jianming Zhang, Bryan A. Plummer, Zhe Lin",
        "abstract": "Human image editing includes tasks like changing a person's pose, their clothing, or editing the image according to a text prompt. However, prior work often tackles these tasks separately, overlooking the benefit of mutual reinforcement from learning them jointly. In this paper, we propose UniHuman, a unified model that addresses multiple facets of human image editing in real-world settings. To enhance the model's generation quality and generalization capacity, we leverage guidance from human visual encoders and introduce a lightweight pose-warping module that can exploit different pose representations, accommodating unseen textures and patterns. Furthermore, to bridge the disparity between existing human editing benchmarks with real-world data, we curated 400K high-quality human image-text pairs for training and collected 2K human images for out-of-domain testing, both encompassing diverse clothing styles, backgrounds, and age groups. Experiments on both in-domain and out-of-domain test sets demonstrate that UniHuman outperforms task-specific models by a significant margin. In user studies, UniHuman is preferred by the users in an average of 77% of cases. Our project is available at https://github.com/NannanLi999/UniHuman.",
        "page": "http://arxiv.org/abs/2312.14985",
        "pdf": "http://arxiv.org/pdf/2312.14985.pdf"
    },
    {
        "title": "Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs",
        "author": "shiyu xuan, Qingpei Guo, Ming Yang, Shiliang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Point2CAD: Reverse Engineering CAD Models from 3D Point Clouds",
        "author": "Yujia Liu, Anton Obukhov, Jan D. Wegner, Konrad Schindler",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Active Object Detection with Knowledge Aggregation and Distillation from Large Models",
        "author": "Dejie Yang, Yang Liu",
        "abstract": "Accurately detecting active objects undergoing state changes is essential for comprehending human interactions and facilitating decision-making. The existing methods for active object detection (AOD) primarily rely on visual appearance of the objects within input, such as changes in size, shape and relationship with hands. However, these visual changes can be subtle, posing challenges, particularly in scenarios with multiple distracting no-change instances of the same category. We observe that the state changes are often the result of an interaction being performed upon the object, thus propose to use informed priors about object related plausible interactions (including semantics and visual appearance) to provide more reliable cues for AOD. Specifically, we propose a knowledge aggregation procedure to integrate the aforementioned informed priors into oracle queries within the teacher decoder, offering more object affordance commonsense to locate the active object. To streamline the inference process and reduce extra knowledge inputs, we propose a knowledge distillation approach that encourages the student decoder to mimic the detection capabilities of the teacher decoder using the oracle query by replicating its predictions and attention. Our proposed framework achieves state-of-the-art performance on four datasets, namely Ego4D, Epic-Kitchens, MECCANO, and 100DOH, which demonstrates the effectiveness of our approach in improving AOD.",
        "page": "http://arxiv.org/abs/2405.12509",
        "pdf": "http://arxiv.org/pdf/2405.12509.pdf"
    },
    {
        "title": "ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations",
        "author": "Rwiddhi Chakraborty, Adrian de Sena Sletten, Michael C. Kampffmeyer",
        "abstract": "Group robustness strategies aim to mitigate learned biases in deep learning models that arise from spurious correlations present in their training datasets. However, most existing methods rely on the access to the label distribution of the groups, which is time-consuming and expensive to obtain. As a result, unsupervised group robustness strategies are sought. Based on the insight that a trained model's classification strategies can be inferred accurately based on explainability heatmaps, we introduce ExMap, an unsupervised two stage mechanism designed to enhance group robustness in traditional classifiers. ExMap utilizes a clustering module to infer pseudo-labels based on a model's explainability heatmaps, which are then used during training in lieu of actual labels. Our empirical studies validate the efficacy of ExMap - We demonstrate that it bridges the performance gap with its supervised counterparts and outperforms existing partially supervised and unsupervised methods. Additionally, ExMap can be seamlessly integrated with existing group robustness learning strategies. Finally, we demonstrate its potential in tackling the emerging issue of multiple shortcut mitigation\\footnote{Code available at \\url{https://github.com/rwchakra/exmap}}.",
        "page": "http://arxiv.org/abs/2403.13870",
        "pdf": "http://arxiv.org/pdf/2403.13870.pdf"
    },
    {
        "title": "FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer",
        "author": "Dongyeong Hwang, Hyunju Kim, Sunwoo Kim, Kijung Shin",
        "abstract": "The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto speech recognition models. Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.",
        "page": "http://arxiv.org/abs/2403.12821",
        "pdf": "http://arxiv.org/pdf/2403.12821.pdf"
    },
    {
        "title": "Mip-Splatting: Alias-free 3D Gaussian Splatting",
        "author": "Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Text2QR:  Harmonizing Aesthetic Customization and Scanning Robustness for Text-Guided QR Code Generation",
        "author": "Guangyang Wu, Xiaohong Liu, Jun Jia, Xuehao Cui, Guangtao Zhai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "UV-IDM: Identity-Conditioned Latent Diffusion Model for Face UV-Texture Generation",
        "author": "Hong Li, Yutang Feng, Song Xue, Xuhui Liu, Boyu Liu, Bohan Zeng, Shanglin Li, Jianzhuang Liu, Shumin Han, Baochang Zhang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",
        "author": "Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, Tao Chen",
        "abstract": "Recent advances in Large Multimodal Models (LMM) have made it possible for various applications in human-machine interactions. However, developing LMMs that can comprehend, reason, and plan in complex and diverse 3D environments remains a challenging topic, especially considering the demand for understanding permutation-invariant point cloud 3D representations of the 3D scene. Existing works seek help from multi-view images, and project 2D features to 3D space as 3D scene representations. This, however, leads to huge computational overhead and performance degradation. In this paper, we present LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and respond to both textual-instructions and visual-prompts. This help LMMs better comprehend human interactions and further help to remove the ambiguities in cluttered 3D scenes. Experiments show that LL3DA achieves remarkable results, and surpasses various 3D vision-language models on both 3D Dense Captioning and 3D Question Answering.",
        "page": "http://arxiv.org/abs/2311.18651",
        "pdf": "http://arxiv.org/pdf/2311.18651.pdf"
    },
    {
        "title": "MMM: Generative Masked Motion Model",
        "author": "Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, Chen Chen",
        "abstract": "Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fine-grained text descriptions, therefore simultaneously achieving high-fidelity and high-speed motion generation. In addition, MMM has innate motion editability. By simply placing mask tokens in the place that needs editing, MMM automatically fills the gaps while guaranteeing smooth transitions between editing and non-editing parts. Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM surpasses current leading methods in generating high-quality motion (evidenced by superior FID scores of 0.08 and 0.429), while offering advanced editing features such as body-part modification, motion in-betweening, and the synthesis of long motion sequences. In addition, MMM is two orders of magnitude faster on a single mid-range GPU than editable motion diffusion models. Our project page is available at \\url{https://exitudio.github.io/MMM-page}.",
        "page": "http://arxiv.org/abs/2312.03596",
        "pdf": "http://arxiv.org/pdf/2312.03596.pdf"
    },
    {
        "title": "Adaptive Hyper-graph Aggregation for Modality-Agnostic Federated Learning",
        "author": "Fan Qi, Shuai Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Wonder3D: Single Image to 3D using Cross-Domain Diffusion",
        "author": "Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Honeybee: Locality-enhanced Projector for Multimodal LLM",
        "author": "Junbum Cha, Woo-Young Kang, Jonghwan Mun, Byungseok Roh",
        "abstract": "In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, our proposed MLLM, Honeybee, remarkably outperforms previous state-of-the-art methods across various benchmarks, including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly higher efficiency. Code and models are available at https://github.com/kakaobrain/honeybee.",
        "page": "http://arxiv.org/abs/2312.06742",
        "pdf": "http://arxiv.org/pdf/2312.06742.pdf"
    },
    {
        "title": "Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement",
        "author": "Zaid Khan, Vijay Kumar BG, Samuel Schulter, Yun Fu, Manmohan Chandraker",
        "abstract": "Visual program synthesis is a promising approach to exploit the reasoning abilities of large language models for compositional computer vision tasks. Previous work has used few-shot prompting with frozen LLMs to synthesize visual programs. Training an LLM to write better visual programs is an attractive prospect, but it is unclear how to accomplish this. No dataset of visual programs for training exists, and acquisition of a visual program dataset cannot be easily crowdsourced due to the need for expert annotators. To get around the lack of direct supervision, we explore improving the program synthesis abilities of an LLM using feedback from interactive experience. We propose a method where we exploit existing annotations for a vision-language task to improvise a coarse reward signal for that task, treat the LLM as a policy, and apply reinforced self-training to improve the visual program synthesis ability of the LLM for that task. We describe a series of experiments on object detection, compositional visual question answering, and image-text retrieval, and show that in each case, the self-trained LLM outperforms or performs on par with few-shot frozen LLMs that are an order of magnitude larger. Website: https://zaidkhan.me/ViReP",
        "page": "http://arxiv.org/abs/2404.04627",
        "pdf": "http://arxiv.org/pdf/2404.04627.pdf"
    },
    {
        "title": "MoMask: Generative Masked Modeling of 3D Human Motions",
        "author": "chuan guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, Li Cheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences",
        "author": "Seungwook Kim, Kejie Li, Xueqing Deng, Yichun Shi, Minsu Cho, Peng Wang",
        "abstract": "Leveraging multi-view diffusion models as priors for 3D optimization have alleviated the problem of 3D consistency, e.g., the Janus face problem or the content drift problem, in zero-shot text-to-3D models. However, the 3D geometric fidelity of the output remains an unresolved issue; albeit the rendered 2D views are realistic, the underlying geometry may contain errors such as unreasonable concavities. In this work, we propose CorrespondentDream, an effective method to leverage annotation-free, cross-view correspondences yielded from the diffusion U-Net to provide additional 3D prior to the NeRF optimization process. We find that these correspondences are strongly consistent with human perception, and by adopting it in our loss design, we are able to produce NeRF models with geometries that are more coherent with common sense, e.g., more smoothed object surface, yielding higher 3D fidelity. We demonstrate the efficacy of our approach through various comparative qualitative results and a solid user study.",
        "page": "http://arxiv.org/abs/2404.10603",
        "pdf": "http://arxiv.org/pdf/2404.10603.pdf"
    },
    {
        "title": "BigGait: Learning Gait Representation You Want by Large Vision Models",
        "author": "Dingqiang Ye, Chao Fan, Jingzhe Ma, Xiaoming Liu, Shiqi Yu",
        "abstract": "Gait recognition stands as one of the most pivotal remote identification technologies and progressively expands across research and industry communities. However, existing gait recognition methods heavily rely on task-specific upstream driven by supervised learning to provide explicit gait representations like silhouette sequences, which inevitably introduce expensive annotation costs and potential error accumulation. Escaping from this trend, this work explores effective gait representations based on the all-purpose knowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a simple yet efficient gait framework, termed BigGait. Specifically, the Gait Representation Extractor (GRE) within BigGait draws upon design principles from established gait representations, effectively transforming all-purpose knowledge into implicit gait representations without requiring third-party supervision signals. Experiments on CCPG, CAISA-B* and SUSTech1K indicate that BigGait significantly outperforms the previous methods in both within-domain and cross-domain tasks in most cases, and provides a more practical paradigm for learning the next-generation gait representation. Finally, we delve into prospective challenges and promising directions in LVMs-based gait recognition, aiming to inspire future work in this emerging topic. The source code is available at https://github.com/ShiqiYu/OpenGait.",
        "page": "http://arxiv.org/abs/2402.19122",
        "pdf": "http://arxiv.org/pdf/2402.19122.pdf"
    },
    {
        "title": "Living Scenes: Multi-object Relocalization and Reconstruction in Changing 3D Environments",
        "author": "Liyuan Zhu, Shengyu Huang, Konrad Schindler, Iro Armeni",
        "abstract": "Research into dynamic 3D scene understanding has primarily focused on short-term change tracking from dense observations, while little attention has been paid to long-term changes with sparse observations. We address this gap with MoRE, a novel approach for multi-object relocalization and reconstruction in evolving environments. We view these environments as \"living scenes\" and consider the problem of transforming scans taken at different points in time into a 3D reconstruction of the object instances, whose accuracy and completeness increase over time. At the core of our method lies an SE(3)-equivariant representation in a single encoder-decoder network, trained on synthetic data. This representation enables us to seamlessly tackle instance matching, registration, and reconstruction. We also introduce a joint optimization algorithm that facilitates the accumulation of point clouds originating from the same instance across multiple scans taken at different points in time. We validate our method on synthetic and real-world data and demonstrate state-of-the-art performance in both end-to-end performance and individual subtasks.",
        "page": "http://arxiv.org/abs/2312.09138",
        "pdf": "http://arxiv.org/pdf/2312.09138.pdf"
    },
    {
        "title": "Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition",
        "author": "Kyle Buettner, Sina Malakouti, Xiang Li, Adriana Kovashka",
        "abstract": "Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geographically diverse descriptive knowledge of categories can enhance robustness. For this purpose, we explore the feasibility of probing a large language model for geography-based object knowledge, and we examine the effects of integrating knowledge into zero-shot and learnable soft prompting with CLIP. Within this exploration, we propose geography knowledge regularization to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set. Accuracy gains over prompting baselines on DollarStreet while training only on Europe data are up to +2.8/1.2/1.6 on target data from Africa/Asia/Americas, and +4.6 overall on the hardest classes. Competitive performance is shown vs. few-shot target training, and analysis is provided to direct future study of geographical robustness.",
        "page": "http://arxiv.org/abs/2401.01482",
        "pdf": "http://arxiv.org/pdf/2401.01482.pdf"
    },
    {
        "title": "DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection",
        "author": "Yuhao Sun, Lingyun Yu, Hongtao Xie, Jiaming Li, Yongdong Zhang",
        "abstract": "With the rapid development of face recognition (FR) systems, the privacy of face images on social media is facing severe challenges due to the abuse of unauthorized FR systems. Some studies utilize adversarial attack techniques to defend against malicious FR systems by generating adversarial examples. However, the generated adversarial examples, i.e., the protected face images, tend to suffer from subpar visual quality and low transferability. In this paper, we propose a novel face protection approach, dubbed DiffAM, which leverages the powerful generative ability of diffusion models to generate high-quality protected face images with adversarial makeup transferred from reference images. To be specific, we first introduce a makeup removal module to generate non-makeup images utilizing a fine-tuned diffusion model with guidance of textual prompts in CLIP space. As the inverse process of makeup transfer, makeup removal can make it easier to establish the deterministic relationship between makeup domain and non-makeup domain regardless of elaborate text prompts. Then, with this relationship, a CLIP-based makeup loss along with an ensemble attack strategy is introduced to jointly guide the direction of adversarial makeup domain, achieving the generation of protected face images with natural-looking makeup and high black-box transferability. Extensive experiments demonstrate that DiffAM achieves higher visual quality and attack success rates with a gain of 12.98% under black-box setting compared with the state of the arts. The code will be available at https://github.com/HansSunY/DiffAM.",
        "page": "http://arxiv.org/abs/2405.09882",
        "pdf": "http://arxiv.org/pdf/2405.09882.pdf"
    },
    {
        "title": "DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation",
        "author": "Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang, Yu Qiao, Hongsheng Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Feedback-Guided Autonomous Driving",
        "author": "Jimuyang Zhang, Zanming Huang, Arijit Ray, Eshed Ohn-Bar",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "LTM: Lightweight Textured Mesh Extraction and Refinement of Large Unbounded Scenes for Efficient Storage and Real-time Rendering",
        "author": "Jaehoon Choi, Rajvi Shah, Qinbo Li, Yipeng Wang, Ayush Saraf, Changil Kim, Jia-Bin Huang, Dinesh Manocha, Suhib Alsisan, Johannes Kopf",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Test-Time Linear Out-of-Distribution Detection",
        "author": "Ke Fan, Tong Liu, Xingyu Qiu, Yikai Wang, Lian Huai, Zeyu Shangguan, Shuang Gou, FENGJIAN LIU, Yuqian Fu, Yanwei Fu, Xingqun Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Matching Anything by Segmenting Anything",
        "author": "Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "InstaGen: Enhancing Object Detection by Training on Synthetic Dataset",
        "author": "Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Narrative Action Evaluation with Prompt-Guided Multimodal Interaction",
        "author": "Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang",
        "abstract": "In this paper, we investigate a new problem called narrative action evaluation (NAE). NAE aims to generate professional commentary that evaluates the execution of an action. Unlike traditional tasks such as score-based action quality assessment and video captioning involving superficial sentences, NAE focuses on creating detailed narratives in natural language. These narratives provide intricate descriptions of actions along with objective evaluations. NAE is a more challenging task because it requires both narrative flexibility and evaluation rigor. One existing possible solution is to use multi-task learning, where narrative language and evaluative information are predicted separately. However, this approach results in reduced performance for individual tasks because of variations between tasks and differences in modality between language information and evaluation information. To address this, we propose a prompt-guided multimodal interaction framework. This framework utilizes a pair of transformers to facilitate the interaction between different modalities of information. It also uses prompts to transform the score regression task into a video-text matching task, thus enabling task interactivity. To support further research in this field, we re-annotate the MTL-AQA and FineGym datasets with high-quality and comprehensive action narration. Additionally, we establish benchmarks for NAE. Extensive experiment results prove that our method outperforms separate learning methods and naive multi-task learning methods. Data and code are released at https://github.com/shiyi-zh0408/NAE_CVPR2024.",
        "page": "http://arxiv.org/abs/2404.14471",
        "pdf": "http://arxiv.org/pdf/2404.14471.pdf"
    },
    {
        "title": "Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition",
        "author": "Zihan Wang, Siyang Song, Cheng Luo, Songhe Deng, Weicheng Xie, Linlin Shen",
        "abstract": "Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions. While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition. Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation. Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling). Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition. Our code is publicly available at https://github.com/CVI-SZU/MDHR.",
        "page": "http://arxiv.org/abs/2404.06443",
        "pdf": "http://arxiv.org/pdf/2404.06443.pdf"
    },
    {
        "title": "Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting",
        "author": "Zijie Chen, Lichao Zhang, Fangsheng Weng, Lili Pan, ZHENZHONG Lan",
        "abstract": "Despite significant progress in the field, it is still challenging to create personalized visual representations that align closely with the desires and preferences of individual users. This process requires users to articulate their ideas in words that are both comprehensible to the models and accurately capture their vision, posing difficulties for many users. In this paper, we tackle this challenge by leveraging historical user interactions with the system to enhance user prompts. We propose a novel approach that involves rewriting user prompts based on a newly collected large-scale text-to-image dataset with over 300k prompts from 3115 users. Our rewriting model enhances the expressiveness and alignment of user prompts with their intended visual outputs. Experimental results demonstrate the superiority of our methods over baseline approaches, as evidenced in our new offline evaluation method and online tests. Our code and dataset are available at https://github.com/zzjchen/Tailored-Visions.",
        "page": "http://arxiv.org/abs/2310.08129",
        "pdf": "http://arxiv.org/pdf/2310.08129.pdf"
    },
    {
        "title": "Imagine Before Go: Self-Supervised Generative Map for Object Goal Navigation",
        "author": "Sixian Zhang, Xinyao Yu, Xinhang Song, Xiaohan Wang, Shuqiang Jiang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Multi-view Aggregation Network for Dichotomous Image Segmentation",
        "author": "Qian Yu, Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu",
        "abstract": "Dichotomous Image Segmentation (DIS) has recently emerged towards high-precision object segmentation from high-resolution natural images. When designing an effective DIS model, the main challenge is how to balance the semantic dispersion of high-resolution targets in the small receptive field and the loss of high-precision details in the large receptive field. Existing methods rely on tedious multiple encoder-decoder streams and stages to gradually complete the global localization and local refinement. Human visual system captures regions of interest by observing them from multiple views. Inspired by it, we model DIS as a multi-view object perception problem and provide a parsimonious multi-view aggregation network (MVANet), which unifies the feature fusion of the distant view and close-up view into a single stream with one encoder-decoder structure. With the help of the proposed multi-view complementary localization and refinement modules, our approach established long-range, profound visual interactions across multiple views, allowing the features of the detailed close-up view to focus on highly slender structures.Experiments on the popular DIS-5K dataset show that our MVANet significantly outperforms state-of-the-art methods in both accuracy and speed. The source code and datasets will be publicly available at \\href{https://github.com/qianyu-dlut/MVANet}{MVANet}.",
        "page": "http://arxiv.org/abs/2404.07445",
        "pdf": "http://arxiv.org/pdf/2404.07445.pdf"
    },
    {
        "title": "EVCap: Retrieval-Augmented Image Captioning with External Visual--Name Memory for Open-World Comprehension",
        "author": "Jiaxuan Li, Duc Minh Vo, Akihiro Sugimoto, Hideki Nakayama",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Plug-and-Play Diffusion Distillation",
        "author": "Yi-Ting Hsiao, Siavash Khodadadeh, Kevin Duarte, Wei-An Lin, Hui Qu, Mingi Kwon, Ratheesh Kalarot",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CLIB-FIQA: Face Image Quality Assessment with Confidence Calibration",
        "author": "Fu-Zhao Ou, Fu-Zhao Ou, Chongyi Li, Shiqi Wang, Sam Kwong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding",
        "author": "Zhihao Zhang, Shengcao Cao, Yu-Xiong Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Polos: Multimodal Metric Learning from Human Feedback for Image Captioning",
        "author": "Yuiga Wada, Kanta Kaneda, Daichi Saito, Komei Sugiura",
        "abstract": "Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We constructed the Polaris dataset, which comprises 131K human judgments from 550 evaluators, which is approximately ten times larger than standard datasets. Our approach achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset, thereby demonstrating its effectiveness and robustness.",
        "page": "http://arxiv.org/abs/2402.18091",
        "pdf": "http://arxiv.org/pdf/2402.18091.pdf"
    },
    {
        "title": "FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models",
        "author": "Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos",
        "abstract": "Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be realized. Specifically, we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs, and low caption quality and diversity. Then, we devise effective solutions for addressing both problems, which essentially require training with multiple true positive pairs. Finally, we propose training with sigmoid loss to address such a requirement. We show very large gains over the current state-of-the-art for both image recognition ($\\sim +6\\%$ on average over 11 datasets) and image retrieval ($\\sim +19\\%$ on Flickr30k and $\\sim +15\\%$ on MSCOCO).",
        "page": "http://arxiv.org/abs/2405.10286",
        "pdf": "http://arxiv.org/pdf/2405.10286.pdf"
    },
    {
        "title": "Differentiable Micro-Mesh Construction",
        "author": "Yishun Dou, Zhong Zheng, Qiaoqiao Jin, Rui Shi, Yuhan Li, Bingbing Ni",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Enhancing Vision-Language Pretraining with Rich Supervisions",
        "author": "Yuan Gao, Kunyu Shi, Pengkai Zhu, Edouard Belval, Oren Nuriel, Srikar Appalaraju, Shabnam Ghadar, Zhuowen Tu, Vijay Mahadevan, Stefano Soatto",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "On the Robustness of Large Multimodal Models Against Image Adversarial Attacks",
        "author": "Xuanming Cui, Alejandro Aparcedo, Young Kyun Jang, Ser-Nam Lim",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations",
        "author": "Daan de Geus, Gijs Dubbelman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Enhanced Motion-Text Alignment for Image-to-Video Transfer Learning",
        "author": "Wei Zhang, Chaoqun Wan, Tongliang Liu, Xinmei Tian, Xu Shen, Jieping Ye",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Countering Personalized Text-to-Image Generation with Influence Watermarks",
        "author": "Hanwen Liu, Zhicheng Sun, Yadong Mu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields",
        "author": "Yunsong Wang, Hanlin Chen, Gim Hee Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SNIDA: Unlocking Few-Shot Object Detection with Non-linear Semantic Decoupling Augmentation",
        "author": "Yanjie Wang, Xu Zou, Luxin Yan, Sheng Zhong, Jiahuan Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Automatic Controllable Colorization via Imagination",
        "author": "Xiaoyan Cong, Yue Wu, Qifeng Chen, Chenyang Lei",
        "abstract": "We propose a framework for automatic colorization that allows for iterative editing and modifications. The core of our framework lies in an imagination module: by understanding the content within a grayscale image, we utilize a pre-trained image generation model to generate multiple images that contain the same content. These images serve as references for coloring, mimicking the process of human experts. As the synthesized images can be imperfect or different from the original grayscale image, we propose a Reference Refinement Module to select the optimal reference composition. Unlike most previous end-to-end automatic colorization algorithms, our framework allows for iterative and localized modifications of the colorization results because we explicitly model the coloring samples. Extensive experiments demonstrate the superiority of our framework over existing automatic colorization algorithms in editability and flexibility. Project page: https://xy-cong.github.io/imagine-colorization.",
        "page": "http://arxiv.org/abs/2404.05661",
        "pdf": "http://arxiv.org/pdf/2404.05661.pdf"
    },
    {
        "title": "Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image Diffusion Models",
        "author": "Shweta Mahajan, Tanzila Rahman, Kwang Moo Yi, Leonid Sigal",
        "abstract": "The quality of the prompts provided to text-to-image diffusion models determines how faithful the generated content is to the user's intent, often requiring `prompt engineering'. To harness visual concepts from target images without prompt engineering, current approaches largely rely on embedding inversion by optimizing and then mapping them to pseudo-tokens. However, working with such high-dimensional vector representations is challenging because they lack semantics and interpretability, and only allow simple vector operations when using them. Instead, this work focuses on inverting the diffusion model to obtain interpretable language prompts directly. The challenge of doing this lies in the fact that the resulting optimization problem is fundamentally discrete and the space of prompts is exponentially large; this makes using standard optimization techniques, such as stochastic gradient descent, difficult. To this end, we utilize a delayed projection scheme to optimize for prompts representative of the vocabulary space in the model. Further, we leverage the findings that different timesteps of the diffusion process cater to different levels of detail in an image. The later, noisy, timesteps of the forward diffusion process correspond to the semantic information, and therefore, prompt inversion in this range provides tokens representative of the image semantics. We show that our approach can identify semantically interpretable and meaningful prompts for a target image which can be used to synthesize diverse images with similar content. We further illustrate the application of the optimized prompts in evolutionary image generation and concept removal.",
        "page": "http://arxiv.org/abs/2312.12416",
        "pdf": "http://arxiv.org/pdf/2312.12416.pdf"
    },
    {
        "title": "PoseGPT: Chatting about 3D Human Pose",
        "author": "Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Michael J. Black",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improved Baselines with Visual Instruction Tuning",
        "author": "Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks",
        "author": "Jiaxin Zhang, Dezhi Peng, Chongyu Liu, Peirong Zhang, Lianwen Jin",
        "abstract": "Document image restoration is a crucial aspect of Document AI systems, as the quality of document images significantly influences the overall performance. Prevailing methods address distinct restoration tasks independently, leading to intricate systems and the incapability to harness the potential synergies of multi-task learning. To overcome this challenge, we propose DocRes, a generalist model that unifies five document image restoration tasks including dewarping, deshadowing, appearance enhancement, deblurring, and binarization. To instruct DocRes to perform various restoration tasks, we propose a novel visual prompt approach called Dynamic Task-Specific Prompt (DTSPrompt). The DTSPrompt for different tasks comprises distinct prior features, which are additional characteristics extracted from the input image. Beyond its role as a cue for task-specific execution, DTSPrompt can also serve as supplementary information to enhance the model's performance. Moreover, DTSPrompt is more flexible than prior visual prompt approaches as it can be seamlessly applied and adapted to inputs with high and variable resolutions. Experimental results demonstrate that DocRes achieves competitive or superior performance compared to existing state-of-the-art task-specific models. This underscores the potential of DocRes across a broader spectrum of document image restoration tasks. The source code is publicly available at https://github.com/ZZZHANG-jx/DocRes",
        "page": "http://arxiv.org/abs/2405.04408",
        "pdf": "http://arxiv.org/pdf/2405.04408.pdf"
    },
    {
        "title": "Bilateral Propagation Network for Depth Completion",
        "author": "Jie Tang, Fei-Peng Tian, Boshi An, Jian Li, Ping Tan",
        "abstract": "Depth completion aims to derive a dense depth map from sparse depth measurements with a synchronized color image. Current state-of-the-art (SOTA) methods are predominantly propagation-based, which work as an iterative refinement on the initial estimated dense depth. However, the initial depth estimations mostly result from direct applications of convolutional layers on the sparse depth map. In this paper, we present a Bilateral Propagation Network (BP-Net), that propagates depth at the earliest stage to avoid directly convolving on sparse data. Specifically, our approach propagates the target depth from nearby depth measurements via a non-linear model, whose coefficients are generated through a multi-layer perceptron conditioned on both \\emph{radiometric difference} and \\emph{spatial distance}. By integrating bilateral propagation with multi-modal fusion and depth refinement in a multi-scale framework, our BP-Net demonstrates outstanding performance on both indoor and outdoor scenes. It achieves SOTA on the NYUv2 dataset and ranks 1st on the KITTI depth completion benchmark at the time of submission. Experimental results not only show the effectiveness of bilateral propagation but also emphasize the significance of early-stage propagation in contrast to the refinement stage. Our code and trained models will be available on the project page.",
        "page": "http://arxiv.org/abs/2403.11270",
        "pdf": "http://arxiv.org/pdf/2403.11270.pdf"
    },
    {
        "title": "Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning",
        "author": "Zichen Miao, Jiang Wang, Ze Wang, Ze Wang, Zhengyuan Yang, Lijuan Wang, Qiang Qiu, Zicheng Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Visual Point Cloud Forecasting enables Scalable Autonomous Driving",
        "author": "Zetong Yang, Li Chen, Yanan Sun, Hongyang Li",
        "abstract": "In contrast to extensive studies on general vision, pre-training for scalable visual autonomous driving remains seldom explored. Visual autonomous driving applications require features encompassing semantics, 3D geometry, and temporal information simultaneously for joint perception, prediction, and planning, posing dramatic challenges for pre-training. To resolve this, we bring up a new pre-training task termed as visual point cloud forecasting - predicting future point clouds from historical visual input. The key merit of this task captures the synergic learning of semantics, 3D structures, and temporal dynamics. Hence it shows superiority in various downstream tasks. To cope with this new problem, we present ViDAR, a general model to pre-train downstream visual encoders. It first extracts historical embeddings by the encoder. These representations are then transformed to 3D geometric space via a novel Latent Rendering operator for future point cloud prediction. Experiments show significant gain in downstream tasks, e.g., 3.1% NDS on 3D detection, ~10% error reduction on motion forecasting, and ~15% less collision rate on planning.",
        "page": "http://arxiv.org/abs/2312.17655",
        "pdf": "http://arxiv.org/pdf/2312.17655.pdf"
    },
    {
        "title": "On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving",
        "author": "Kaituo Feng, Changsheng Li, Dongchun Ren, Ye Yuan, Guoren Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models",
        "author": "Yusuf Dalva, Pinar Yanardag",
        "abstract": "Generative models have been very popular in the recent years for their image generation capabilities. GAN-based models are highly regarded for their disentangled latent space, which is a key feature contributing to their success in controlled image editing. On the other hand, diffusion models have emerged as powerful tools for generating high-quality images. However, the latent space of diffusion models is not as thoroughly explored or understood. Existing methods that aim to explore the latent space of diffusion models usually relies on text prompts to pinpoint specific semantics. However, this approach may be restrictive in areas such as art, fashion, or specialized fields like medicine, where suitable text prompts might not be available or easy to conceive thus limiting the scope of existing work. In this paper, we propose an unsupervised method to discover latent semantics in text-to-image diffusion models without relying on text prompts. Our method takes a small set of unlabeled images from specific domains, such as faces or cats, and a pre-trained diffusion model, and discovers diverse semantics in unsupervised fashion using a contrastive learning objective. Moreover, the learned directions can be applied simultaneously, either within the same domain (such as various types of facial edits) or across different domains (such as applying cat and face edits within the same image) without interfering with each other. Our extensive experiments show that our method achieves highly disentangled edits, outperforming existing approaches in both diffusion-based and GAN-based latent space editing methods.",
        "page": "http://arxiv.org/abs/2312.05390",
        "pdf": "http://arxiv.org/pdf/2312.05390.pdf"
    },
    {
        "title": "Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models",
        "author": "Nikita Starodubcev, Dmitry Baranchuk, Artem Fedorov, Artem Babenko",
        "abstract": "Knowledge distillation methods have recently shown to be a promising direction to speedup the synthesis of large-scale diffusion models by requiring only a few inference steps. While several powerful distillation methods were recently proposed, the overall quality of student samples is typically lower compared to the teacher ones, which hinders their practical usage. In this work, we investigate the relative quality of samples produced by the teacher text-to-image diffusion model and its distilled student version. As our main empirical finding, we discover that a noticeable portion of student samples exhibit superior fidelity compared to the teacher ones, despite the \"approximate\" nature of the student. Based on this finding, we propose an adaptive collaboration between student and teacher diffusion models for effective text-to-image synthesis. Specifically, the distilled model produces the initial sample, and then an oracle decides whether it needs further improvements with a slow teacher model. Extensive experiments demonstrate that the designed pipeline surpasses state-of-the-art text-to-image alternatives for various inference budgets in terms of human preference. Furthermore, the proposed approach can be naturally used in popular applications such as text-guided image editing and controllable generation.",
        "page": "http://arxiv.org/abs/2312.10835",
        "pdf": "http://arxiv.org/pdf/2312.10835.pdf"
    },
    {
        "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior",
        "author": "Fangfu Liu, Diankun Wu, Yi Wei, Yongming Rao, Yueqi Duan",
        "abstract": "Recently, 3D content creation from text prompts has demonstrated remarkable progress by utilizing 2D and 3D diffusion models. While 3D diffusion models ensure great multi-view consistency, their ability to generate high-quality and diverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion models find a distillation approach that achieves excellent generalization and rich details without any 3D data. However, 2D lifting methods suffer from inherent view-agnostic ambiguity thereby leading to serious multi-face Janus issues, where text prompts fail to provide sufficient guidance to learn coherent 3D results. Instead of retraining a costly viewpoint-aware model, we study how to fully exploit easily accessible coarse 3D knowledge to enhance the prompts and guide 2D lifting optimization for refinement. In this paper, we propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. Specifically, we design a pair of guiding strategies derived from the coarse 3D prior generated by the 3D diffusion model: a structural guidance for geometric fidelity and a semantic guidance for 3D coherence. Employing the two types of guidance, the 2D diffusion model enriches the 3D content with diversified and high-quality results. Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency.",
        "page": "http://arxiv.org/abs/2312.06655",
        "pdf": "http://arxiv.org/pdf/2312.06655.pdf"
    },
    {
        "title": "Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts",
        "author": "Fei Ni, Jianye Hao, Shiguang Wu, Longxin Kou, Jiashun Liu, YAN ZHENG, Bin Wang, Yuzheng Zhuang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Improving Distant 3D Object Detection Using 2D Box Supervision",
        "author": "Zetong Yang, Zhiding Yu, Christopher Choy, Renhao Wang, Anima Anandkumar, Jose M. Alvarez",
        "abstract": "Improving the detection of distant 3d objects is an important yet challenging task. For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information. As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant objects, which hampers the capability of existing detectors for long-range scenarios. We address this challenge by considering only 2D box supervision for distant objects since they are easy to annotate. We propose LR3D, a framework that learns to recover the missing depth of distant objects. LR3D adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close objects. This mapping allows the depth estimation of distant objects conditioned on their 2D boxes, making long-range 3D detection with 2D supervision feasible. Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant objects (over 200m) with comparable accuracy to full 3D supervision. Our framework is general, and could widely benefit 3D detection methods to a large extent.",
        "page": "http://arxiv.org/abs/2403.09230",
        "pdf": "http://arxiv.org/pdf/2403.09230.pdf"
    },
    {
        "title": "Minimal Perspective Autocalibration",
        "author": "Andrea Porfiri Dal Cin, Timothy Duff, Luca Magri, Tomas Pajdla",
        "abstract": "We introduce a new family of minimal problems for reconstruction from multiple views. Our primary focus is a novel approach to autocalibration, a long-standing problem in computer vision. Traditional approaches to this problem, such as those based on Kruppa's equations or the modulus constraint, rely explicitly on the knowledge of multiple fundamental matrices or a projective reconstruction. In contrast, we consider a novel formulation involving constraints on image points, the unknown depths of 3D points, and a partially specified calibration matrix $K$. For $2$ and $3$ views, we present a comprehensive taxonomy of minimal autocalibration problems obtained by relaxing some of these constraints. These problems are organized into classes according to the number of views and any assumed prior knowledge of $K$. Within each class, we determine problems with the fewest -- or a relatively small number of -- solutions. From this zoo of problems, we devise three practical solvers. Experiments with synthetic and real data and interfacing our solvers with COLMAP demonstrate that we achieve superior accuracy compared to state-of-the-art calibration methods. The code is available at https://github.com/andreadalcin/MinimalPerspectiveAutocalibration",
        "page": "http://arxiv.org/abs/2405.05605",
        "pdf": "http://arxiv.org/pdf/2405.05605.pdf"
    },
    {
        "title": "SVGDreamer: Text Guided SVG Generation with Diffusion Model",
        "author": "XiMing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Dong Xu, Qian Yu",
        "abstract": "Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduces attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to address issues of shape over-smoothing, color over-saturation, limited diversity, and slow convergence of the existing text-to-SVG generation methods by modeling SVGs as distributions of control points and colors. Furthermore, VPSD leverages a reward model to re-weight vector particles, which improves aesthetic appeal and accelerates convergence. Extensive experiments are conducted to validate the effectiveness of SVGDreamer, demonstrating its superiority over baseline methods in terms of editability, visual quality, and diversity. Project page: \\href{https://ximinng.github.io/SVGDreamer-project/}{https://ximinng.github.io/SVGDreamer-project/}",
        "page": "http://arxiv.org/abs/2312.16476",
        "pdf": "http://arxiv.org/pdf/2312.16476.pdf"
    },
    {
        "title": "Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning",
        "author": "xin zhang, Jiawei Du, Weiying Xie, Yunsong Li, Joey Tianyi Zhou",
        "abstract": "Dataset pruning aims to construct a coreset capable of achieving performance comparable to the original, full dataset. Most existing dataset pruning methods rely on snapshot-based criteria to identify representative samples, often resulting in poor generalization across various pruning and cross-architecture scenarios. Recent studies have addressed this issue by expanding the scope of training dynamics considered, including factors such as forgetting event and probability change, typically using an averaging approach. However, these works struggle to integrate a broader range of training dynamics without overlooking well-generalized samples, which may not be sufficiently highlighted in an averaging manner. In this study, we propose a novel dataset pruning method termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS utilizes a dual-depth strategy to achieve a balance between incorporating extensive training dynamics and identifying representative samples for dataset pruning. In the first depth, we estimate the series of each sample's individual contributions spanning the training progress, ensuring comprehensive integration of training dynamics. In the second depth, we focus on the variability of the sample-wise contributions identified in the first depth to highlight well-generalized samples. Extensive experiments conducted on CIFAR and ImageNet datasets verify the superiority of TDDS over previous SOTA methods. Specifically on CIFAR-100, our method achieves 54.51% accuracy with only 10% training data, surpassing random selection by 7.83% and other comparison methods by at least 12.69%.",
        "page": "http://arxiv.org/abs/2311.13613",
        "pdf": "http://arxiv.org/pdf/2311.13613.pdf"
    },
    {
        "title": "GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo",
        "author": "Jiang Wu, Rui Li, Haofei Xu, Wenxun Zhao, Yu Zhu, Jinqiu Sun, Yanning Zhang",
        "abstract": "Matching cost aggregation plays a fundamental role in learning-based multi-view stereo networks. However, directly aggregating adjacent costs can lead to suboptimal results due to local geometric inconsistency. Related methods either seek selective aggregation or improve aggregated depth in the 2D space, both are unable to handle geometric inconsistency in the cost volume effectively. In this paper, we propose GoMVS to aggregate geometrically consistent costs, yielding better utilization of adjacent geometries. More specifically, we correspond and propagate adjacent costs to the reference pixel by leveraging the local geometric smoothness in conjunction with surface normals. We achieve this by the geometric consistent propagation (GCP) module. It computes the correspondence from the adjacent depth hypothesis space to the reference depth space using surface normals, then uses the correspondence to propagate adjacent costs to the reference geometry, followed by a convolution for aggregation. Our method achieves new state-of-the-art performance on DTU, Tanks & Temple, and ETH3D datasets. Notably, our method ranks 1st on the Tanks & Temple Advanced benchmark.",
        "page": "http://arxiv.org/abs/2404.07992",
        "pdf": "http://arxiv.org/pdf/2404.07992.pdf"
    },
    {
        "title": "Paint3D: Paint Anything 3D with Lighting-less Texture Diffusion Models",
        "author": "Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, Gang Yu",
        "abstract": "This paper presents Paint3D, a novel coarse-to-fine generative framework that is capable of producing high-resolution, lighting-less, and diverse 2K UV texture maps for untextured 3D meshes conditioned on text or image inputs. The key challenge addressed is generating high-quality textures without embedded illumination information, which allows the textures to be re-lighted or re-edited within modern graphics pipelines. To achieve this, our method first leverages a pre-trained depth-aware 2D diffusion model to generate view-conditional images and perform multi-view texture fusion, producing an initial coarse texture map. However, as 2D models cannot fully represent 3D shapes and disable lighting effects, the coarse texture map exhibits incomplete areas and illumination artifacts. To resolve this, we train separate UV Inpainting and UVHD diffusion models specialized for the shape-aware refinement of incomplete areas and the removal of illumination artifacts. Through this coarse-to-fine process, Paint3D can produce high-quality 2K UV textures that maintain semantic consistency while being lighting-less, significantly advancing the state-of-the-art in texturing 3D objects.",
        "page": "http://arxiv.org/abs/2312.13913",
        "pdf": "http://arxiv.org/pdf/2312.13913.pdf"
    },
    {
        "title": "Closely Interactive Human Reconstruction with Proxemics and Physics-Guided Adaption",
        "author": "Buzhen Huang, Chen Li, Chongyang Xu, Liang Pan, Yangang Wang, Gim Hee Lee",
        "abstract": "Existing multi-person human reconstruction approaches mainly focus on recovering accurate poses or avoiding penetration, but overlook the modeling of close interactions. In this work, we tackle the task of reconstructing closely interactive humans from a monocular video. The main challenge of this task comes from insufficient visual information caused by depth ambiguity and severe inter-person occlusion. In view of this, we propose to leverage knowledge from proxemic behavior and physics to compensate the lack of visual information. This is based on the observation that human interaction has specific patterns following the social proxemics. Specifically, we first design a latent representation based on Vector Quantised-Variational AutoEncoder (VQ-VAE) to model human interaction. A proxemics and physics guided diffusion model is then introduced to denoise the initial distribution. We design the diffusion model as dual branch with each branch representing one individual such that the interaction can be modeled via cross attention. With the learned priors of VQ-VAE and physical constraint as the additional information, our proposed approach is capable of estimating accurate poses that are also proxemics and physics plausible. Experimental results on Hi4D, 3DPW, and CHI3D demonstrate that our method outperforms existing approaches. The code is available at \\url{https://github.com/boycehbz/HumanInteraction}.",
        "page": "http://arxiv.org/abs/2404.11291",
        "pdf": "http://arxiv.org/pdf/2404.11291.pdf"
    },
    {
        "title": "Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following",
        "author": "Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, Jingren Zhou",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds",
        "author": "Shengjun Zhang, Xin Fei, Yueqi Duan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "SuperSVG: Superpixel-based Scalable Vector Graphics Synthesis",
        "author": "Teng Hu, Ran Yi, Baihong Qian, Jiangning Zhang, Paul L. Rosin, Yu-Kun Lai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Video ReCap: Recursive Captioning of Hour-Long Videos",
        "author": "Md Mohaiminul Islam, Vu Bao Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis",
        "author": "Yufei Ye, Abhinav Gupta, Kris Kitani, Shubham Tulsiani",
        "abstract": "We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model, trained by aggregating seven diverse real-world interaction datasets spanning across 155 categories, represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines. Project website: https://judyye.github.io/ghop-www",
        "page": "http://arxiv.org/abs/2404.12383",
        "pdf": "http://arxiv.org/pdf/2404.12383.pdf"
    },
    {
        "title": "MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation",
        "author": "Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "IQ-VFI: Implicit Quadratic Motion Estimation for Video Frame Interpolation",
        "author": "Mengshun Hu, Kui Jiang, Zhihang Zhong, Zheng Wang, Yinqiang Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Part-aware Unified Representation of Language and Skeleton for Zero-shot Action Recognition",
        "author": "Anqi Zhu, Qiuhong Ke, Mingming Gong, James Bailey",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CoGS: Controllable Gaussian Splatting",
        "author": "Heng Yu, Joel Julin, Zolt\u00e1n \u00c1. Milacski, Koichiro Niinuma, L\u00e1szl\u00f3 A. Jeni",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "A Bayesian Approach to OOD Robustness in Image Classification",
        "author": "Prakhar Kaushik, Adam Kortylewski, Alan L. Yuille",
        "abstract": "An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transitional dictionary of vMF kernels that are intermediate between the source and target domains and train the generative model on this dictionary using the annotations on the source domain, followed by iterative refinement. This approach, termed Unsupervised Generative Transition (UGT), performs very well in OOD scenarios even when occlusion is present. UGT is evaluated on different OOD benchmarks including the OOD-CV dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image corruptions (including adding occluders), and synthetic-to-real domain transfer, and does well in all scenarios outperforming SOTA alternatives (e.g. up to 10% top-1 accuracy on Occluded OOD-CV dataset).",
        "page": "http://arxiv.org/abs/2403.07277",
        "pdf": "http://arxiv.org/pdf/2403.07277.pdf"
    },
    {
        "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action",
        "author": "Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi",
        "abstract": "We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community.",
        "page": "http://arxiv.org/abs/2312.17172",
        "pdf": "http://arxiv.org/pdf/2312.17172.pdf"
    },
    {
        "title": "Leveraging Predicate and Triplet Learning for Scene Graph Generation",
        "author": "Jiankai Li, Yunhong Wang, Xiefan Guo, Ruijie Yang, Weixin Li",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Semantic Shield: Defending Vision-Language Models Against Backdooring and Poisoning via Fine-grained Knowledge Alignment",
        "author": "Alvi Md Ishmam, Chris Thomas",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Action Detection via an Image Diffusion Process",
        "author": "Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Jun Liu",
        "abstract": "Action detection aims to localize the starting and ending points of action instances in untrimmed videos, and predict the classes of those instances. In this paper, we make the observation that the outputs of the action detection task can be formulated as images. Thus, from a novel perspective, we tackle action detection via a three-image generation process to generate starting point, ending point and action-class predictions as images via our proposed Action Detection Image Diffusion (ADI-Diff) framework. Furthermore, since our images differ from natural images and exhibit special properties, we further explore a Discrete Action-Detection Diffusion Process and a Row-Column Transformer design to better handle their processing. Our ADI-Diff framework achieves state-of-the-art results on two widely-used datasets.",
        "page": "http://arxiv.org/abs/2404.01051",
        "pdf": "http://arxiv.org/pdf/2404.01051.pdf"
    },
    {
        "title": "Disentangled Prompt Representation for Domain Generalization",
        "author": "De Cheng, Zhipeng Xu, XINYANG JIANG, Nannan Wang, Dongsheng Li, Xinbo Gao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios",
        "author": "Jingbo Wang, Zhengyi Luo, Ye Yuan, Yixuan LI, Bo Dai",
        "abstract": "We address the challenge of content diversity and controllability in pedestrian simulation for driving scenarios. Recent pedestrian animation frameworks have a significant limitation wherein they primarily focus on either following trajectory [46] or the content of the reference video [57], consequently overlooking the potential diversity of human motion within such scenarios. This limitation restricts the ability to generate pedestrian behaviors that exhibit a wider range of variations and realistic motions and therefore restricts its usage to provide rich motion content for other components in the driving simulation system, e.g., suddenly changed motion to which the autonomous vehicle should respond. In our approach, we strive to surpass the limitation by showcasing diverse human motions obtained from various sources, such as generated human motions, in addition to following the given trajectory. The fundamental contribution of our framework lies in combining the motion tracking task with trajectory following, which enables the tracking of specific motion parts (e.g., upper body) while simultaneously following the given trajectory by a single policy. This way, we significantly enhance both the diversity of simulated human motion within the given scenario and the controllability of the content, including language-based control. Our framework facilitates the generation of a wide range of human motions, contributing to greater realism and adaptability in pedestrian simulations for driving scenarios. More information is on our project page https://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html .",
        "page": "http://arxiv.org/abs/2404.19722",
        "pdf": "http://arxiv.org/pdf/2404.19722.pdf"
    },
    {
        "title": "SAOR: Single-View Articulated Object Reconstruction",
        "author": "Mehmet Aygun, Oisin Mac Aodha",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Incremental Residual Concept Bottleneck Models",
        "author": "Chenming Shang, Shiji Zhou, Hengyuan Zhang, Xinzhe Ni, Yujiu Yang, Yuwang Wang",
        "abstract": "Concept Bottleneck Models (CBMs) map the black-box visual representations extracted by deep neural networks onto a set of interpretable concepts and use the concepts to make predictions, enhancing the transparency of the decision-making process. Multimodal pre-trained models can match visual representations with textual concept embeddings, allowing for obtaining the interpretable concept bottleneck without the expertise concept annotations. Recent research has focused on the concept bank establishment and the high-quality concept selection. However, it is challenging to construct a comprehensive concept bank through humans or large language models, which severely limits the performance of CBMs. In this work, we propose the Incremental Residual Concept Bottleneck Model (Res-CBM) to address the challenge of concept completeness. Specifically, the residual concept bottleneck model employs a set of optimizable vectors to complete missing concepts, then the incremental concept discovery module converts the complemented vectors with unclear meanings into potential concepts in the candidate concept bank. Our approach can be applied to any user-defined concept bank, as a post-hoc processing method to enhance the performance of any CBMs. Furthermore, to measure the descriptive efficiency of CBMs, the Concept Utilization Efficiency (CUE) metric is proposed. Experiments show that the Res-CBM outperforms the current state-of-the-art methods in terms of both accuracy and efficiency and achieves comparable performance to black-box models across multiple datasets.",
        "page": "http://arxiv.org/abs/2404.08978",
        "pdf": "http://arxiv.org/pdf/2404.08978.pdf"
    },
    {
        "title": "Improving Transferable Targeted Adversarial Attacks with Model Self-Enhancement",
        "author": "Han Wu, Guanyan Ou, Weibin Wu, Zibin Zheng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding",
        "author": "Jin-Chuan Shi, Miao Wang, Haobin Duan, Shaohua Guan",
        "abstract": "Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language-embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language-embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.",
        "page": "http://arxiv.org/abs/2311.18482",
        "pdf": "http://arxiv.org/pdf/2311.18482.pdf"
    },
    {
        "title": "Density-Adaptive Model Based on Motif Matrix for Multi-Agent Trajectory Prediction",
        "author": "Di Wen, Haoran Xu, Zhaocheng He, Zhe Wu, Guang Tan, Peixi Peng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Accurate Post-training Quantization for Diffusion Models",
        "author": "Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu",
        "abstract": "In this paper, we propose an accurate data-free post-training quantization framework of diffusion models (ADP-DM) for efficient image generation. Conventional data-free quantization methods learn shared quantization functions for tensor discretization regardless of the generation timesteps, while the activation distribution differs significantly across various timesteps. The calibration images are acquired in random timesteps which fail to provide sufficient information for generalizable quantization function learning. Both issues cause sizable quantization errors with obvious image generation performance degradation. On the contrary, we design group-wise quantization functions for activation discretization in different timesteps and sample the optimal timestep for informative calibration image generation, so that our quantized diffusion model can reduce the discretization errors with negligible computational overhead. Specifically, we partition the timesteps according to the importance weights of quantization functions in different groups, which are optimized by differentiable search algorithms. We also select the optimal timestep for calibration image generation by structural risk minimizing principle in order to enhance the generalization ability in the deployment of quantized diffusion model. Extensive experimental results show that our method outperforms the state-of-the-art post-training quantization of diffusion model by a sizable margin with similar computational cost.",
        "page": "http://arxiv.org/abs/2305.18723",
        "pdf": "http://arxiv.org/pdf/2305.18723.pdf"
    },
    {
        "title": "View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network",
        "author": "Quan Zhang, Lei Wang, Vishal M. Patel, Xiaohua Xie, Jianhuang Lai",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DUSt3R: Geometric 3D Vision Made Easy",
        "author": "Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "InceptionNeXt: When Inception Meets ConvNeXt",
        "author": "Weihao Yu, Pan Zhou, Shuicheng Yan, Xinchao Wang",
        "abstract": "Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an identity mapping. With this new Inception depthwise convolution, we build a series of networks, namely IncepitonNeXt, which not only enjoy high throughputs but also maintain competitive performance. For instance, InceptionNeXt-T achieves 1.6x higher training throughputs than ConvNeX-T, as well as attains 0.2% top-1 accuracy improvement on ImageNet-1K. We anticipate InceptionNeXt can serve as an economical baseline for future architecture design to reduce carbon footprint. Code is available at https://github.com/sail-sg/inceptionnext.",
        "page": "http://arxiv.org/abs/2303.16900",
        "pdf": "http://arxiv.org/pdf/2303.16900.pdf"
    },
    {
        "title": "Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval",
        "author": "Rohan Sarkar, Avinash Kak",
        "abstract": "In the context of pose-invariant object recognition and retrieval, we demonstrate that it is possible to achieve significant improvements in performance if both the category-based and the object-identity-based embeddings are learned simultaneously during training. In hindsight, that sounds intuitive because learning about the categories is more fundamental than learning about the individual objects that correspond to those categories. However, to the best of what we know, no prior work in pose-invariant learning has demonstrated this effect. This paper presents an attention-based dual-encoder architecture with specially designed loss functions that optimize the inter- and intra-class distances simultaneously in two different embedding spaces, one for the category embeddings and the other for the object-level embeddings. The loss functions we have proposed are pose-invariant ranking losses that are designed to minimize the intra-class distances and maximize the inter-class distances in the dual representation spaces. We demonstrate the power of our approach with three challenging multi-view datasets, ModelNet-40, ObjectPI, and FG3D. With our dual approach, for single-view object recognition, we outperform the previous best by 20.0% on ModelNet40, 2.0% on ObjectPI, and 46.5% on FG3D. On the other hand, for single-view object retrieval, we outperform the previous best by 33.7% on ModelNet40, 18.8% on ObjectPI, and 56.9% on FG3D.",
        "page": "http://arxiv.org/abs/2403.00272",
        "pdf": "http://arxiv.org/pdf/2403.00272.pdf"
    },
    {
        "title": "Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding",
        "author": "Chaolei Tan, Jianhuang Lai, Wei-Shi Zheng, Jian-Fang Hu",
        "abstract": "Video Paragraph Grounding (VPG) is an emerging task in video-language understanding, which aims at localizing multiple sentences with semantic relations and temporal order from an untrimmed video. However, existing VPG approaches are heavily reliant on a considerable number of temporal labels that are laborious and time-consuming to acquire. In this work, we introduce and explore Weakly-Supervised Video Paragraph Grounding (WSVPG) to eliminate the need of temporal annotations. Different from previous weakly-supervised grounding frameworks based on multiple instance learning or reconstruction learning for two-stage candidate ranking, we propose a novel siamese learning framework that jointly learns the cross-modal feature alignment and temporal coordinate regression without timestamp labels to achieve concise one-stage localization for WSVPG. Specifically, we devise a Siamese Grounding TRansformer (SiamGTR) consisting of two weight-sharing branches for learning complementary supervision. An Augmentation Branch is utilized for directly regressing the temporal boundaries of a complete paragraph within a pseudo video, and an Inference Branch is designed to capture the order-guided feature correspondence for localizing multiple sentences in a normal video. We demonstrate by extensive experiments that our paradigm has superior practicability and flexibility to achieve efficient weakly-supervised or semi-supervised learning, outperforming state-of-the-art methods trained with the same or stronger supervision.",
        "page": "http://arxiv.org/abs/2403.11463",
        "pdf": "http://arxiv.org/pdf/2403.11463.pdf"
    },
    {
        "title": "MLP Can Be A Good Transformer Learner",
        "author": "Sihao Lin, Pumeng Lyu, Dongrui Liu, Tao Tang, Xiaodan Liang, Andy Song, Xiaojun Chang",
        "abstract": "Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.",
        "page": "http://arxiv.org/abs/2404.05657",
        "pdf": "http://arxiv.org/pdf/2404.05657.pdf"
    },
    {
        "title": "Learning Continual Compatible Representation for Re-indexing Free Lifelong Person Re-identification",
        "author": "Zhenyu Cui, Jiahuan Zhou, Xun Wang, Manyu Zhu, Yuxin Peng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards a Perceptual Evaluation Framework for Lighting Estimation",
        "author": "Justine Giroux, Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Javier Vazquez-Corral, Jean-Fran\u00e7ois Lalonde",
        "abstract": "Progress in lighting estimation is tracked by computing existing image quality assessment (IQA) metrics on images from standard datasets. While this may appear to be a reasonable approach, we demonstrate that doing so does not correlate to human preference when the estimated lighting is used to relight a virtual scene into a real photograph. To study this, we design a controlled psychophysical experiment where human observers must choose their preference amongst rendered scenes lit using a set of lighting estimation algorithms selected from the recent literature, and use it to analyse how these algorithms perform according to human perception. Then, we demonstrate that none of the most popular IQA metrics from the literature, taken individually, correctly represent human perception. Finally, we show that by learning a combination of existing IQA metrics, we can more accurately represent human preference. This provides a new perceptual framework to help evaluate future lighting estimation algorithms.",
        "page": "http://arxiv.org/abs/2312.04334",
        "pdf": "http://arxiv.org/pdf/2312.04334.pdf"
    },
    {
        "title": "RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos",
        "author": "Hongchi Xia, Yang Fu, Sifei Liu, Xiaolong Wang",
        "abstract": "We introduce a new RGB-D object dataset captured in the wild called WildRGB-D. Unlike most existing real-world object-centric datasets which only come with RGB capturing, the direct capture of the depth channel allows better 3D annotations and broader downstream applications. WildRGB-D comprises large-scale category-level RGB-D object videos, which are taken using an iPhone to go around the objects in 360 degrees. It contains around 8500 recorded objects and nearly 20000 RGB-D videos across 46 common object categories. These videos are taken with diverse cluttered backgrounds with three setups to cover as many real-world scenarios as possible: (i) a single object in one video; (ii) multiple objects in one video; and (iii) an object with a static hand in one video. The dataset is annotated with object masks, real-world scale camera poses, and reconstructed aggregated point clouds from RGBD videos. We benchmark four tasks with WildRGB-D including novel view synthesis, camera pose estimation, object 6d pose estimation, and object surface reconstruction. Our experiments show that the large-scale capture of RGB-D objects provides a large potential to advance 3D object learning. Our project page is https://wildrgbd.github.io/.",
        "page": "http://arxiv.org/abs/2401.12592",
        "pdf": "http://arxiv.org/pdf/2401.12592.pdf"
    },
    {
        "title": "Aligning and Prompting Everything All at Once for Universal Visual Perception",
        "author": "Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, Shaohui Lin, Rongrong Ji",
        "abstract": "Vision foundation models have been explored recently to build general-purpose vision systems. However, predominant paradigms, driven by casting instance-level tasks as an object-word alignment, bring heavy cross-modality interaction, which is not effective in prompting object detection and visual grounding. Another line of work that focuses on pixel-level tasks often encounters a large annotation gap of things and stuff, and suffers from mutual interference between foreground-object and background-class segmentation. In stark contrast to the prevailing methods, we present APE, a universal visual perception model for aligning and prompting everything all at once in an image to perform diverse tasks, i.e., detection, segmentation, and grounding, as an instance-level sentence-object matching paradigm. Specifically, APE advances the convergence of detection and grounding by reformulating language-guided grounding as open-vocabulary detection, which efficiently scales up model prompting to thousands of category vocabularies and region descriptions while maintaining the effectiveness of cross-modality fusion. To bridge the granularity gap of different pixel-level tasks, APE equalizes semantic and panoptic segmentation to proxy instance learning by considering any isolated regions as individual instances. APE aligns vision and language representation on broad data with natural and challenging characteristics all at once without task-specific fine-tuning. The extensive experiments on over 160 datasets demonstrate that, with only one-suit of weights, APE outperforms (or is on par with) the state-of-the-art models, proving that an effective yet universal perception for anything aligning and prompting is indeed feasible. Codes and trained models are released at https://github.com/shenyunhang/APE.",
        "page": "http://arxiv.org/abs/2312.02153",
        "pdf": "http://arxiv.org/pdf/2312.02153.pdf"
    },
    {
        "title": "OmniGlue: Generalizable Feature Matching with Foundation Model Guidance",
        "author": "Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, Andr\u00e9 Araujo",
        "abstract": "The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time. Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors. We perform comprehensive experiments on a suite of $7$ datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue's novel components lead to relative gains on unseen domains of $20.9\\%$ with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by $9.5\\%$ relatively.Code and model can be found at https://hwjiang1510.github.io/OmniGlue",
        "page": "http://arxiv.org/abs/2405.12979",
        "pdf": "http://arxiv.org/pdf/2405.12979.pdf"
    },
    {
        "title": "LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation",
        "author": "Linfeng Yuan, Miaojing Shi, Zijie Yue, Qijun Chen",
        "abstract": "Referring video object segmentation (RVOS) aims to segment the target instance referred by a given text expression in a video clip. The text expression normally contains sophisticated description of the instance's appearance, action, and relation with others. It is therefore rather difficult for a RVOS model to capture all these attributes correspondingly in the video; in fact, the model often favours more on the action- and relation-related visual attributes of the instance. This can end up with partial or even incorrect mask prediction of the target instance. We tackle this problem by taking a subject-centric short text expression from the original long text expression. The short one retains only the appearance-related information of the target instance so that we can use it to focus the model's attention on the instance's appearance. We let the model make joint predictions using both long and short text expressions; and insert a long-short cross-attention module to interact the joint features and a long-short predictions intersection loss to regulate the joint predictions. Besides the improvement on the linguistic part, we also introduce a forward-backward visual consistency loss, which utilizes optical flows to warp visual features between the annotated frames and their temporal neighbors for consistency. We build our method on top of two state of the art pipelines. Extensive experiments on A2D-Sentences, Refer-YouTube-VOS, JHMDB-Sentences and Refer-DAVIS17 show impressive improvements of our method.Code is available at https://github.com/LinfengYuan1997/Losh.",
        "page": "http://arxiv.org/abs/2306.08736",
        "pdf": "http://arxiv.org/pdf/2306.08736.pdf"
    },
    {
        "title": "Diffusion-FOF: Single-view Clothed Human Reconstruction via Diffusion-based Fourier Occupancy Field",
        "author": "Yuanzhen Li, Fei LUO, Chunxia Xiao",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Leveraging Frame Affinity for sRGB-to-RAW Video De-rendering",
        "author": "Chen Zhang, Wencheng Han, Yang Zhou, Jianbing Shen, Cheng-Zhong Xu, Wentao Liu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Investigating Compositional Challenges in Vision-Language Models for Visual Grounding",
        "author": "Yunan Zeng, Yan Huang, Jinjin Zhang, Zequn Jie, Zhenhua Chai, Liang Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "eTraM: Event-based Traffic Monitoring Dataset",
        "author": "Aayush Atul Verma, Bharatesh Chakravarthi, Arpitsinh Vaghela, Hua Wei, 'YZ' Yezhou Yang",
        "abstract": "Event cameras, with their high temporal and dynamic range and minimal memory usage, have found applications in various fields. However, their potential in static traffic monitoring remains largely unexplored. To facilitate this exploration, we present eTraM - a first-of-its-kind, fully event-based traffic monitoring dataset. eTraM offers 10 hr of data from different traffic scenarios in various lighting and weather conditions, providing a comprehensive overview of real-world situations. Providing 2M bounding box annotations, it covers eight distinct classes of traffic participants, ranging from vehicles to pedestrians and micro-mobility. eTraM's utility has been assessed using state-of-the-art methods for traffic participant detection, including RVT, RED, and YOLOv8. We quantitatively evaluate the ability of event-based models to generalize on nighttime and unseen scenes. Our findings substantiate the compelling potential of leveraging event cameras for traffic monitoring, opening new avenues for research and application. eTraM is available at https://eventbasedvision.github.io/eTraM",
        "page": "http://arxiv.org/abs/2403.19976",
        "pdf": "http://arxiv.org/pdf/2403.19976.pdf"
    },
    {
        "title": "Separating the \"Chirp\" from the \"Chat\": Self-supervised Visual Grounding of Sound and Language",
        "author": "Mark Hamilton, Andrew Zisserman, John Hershey, William Freeman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models by Inverting Stable Diffusion",
        "author": "George Cazenavette, Avneesh Sud, Thomas Leung, Ben Usman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Overcoming Data Limitations for High-Quality Video Diffusion Models",
        "author": "Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, CHAO WENG, Ying Shan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MirageRoom: 3D Scene Segmentation with 2D Pre-trained Models by Mirage Projection",
        "author": "Haowen Sun, Yueqi Duan, Juncheng Yan, Yifan Liu, Jiwen Lu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Polarization Wavefront Lidar: Learning Large Scene Reconstruction from Polarized Wavefronts",
        "author": "Dominik Scheuble, Chenyang Lei, Mario Bijelic, Seung-Hwan Baek, Felix Heide",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "CAD: Photorealistic 3D Generation via Adversarial Distillation",
        "author": "Ziyu Wan, Despoina Paschalidou, Ian Huang, Hongyu Liu, Bokui Shen, Xiaoyu Xiang, Jing Liao, Leonidas Guibas",
        "abstract": "The increased demand for 3D data in AR/VR, robotics and gaming applications, gave rise to powerful generative pipelines capable of synthesizing high-quality 3D objects. Most of these models rely on the Score Distillation Sampling (SDS) algorithm to optimize a 3D representation such that the rendered image maintains a high likelihood as evaluated by a pre-trained diffusion model. However, finding a correct mode in the high-dimensional distribution produced by the diffusion model is challenging and often leads to issues such as over-saturation, over-smoothing, and Janus-like artifacts. In this paper, we propose a novel learning paradigm for 3D synthesis that utilizes pre-trained diffusion models. Instead of focusing on mode-seeking, our method directly models the distribution discrepancy between multi-view renderings and diffusion priors in an adversarial manner, which unlocks the generation of high-fidelity and photorealistic 3D content, conditioned on a single image and prompt. Moreover, by harnessing the latent space of GANs and expressive diffusion model priors, our method facilitates a wide variety of 3D applications including single-view reconstruction, high diversity generation and continuous 3D interpolation in the open domain. The experiments demonstrate the superiority of our pipeline compared to previous works in terms of generation quality and diversity.",
        "page": "http://arxiv.org/abs/2312.06663",
        "pdf": "http://arxiv.org/pdf/2312.06663.pdf"
    },
    {
        "title": "Generative Quanta Color Imaging",
        "author": "Vishal Purohit, Junjie Luo, Yiheng Chi, Qi Guo, Stanley H. Chan, Qiang Qiu",
        "abstract": "The astonishing development of single-photon cameras has created an unprecedented opportunity for scientific and industrial imaging. However, the high data throughput generated by these 1-bit sensors creates a significant bottleneck for low-power applications. In this paper, we explore the possibility of generating a color image from a single binary frame of a single-photon camera. We evidently find this problem being particularly difficult to standard colorization approaches due to the substantial degree of exposure variation. The core innovation of our paper is an exposure synthesis model framed under a neural ordinary differential equation (Neural ODE) that allows us to generate a continuum of exposures from a single observation. This innovation ensures consistent exposure in binary images that colorizers take on, resulting in notably enhanced colorization. We demonstrate applications of the method in single-image and burst colorization and show superior generative performance over baselines. Project website can be found at https://vishal-s-p.github.io/projects/2023/generative_quanta_color.html.",
        "page": "http://arxiv.org/abs/2403.19066",
        "pdf": "http://arxiv.org/pdf/2403.19066.pdf"
    },
    {
        "title": "Training Like a Medical Resident: Context-Prior Learning Toward Universal Medical Image Segmentation",
        "author": "Yunhe Gao",
        "abstract": "A major focus of clinical imaging workflow is disease diagnosis and management, leading to medical imaging datasets strongly tied to specific clinical objectives. This scenario has led to the prevailing practice of developing task-specific segmentation models, without gaining insights from widespread imaging cohorts. Inspired by the training program of medical radiology residents, we propose a shift towards universal medical image segmentation, a paradigm aiming to build medical image understanding foundation models by leveraging the diversity and commonality across clinical targets, body regions, and imaging modalities. Towards this goal, we develop Hermes, a novel context-prior learning approach to address the challenges of data heterogeneity and annotation differences in medical image segmentation. In a large collection of eleven diverse datasets (2,438 3D images) across five modalities (CT, PET, T1, T2 and cine MRI) and multiple body regions, we demonstrate the merit of the universal paradigm over the traditional paradigm on addressing multiple tasks within a single model. By exploiting the synergy across tasks, Hermes achieves state-of-the-art performance on all testing datasets and shows superior model scalability. Results on two additional datasets reveals Hermes' strong performance for transfer learning, incremental learning, and generalization to downstream tasks. Hermes's learned priors demonstrate an appealing trait to reflect the intricate relations among tasks and modalities, which aligns with the established anatomical and imaging principles in radiology. The code is available: https://github.com/yhygao/universal-medical-image-segmentation.",
        "page": "http://arxiv.org/abs/2306.02416",
        "pdf": "http://arxiv.org/pdf/2306.02416.pdf"
    },
    {
        "title": "Structured Model Probing: Empowering Efficient Transfer Learning by Structured Regularization",
        "author": "Zhi-Fan Wu, Chaojie Mao, Xue Wang, Jianwen Jiang, Yiliang Lv, Rong Jin",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "WALT3D: Generating Realistic Training Data from Time-Lapse Imagery for Reconstructing Dynamic Objects under Occlusion",
        "author": "Khiem Vuong, N. Dinesh Reddy, Robert Tamburo, Srinivasa G. Narasimhan",
        "abstract": "Current methods for 2D and 3D object understanding struggle with severe occlusions in busy urban environments, partly due to the lack of large-scale labeled ground-truth annotations for learning occlusion. In this work, we introduce a novel framework for automatically generating a large, realistic dataset of dynamic objects under occlusions using freely available time-lapse imagery. By leveraging off-the-shelf 2D (bounding box, segmentation, keypoint) and 3D (pose, shape) predictions as pseudo-groundtruth, unoccluded 3D objects are identified automatically and composited into the background in a clip-art style, ensuring realistic appearances and physically accurate occlusion configurations. The resulting clip-art image with pseudo-groundtruth enables efficient training of object reconstruction methods that are robust to occlusions. Our method demonstrates significant improvements in both 2D and 3D reconstruction, particularly in scenarios with heavily occluded objects like vehicles and people in urban scenes.",
        "page": "http://arxiv.org/abs/2403.19022",
        "pdf": "http://arxiv.org/pdf/2403.19022.pdf"
    },
    {
        "title": "Data Valuation and Detections in Federated Learning",
        "author": "Wenqian Li, Shuran Fu, Fengrui Zhang, Yan Pang",
        "abstract": "Federated Learning (FL) enables collaborative model training while preserving the privacy of raw data. A challenge in this framework is the fair and efficient valuation of data, which is crucial for incentivizing clients to contribute high-quality data in the FL task. In scenarios involving numerous data clients within FL, it is often the case that only a subset of clients and datasets are pertinent to a specific learning task, while others might have either a negative or negligible impact on the model training process. This paper introduces a novel privacy-preserving method for evaluating client contributions and selecting relevant datasets without a pre-specified training algorithm in an FL task. Our proposed approach FedBary, utilizes Wasserstein distance within the federated context, offering a new solution for data valuation in the FL framework. This method ensures transparent data valuation and efficient computation of the Wasserstein barycenter and reduces the dependence on validation datasets. Through extensive empirical experiments and theoretical analyses, we demonstrate the potential of this data valuation method as a promising avenue for FL research.",
        "page": "http://arxiv.org/abs/2311.05304",
        "pdf": "http://arxiv.org/pdf/2311.05304.pdf"
    },
    {
        "title": "Unveiling the Unknown: Unleashing the Power of Unknown to Known in Open-Set Source-Free Domain Adaptation",
        "author": "Fuli Wan, Han Zhao, Xu Yang, Cheng Deng",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "AutoAD III: The Prequel -- Back to the Pixels",
        "author": "Tengda Han, Max Bain, Arsha Nagrani, G\u00fcl Varol, Weidi Xie, Andrew Zisserman",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner",
        "author": "Mengfei Xia, Yujun Shen, Changsong Lei, Yu Zhou, Deli Zhao, Ran Yi, Wenping Wang, Yong-Jin Liu",
        "abstract": "A diffusion model, which is formulated to produce an image using thousands of denoising steps, usually suffers from a slow inference speed. Existing acceleration algorithms simplify the sampling by skipping most steps yet exhibit considerable performance degradation. By viewing the generation of diffusion models as a discretized integrating process, we argue that the quality drop is partly caused by applying an inaccurate integral direction to a timestep interval. To rectify this issue, we propose a timestep aligner that helps find a more accurate integral direction for a particular interval at the minimum cost. Specifically, at each denoising step, we replace the original parameterization by conditioning the network on a new timestep, which is obtained by aligning the sampling distribution to the real distribution. Extensive experiments show that our plug-in design can be trained efficiently and boost the inference performance of various state-of-the-art acceleration methods, especially when there are few denoising steps. For example, when using 10 denoising steps on the popular LSUN Bedroom dataset, we improve the FID of DDIM from 9.65 to 6.07, simply by adopting our method for a more appropriate set of timesteps. Code will be made publicly available.",
        "page": "http://arxiv.org/abs/2310.09469",
        "pdf": "http://arxiv.org/pdf/2310.09469.pdf"
    },
    {
        "title": "Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos",
        "author": "Sagnik Majumder, Ziad Al-Halah, Kristen Grauman",
        "abstract": "We propose a self-supervised method for learning representations based on spatial audio-visual correspondences in egocentric videos. Our method uses a masked auto-encoding framework to synthesize masked binaural (multi-channel) audio through the synergy of audio and vision, thereby learning useful spatial relationships between the two modalities. We use our pretrained features to tackle two downstream video tasks requiring spatial understanding in social scenarios: active speaker detection and spatial audio denoising. Through extensive experiments, we show that our features are generic enough to improve over multiple state-of-the-art baselines on both tasks on two challenging egocentric video datasets that offer binaural audio, EgoCom and EasyCom. Project: http://vision.cs.utexas.edu/projects/ego_av_corr.",
        "page": "http://arxiv.org/abs/2307.04760",
        "pdf": "http://arxiv.org/pdf/2307.04760.pdf"
    },
    {
        "title": "Diversity-aware Channel Pruning for StyleGAN Compression",
        "author": "Jiwoo Chung, Sangeek Hyun, Sang-Heon Shim, Jae-Pil Heo",
        "abstract": "StyleGAN has shown remarkable performance in unconditional image generation. However, its high computational cost poses a significant challenge for practical applications. Although recent efforts have been made to compress StyleGAN while preserving its performance, existing compressed models still lag behind the original model, particularly in terms of sample diversity. To overcome this, we propose a novel channel pruning method that leverages varying sensitivities of channels to latent vectors, which is a key factor in sample diversity. Specifically, by assessing channel importance based on their sensitivities to latent vector perturbations, our method enhances the diversity of samples in the compressed model. Since our method solely focuses on the channel pruning stage, it has complementary benefits with prior training schemes without additional training cost. Extensive experiments demonstrate that our method significantly enhances sample diversity across various datasets. Moreover, in terms of FID scores, our method not only surpasses state-of-the-art by a large margin but also achieves comparable scores with only half training iterations.",
        "page": "http://arxiv.org/abs/2403.13548",
        "pdf": "http://arxiv.org/pdf/2403.13548.pdf"
    },
    {
        "title": "SimAC: A Simple Anti-Customization Method for Protecting Face Privacy against Text-to-Image Synthesis of Diffusion Models",
        "author": "Feifei Wang, Zhentao Tan, Tianyi Wei, Yue Wu, Qidong Huang",
        "abstract": "Despite the success of diffusion-based customization methods on visual content creation, increasing concerns have been raised about such techniques from both privacy and political perspectives. To tackle this issue, several anti-customization methods have been proposed in very recent months, predominantly grounded in adversarial attacks. Unfortunately, most of these methods adopt straightforward designs, such as end-to-end optimization with a focus on adversarially maximizing the original training loss, thereby neglecting nuanced internal properties intrinsic to the diffusion model, and even leading to ineffective optimization in some diffusion time steps.In this paper, we strive to bridge this gap by undertaking a comprehensive exploration of these inherent properties, to boost the performance of current anti-customization approaches. Two aspects of properties are investigated: 1) We examine the relationship between time step selection and the model's perception in the frequency domain of images and find that lower time steps can give much more contributions to adversarial noises. This inspires us to propose an adaptive greedy search for optimal time steps that seamlessly integrates with existing anti-customization methods. 2) We scrutinize the roles of features at different layers during denoising and devise a sophisticated feature-based optimization framework for anti-customization.Experiments on facial benchmarks demonstrate that our approach significantly increases identity disruption, thereby protecting user privacy and copyright. Our code is available at: https://github.com/somuchtome/SimAC.",
        "page": "http://arxiv.org/abs/2312.07865",
        "pdf": "http://arxiv.org/pdf/2312.07865.pdf"
    },
    {
        "title": "RobustSAM: Segment Anything Robustly on Degraded Images",
        "author": "Wei-Ting Chen, Yu Jiet Vong, Sy-Yen Kuo, Sizhuo Ma, Jian Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learned Trajectory Embedding for Subspace Clustering",
        "author": "Yaroslava Lochman, Christopher Zach, Carl Olsson",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Dynamic Prompt Optimizing for Text-to-Image Generation",
        "author": "Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, Qing Yang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Grounded Question-Answering in Long Egocentric Videos",
        "author": "Shangzhe Di, Weidi Xie",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Inclusion Matching for Animation Paint Bucket Colorization",
        "author": "Yuekun Dai, Shangchen Zhou, Blake Li, Chongyi Li, Chen Change Loy",
        "abstract": "Colorizing line art is a pivotal task in the production of hand-drawn cel animation. This typically involves digital painters using a paint bucket tool to manually color each segment enclosed by lines, based on RGB values predetermined by a color designer. This frame-by-frame process is both arduous and time-intensive. Current automated methods mainly focus on segment matching. This technique migrates colors from a reference to the target frame by aligning features within line-enclosed segments across frames. However, issues like occlusion and wrinkles in animations often disrupt these direct correspondences, leading to mismatches. In this work, we introduce a new learning-based inclusion matching pipeline, which directs the network to comprehend the inclusion relationships between segments rather than relying solely on direct visual correspondences. Our method features a two-stage pipeline that integrates a coarse color warping module with an inclusion matching module, enabling more nuanced and accurate colorization. To facilitate the training of our network, we also develope a unique dataset, referred to as PaintBucket-Character. This dataset includes rendered line arts alongside their colorized counterparts, featuring various 3D characters. Extensive experiments demonstrate the effectiveness and superiority of our method over existing techniques.",
        "page": "http://arxiv.org/abs/2403.18342",
        "pdf": "http://arxiv.org/pdf/2403.18342.pdf"
    },
    {
        "title": "DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery",
        "author": "Yixuan Zhu, Ao Li, Yansong Tang, Wenliang Zhao, Jie Zhou, Jiwen Lu",
        "abstract": "The recovery of occluded human meshes presents challenges for current methods due to the difficulty in extracting effective image features under severe occlusion. In this paper, we introduce DPMesh, an innovative framework for occluded human mesh recovery that capitalizes on the profound diffusion prior about object structure and spatial relationships embedded in a pre-trained text-to-image diffusion model. Unlike previous methods reliant on conventional backbones for vanilla feature extraction, DPMesh seamlessly integrates the pre-trained denoising U-Net with potent knowledge as its image backbone and performs a single-step inference to provide occlusion-aware information. To enhance the perception capability for occluded poses, DPMesh incorporates well-designed guidance via condition injection, which produces effective controls from 2D observations for the denoising U-Net. Furthermore, we explore a dedicated noisy key-point reasoning approach to mitigate disturbances arising from occlusion and crowded scenarios. This strategy fully unleashes the perceptual capability of the diffusion prior, thereby enhancing accuracy. Extensive experiments affirm the efficacy of our framework, as we outperform state-of-the-art methods on both occlusion-specific and standard datasets. The persuasive results underscore its ability to achieve precise and robust 3D human mesh recovery, particularly in challenging scenarios involving occlusion and crowded scenes.",
        "page": "http://arxiv.org/abs/2404.01424",
        "pdf": "http://arxiv.org/pdf/2404.01424.pdf"
    },
    {
        "title": "Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery",
        "author": "Siddharth Tourani, Ahmed Alwheibi, Arif Mahmood, Muhammad Haris Khan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "OVMR: Open-Vocabulary Recognition with Multi-Modal References",
        "author": "Zehong Ma, Shiliang Zhang, Longhui Wei, Qi Tian",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "An edit friendly ddpm noise space: inversion and manipulations",
        "author": "Inbar Huberman-Spiegelglas, Vladimir Kulikov, Tomer Michaeli",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) employ a sequence of white Gaussian noise samples to generate an image. In analogy with GANs, those noise maps could be considered as the latent code associated with the generated image. However, this native noise space does not possess a convenient structure, and is thus challenging to work with in editing tasks. Here, we propose an alternative latent noise space for DDPM that enables a wide range of editing operations via simple means, and present an inversion method for extracting these edit-friendly noise maps for any given image (real or synthetically generated). As opposed to the native DDPM noise space, the edit-friendly noise maps do not have a standard normal distribution and are not statistically independent across timesteps. However, they allow perfect reconstruction of any desired image, and simple transformations on them translate into meaningful manipulations of the output image (e.g. shifting, color edits). Moreover, in text-conditional models, fixing those noise maps while changing the text prompt, modifies semantics while retaining structure. We illustrate how this property enables text-based editing of real images via the diverse DDPM sampling scheme (in contrast to the popular non-diverse DDIM inversion). We also show how it can be used within existing diffusion-based editing methods to improve their quality and diversity. Webpage: https://inbarhub.github.io/DDPM_inversion",
        "page": "http://arxiv.org/abs/2304.06140",
        "pdf": "http://arxiv.org/pdf/2304.06140.pdf"
    },
    {
        "title": "Improved Implicit Neural Representation with Fourier Reparameterized Training",
        "author": "Kexuan Shi, Xingyu Zhou, Shuhang Gu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation",
        "author": "You Wu, Kean Liu, Xiaoyue Mi, Fan Tang, Juan Cao, Jintao Li",
        "abstract": "Concept personalization methods enable large text-to-image models to learn specific subjects (e.g., objects/poses/3D models) and synthesize renditions in new contexts. Given that the image references are highly biased towards visual attributes, state-of-the-art personalization models tend to overfit the whole subject and cannot disentangle visual characteristics in pixel space. In this study, we proposed a more challenging setting, namely fine-grained visual appearance personalization. Different from existing methods, we allow users to provide a sentence describing the desired attributes. A novel decoupled self-augmentation strategy is proposed to generate target-related and non-target samples to learn user-specified visual attributes. These augmented data allow for refining the model's understanding of the target attribute while mitigating the impact of unrelated attributes. At the inference stage, adjustments are conducted on semantic space through the learned target and non-target embeddings to further enhance the disentanglement of target attributes. Extensive experiments on various kinds of visual attributes with SOTA personalization methods show the ability of the proposed method to mimic target visual appearance in novel contexts, thus improving the controllability and flexibility of personalization.",
        "page": "http://arxiv.org/abs/2403.20231",
        "pdf": "http://arxiv.org/pdf/2403.20231.pdf"
    },
    {
        "title": "DaReNeRF: Direction-aware Representation for Dynamic Scenes",
        "author": "Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer",
        "author": "Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, Tao Chen",
        "abstract": "Vision-Language Transformers (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens. Existing token pruning research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token pruning process, causing the important tokens for one modality to be falsely pruned in another modality branch. Meanwhile, existing VLT pruning works also lack the flexibility to dynamically compress each layer based on different input samples. To this end, we propose a novel framework named Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for accelerating various VLTs. Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities. We further design a novel Dynamic Token Pruning (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances. Extensive experiments on various benchmarks demonstrate that MADTP significantly reduces the computational complexity of kinds of multimodal models while preserving competitive performance. Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation.",
        "page": "http://arxiv.org/abs/2403.02991",
        "pdf": "http://arxiv.org/pdf/2403.02991.pdf"
    },
    {
        "title": "COCONut: Modernizing COCO Segmentation",
        "author": "Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, Liang-Chieh Chen",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Towards Automated Movie Trailer Generation",
        "author": "Dawit Argaw Argaw, Mattia Soldan, Alejandro Pardo, Chen Zhao, Fabian Caba Heilbron, Joon Chung, Bernard Ghanem",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "PredToken: Predicting Unknown Tokens and Beyond with Coarse-to-Fine Iterative Decoding",
        "author": "Xuesong Nie, Haoyuan Jin, Yunfeng Yan, Xi Chen, Zhihang Zhu, Donglian Qi",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Learning Object State Changes in Videos: An Open-World Perspective",
        "author": "Zihui Xue, Kumar Ashutosh, Kristen Grauman",
        "abstract": "Object State Changes (OSCs) are pivotal for video understanding. While humans can effortlessly generalize OSC understanding from familiar to unknown objects, current approaches are confined to a closed vocabulary. Addressing this gap, we introduce a novel open-world formulation for the video OSC problem. The goal is to temporally localize the three stages of an OSC -- the object's initial state, its transitioning state, and its end state -- whether or not the object has been observed during training. Towards this end, we develop VidOSC, a holistic learning approach that: (1) leverages text and vision-language models for supervisory signals to obviate manually labeling OSC training data, and (2) abstracts fine-grained shared state representations from objects to enhance generalization. Furthermore, we present HowToChange, the first open-world benchmark for video OSC localization, which offers an order of magnitude increase in the label space and annotation volume compared to the best existing benchmark. Experimental results demonstrate the efficacy of our approach, in both traditional closed-world and open-world scenarios.",
        "page": "http://arxiv.org/abs/2312.11782",
        "pdf": "http://arxiv.org/pdf/2312.11782.pdf"
    },
    {
        "title": "G$^3$-LQ: Marrying Hyperbolic Alignment with Explicit Semantic-Geometric Modeling for 3D Visual Grounding",
        "author": "Yuan Wang, Yali Li, Shengjin Wang",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "NightCC: Nighttime Color Constancy via  Adaptive Channel Masking",
        "author": "Shuwei Li, Robby T. Tan",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "DYSON: Dynamic Feature Space Self-Organization for Online Task-Free Class Incremental Learning",
        "author": "Yuhang He, YingJie Chen, Yuhan Jin, Songlin Dong, Xing Wei, Yihong Gong",
        "abstract": null,
        "page": null,
        "pdf": null
    },
    {
        "title": "Harnessing Large Language Models for Training-free Video Anomaly Detection",
        "author": "Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, Elisa Ricci",
        "abstract": "Video anomaly detection (VAD) aims to temporally locate abnormal events in a video. Existing works mostly rely on training deep models to learn the distribution of normality with either video-level supervision, one-class supervision, or in an unsupervised setting. Training-based methods are prone to be domain-specific, thus being costly for practical deployment as any domain change will involve data collection and model training. In this paper, we radically depart from previous efforts and propose LAnguage-based VAD (LAVAD), a method tackling VAD in a novel, training-free paradigm, exploiting the capabilities of pre-trained large language models (LLMs) and existing vision-language models (VLMs). We leverage VLM-based captioning models to generate textual descriptions for each frame of any test video. With the textual scene description, we then devise a prompting mechanism to unlock the capability of LLMs in terms of temporal aggregation and anomaly score estimation, turning LLMs into an effective video anomaly detector. We further leverage modality-aligned VLMs and propose effective techniques based on cross-modal similarity for cleaning noisy captions and refining the LLM-based anomaly scores. We evaluate LAVAD on two large datasets featuring real-world surveillance scenarios (UCF-Crime and XD-Violence), showing that it outperforms both unsupervised and one-class methods without requiring any training or data collection.",
        "page": "http://arxiv.org/abs/2404.01014",
        "pdf": "http://arxiv.org/pdf/2404.01014.pdf"
    }
]